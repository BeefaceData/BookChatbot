{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8acc1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Beeface\\.virtualenvs\\BookChatbot-9Q2uoHrH\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7474cc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Model_Tokenizer/qwen3_0.6_tokenizer\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Model_Tokenizer/qwen3_0.6_model\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beeb8707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: <think>\n",
      "Okay, the user is asking for a short introduction to a large language model. Let me start by recalling what a large language model is. It's a type of machine learning model designed to understand and generate human language. But I should make sure to explain it clearly without getting too technical.\n",
      "\n",
      "First, I need to mention that it's a powerful tool that can understand and create text. Maybe start with the key points: it's based on deep learning, can understand language, and can generate text. But I should keep it concise. Also, maybe include an example of its use, like answering questions or writing articles, to show its practical application.\n",
      "\n",
      "Wait, the user might be looking for a brief but informative intro. I should avoid jargon and keep it simple. Let me check if there's any important detail I'm missing. Oh, right, the model can learn from a large dataset, which is a key feature. Also, it's used in various fields like AI, customer service, etc. \n",
      "\n",
      "I should structure the introduction to start with the definition, then the key features, and a brief example. Make sure it's all in one sentence or two. Let me put it all together now.\n",
      "</think>\n",
      "content: A large language model (LLM) is a type of artificial intelligence that enables machines to understand and generate human language. It uses deep learning techniques to analyze vast amounts of text and produce meaningful responses, making it useful for tasks like writing articles, answering questions, or creating content.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# print(model.eval())\n",
    "\n",
    "# prepare the model input\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d20dcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader, DirectoryLoader, UnstructuredWordDocumentLoader # Loads text documents.\n",
    "from langchain_community.vectorstores import Chroma, Weaviate # A vector store for efficiently storing and searching vector embeddings.\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings # Converts text chunks into vector embeddings using a pre-trained model.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # Splits large text into smaller, manageable chunks.\n",
    "import os # Used for operating system interactions (though not used explicitly in this script).\n",
    "import shutil\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# from weaviate.classes.init import Auth\n",
    "# from weaviate.classes.config import Configure\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "YOUR_WEAVIATE_KEY = os.environ[\"YOUR_WEAVIATE_KEY\"]\n",
    "YOUR_WEAVIATE_CLUSTER = os.environ[\"YOUR_WEAVIATE_CLUSTER\"]\n",
    "YOUR_HUGGINGFACE_KEY = os.environ[\"YOUR_HUGGINGFACE_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "290d378c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [WinError 10060] A connection attempt failed because\n",
      "[nltk_data]     the connected party did not properly respond after a\n",
      "[nltk_data]     period of time, or established connection failed\n",
      "[nltk_data]     because connected host has failed to respond>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ff807c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error loading file PDF\\Agents_Companion_v2.pdf\n"
     ]
    },
    {
     "ename": "PDFInfoNotInstalledError",
     "evalue": "Unable to get page count. Is poppler installed and in PATH?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Beeface\\.virtualenvs\\BookChatbot-9Q2uoHrH\\Lib\\site-packages\\pdf2image\\pdf2image.py:581\u001b[39m, in \u001b[36mpdfinfo_from_path\u001b[39m\u001b[34m(pdf_path, userpw, ownerpw, poppler_path, rawdates, timeout, first_page, last_page)\u001b[39m\n\u001b[32m    580\u001b[39m     env[\u001b[33m\"\u001b[39m\u001b[33mLD_LIBRARY_PATH\u001b[39m\u001b[33m\"\u001b[39m] = poppler_path + \u001b[33m\"\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m + env.get(\u001b[33m\"\u001b[39m\u001b[33mLD_LIBRARY_PATH\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m proc = \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPIPE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py:1026\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[39m\n\u001b[32m   1023\u001b[39m             \u001b[38;5;28mself\u001b[39m.stderr = io.TextIOWrapper(\u001b[38;5;28mself\u001b[39m.stderr,\n\u001b[32m   1024\u001b[39m                     encoding=encoding, errors=errors)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1036\u001b[39m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py:1538\u001b[39m, in \u001b[36mPopen._execute_child\u001b[39m\u001b[34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[39m\n\u001b[32m   1537\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1538\u001b[39m     hp, ht, pid, tid = \u001b[43m_winapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1539\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[32m   1540\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1541\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1542\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1544\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1545\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1546\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1547\u001b[39m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[32m   1548\u001b[39m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1551\u001b[39m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[32m   1552\u001b[39m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPDFInfoNotInstalledError\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m loader = DirectoryLoader(\u001b[33m\"\u001b[39m\u001b[33mPDF\u001b[39m\u001b[33m\"\u001b[39m, glob=\u001b[33m\"\u001b[39m\u001b[33m**/*.pdf\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# :contentReference[oaicite:17]{index=17}\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m docs = \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(docs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m documents loaded.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(docs[\u001b[32m0\u001b[39m].page_content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m characters in your documents\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Beeface\\.virtualenvs\\BookChatbot-9Q2uoHrH\\Lib\\site-packages\\langchain_community\\document_loaders\\directory.py:117\u001b[39m, in \u001b[36mDirectoryLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> List[Document]:\n\u001b[32m    116\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load documents.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Beeface\\.virtualenvs\\BookChatbot-9Q2uoHrH\\Lib\\site-packages\\langchain_community\\document_loaders\\directory.py:195\u001b[39m, in \u001b[36mDirectoryLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m items:\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lazy_load_file(i, p, pbar)\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pbar:\n\u001b[32m    198\u001b[39m     pbar.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Beeface\\.virtualenvs\\BookChatbot-9Q2uoHrH\\Lib\\site-packages\\langchain_community\\document_loaders\\directory.py:233\u001b[39m, in \u001b[36mDirectoryLoader._lazy_load_file\u001b[39m\u001b[34m(self, item, path, pbar)\u001b[39m\n\u001b[32m    231\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    232\u001b[39m         logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError loading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(item)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pbar:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Beeface\\.virtualenvs\\BookChatbot-9Q2uoHrH\\Lib\\site-packages\\langchain_community\\document_loaders\\directory.py:223\u001b[39m, in \u001b[36mDirectoryLoader._lazy_load_file\u001b[39m\u001b[34m(self, item, path, pbar)\u001b[39m\n\u001b[32m    221\u001b[39m loader = \u001b[38;5;28mself\u001b[39m.loader_cls(\u001b[38;5;28mstr\u001b[39m(item), **\u001b[38;5;28mself\u001b[39m.loader_kwargs)\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubdoc\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Beeface\\.virtualenvs\\BookChatbot-9Q2uoHrH\\Lib\\site-packages\\langchain_community\\document_loaders\\unstructured.py:107\u001b[39m, in \u001b[36mUnstructuredBaseLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlazy_load\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterator[Document]:\n\u001b[32m    106\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     elements = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28mself\u001b[39m._post_process_elements(elements)\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33melements\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Beeface\\.virtualenvs\\BookChatbot-9Q2uoHrH\\Lib\\site-packages\\langchain_community\\document_loaders\\unstructured.py:228\u001b[39m, in \u001b[36mUnstructuredFileLoader._get_elements\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.file_path, Path):\n\u001b[32m    227\u001b[39m     \u001b[38;5;28mself\u001b[39m.file_path = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.file_path)\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43munstructured_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Beeface\\.virtualenvs\\BookChatbot-9Q2uoHrH\\Lib\\site-packages\\unstructured\\partition\\auto.py:212\u001b[39m, in \u001b[36mpartition\u001b[39m\u001b[34m(filename, file, encoding, content_type, url, headers, ssl_verify, request_timeout, strategy, skip_infer_table_types, ocr_languages, languages, detect_language_per_element, pdf_infer_table_structure, extract_images_in_pdf, extract_image_block_types, extract_image_block_output_dir, extract_image_block_to_payload, data_source_metadata, metadata_filename, hi_res_model_name, model_name, starting_page_number, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file_type == FileType.PDF:\n\u001b[32m    211\u001b[39m     partition_pdf = partitioner_loader.get(file_type)\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     elements = \u001b[43mpartition_pdf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlanguages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhi_res_model_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhi_res_model_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextract_images_in_pdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_images_in_pdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextract_image_block_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_image_block_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextract_image_block_output_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_image_block_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextract_image_block_to_payload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_image_block_to_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstarting_page_number\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstarting_page_number\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m augment_metadata(elements)\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file_type.partitioner_shortname == \u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Beeface\\.virtualenvs\\BookChatbot-9Q2uoHrH\\Lib\\site-packages\\unstructured\\documents\\elements.py:585\u001b[39m, in \u001b[36mprocess_metadata.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    583\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    584\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: _P.args, **kwargs: _P.kwargs) -> \u001b[38;5;28mlist\u001b[39m[Element]:\n\u001b[32m--> \u001b[39m\u001b[32m585\u001b[39m     elements = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    586\u001b[39m     call_args = get_call_args_applying_defaults(func, *args, **kwargs)\n\u001b[32m    588\u001b[39m     unique_element_ids: \u001b[38;5;28mbool\u001b[39m = call_args.get(\u001b[33m\"\u001b[39m\u001b[33munique_element_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Beeface\\.virtualenvs\\BookChatbot-9Q2uoHrH\\Lib\\site-packages\\unstructured\\file_utils\\filetype.py:816\u001b[39m, in \u001b[36madd_filetype.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    814\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    815\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: _P.args, **kwargs: _P.kwargs) -> \u001b[38;5;28mlist\u001b[39m[Element]:\n\u001b[32m--> \u001b[39m\u001b[32m816\u001b[39m     elements = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    818\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m elements:\n\u001b[32m    819\u001b[39m         \u001b[38;5;66;03m# NOTE(robinson) - Attached files have already run through this logic\u001b[39;00m\n\u001b[32m    820\u001b[39m         \u001b[38;5;66;03m# in their own partitioning function\u001b[39;00m\n\u001b[32m    821\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m element.metadata.attached_to_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Beeface\\.virtualenvs\\BookChatbot-9Q2uoHrH\\Lib\\site-packages\\unstructured\\file_utils\\filetype.py:774\u001b[39m, in \u001b[36madd_metadata.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: _P.args, **kwargs: _P.kwargs) -> \u001b[38;5;28mlist\u001b[39m[Element]:\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m     elements = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    775\u001b[39m     call_args = get_call_args_applying_defaults(func, *args, **kwargs)\n\u001b[32m    777\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m call_args.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata_filename\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Beeface\\.virtualenvs\\BookChatbot-9Q2uoHrH\\Lib\\site-packages\\unstructured\\chunking\\dispatch.py:74\u001b[39m, in \u001b[36madd_chunking_strategy.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"The decorated function is replaced with this one.\"\"\"\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# -- call the partitioning function to get the elements --\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m elements = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# -- look for a chunking-strategy argument --\u001b[39;00m\n\u001b[32m     77\u001b[39m call_args = get_call_args_applying_defaults(func, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Beeface\\.virtualenvs\\BookChatbot-9Q2uoHrH\\Lib\\site-packages\\unstructured\\partition\\pdf.py:228\u001b[39m, in \u001b[36mpartition_pdf\u001b[39m\u001b[34m(filename, file, include_page_breaks, strategy, infer_table_structure, ocr_languages, languages, metadata_filename, metadata_last_modified, chunking_strategy, hi_res_model_name, extract_images_in_pdf, extract_image_block_types, extract_image_block_output_dir, extract_image_block_to_payload, starting_page_number, extract_forms, form_extraction_skip_tables, password, pdfminer_line_margin, pdfminer_char_margin, pdfminer_line_overlap, pdfminer_word_margin, **kwargs)\u001b[39m\n\u001b[32m    225\u001b[39m exactly_one(filename=filename, file=file)\n\u001b[32m    227\u001b[39m languages = check_language_args(languages \u001b[38;5;129;01mor\u001b[39;00m [], ocr_languages)\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpartition_pdf_or_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_page_breaks\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_page_breaks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlanguages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhi_res_model_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhi_res_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_images_in_pdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_images_in_pdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_image_block_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_image_block_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_image_block_output_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_image_block_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_image_block_to_payload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_image_block_to_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstarting_page_number\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstarting_page_number\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_forms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_forms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mform_extraction_skip_tables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mform_extraction_skip_tables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpdfminer_line_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdfminer_line_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpdfminer_char_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdfminer_char_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpdfminer_line_overlap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdfminer_line_overlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpdfminer_word_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdfminer_word_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Beeface\\.virtualenvs\\BookChatbot-9Q2uoHrH\\Lib\\site-packages\\unstructured\\partition\\pdf.py:379\u001b[39m, in \u001b[36mpartition_pdf_or_image\u001b[39m\u001b[34m(filename, file, is_image, include_page_breaks, strategy, infer_table_structure, languages, metadata_last_modified, hi_res_model_name, extract_images_in_pdf, extract_image_block_types, extract_image_block_output_dir, extract_image_block_to_payload, starting_page_number, extract_forms, form_extraction_skip_tables, password, pdfminer_line_margin, pdfminer_char_margin, pdfminer_line_overlap, pdfminer_word_margin, ocr_agent, table_ocr_agent, **kwargs)\u001b[39m\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m strategy == PartitionStrategy.OCR_ONLY:\n\u001b[32m    377\u001b[39m     \u001b[38;5;66;03m# NOTE(robinson): Catches file conversion warnings when running with PDFs\u001b[39;00m\n\u001b[32m    378\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m warnings.catch_warnings():\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m         elements = \u001b[43m_partition_pdf_or_image_with_ocr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m            \u001b[49m\u001b[43minclude_page_breaks\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_page_breaks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlanguages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m            \u001b[49m\u001b[43mocr_languages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mocr_languages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m            \u001b[49m\u001b[43mis_image\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlast_modified\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstarting_page_number\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstarting_page_number\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    391\u001b[39m         out_elements = _process_uncategorized_text_elements(elements)\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out_elements\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Beeface\\.virtualenvs\\BookChatbot-9Q2uoHrH\\Lib\\site-packages\\unstructured\\partition\\pdf.py:931\u001b[39m, in \u001b[36m_partition_pdf_or_image_with_ocr\u001b[39m\u001b[34m(filename, file, include_page_breaks, languages, ocr_languages, is_image, metadata_last_modified, starting_page_number, password, **kwargs)\u001b[39m\n\u001b[32m    929\u001b[39m         elements.extend(page_elements)\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpage_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_pdf_to_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstarting_page_number\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpage_elements\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_partition_pdf_or_image_with_ocr_from_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m            \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlanguages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   (...)\u001b[39m\u001b[32m    941\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43melements\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_elements\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Beeface\\.virtualenvs\\BookChatbot-9Q2uoHrH\\Lib\\site-packages\\unstructured\\partition\\pdf_image\\pdf_image_utils.py:407\u001b[39m, in \u001b[36mconvert_pdf_to_images\u001b[39m\u001b[34m(filename, file, chunk_size, password)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    406\u001b[39m     f_bytes = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     info = \u001b[43mpdf2image\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpdfinfo_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserpw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    409\u001b[39m total_pages = info[\u001b[33m\"\u001b[39m\u001b[33mPages\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m start_page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, total_pages + \u001b[32m1\u001b[39m, chunk_size):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Beeface\\.virtualenvs\\BookChatbot-9Q2uoHrH\\Lib\\site-packages\\pdf2image\\pdf2image.py:607\u001b[39m, in \u001b[36mpdfinfo_from_path\u001b[39m\u001b[34m(pdf_path, userpw, ownerpw, poppler_path, rawdates, timeout, first_page, last_page)\u001b[39m\n\u001b[32m    604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m d\n\u001b[32m    606\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PDFInfoNotInstalledError(\n\u001b[32m    608\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to get page count. Is poppler installed and in PATH?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     )\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m    611\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PDFPageCountError(\n\u001b[32m    612\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnable to get page count.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr.decode(\u001b[33m'\u001b[39m\u001b[33mutf8\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    613\u001b[39m     )\n",
      "\u001b[31mPDFInfoNotInstalledError\u001b[39m: Unable to get page count. Is poppler installed and in PATH?"
     ]
    }
   ],
   "source": [
    "# loader = DirectoryLoader(\"PDF\", glob=\"**/*.pdf\")  # :contentReference[oaicite:17]{index=17}\n",
    "# docs = loader.load()\n",
    "# print(f\"You have {len(docs)} documents loaded.\")\n",
    "# print(f\"You have {len(docs[0].page_content)} characters in your documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95620644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: PDF\\Agents_Companion_v2.pdf\n",
      "Loading: PDF\\Agents_v8.pdf\n",
      "Loading: PDF\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf\n",
      "Loading: PDF\\Prompt Engineering_v7 (1).pdf\n",
      "Loading: PDF\\Solving Domain-Specific problems using LLMs_v7.pdf\n",
      "Loading: PDF\\whitepaper_emebddings_vectorstores_v2.pdf\n",
      "Loading: PDF\\whitepaper_Foundational Large Language models & text generation_v2.pdf\n",
      "You have 466 documents loaded.\n",
      "You have 195 characters in your documents\n"
     ]
    }
   ],
   "source": [
    "pdf_folder = \"PDF\"  # Folder containing your PDF files\n",
    "all_docs = []\n",
    "\n",
    "for filename in os.listdir(pdf_folder):\n",
    "    if filename.lower().endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_folder, filename)\n",
    "        print(f\"Loading: {pdf_path}\")\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        docs = loader.load()\n",
    "        all_docs.extend(docs)  # Append the loaded pages/documents to the full list\n",
    "\n",
    "print(f\"You have {len(all_docs)} documents loaded.\")\n",
    "print(f\"You have {len(all_docs[0].page_content)} characters in your documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0748fb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 76 documents loaded.\n",
      "You have 195 characters in your documents\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"PDF/Agents_Companion_v2.pdf\")  # :contentReference[oaicite:17]{index=17}\n",
    "docs = loader.load()\n",
    "print(f\"You have {len(docs)} documents loaded.\")\n",
    "print(f\"You have {len(docs[0].page_content)} characters in your documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6368c505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 0, 'page_label': '1'}, page_content='Agents Companion \\n \\nAuthors: Antonio Gulli, Lavi Nigam,  \\nJulia Wiesinger, Vladimir Vuskovic,  \\nIrina Sigler, Ivan Nardini, Nicolas Stroppa, \\nSokratis Kartakis, Narek Saribekyan,  \\nand Alan Bount'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 1, 'page_label': '2'}, page_content='Agents Companion\\nFebruary 2025\\n2\\nAcknowledgements\\nEditors & curators\\nAnant Nawalgaria\\nContent contributors\\nAnant Nawalgaria\\nSteven Johnson\\nHussain Chinoy \\nDesigner\\nMichael Lanning'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 2, 'page_label': '3'}, page_content='Introduction 6\\nAgent Ops 8\\nAgent Success Metrics 12\\nAgent Evaluation 14\\nAssessing Agent Capabilities 15\\nEvaluating Trajectory and Tool Use 17\\nEvaluating the Final Response 20\\nHuman-in-the-Loop Evaluation 21\\nMore about Agent Evaluation 22\\nMultiple Agents & Their Evaluation 23\\nUnderstanding Multi-Agent Architectures 24\\nMulti-Agent Design Patterns and Their Business Impact 25\\nImportant components of Agents 28\\nChallenges in Multi-Agent systems  31\\nMulti-Agent Evaluation 32\\nTable of contents'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 3, 'page_label': '4'}, page_content='Agentic RAG: A Critical Evolution in Retrieval-Augmented Generation 33\\nAgentic RAG and its Importance 34\\nBetter Search, Better RAG 36\\nAgents in the enterprise 38\\nManager of agents 38\\nGoogle Agentspace 40\\nNotebookLM Enterprise 41\\nGoogle AgentSpace Enterprise 43\\nFrom agents to contractors 46\\nContracts 46\\nContract Lifecycle 49\\nContract execution 49\\nContract Negotiation 50\\nContract Feedback 51\\nSubcontracts 51\\nAutomotive AI: Real World Use of Multi-Agent Architecture 54\\nSpecialized Agents 54\\nConversational Navigation Agent 54\\nConversational Media Search Agent 56\\nMessage Composition Agent 56\\nCar Manual Agent 57\\nGeneral Knowledge Agent 58\\nPatterns in Use 58'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 4, 'page_label': '5'}, page_content='Hierarchical Pattern 58\\nDiamond Pattern 60\\nPeer-to-Peer 62\\nCollaborative Pattern 64\\nResponse Mixer Agent 66\\nAdaptive Loop Pattern 67\\nAdvantages of Multi-Agent Architecture for Automotive AI 68\\nAgent Builder 69\\nSummary 70\\nEndnotes 74'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 5, 'page_label': '6'}, page_content='Agents Companion\\nFebruary 2025\\n6\\nIntroduction\\nGenerative AI agents mark a leap forward from traditional, standalone language models, \\noffering a dynamic approach to problem-solving and interaction. As defined in the original \\nAgents paper, an agent is an application engineered to achieve specific objectives by \\nperceiving its environment and strategically acting upon it using the tools at its disposal. \\nThe fundamental principle of an agent lies in its synthesis of reasoning, logic, and access to \\nexternal information, enabling it to perform tasks and make decisions beyond the inherent \\ncapabilities of the underlying model. These agents possess the capacity for autonomous \\noperation, independently pursuing their goals and proactively determining subsequent \\nactions, often without explicit instructions.\\nThe future of AI is agentic.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 6, 'page_label': '7'}, page_content='Agents Companion\\nFebruary 2025\\n7\\nThe architecture of an agent is composed of three essential elements that drive its behavior \\nand decision-making:\\n• Model: Within the agent\\'s framework, the term \"model\" pertains to the language \\nmodel (LM) that functions as the central decision-making unit, employing instruction-\\nbased reasoning and logical frameworks. The model can vary from general-purpose to \\nmultimodal or fine-tuned, depending on the agent\\'s specific requirements.\\n• Tools: Tools are critical for bridging the divide between the agent\\'s internal capabilities \\nand the external world, facilitating interaction with external data and services. These \\ntools empower agents to access and process real-world information. Tools can include \\nextensions, functions, and data stores. Extensions bridge the gap between an API and \\nan agent, enabling agents to seamlessly execute APIs. Functions are self-contained \\nmodules of code that accomplish specific tasks. Data stores provide access to dynamic \\nand up-to-date information, ensuring a model’s responses remain grounded in factuality \\nand relevance.\\n• Orchestration layer: The orchestration layer is a cyclical process that dictates how the \\nagent assimilates information, engages in internal reasoning, and leverages that reasoning \\nto inform its subsequent action or decision. This layer is responsible for maintaining \\nmemory, state, reasoning, and planning. It employs prompt engineering frameworks to \\nsteer reasoning and planning, facilitating more effective interaction with the environment \\nand task completion. Reasoning techniques such as ReAct, Chain-of-Thought (CoT), and \\nTree-of-Thoughts (ToT) can be applied within this layer.\\nBuilding on these foundational concepts, this companion paper is designed for developers \\nand serves as a \"102\" guide to more advanced topics. It offers in-depth explorations of agent \\nevaluation methodologies and practical applications of Google agent products for enhancing \\nagent capabilities in solving complex, real-world problems.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 7, 'page_label': '8'}, page_content=\"Agents Companion\\nFebruary 2025\\n8\\nWhile exploring these theoretical concepts, we'll examine how they manifest in real-world \\nimplementations, with a particular focus on automotive AI as a compelling case study. The \\nautomotive domain exemplifies the challenges and opportunities of multi-agent architectures \\nin production environments. Modern vehicles demand conversational interfaces that function \\nwith or without connectivity, balance between on-device and cloud processing for both \\nsafety and user experience, and seamlessly coordinate specialized capabilities across \\nnavigation, media control, messaging, and vehicle systems. Through this automotive lens, \\nwe'll see how different coordination patterns -- hierarchical, collaborative, and peer-to-\\npeer -- come together to create robust, responsive user experiences in environments with \\nsignificant constraints. This case study illustrates the practical application of multi-agent \\nsystems that businesses across industries can adapt to their specific domains.\\n Anyone who has built with gen AI quickly realizes it’s easy to get from an idea to a proof of \\nconcept, but it can be quite difficult to ensure high quality results and get to production - gen \\nAI agents are no exception. Quality and Reliability are the most cited concerns for deploying \\nto production, and the “Agent Ops” process is a solution to optimize agent building.\\nAgent Ops\\nOver the past two years, the field of Generative AI (GenAI) has undergone significant \\nchanges, with enterprise customers focusing on how to operationalize related solutions. This \\nhas resulted in various terms describing the operationalization of GenAI, such as MLOps for \\nGenAI, LLMOps, FMOps, and GenAIOps.\\nAgent and Operations (AgentOps) is a subcategory of GenAIOps that focuses on the efficient \\noperationalization of Agents. Its main additional components include internal and external \\ntool management, agent brain prompt (goal, profile, instructions) and orchestration, memory, \\nand task decomposition.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 8, 'page_label': '9'}, page_content='Agents Companion\\nFebruary 2025\\n9\\nFigure 1. Relationship between DevOps, MLOps, and AgentOps. 13\\nEach of these “Ops” requires capabilities like version control, automated deployments \\nthrough CI/CD, testing, logging, security and (critically) metrics. Each system often \\nimplements some form of optimization based on metrics – measuring what your system \\nis and isn’t doing, measuring the outcomes and business metrics, and automating the \\nprocesses for more holistic metrics, and incrementally improving step by step. This practice \\nmight be called “A/B experimentation” or “ML Ops” or “Metrics Driven development\", but \\nthey derive from the same general approach and we will rely on those principles for Agent \\nOps as well.\\nRemember that new practices don\\'t replace the old. DevOps and MLOps best practices \\nare still necessary for AgentOps, as they are dependencies. For example, Agent tool use, \\nwhere APIs are invoked based on agent orchestration, often uses the same APIs you would'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 9, 'page_label': '10'}, page_content='Agents Companion\\nFebruary 2025\\n10\\ninvoke with non-agentic software. Authentication and secret management, security, privacy, \\nexception handling, throttling, quotas, and scalability are still critical and require careful API \\ndesign in addition to Agent design.\\nLet’s go ahead and define these “ops” terms to help distinguish between them:\\n• Development and Operations (DevOps) is the practice of efficiently productionizing \\ndeterministic software applications by integrating the elements of people, processes, and \\ntechnology. DevOps serves as the foundation for all the following terms.\\n• Machine Learning Operations (MLOps) builds upon the capabilities of DevOps and \\nconcentrates on the efficient productionization of ML models. The primary distinction is \\nthat the output of an ML model is non-deterministic and relies on the input data (garbage \\nin, garbage out).\\n• Foundation Model Operations (FMOps) expands upon the capabilities of MLOps \\nand focuses on the efficient productionization of pre-trained (trained from scratch) or \\ncustomized (fine-tuned) FMs.\\n• Prompt and Operations (PromptOps) is a subcategory of GenAIOps that focuses \\non operationalizing prompts effectively. Its main additional capabilities include prompt \\nstorage, lineage, metadata management (including evaluation scores), a centralized \\nprompt template registry, and a prompt optimizer.\\n• RAG and Operations (RAGOps) is a subcategory of GenAIOps that centers on efficiently \\noperationalizing RAG solutions. Its primary additional capabilities include the retrieval \\nprocess through offline data preparation (encompassing cleaning, chunking, vectorization, \\nsimilarity search, and re-ranking) and the generation process through prompt \\naugmentation and grounding.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 10, 'page_label': '11'}, page_content='Agents Companion\\nFebruary 2025\\n11\\n• Agent and Operations (AgentOps) is a subcategory of GenAIOps that focuses on the \\nefficient operationalization of Agents. Its main additional components include internal \\nand external tool management, agent brain prompt (goal, profile, instructions) and \\norchestration, memory, and task decomposition.\\nFigure 2. Each of these “Ops” are about technology, processes, and people 14\\nAll of these “Ops” are, in essence, the harmonious blend of people, processes, and \\ntechnologies working together to efficiently deploy machine learning solutions into a \\nlive production environment. It’s crucial to recovgnize that Ops extends beyond mere \\ntechnology; it’s not simply about constructing and delivering a ML pipeline. Successful Ops \\nimplementations delve deeper, considering the customer’s operational model, their existing \\nbusiness units, and their overall organizational structure. This holistic approach ensures that \\nthe technology is tailored to their specific needs, seamlessly integrating into the business \\nand maximizing value.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 11, 'page_label': '12'}, page_content='Agents Companion\\nFebruary 2025\\n12\\nThe next section will cover Agent Evaluation in detail, which is a significant part of the story \\nfor Agent Ops and automation to capture useful metrics. Before we go there, let’s start with a \\nthought experiment; imagine setting up an A/B experiment in production for your new Agent.  \\nThe treatment arm gets your new agent and the control arm does not.  In that scenario, what \\nmetrics are you measuring to determine if the treatment arm is doing better? What metrics \\nare you measuring to determine ROI for the project? Is it a goal being accomplished, or \\nsales totals, or a set of critical steps in a user journey? Those metrics must be understood, \\ninstrumented and easily analyzed in addition to more detailed Agent Evaluation metrics.\\nAgent Success Metrics\\nMetrics are critical to building, monitoring, and comparing revisions of Agents. Business \\nmetrics, like revenue or user engagement, are probably outside of the scope of the agent \\nitself but these should be the north star metric for your agents.\\nMost Agents are designed around accomplishing goals, so goal completion rate is a key \\nmetric to track. Similarly, a goal might be broken down into a few critical tasks or critical \\nuser interactions. Each of these critical tasks and interactions should be independently \\ninstrumented and measured.\\nSo before we get into the details of the Agent itself, we already have several metrics \\nidentified which you should be able to easily track on a dashboard. Each business metric, \\ngoal, or critical interaction, will be aggregated in a familiar fashion: attempts, successes, \\nrates, etc. Additionally, metrics you should be able to get from any application telemetry \\nsystem are very important to track for agents as well, metrics like latency, errors, etc.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 12, 'page_label': '13'}, page_content='Agents Companion\\nFebruary 2025\\n13\\nNone of these metrics are specific to Agents, you could track them for any software, but they \\nare even more important for Agent builders. Deterministic code does only what you tell it to \\ndo, whereas an agent can do a lot more, relying on LLMs which are trained on huge amounts \\nof data. Instrumentation of these high level metrics is an important part of observability. \\nYou can think of them as Key Performance Indicators (KPI) for the agent, and they allow for \\nobservability in the aggregate, a higher level perspective of your agents.\\nHuman feedback is one of the more critical metrics to track as well. A simple 👍👎  or user \\nfeedback form, within the context of an agent or task can go a long way to understanding \\nwhere your agent does well and where it needs improvement. This feedback can come from \\nend users of a consumer system, but also employees, QA testers, and process or domain \\nexperts reviewing the agent.\\nMore detailed observability is also very important for agent building, being able to see and \\nunderstand what the agent is doing and why it’s doing that. An agent can be instrumented \\nwith “trace” to log all of the inner workings of the agent, not only the critically important \\ntasks and user interactions. You could conceptually measure every internal step as metrics, \\nbut that is rarely done. Instead these detailed traces are used to debug an agent when \\nmetrics or manual testing show a problem, you can dig into details and see what went wrong.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 13, 'page_label': '14'}, page_content=\"Agents Companion\\nFebruary 2025\\n14\\nFigure 3: An example of Cloud Observability showing traces for an agent with tools and LLM \\nOpenTelemetry spans. 15\\nSo far we’ve been talking about business metrics, goals, tasks, human feedback, and traces \\n– those are all ways of understanding the actions and impact of your agents, in production.  \\nWhile developing an agent, in addition to manual testing, automated testing will be much \\nmore efficient in the long run and provide greater insights into the behavior of agents.\\nAgent Evaluation\\nTo bridge the gap between a proof-of-concept and a production-ready AI agent, a robust \\nand automated evaluation framework is essential. Unlike evaluating generative models, where \\nthe focus is primarily on the final output, agent evaluation requires a deeper understanding of \\nthe decision-making process. Agent evaluation can be broken down into three components \\nthat we discuss in this chapter:\\n1. Assessing Agent Capabilities: Evaluating an agent's core abilities, such as its capacity to \\nunderstand instructions and reason logically.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 14, 'page_label': '15'}, page_content=\"Agents Companion\\nFebruary 2025\\n15\\n2. Evaluating Trajectory and Tool Use: Analyzing the steps an agent takes to reach a \\nsolution, including its choice of tools, strategies, and the efficiency of its approach.\\n3. Evaluating the Final Response: Assessing the quality, relevance, and correctness of the \\nagent's final output.\\nAssessing Agent Capabilities\\nBefore evaluating your specific agentic use cases, publicly available benchmarks and \\ntechnical reports can provide insight into core capabilities and limitations to consider when \\nbuilding out your agentic use cases. Public benchmarks exist for most fundamental agentic \\ncapabilities like model performance, hallucinations, tool calling and planning. For example, \\ntool calling, the ability to select and use appropriate tools, is demonstrated by benchmarks \\nlike the Berkeley Function-Calling Leaderboard (BFCL)16 and τ-bench17 that also outlines \\ncommon mistakes. Another example, PlanBench18 aims to assess planning and reasoning, \\nacross several domains and specific capabilities. \\nBut tool calling and planning is not the only capability you should consider. Agents inherit \\nbehaviors from their LLMs and each of their other components. Likewise, agent and user \\ninteractions have a history in traditional conversational design systems and workflow \\nsystems, and therefore can inherit the set of metrics and measurements that are used to \\ndetermine the efficacy of these systems.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 15, 'page_label': '16'}, page_content='Agents Companion\\nFebruary 2025\\n16\\nFigure 4: A high level diagram of AgentBench. 19\\nHolistic agent benchmarks like AgentBench19 try to capture end to end performance across \\nseveral scenarios. This is more realistic, when the scenario reflects your agent use case, \\nbut not if it’s testing capabilities your agent doesn’t implement. It is difficult to simulate \\nthe environment, tools, instructions, and use case requirements in ways that are both \\nspecific and general at the same time. Companies and organizations are setting up public \\nbenchmarks for specialized use cases, like Adyen’s Data Analyst leaderboard DBAStep20 \\nwhich may give you a more targeted evaluation - if you understand both the evaluation \\napproach and the agents who are on the leaderboard.\\nPublic benchmarks are a valuable starting point, to get a feeling for what is possible and \\nidentify pitfalls to look out for. Most benchmark assessments include discussions of common \\nfailure modes that can guide you in setting up your own, use-case specific evaluation \\nframework.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 16, 'page_label': '17'}, page_content='Agents Companion\\nFebruary 2025\\n17\\nBeyond public evaluations, you will want to evaluate the behavior of your agent across a \\nvariety of scenarios to ensure it does what you want it to do. You are simulating interactions \\nwith your agent and evaluating how it responds. This includes the evaluating final response \\nand also the set of steps it takes along the way (trajectory). Those are the 2 most common \\nand practical approaches we recommend you start with. There are many other evaluation \\ntechniques you can use beyond these, either for finer details on sub-components or \\nbroader approaches.\\nSoftware engineers will compare this to automated testing of code.  Investing in automated \\ntests saves you time and gives you confidence in the software you are building. With agents, \\nthis automation pays off faster, in both time and confidence. Curating the evaluation data \\nset will be extremely important for accurately representing the use case your agent will \\nencounter, even more so than in software testing. \\nEvaluating Trajectory and Tool Use\\nAn agent usually does several actions before it responds back to the user.  It might compare \\nthe user input with session history to disambiguate a term, or lookup a policy document, \\nsearch a knowledge base or invoke an API to save a ticket. Each of those actions is a step on \\na path, also called a “trajectory” of actions. Every time your agent does something, there’s a \\ntrajectory of actions under the hood.\\nComparing the trajectory that you expect the agent to take vs the trajectory that the agent \\nactually took, is particularly useful for developers who want to debug their application, \\nidentifying errors or inefficiencies, and ultimately improving performance.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 17, 'page_label': '18'}, page_content='Agents Companion\\nFebruary 2025\\n18\\nFigure 5: LangSmith diagram of final evaluation and trajectory. 21  \\nThe following six ground-truth-based automated trajectory evaluations provide different \\nlenses to assess the performance of an agent:\\n1. Exact match: Requires the AI agent to produce a sequence of actions (a \"trajectory\") that \\nperfectly mirrors the ideal solution. This is the most rigid metric, allowing no deviation from \\nthe expected path. \\n2. In-order match: This metric assesses an agent\\'s ability to complete the expected \\ntrajectory, while accommodating extra, unpenalized actions. Success is defined by \\ncompleting the core steps in order, with flexibility for additional actions.\\n3. Any-order match: Compared to in-order match, this metric now disregards the order. It \\nasks if the agent included all necessary actions, but does not look into the order of actions \\ntaken and also allows for extra steps.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 18, 'page_label': '19'}, page_content=\"Agents Companion\\nFebruary 2025\\n19\\n4. Precision: How many of the tool calls in the predicted trajectory are actually relevant or \\ncorrect according to the reference trajectory?\\n5. Recall: How many of the essential tool calls from the reference trajectory are actually \\ncaptured in the predicted trajectory?\\n6. Single-tool use: Understand if a specific action is within the agent's trajectory. This \\nmetric is useful to understand if the agent has learned to utilize a particular tool yet.\\nFigure 6: A radar chart plotting a single trajectory evaluation with a few metrics. 24\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 19, 'page_label': '20'}, page_content=\"Agents Companion\\nFebruary 2025\\n20\\nConsider these metrics as different lenses for analyzing and debugging your agent's \\ntrajectory. Each metric offers a unique perspective, but not all will be relevant to every \\nsituation. For instance, some use cases demand strict adherence to the ideal trajectory, while \\nothers allow for more creative deviations. A clear limitation of this evaluation approach is that \\nyou need to have a reference trajectory in place for this to work. While ground-truth-based \\nautomated trajectory evaluations that are discussed here are prevalent in popular libraries. \\nResearch is advancing the use of agent autoraters for more efficient evaluation, for example \\nAgent as a Judge, 202422.\\nEvaluating the Final Response\\nThe final response evaluation boils down to a simple question: Does your agent achieve its \\ngoals? You can define custom success criteria, tailored to your specific needs, to measure \\nthis. For example, you could assess whether a retail chatbot accurately answers product \\nquestions, or whether a research agent effectively summarizes findings with the appropriate \\ntone and style. To automate this process, you can use autorater. An autorater is an LLM \\nthat acts as a judge. Given the input prompts and the generated response, it mirrors \\nhuman evaluation by assessing the response against a set of user-provided criteria. For \\nthis evaluation to work, it is crucial to consider that given the absence of ground-truth, you \\nneed to be very precise in defining your evaluation criteria, as this is the core of what your \\nevaluation is looking at. You find a number of predefined criteria in various libraries, treat \\nthem as a starting point and tweak them to provide your definition of good.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 20, 'page_label': '21'}, page_content=\"Agents Companion\\nFebruary 2025\\n21\\nHuman-in-the-Loop Evaluation\\nThe fields of agent development and agent evaluation are rapidly evolving. Evaluating \\nAI agents presents significant challenges, including defining clear objectives, designing \\nrealistic environments, managing stochastic behavior, and ensuring fairness and bias \\nmitigation, particularly in socially impactful applications. Therefore, it's crucial to incorporate \\na human-in-the-loop approach alongside the automated evaluations discussed previously \\n(which involve predefined metrics and autoraters). Human-in-the-loop is valuable for tasks \\nrequiring subjective judgment or creative problem-solving, it can also serve to calibrate and \\ndouble-check if your automated evaluation approaches actually work and align with your \\npreferences. Key benefits include:\\n• Subjectivity: Humans can evaluate qualities that are difficult to quantify, such as \\ncreativity, common sense, and nuance.\\n• Contextual Understanding: Human evaluators can consider the broader context of the \\nagent's actions and their implications.\\n• Iterative Improvement: Human feedback provides valuable insights for refining the \\nagent's behavior and learning process.\\n• Evaluating the evaluator: Human feedback can provide a signal to calibrate and refine \\nyour autoraters. \\nTo implement human-in-the-loop evaluation, consider these methods:\\n• Direct Assessment: Human experts directly rate or score the agent's performance on \\nspecific tasks.\\n• Comparative Evaluation: Experts compare the agent's performance to that of other \\nagents or your previous iterations.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 21, 'page_label': '22'}, page_content=\"Agents Companion\\nFebruary 2025\\n22\\n• User Studies: Participants interact with the agent and provide feedback on its behavior, \\nusability, and overall effectiveness.\\nMore about Agent Evaluation\\nIn this section we cover agent evaluation from the practical perspective. But this is just the tip \\nof the iceberg.  Agent evaluation presents many challenges. Evaluation data for your agents \\nmay be difficult to find. While synthetic data or LLMs as judges can be used, evaluations \\nmay still be incomplete. Also, LLM-as-a-Judge metrics, for example, may prioritize final \\noutcomes over the agent's reasoning and intermediate actions, potentially missing key \\ninsights. Additionally, as evaluations for agent systems have a history in conversational and \\nworkflow systems, there is so much to explore on how to inherit methods and metrics to \\nevaluate agent's capabilities, such as the ability to improve task performance over multiple \\ninteractions. Evaluations for multi-modal generations pose additional complexities; images, \\naudio, and video evaluations require their own evaluation methods and metrics. Finally, real-\\nworld environments pose further challenges, as they are dynamic and unpredictable, making \\nit difficult to evaluate agents in controlled settings.\\nLooking ahead, to solve these open challenges, the field of agent evaluation is evolving \\nrapidly. Key trends include a shift towards process-based evaluation, prioritizing the \\nunderstanding of agent reasoning; an increase in AI-assisted evaluation methods for \\nimproved scalability; and a stronger focus on real-world application contexts. The \\ndevelopment of new standardized benchmarks is also gaining traction, facilitating objective \\ncomparisons between agents, while increased emphasis on explainability and interpretability \\naims to provide deeper insights into agent behavior.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 22, 'page_label': '23'}, page_content='Agents Companion\\nFebruary 2025\\n23\\nTable 1: A table comparing strengths and weaknesses of automated evaluations for Agents.\\nAt this point it should be clear that only by continually refining evaluation methods, we will \\nensure that AI agents are developed and deployed responsibly, effectively, and ethically in \\nthe coming future. \\nMultiple Agents & Their Evaluation\\nAgent evaluation, which assesses the effectiveness, reliability, and adaptability of \\nautonomous AI agents, as seen in the previous section, has emerged as a key focus area. \\nWe have seen a significant evolution in AI systems, transitioning towards multi-agent \\narchitectures—where multiple specialized agents collaborate to achieve complex objectives. \\nA multi-agent system is like a team of experts, each specializing in a particular area, working \\ntogether to solve a complex problem. Each agent is an independent entity, potentially using a \\ndifferent LLM, and with its own unique role and context. Agents communicate and collaborate \\nto achieve a common goal. This approach differs from traditional single-agent systems, \\nwhere one LLM handles all aspects of a task.\\nEvaluation Method  Strengths  Weaknesses\\nHuman Evaluation\\nCaptures nuanced behavior, \\nconsiders human factors\\nSubjective, time-consuming, \\nexpensive, difficult to scale\\nLLM-as-a-Judge Scalable, efficient, consistent\\nMay overlook intermediate steps, \\nlimited by LLM capabilities\\nAutomated Metrics Objective, scalable, efficient\\nMay not capture full capabilities, \\nsusceptible to gaming'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 23, 'page_label': '24'}, page_content=\"Agents Companion\\nFebruary 2025\\n24\\nMulti-agent systems offer several advantages over single-agent systems:\\n• Enhanced Accuracy: Agents can cross-check each other's work, leading to more \\naccurate results.\\n• Improved Efficiency: Agents can work in parallel, speeding up task completion.\\n• Better Handling of Complex Tasks: Large tasks can be broken down into smaller, more \\nmanageable subtasks, with each agent focusing on a specific aspect.\\n• Increased Scalability: The system can be easily scaled by adding more agents with \\nspecialized capabilities.\\n• Improved Fault Tolerance: If one agent fails, others can take over its responsibilities.\\n• Reduced Hallucinations and Bias: By combining the perspectives of multiple agents, \\nthe system can reduce the effects of hallucinations and bias, leading to more reliable and \\ntrustworthy outputs.\\nUnderstanding Multi-Agent Architectures\\nUnlike traditional monolithic AI systems, multi-agent architectures break down a problem \\ninto distinct tasks handled by specialized agents. Each agent operates with defined roles, \\ninteracting dynamically with others to optimize decision-making, knowledge retrieval, and \\nexecution. These architectures enable more structured reasoning, decentralized problem-\\nsolving, and scalable task automation, offering a paradigm shift from single-agent workflows.\\nAt their core, multi-agent systems leverage principles of modularity, collaboration, and \\nhierarchy to create a robust AI ecosystem. Agents within these systems can be categorized \\nbased on their function for example:\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 24, 'page_label': '25'}, page_content='Agents Companion\\nFebruary 2025\\n25\\n• Planner Agents: Responsible for breaking down high-level objectives into \\nstructured sub-tasks.\\n• Retriever Agents: Optimize knowledge acquisition by dynamically fetching relevant data \\nfrom external sources.\\n• Execution Agents: Perform computations, generate responses, or interact with APIs.\\n• Evaluator Agents: Monitor and validate responses, ensuring coherence and alignment \\nwith objectives.\\nThrough these components, multi-agent architectures extend beyond simple prompt-based \\ninteractions, enabling AI-driven workflows that are adaptive, explainable, and efficient.\\nMulti-Agent Design Patterns and Their Business Impact\\nTo design effective multi-agent architectures, specific design patterns have emerged. These \\npatterns define interaction protocols, delegation mechanisms, and role distributions, allowing \\nbusinesses to implement AI-driven automation in structured ways. Some common design \\npatterns include:'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 25, 'page_label': '26'}, page_content='Agents Companion\\nFebruary 2025\\n26\\nTable 2: A table comparing types of multi-agent systems.\\nType of Multi-Agent System Description Example\\nSequential\\nAgents work in a sequential \\nmanner, with each agent \\ncompleting its task before \\npassing the output to the \\nnext agent.\\nAn assembly line, where each \\nworker performs a specific \\noperation before passing the \\nproduct to the next worker.\\nHierarchical\\nAgents are organized in a \\nhierarchical structure, with a \\n\"manager\" agent coordinating the \\nworkflow and delegating tasks to \\n\"worker\" agents.\\nA system with a leader agent \\nresponsible for making strategic \\ndecisions and follower agents \\nexecuting tasks based on the \\nleader\\'s instructions.\\nCollaborative\\nAgents work together \\ncollaboratively, sharing \\ninformation and resources to \\nachieve a common goal.\\nA team of researchers working \\non a project, where each member \\ncontributes their expertise \\nand insights.\\nCompetitive\\nAgents may compete with \\neach other to achieve the \\nbest outcome.\\nLLMs act as individual players in a \\ngame like Overcooked-AI, where \\nthey must coordinate their actions \\nto achieve a shared goal while \\ncompeting for resources and \\noptimizing individual performance.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 26, 'page_label': '27'}, page_content='Agents Companion\\nFebruary 2025\\n27\\nFigure 7: An image depicting different multi-agent topologies, from LangGraph documentation. 23\\nThe choice of design pattern depends on the specific application and the desired level of \\ninteraction between agents.\\nBusinesses benefit from these patterns by reducing operational bottlenecks, improving \\nknowledge retrieval, and enhancing automation reliability. Multi-agent systems enable \\ncompanies to scale AI deployments while ensuring agility in decision-making and \\nworkflow execution.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 27, 'page_label': '28'}, page_content='Agents Companion\\nFebruary 2025\\n28\\nImportant components of Agents\\nThe architecture of LLM-based AI agents consists of several interrelated components \\nessential for autonomous operation and intelligent interaction:\\n• Interaction Wrapper: This component serves as the interface between the agent \\nand its environment, managing communication and adapting to various input and \\noutput modalities.\\n• Memory Management: This includes short-term working memory for immediate context, \\ncache, and sessions . It also can include long-term storage for learned patterns and \\nexperiences, as episodes, examples, skills or reference data. It also includes “reflection” \\nto decide which short term items (eg: user preference) should be copied into long term \\nmemory (eg: user profile), and if that can be shared across agents, tasks, or sessions.\\n• Cognitive Functionality: This is often underpinned by Chain-of-Thought (CoT), ReACT, \\nreasoning, thinking, or a planner subsystem - it allows agents to decompose complex \\ntasks into logical steps and engage in self-correction. In some cases this also includes \\nuser intent refinement, to ask a clarifying question if uncertain.\\n• Tool Integration: This subsystem enables agents to utilize external tools, expanding \\ntheir capabilities beyond natural language processing. Dynamic tool registries allowing \\ndiscovery, registration, and “Tool RAG”.\\n• Flow / Routing: This governs connections with other agents, facilitating dynamic neighbor \\ndiscovery and efficient communication within the multi-agent system.  This might be \\nimplemented as a delegation of a task to a background agent, or handoff of the user \\ninteraction to an agent, or the use of an agent as a tool.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 28, 'page_label': '29'}, page_content='Agents Companion\\nFebruary 2025\\n29\\n• Feedback Loops / Reinforcement Learning: These enable continuous learning and \\nadaptation by processing interaction outcomes and refining decision-making strategies.  \\nFor gen AI agents this rarely takes the form of traditional RL training, but the performance \\nmetrics of the past can be incorporated into future decision making.\\n• Agent Communication: Effective communication between agents is crucial for the \\nsuccess of multi-agent systems. The Agent to Agent communication protocol facilitates \\nstructured and efficient communication among agents, enabling them to achieve \\nconsensus and address complex problems collaboratively\\n• Remote Agent Communication: Agent to Agent communication within an organization \\nis critical to allows agents to share messages, tasks, and knowledge.  Once your multi-\\nagent system includes a remote agent, the communication protocol becomes even more \\nimportant. Asynchronous tasks and sessions need to be durable, and updated with \\nnotifications while end users are not in session.  Negotiations between Agents must allow \\nfor bringing a user into session and for supported UX capabilities.  \\n• Agent & Tool Registry (mesh): As you go beyond a handful of tools or a handful of \\nagents, you need a robust system to discover, register, administer, select and utilize from \\na “mesh” of tools or agents.  Critically important is the ontology and description of the \\ntools and agents, their capabilities and requirements, and their performance metrics. Your \\nagents will make a plan and choose which tool or which agent to use from such a system, \\nand those choices are informed by the data in the system'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 29, 'page_label': '30'}, page_content='Agents Companion\\nFebruary 2025\\n30\\nFigure 8: An image demonstrating the process of a user interacting with multiple, self-coordinating agents. 24  \\nThese architectural elements provide a robust foundation for the autonomous operation and \\ncollaborative capabilities of LLM agents within multi-agent systems.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 30, 'page_label': '31'}, page_content='Agents Companion\\nFebruary 2025\\n31\\nChallenges in Multi-Agent systems \\nWhile multi-agent systems offer numerous advantages, they also face several challenges:\\n• Task Communication: Today most agent frameworks communicate in messages, not \\nstructured async tasks.\\n• Task Allocation: Efficiently dividing complex tasks among different agents can be \\nchallenging, and feedback loops are often left up to the developer to implement.\\n• Coordinating Reasoning: Getting agents to debate and reason together effectively \\nrequires sophisticated coordination mechanisms.\\n• Managing Context: Keeping track of all the information, tasks, and conversations \\nbetween agents can be overwhelming.\\n• Time and Cost: Multi-agent interactions can be computationally expensive and time-\\nconsuming.  This results in higher runtime prices and more user latency.\\n• Complexity: In the same way that microservice architecture allows each microservice \\nmore flexibility and simplicity, the system as a whole usually becomes more complex.  \\nAddressing these challenges is crucial for developing robust and efficient \\nmulti-agent systems.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 31, 'page_label': '32'}, page_content='Agents Companion\\nFebruary 2025\\n32\\nMulti-Agent Evaluation\\nLuckily, the evaluation of multi-agent systems is a clear progression of evaluating single \\nagent systems. Agent Success Metrics are unchanged, business metrics as your north star, \\ngoals and critical task success metrics, application telemetry metrics like latency and errors.  \\nInstrumenting the multi-agent system with trace will help debug and understand what is \\nhappening during complex interactions.\\nIn the Agent Evaluation section we discussed Evaluating Trajectories and Evaluating the Final \\nResponse as the 2 best approaches to automated evaluation of an agent, and this remains \\nthe case for multi-agent systems. For a multi-agent system, a trajectory of actions might \\ninclude several or even all of your agents. Even though several agents may collaborate on a \\ntask, a single final answer is returned to the user at the end and can be evaluated in isolation.\\nBecause a multi-agent system probably has more steps, you can drill down and evaluate at \\nevery step.  You can evaluate each of your agents in isolation and the system as a whole.  \\nTrajectory evaluations are a scalable approach to do exactly this.\\nThere are some questions you need to ask, which are unique to multi-agent \\nsystems, including:\\n• Cooperation and Coordination: How well do agents work together and coordinate their \\nactions to achieve common goals?\\n• Planning and Task Assignment: Did we come up with the right plan, and did we stick to \\nit?  Did child agents deviate from the main plan or get lost in a cul-de-sac?\\n• Agent Utilization: How effectively do agents select the right agent and choose to use the \\nagent as a tool, delegate a background task, or transfer the user?'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 32, 'page_label': '33'}, page_content=\"Agents Companion\\nFebruary 2025\\n33\\n• Scalability: Does the system's quality improve as more agents are added?  Does the \\nlatency go down?  Are we being more efficient or less?\\nThese types of questions can guide developers to identify areas for improvement in the \\nmulti-agent system. You will answer these questions using the same tools you use for single \\nagent systems, but the complexity of the analysis goes up.\\nAgentic RAG: A Critical Evolution in \\nRetrieval-Augmented Generation\\nA significant advancement in multi-agent architectures is Agentic Retrieval-Augmented \\nGeneration (Agentic RAG). Traditional RAG pipelines rely on a static approach—retrieving \\nknowledge from vector databases and feeding it into an LLM for synthesis. However, this \\napproach often fails when dealing with ambiguous, multi-step, or multi-perspective queries.\\nAgentic RAG introduces autonomous retrieval agents that actively refine their search \\nbased on iterative reasoning. These agents enhance retrieval in the following ways:\\n• Context-Aware Query Expansion: Instead of relying on a single search pass, agents \\ngenerate multiple query refinements to retrieve more relevant and comprehensive results.\\n• Multi-Step Reasoning: Agents decompose complex queries into smaller logical steps, \\nretrieving information sequentially to build structured responses.\\n• Adaptive Source Selection: Instead of fetching data from a single vector database, \\nretrieval agents dynamically select the best knowledge sources based on context.\\n• Validation and Correction: Evaluator agents cross-check retrieved knowledge for \\nhallucinations and contradictions before integrating it into the final response.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 33, 'page_label': '34'}, page_content=\"Agents Companion\\nFebruary 2025\\n34\\nThis approach significantly improves response accuracy, explainability, and adaptability, \\nmaking it a crucial innovation for enterprises dealing with complex knowledge retrieval tasks \\nin areas like legal research, scientific discovery, and business intelligence.\\nAgentic RAG and its Importance\\nAgentic RAG (Retrieval-Augmented Generation) is an advanced approach that combines the \\nstrengths of RAG with the autonomy of AI agents. Traditional RAG systems retrieve relevant \\ninformation from external knowledge sources to enhance LLM responses. Agentic RAG \\ntakes this a step further by employing intelligent agents to orchestrate the retrieval process, \\nevaluate the retrieved information, and make decisions about how to best utilize it.\\nAgentic RAG offers several advantages over traditional RAG:\\n• Improved Accuracy: Agents can evaluate the quality of retrieved information and make \\ndecisions about which sources to trust, leading to more accurate and reliable responses.\\n• Enhanced Contextual Understanding: Agents can consider the context of the user's \\nquery and the retrieved information to generate more relevant and meaningful responses.\\n• Increased Adaptability: Agents can adapt to changing information needs and \\ndynamically adjust their retrieval strategies to provide the most up-to-date and relevant \\ninformation. This adaptability is crucial in complex domains where information is constantly \\nevolving, such as healthcare, finance, and legal research.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 34, 'page_label': '35'}, page_content='Agents Companion\\nFebruary 2025\\n35\\nFigure 9: A diagram of Agentic RAG from Vectorize.io. 25\\nAgentic RAG is particularly valuable in complex domains where information is constantly \\nevolving, such as healthcare, finance, and legal research. For example, in healthcare, agentic \\nRAG can help navigate complicated medical databases, research papers, and patient \\nrecords, providing doctors with comprehensive and accurate information.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 35, 'page_label': '36'}, page_content='Agents Companion\\nFebruary 2025\\n36\\nBetter Search, Better RAG\\nAlmost all RAG approaches require a search engine to index and retrieve relevant data. The \\nintroduction of agents allows for refinement of query, filtering, ranking, and the final answer.  \\nAgentic RAG agents are executing several searches to retrieve information.\\nFor developers who are trying to optimize existing RAG implementations, it is usually most \\nvaluable to improve search results (measured in recall) prior to introducing agents.  Some of \\nthe main techniques to improve search performance are:\\n• Parse source documents and chunk them: Vertex AI Layout Parser can handle complex \\ndocument layouts, embedded tables, and embedded images like charts, and uses a \\nsemantic chunker to keep chunks on topic with a hierarchy of headings.\\n• Add metadata to your chunks: synonyms, keywords, authors, dates, tags and categories \\nallow your searches to boost, bury, and filter; these allow your users or your agents more \\ncontrol over search results.\\n• Fine tune the embedding model or add a search adaptor which changes embedding \\nspace: these allow the searchable index of vectors to represent your domain better than a \\ngeneral purpose embedding model.\\n• A faster vector database can improve search quality: to search embeddings, you must \\nmake a tradeoff between speed and accuracy, upgrading to an ultra-fast Vertex AI \\nVector Search can improve both latency and quality\\n• Use a ranker: vector searches are fast but approximate, they should return dozens or \\nhundreds of results which need to be re-ranked by a more sophisticated system to ensure \\nthe top few results are the most relevant or best answer.\\n• Implement check grounding: as a safeguard on grounded generation, you can ensure \\neach phrase is actually citable by retrieved chunks.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 36, 'page_label': '37'}, page_content='Agents Companion\\nFebruary 2025\\n37\\nFigure 10: A diagram of common RAG and search components, showing Vertex AI Search 26 , search builder \\nAPIs 27 , and RAG Engine. 28\\nVertex AI Search26 is a powerful search engine providing Google quality search for your \\ndata and can be used with any RAG or Agentic RAG implementation. Each of the above \\ncomponents is automatically available within Vertex AI Search, without any development \\ntime at all.  For developers who want to build their own search engine, each of the above \\ncomponents is exposed as a stand alone API27, and RAG Engine28 can orchestrate the whole \\npipeline easily, with a LlamaIndex like python based interface.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 37, 'page_label': '38'}, page_content='Agents Companion\\nFebruary 2025\\n38\\nAgents in the enterprise\\nManager of agents\\n2025 is the year of the agents, and this will particularly be true in the context of enterprises \\nthat will develop and use agents that will assist employees to perform specific tasks \\nor autonomously run in the background performing automation. Business analysts can \\neffortlessly uncover industry trends and create compelling, data-driven presentations \\nfueled by AI-generated insights. HR teams can revolutionize the employee experience with \\nstreamlined onboarding, even for complex tasks like 401k selection. Software engineers \\ncan proactively identify and resolve bugs, enabling them to build and iterate with greater \\nefficiency, and accelerate deployment cycles. Marketers can unlock deeper performance \\nanalysis, optimize content recommendations, and fine-tune campaigns effortlessly to achieve \\nbetter results. \\nWe see two types of agents emerging:\\n1. “Assistants”: Agents that interact with the user, take a task, execute it, and come back \\nto the user. Conversational agents popularized by frameworks like Gems or GPTs usually \\nbelong to this category. Assistants can be general - able to help on a variety of tasks - or \\nspecialized to a particular domain, or tasks. Examples include agents that help schedule \\nmeetings, analyze data, write code, write marketing press releases, help sellers with \\nsales opportunities, or even agents that perform deep research on a particular topic \\nas requested by the user. These agents can be synchronous and return the requested \\ninformation or perform the requested task fast, or they run for a longer period of time \\nbefore returning (like the deep research agents).'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 38, 'page_label': '39'}, page_content='Agents Companion\\nFebruary 2025\\n39\\n2. \"Automation agents”: Agents that run in the background, listen to events, monitor \\nchanges in systems or data, and then make smart decisions and act. Action might include \\nacting on backend systems, performing some tests to validate the observation, fixing \\nproblems, notifying the right employees, etc. This can really be seen as the backbone of \\nthe automation of the future. While in the past we had to write special code to specify \\nthe logic of automations, now we can start relying on smart and general decision making \\nabilities of AI agents.\\nRather than simply invoking agents to perform a task and wait for the output, knowledge \\nworkers will increasingly become managers of agents. They will be assigning tasks to multiple \\nagents and manage them, check if any of agents need help or require approval to proceed, \\nuse the output of specific agents to start new tasks, monitor execution of long running tasks \\nto review and steer them in the right direction. Novel user interfaces to allow this type of \\nvirtual team management will emerge to allow orchestration, monitoring and managing a \\nmulti-agent system with agents performing tasks, calling (or even creating) other agents.\\nGoogle Agentspace aims at providing this type of experience and allow to:\\n• Create new agents by using a no/low code interface or a full code framework to \\nimplement both specialized assistants and automation agents\\n• Configure and manage the agents access for company administrators\\n• Invoke the right agents when appropriate\\n• Monitor, manage, and orchestrate multiple agents in a UI suited for team management'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 39, 'page_label': '40'}, page_content=\"Agents Companion\\nFebruary 2025\\n40\\nGoogle Agentspace\\nGoogle Agentspace29 offers a suite of AI-driven tools designed to elevate enterprise \\nproductivity by facilitating access to pertinent information and automating intricate, \\nagentic workflows. It harnesses the advanced reasoning capabilities of Gemini, the power \\nof Google's search infrastructure, and secure access to enterprise data, irrespective of its \\nphysical location.\\nAgentspace directly addresses the limitations inherent in traditional knowledge management \\nsystems, which frequently fall short in areas such as personalization, automated answer \\ngeneration, contextual comprehension, and comprehensive information retrieval. It \\nempowers employees with efficient information access by consolidating disparate content \\nsources, generating grounded and personalized responses, and streamlining operational \\nworkflows. Key functionalities include the capacity to ingest a wide variety of data formats, \\nsynchronize data from Software-as-a-Service (SaaS) platforms, deliver access-controlled \\nsearch results and AI-generated answers, and integrate AI assistance and intelligent agents \\ninto cohesive workflows.\\nThe architecture of Agentspace Enterprise is founded upon several core principles.  \\nParamount among these is built-in trust, which prioritizes security, explainability, and \\ngovernance through features such as single sign-on (SSO) authentication, an integrated \\npermissions model, and user-level access controls. Google's advanced intelligence is \\nleveraged to discern user behavior and content patterns through machine learning, resulting \\nin the delivery of highly relevant results via semantic understanding, knowledge graphs, \\nand LLMs. Universal connectivity is achieved by connecting to a diverse array of enterprise \\nsystems with on-demand and automated data refreshes, thereby eliminating information \\nsilos. Enterprise-level customization and user-level personalization are facilitated through \\ngranular controls for search functionality, recommendations, LLMs, and knowledge graphs, \\nproviding tailored experiences based on individual user roles, permissions, and search\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 40, 'page_label': '41'}, page_content='Agents Companion\\nFebruary 2025\\n41\\nhistory. Real-time feedback and adaptation mechanisms enable the continuous refinement \\nof results through machine learning and user input. Blended Retrieval Augmented Generation \\n(RAG) allows for customizable data blending, powering generative AI applications grounded \\nin enterprise data. Finally, scalability is a critical design consideration, with the system \\nengineered to accommodate growth across geographical regions, languages, and peak \\nusage demands.\\nSecurity is always top of mind. Google Agentspace is built on the Google Cloud secure-by-\\ndesign infrastructure, giving you the peace of mind to confidently deploy AI agents across \\nyour organization. It provides granular IT controls, including role-based access control \\n(RBAC), VPC Service Controls, and IAM integration, ensuring your data remains protected \\nand compliant at all times.Security is a foundational principle of Agentspace. Built upon the \\nsecure infrastructure of Google Cloud, it provides a robust environment for the deployment \\nof AI agents. Granular IT controls, encompassing role-based access control (RBAC), \\nVirtual Private Cloud (VPC) Service Controls, and Identity and Access Management (IAM) \\nintegration, guarantee data protection and regulatory compliance. These security measures \\nare essential for the safeguarding of sensitive information and give users the peace of mind \\nto confidently deploy AI agents across their organization.\\nNotebookLM Enterprise\\nNotebookLM30 is a research and learning tool designed to streamline the process of \\nunderstanding and synthesizing complex information. It empowers users to upload various \\nsource materials, including documents, notes, and other relevant files, and leverages the \\npower of artificial intelligence to facilitate deeper comprehension. Imagine researching a \\nmultifaceted topic; NotebookLM allows you to consolidate all your scattered resources into'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 41, 'page_label': '42'}, page_content='Agents Companion\\nFebruary 2025\\n42\\na single, organized workspace. In essence, NotebookLM functions as a dedicated research \\nassistant, accelerating the research process and enabling users to move beyond mere \\ninformation collection to genuine understanding.\\nFigure 11: NotebookLM Enterprise 30  configured with a few mock earning reports.\\nNotebookLM Plus builds upon this foundation, offering a premium tier with enhanced \\nfeatures and capabilities. While the core functionality of uploading sources, asking questions, \\nand generating summaries remains, NotebookLM Plus typically adds features like increased \\nstorage for source materials, enabling work with larger and more complex projects. It'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 42, 'page_label': '43'}, page_content='Agents Companion\\nFebruary 2025\\n43\\nmay also include more sophisticated AI-powered analysis tools, such as more nuanced \\nsummarization options, enhanced question-answering capabilities, or the ability to identify \\nconnections and relationships between different sources more effectively.\\nBuilding upon the foundation of NotebookLM Plus, NotebookLM Enterprise30 brings these \\npowerful capabilities to the enterprise environment, streamlining how employees interact \\nwith and derive insights from their data. This enterprise-grade version enables users to not \\nonly upload and synthesize information, but also to uncover hidden patterns and engage with \\ndata in innovative ways. A prime example is the AI-generated audio summary feature, which \\nenhances comprehension and facilitates efficient knowledge absorption by allowing users to \\n\"listen\" to their research. \\nTechnically, NotebookLM, both in its consumer and enterprise forms, employs LLMs to \\nprocess uploaded documents, extract key concepts, and generate summaries. The audio \\nsummary feature uses text-to-speech (TTS) technology incorporating advanced prosody \\ncontrol to ensure clarity and naturalness. Critically, NotebookLM Enterprise incorporates \\nenterprise-grade security and privacy features, ensuring that sensitive company data is \\nhandled with the utmost care and protected in accordance with organizational policies.`\\nGoogle AgentSpace Enterprise\\nGoogle Agentspace furnishes employees with a unified, company-branded, multimodal \\nsearch agent that serves as the definitive source of enterprise information. Drawing upon \\nGoogle\\'s extensive search capabilities, Agentspace offers unparalleled conversational \\nassistance. Employees get answers to complex queries, proactive recommendations, \\nand unified access to information from any source. This functionality extends to both \\nunstructured data, such as documents and emails, and structured data, like tables.  \\nIntegrated translation capabilities ensure comprehensive understanding, regardless of'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 43, 'page_label': '44'}, page_content=\"Agents Companion\\nFebruary 2025\\n44\\nthe original language of the information. Pre-built connectors for widely used third-party \\napplications, including Confluence, Google Drive, Jira, Microsoft SharePoint, ServiceNow, \\nand others, empower employees to seamlessly access and query relevant data sources, \\nfacilitating more informed decision-making.\\nFigure 12: Agentspace 29  customized for Cymbal bank, connected to several data stores.\\nMuch more than just information retrieval, agents can take actions in the real world, manage \\nasync tasks and workflows, and help employees get work done. A gallery of agents can be \\nconfigured based on an organization's unique needs and will help with deep research, idea \\ngeneration and merit based refinement, creative asset generation, data analytics and more.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 44, 'page_label': '45'}, page_content='Agents Companion\\nFebruary 2025\\n45\\nAgentspace Enterprise Plus facilitates the creation of custom AI agents tailored to specific \\nbusiness functions. This platform enables the development and deployment of contextually \\naware generative AI agents, empowering employees across departments—marketing, \\nfinance, legal, engineering, and more—to conduct more effective research, rapidly generate \\ncontent, and automate repetitive tasks, including multi-step workflows. A centralized \\ninterface streamlines the discovery and access of these specialized agents, promoting \\nscalable AI adoption. Custom agents will connect with internal and external systems and \\ndata, be tailored to company domain and policies, and potentially even utilize machine \\nlearning models trained on proprietary business data. The platform provides builders tools \\nfor agent development, deployment, and lifecycle managemen'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 45, 'page_label': '46'}, page_content='Agents Companion\\nFebruary 2025\\n46\\nFrom agents to contractors\\nThe common interface to define AI agents across various tools and platforms today is very \\nsimple, and usually includes specifying the goal, some textual instructions, the tools that the \\nagent can use, and a set of examples. While this might be sufficient to prototype demos, it \\nleads to potentially underspecified definitions, and might be one of the leading reasons that \\nAI agents can struggle to get from prototype-to-production. \\nWe propose to evolve the Agent interface to evolve them into ”Contract adhering agents” \\nwhich are aimed at solving complex tasks using AI Agents, more specifically in contexts \\nwhere stakes are high. \\nContracts\\nThe key idea of contractors is to specify and the standard the contracts between the \\nrequester and the agents, making it possible to: \\n1. Define the outcomes as precisely as possible, similarly to what we would do in a \\nreal world scenario when agreeing on a service with a company we are contracting to \\ndo something for us. This allows the agent (contractor) to validate against the desired \\noutcomes and iterate until the desired objective is achieved. \\n2. Make it possible to negotiate the task as well as clarifying and refining it, in order \\nto avoid any ambiguity in the definition, and fill any gap in common understanding of \\nthe goals.\\n3. Define the rules for the contractors to generate new subcontracts needed to solve the \\nbigger one in a standard fashion (cf. section below on subcontracts).'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 46, 'page_label': '47'}, page_content='Agents Companion\\nFebruary 2025\\n47\\n4. \\nTable 3: An example data model for the contracts.\\nContract, initial definition\\nFields Description Required\\nTask/Project\\ndescription\\nProvide a detailed description of what we expect the\\ncontractor to achieve. It should be as specific and as non-\\nambiguous as possible.\\nYes\\nDeliverables &\\nSpecifications\\nDescribe precisely the expected outcomes and\\ndeliverables from the contractor&#39;s task, including a list of\\nspecifications clarifying what makes the deliverable\\nacceptable as outcome and details on how to verify that\\nthe deliverable is fulfilling the expectation.\\nYes\\nScope\\nClarify the scope of the tasks that the contractor is\\nresponsible for completing, going into separate detail\\nabout every aspect of the task. Also used to clarify what is out \\nof scope.\\nNo \\nExpected Cost\\nGives expectation in terms of cost for the task completion.\\nThis is usually a function of the complexity of the task\\ncombined with what tools will be used.\\nYes\\nExpected Duration Gives expectation in terms of duration for the task completion. Yes\\nInput Sources Specify what input sources can be used and considered to be \\nuseful to complete the task. No\\nReporting and\\nFeedback\\nSpecifies how the feedback loop should look like: how often we \\nexpect updates on the progress, and what mechanism/surface \\nis used to provide feedback (emails, APIs, etc.).\\nYes'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 47, 'page_label': '48'}, page_content='Agents Companion\\nFebruary 2025\\n48\\nTable 4: An example data model for the iteration of messaging between contractors.\\nContract Iteration: Feedback & Negotiation\\nFields Description Required\\nUnderspecification Highlight aspects that are underspecified or\\nneed clarification from the task initiator. No\\nCost negotiation Cost considered too high to complete the task. No\\nRisk Highlights potential risks in fulfilling the contract. No \\nAdditional input \\nneeded\\nExpress the kinds of additional data or information that would \\nbe useful to fulfill the contract. No'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 48, 'page_label': '49'}, page_content='Agents Companion\\nFebruary 2025\\n49\\nContract Lifecycle\\nThe following illustration shows the lifecycle of defining, negotiating and executing the \\ncontract: \\nFigure 13: A diagram of the agent as contractor lifecycle from Agentspace.\\nContract execution\\nThis requires the contractor runtime to be able to fulfill the contracts and solve the \\ntasks behind contracts according to their defined specifications. Prioritizing quality and \\ncompleteness over latency enables to fully leverage the capabilities of LLMs, for example'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 49, 'page_label': '50'}, page_content='Agents Companion\\nFebruary 2025\\n50\\nby generating different solutions, and reviewing, scoring, and evolving them. An example of \\nthis methodology is shown below in the Co-Scientist study. The engine can iterate and self-\\nvalidate the results and deliverables based on the provided expectations, and can improve \\nand self-correct until the validators are fulfilled. The ability to concretely validate a solution \\nagainst a set of objective criterias has proven to work extremely well in the context of AI and \\nhas been at the core of successful AI systems such as for example Alpha-Code.\\nContract Negotiation\\nOne core hypothesis behind contracts in the context of automation agents specifically is that \\nmany tasks in the enterprise world can draw significant benefit from leveraging the power of \\nLLMs when used in a less-constrained manner (latency and cost wise).\\nBeing able to tackle more and more complex tasks and making it possible for customers to \\nbe able to rely on and trust the results of contractors will ensure real value for companies. \\nEven that being said, we need to have a notion of relative priority in order to make sure that \\ntasks are appropriately prioritized as well as resources fairly allocated. We thus introduce a \\nnotion of cost (typically relative per customer or contract initiator) which can be discussed \\nand negotiated between the contract initiator and the contractor, in order to make sure that \\nthe contract receives the adequate resources relative to the other contracts initiated by the \\ncontract initiator. The contractors can also negotiate other aspects of the contracts, for \\nexample in terms of specification and deliverables (cf. also section below on feedback).'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 50, 'page_label': '51'}, page_content='Agents Companion\\nFebruary 2025\\n51\\nContract Feedback\\nContracts give a vehicle to provide feedback and in particular resolve ambiguities. As tasks \\nbecome more and more complex, it is critical to be able to raise ambiguities or other issues \\nrelated to the tasks specifications as early as possible.  Contractors can give feedback on the \\ncontract just after having received the contract (initial contract assessment), and then at a \\nfrequency predefined in the contract.\\nThis feedback will contain clarification requests, or other types of feedback about the \\nunderspecification or misspecification of tasks (inconsistencies, conflicting specs, \\nclarification, etc.).\\nSubcontracts\\nAlthough not part of the contract definition and specification directly, the ability to \\ndecompose a task into subtasks by generating subcontracts is a core concept that will be \\nused to power the contractors’ engine.\\nWhen a task is considered too complex to be tackled directly, contractors can decide to \\ndecompose the task into smaller and easier tasks, which will be added to the execution \\nqueue for solving.  This is made possible only through the contract formalization described \\nabove, which makes it possible for the contractors to generate, process and manipulate \\nother contracts in a uniform and standardized way.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 51, 'page_label': '52'}, page_content='Agents Companion\\nFebruary 2025\\n52\\nGoogle\\'s Co-Scientist: A Case Study in \\nMulti-Agent Intelligence\\nGoogle\\'s AI co-scientist is a prime example of a multi-agent LLM system applied to scientific \\nresearch. This system utilizes a team of specialized agents, each with its own role and \\nexpertise, to accelerate the pace of scientific discovery. These agents collaborate to \\ngenerate, evaluate, and refine hypotheses, mirroring the iterative process of scientific inquiry.\\nThe co-scientist system employs a \"generate, debate, and evolve\" approach, drawing \\ninspiration from the scientific method. This approach involves generating diverse hypotheses, \\ncritically evaluating their potential, and refining them through ongoing feedback and analysis. \\nThe system leverages the strengths of different LLMs, each specializing in a particular aspect \\nof the research process, to achieve a more comprehensive and robust outcome.\\nFor instance, in a study on liver fibrosis treatments, the co-scientist not only identified \\nexisting drugs but also proposed new mechanisms and promising drug candidates, \\ndemonstrating its potential to generate novel insights. Some of its major components are: \\n• Data Processing Agents: aggregate and structure large volumes of experimental data.\\n• Hypothesis Generators: propose potential explanations based on existing research and \\nnew findings.\\n• Validation Agents: run simulations and verify results before presenting them \\nto researchers.\\n• Collaboration Agents: communicate findings across different research teams, enhancing \\ninterdisciplinary cooperation.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 52, 'page_label': '53'}, page_content='Agents Companion\\nFebruary 2025\\n53\\nFigure 14: A process diagram of the AI co-scientist system 31\\nThis approach exemplifies how multi-agent architectures move beyond static AI interactions, \\ninstead fostering dynamic, evolving intelligence capable of handling sophisticated tasks in \\nresearch, enterprise automation, and knowledge management.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 53, 'page_label': '54'}, page_content='Agents Companion\\nFebruary 2025\\n54\\nAutomotive AI: Real World Use of \\nMulti-Agent Architecture\\nTo illustrate how multi-agent concepts manifest in practice, let\\'s examine a comprehensive \\nmulti-agent system designed for automotive conversational AI, where several specialized \\nagents collaborate to provide an intuitive and seamless in-car experience.\\nFirst, we’ll look at some examples of specialized agents, followed by some common patterns \\nfor combining them to form a multi-agent system.\\nSpecialized Agents\\nLet’s first look at some examples of specialized agents. \\nConversational Navigation Agent\\nThe Conversational Navigation Agent is specialized in helping users find locations, suggest \\nplaces, and navigate using APIs like Google Places and Maps. This agent understands \\nnavigation intents like \"Find me a sushi restaurant nearby,\" queries the Places API for relevant \\nresults, re-ranks these results based on user preferences and historical data, and sends \\nnavigation commands to the car\\'s built-in system.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 54, 'page_label': '55'}, page_content='Agents Companion\\nFebruary 2025\\n55\\nExample Interaction:\\nUser: \"I need to take a break for lunch. What are good restaurants on my route \\nto Munich?\"\\nNavigation Agent: \"I found three well-rated restaurants along your route on the A9. In 25 \\nkilometers, there\\'s \\'Gasthaus Alte Mühle\\' at the Ingolstadt exit with 4.5 stars - reviewers \\npraise their traditional Bavarian dishes and beer garden. In 45 kilometers, \\'La Strada\\' at \\nthe service area has 4.1 stars with good reviews for quick Italian meals. Or if you can wait \\n70 kilometers, \\'Zeitlos\\' near Allershausen has 4.7 stars - it\\'s a bit fancier with farm-to-table \\ncuisine. Which one interests you most?\"\\nUser: \"The Bavarian place sounds good. Is there convenient parking there, and how far is \\nit from the motorway?\"\\nNavigation Agent: \"Gasthaus Alte Mühle has a dedicated parking area with about 40 \\nspaces. It\\'s only 1.2 kilometers from the A9 exit at Ingolstadt-Nord - about a 2-minute drive. \\nThey\\'re known for their Schweinebraten and homemade dumplings. It gets busy around \\nnoon, but it\\'s currently showing moderate occupancy. Would you like me to set this as your \\ndestination and guide you there? You\\'ll arrive around 12:15 if we head there now.\"\\nUser: \"The Bavarian place sounds good. Is there convenient parking there, and how far is \\nit from the motorway?\"\\nNavigation Agent: \"Gasthaus Alte Mühle has a dedicated parking area with about 40 \\nspaces. It\\'s only 1.2 kilometers from the A9 exit at Ingolstadt-Nord - about a 2-minute drive. \\nThey\\'re known for their Schweinebraten and homemade dumplings. It gets busy around \\nnoon, but it\\'s currently showing moderate occupancy. Would you like me to set this as your \\ndestination and guide you there? You\\'ll arrive around 12:15 if we head there now.\"'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 55, 'page_label': '56'}, page_content='Agents Companion\\nFebruary 2025\\n56\\nConversational Media Search Agent\\nThe Conversational Media Search Agent is specialized in helping users find and play music, \\naudiobooks, and podcasts. This agent retrieves media from local databases or streaming \\nservices, uses web search for additional content when necessary, suggests media based on \\ncontextual factors like mood, weather, and time of day, and handles voice commands such \\nas \"Play a relaxing playlist for my drive.\" It also identifies similar artists or content when users \\nask questions like \"Who sounds like John Coltrane?\"\\nExample Interaction:\\nUser: \"Play something upbeat to help me stay alert on this long motorway drive.\"\\nMedia Search Agent: \"I\\'ll play an upbeat driving playlist to help keep you energized. \\nStarting with \\'Don\\'t Stop Me Now\\' by Queen, followed by some Daft Punk and Stromae \\ntracks. I\\'ve added some popular European radio hits that match your preferences. I\\'ve \\nadjusted the volume for optimal road awareness. Just say \\'next\\' if you want to skip \\na song.\"\\nMessage Composition Agent\\nThe Message Composition Agent is specialized in helping users draft, summarize, and send \\nmessages or emails while driving. This agent parses voice commands like \"Tell Alex I\\'m \\nrunning late,\" uses natural language models to compose contextually appropriate messages, \\noffers draft previews with user modification options, and integrates with various messaging \\napps including SMS, WhatsApp, and email.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 56, 'page_label': '57'}, page_content='Agents Companion\\nFebruary 2025\\n57\\nExample Interaction:\\nUser: \"There\\'s heavy traffic on the E40. Let my team know I\\'ll be 20 minutes late to the \\nmorning standup.\"\\nMedia Search Agent: \"I\\'ve prepared a message to your work team: \\'Stuck in traffic on \\nthe E40. Will be about 20 minutes late to our morning standup. Please start without me.\\' \\nShould I send this to your \\'Work Team\\' chat group, or would you like to edit it first?\"\\nCar Manual Agent\\nThe Car Manual Agent is specialized in providing answers to car-related questions by \\nleveraging a Retrieval-Augmented Generation (RAG) system. This agent retrieves relevant \\nsections from the car manual based on user queries, summarizes and contextualizes this \\ninformation using an LLM, and links users to detailed documentation or instructional videos \\nwhen appropriate.\\nExample Interaction:\\nUser: \"How do I turn off the lane-keeping assist feature in this Volkswagen?\"\\nMedia Search Agent: \"To turn off lane-keeping assist in your Volkswagen, press the \\nbutton with the car-and-lane icon located on the left side of your steering wheel. You\\'ll \\nsee a confirmation in the driver display when it\\'s deactivated. You can also disable \\nit permanently in the infotainment system under Settings > Driver Assistance > Lane \\nKeeping. Would you like me to walk you through the settings menu instead?\"'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 57, 'page_label': '58'}, page_content='Agents Companion\\nFebruary 2025\\n58\\nGeneral Knowledge Agent\\nThe General Knowledge Agent is specialized in answering factual questions about the world, \\nhistory, science, culture, and other general topics. This agent accesses a broad knowledge \\nbase to answer factual questions, provides biographical information about people, offers \\ncontextual explanations of concepts and phenomena, maintains grounding in factual \\ninformation while avoiding hallucinations, and responds to follow-up questions by maintaining \\ncontextual awareness.\\nExample Interaction:\\nUser: \"Tell me an interesting fact about Salzburg.\"\\nMedia Search Agent: \"Salzburg, Austria is the birthplace of Mozart, one of history\\'s most \\nfamous composers. Beyond that well-known fact, Salzburg\\'s old town is a UNESCO World \\nHeritage site, and parts of the beloved musical \\'The Sound of Music\\' were filmed there in \\n1964. The Salzburg Festival, one of the world\\'s most prestigious music and drama festivals, \\nhas been held there annually since 1920.\"\\nPatterns in Use\\nLet’s look at the some common ways used to combine these agents.\\nHierarchical Pattern\\nA central Orchestrator Agent classifies queries and routes them to specialized agents. For \\nexample, when a user asks \"Find me a sushi restaurant nearby,\" the Orchestrator detects \\nthis as a navigation request and routes it to the Navigation Agent, which handles location \\nsearches and interaction with maps APIs.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 58, 'page_label': '59'}, page_content='Agents Companion\\nFebruary 2025\\n59\\nFigure 15. Hierarchical pattern.\\nThe Orchestrator Agent is specialized in routing queries to the most appropriate specialized \\nagents and managing the overall conversation flow. This agent analyzes user queries to \\ndetermine domain and intent, directs requests to the most suitable specialized agent, \\nmaintains conversation context across multiple turns, and manages fallback strategies when \\nspecialized agents cannot provide satisfactory responses.\\nExample Interaction:\\nFirst Turn:\\nUser: \"Is there a service area ahead with clean toilets?\"\\nOrchestrator Agent: [internally routes to Navigation Agent]\\nNavigation Agent: \"There\\'s a service area 20 kilometers ahead on the E19. It\\'s Aire \\nde Waremme, which was renovated last year and has good ratings for facilities and \\ncleanliness. It also has a Paul bakery and a quick-service restaurant.\"\\nSecond Turn:'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 59, 'page_label': '60'}, page_content='Agents Companion\\nFebruary 2025\\n60\\nUser: \"Will it rain at my destination this evening?\"\\nOrchestrator Agent: [recognizes weather query and routes to Weather \\nInformation Agent]\\nWeather Information Agent: \"At your destination in Brussels, the forecast shows a 60% \\nchance of light rain in the evening, starting around 19:00. Temperatures will be around \\n14°C. I\\'d recommend bringing an umbrella if you plan to be outdoors.\"\\nDiamond Pattern\\nIn this variation of the hierarchical pattern, responses from specialized agents pass \\nthrough a central moderation agent before reaching the user. For instance, the Navigation \\nAgent generates a factual response about a nearby restaurant, which then goes through \\na Rephraser Agent that adjusts the tone and style based on user preferences before \\ntext-to-speech synthesis.\\nThe Rephraser Agent is specialized in adapting the tone, style, and presentation of responses \\nto match user preferences and context. This agent transforms technical information into \\nconversational language, adjusts response length based on driving conditions and urgency, \\npersonalizes communication style to match user preferences, and formats complex \\ninformation for easier comprehension while driving.\\nExample Interaction:\\n[Raw response from Car Manual Agent before rephrasing]: \"In the event of a tyre \\npressure monitoring system alert, the vehicle has detected that one or more tyres have \\nfallen below the recommended inflation pressure of 2.2 bar. The affected tyre\\'s pressure is \\napproximately 1.8 bar, which is 18.75% below recommended levels. This does not constitute \\nan immediate safety hazard, but prolonged driving with underinflated tyres can lead to'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 60, 'page_label': '61'}, page_content='Agents Companion\\nFebruary 2025\\n61\\ndecreased fuel efficiency, reduced tyre longevity, and potentially compromised handling \\ncharacteristics. It is advisable to proceed to a service station at your earliest convenience \\nto reinflate the tyre to proper specifications.\"\\nRephraser Agent: [Adapts for driving context, adjusts information density and urgency]\\nFinal Response to User: \"Your front right tyre is a bit low - down to 1.8 bar. It\\'s safe to \\nkeep driving, but you\\'ll want to add some air at the next petrol station. Low tyre pressure \\ncan affect your fuel economy and make the tyre wear out faster. I can help you find a \\nnearby place to fill up if you\\'d like.\"\\nFigure 16. Diamond pattern.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 61, 'page_label': '62'}, page_content='Agents Companion\\nFebruary 2025\\n62\\nExample transformation:\\nInitial Response (factual & neutral): \"I found a highly-rated sushi restaurant nearby. It\\'s \\ncalled Sakura Sushi, located at 123 Main Street. It has a 4.7-star rating and is open until 10 \\nPM. Would you like me to start navigation?\"\\nAfter rephrasing (playful style): \"Sushi craving? Say no more! Head over to Sakura Sushi \\nat 123 Main Street, where the fish is fresh and the soy sauce flows freely! Rated 4.7 stars, \\nopen till 10 PM. Ready for an umami adventure?”\\nPeer-to-Peer\\nAgents can hand off queries to one another when they detect that the orchestration \\nmade a routing mistake. This creates a more resilient system that can recover from \\ninitial misclassifications.\\nFigure 17. Peer-to-peer.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 62, 'page_label': '63'}, page_content='Agents Companion\\nFebruary 2025\\n63\\nExample scenario:\\n1. User asks: \"Find a place to eat sushi nearby\"\\n2. The Orchestrator correctly routes this to the Conversational Navigation Agent, which \\nprovides information about nearby sushi restaurants.\\n3. User follows up: \"How big is New York\\'s Central Park?\"\\n4. The Orchestrator might initially route this to the Conversational Navigation Agent again \\n(based on the previous navigation-related conversation).\\n5. However, the Conversational Navigation Agent recognizes this as a general knowledge \\nquestion rather than a navigation request, and hands it off to the General Knowledge \\nAgent, which can provide factual information about Central Park\\'s size.\\nAdvantages of peer-to-peer hand-off compared to centralized orchestration:\\n1. Resilience to misclassification: Even if the central orchestrator makes an error in \\nrouting, specialized agents can recognize when a query falls outside their domain and \\nredirect appropriately.\\n2. Domain expertise in routing: Specialized agents often have better understanding of the \\nboundaries of their own domains. The Media Search Agent knows exactly what kinds of \\nmusic-related queries it can handle better than a general orchestrator would.\\n3. Reduced orchestration complexity: The central orchestrator doesn\\'t need perfect \\naccuracy in initial routing, reducing the complexity of its decision-making logic.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 63, 'page_label': '64'}, page_content=\"Agents Companion\\nFebruary 2025\\n64\\nCollaborative Pattern\\nThe Collaborative Pattern involves multiple agents working on complementary aspects of \\nthe same task, with a Response Mixer Agent that combines elements from different agent \\nresponses to create a comprehensive answer. This approach recognizes that different agents \\ncontribute valuable pieces to a complete solution based on their specialized expertise. The \\npattern is particularly valuable when:\\n1. Different aspects of a query require different types of expertise (e.g., technical \\nspecifications, practical advice, and conceptual explanations)\\n2. No single agent has complete information to fully address the user's needs\\n3. The user would benefit from multiple perspectives on the same question\\n4. Different specialized agents have access to distinct knowledge bases or \\nreasoning capabilities\\nUnlike the competitive approach where responses compete, the collaborative pattern \\nassumes that responses from different agents are complementary rather than redundant. \\nThe Response Mixer Agent identifies the most valuable information from each source and \\nsynthesizes it into a cohesive answer that leverages the unique strengths of each specialist.\\nFor example, when asked about handling hydroplaning, the Car Manual Agent contributes \\nvehicle-specific safety system information, the Driving Tips Agent provides practical driving \\ntechniques, and the General Knowledge Agent explains the physics behind the phenomenon. \\nTogether, they create a more complete and useful response than any single agent could \\nprovide alone.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 64, 'page_label': '65'}, page_content='Agents Companion\\nFebruary 2025\\n65\\nFigure 18. Collaborative pattern.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 65, 'page_label': '66'}, page_content='Agents Companion\\nFebruary 2025\\n66\\nResponse Mixer Agent\\nThe Response Mixer is specialized in picking the best responses from multiple agents \\nand merging them when needed. This agent evaluates answers based on their accuracy \\nand relevance, removes incorrect information, and combines useful parts from different \\nresponses to create the most helpful answer for the user.\\nExample Interaction:\\nUser: \"How do I handle aquaplaning?\"\\n[Behind the scenes, multiple agents generate responses:]\\nCar Manual Agent Response: \"Your Audi is equipped with Electronic Stability Control \\n(ESC) and Anti-lock Braking System (ABS) which help prevent aquaplaning by maintaining \\ntraction control.\" [Confidence score: 71%]\\nGeneral Knowledge Agent Response: \"Aquaplaning occurs when your tyres lose contact \\nwith the road surface due to water buildup between the tyres and the road.\" [Confidence \\nscore: 65%]\\nSafety Tips Agent Response: \"If you experience aquaplaning, ease off the accelerator, \\navoid sudden braking or steering, and gently steer in the direction you want to go until you \\nregain traction.\" [Confidence score: 94%]\\nResponse Mixer Agent: [Picks the most relevant parts and merges them into one \\ncomplete answer]\\nFinal Response to User: \"If you experience aquaplaning, which happens when your \\ntyres lose contact with the road due to water buildup, ease off the accelerator, avoid \\nsudden braking or steering, and gently steer in the direction you want to go until you \\nregain traction. Your Audi\\'s Electronic Stability Control and ABS systems will help maintain \\ncontrol, but proper driving technique is still essential.\"'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 66, 'page_label': '67'}, page_content='Agents Companion\\nFebruary 2025\\n67\\nThis competitive approach ensures that critical safety information isn\\'t missed due to \\nmisrouting, and the user receives the most helpful possible response even when their query \\nhas multiple valid interpretations.\\nAdaptive Loop Pattern\\nThe Adaptive Loop Pattern involves iterative refinement through repeated attempts, \\nprogressively improving results until they meet desired criteria.\\nThe Conversational Navigation Agent can implement a loop pattern to iteratively improve \\nsearch results when initial queries don\\'t yield satisfactory outcomes.\\nExample Interaction:\\n1. User asks: \"Find a nice Italian restaurant that serves vegan options\"\\n2. The Navigation Agent queries Places API with these parameters\\n3. The agent evaluates the results and finds no restaurants meeting all criteria\\n4. Instead of returning \"no results,\" the agent automatically reformulates the query:\\n• First loop: Searches for \"Italian restaurants with vegetarian options\"\\n• Second loop: Broadens to \"Italian restaurants\" and then filters for those mentioning \\nplant-based options\\n• Third loop: If still unsuccessful, searches for \"vegan restaurants\" and filters for \\nItalian-influenced cuisine\\n5. The agent presents the best matches from these progressive searches, explaining how \\nthey relate to the original request'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 67, 'page_label': '68'}, page_content=\"Agents Companion\\nFebruary 2025\\n68\\nThis looping behavior enables more robust search capabilities that adapt to availability and \\ncontext, providing useful results even when exact matches aren't available.\\nAdvantages of Multi-Agent Architecture for Automotive AI\\nMulti-agent systems bring powerful advantages to automotive AI by breaking down complex \\ntasks into specialized roles. Think of it as assembling a team of experts rather than relying on \\none generalist.\\nIn this approach, each agent specializes in what it does best. The Navigation Agent focuses \\nsolely on finding locations and planning routes. The Media Search Agent becomes an \\nexpert in music and podcasts. The Car Manual Agent specializes in vehicle features and \\ntroubleshooting. By focusing on specific domains, each agent develops deeper capabilities in \\nits area.\\nThis specialization makes the entire system more efficient. When an agent handles a \\nnarrower set of tasks, it becomes simpler to optimize its performance. The result is higher \\nquality responses delivered more quickly and at lower computational cost. Performance \\nimproves because the system can match the right resources to each task. Simple requests \\nuse minimal processing power, while complex questions tap into more powerful resources \\nonly when needed.\\nSpeed matters in a vehicle, and multi-agent systems deliver where it counts. Critical \\nfunctions like adjusting climate controls or opening windows run on fast, on-device \\nagents for immediate response. Meanwhile, less urgent tasks like finding restaurant \\nrecommendationscan use cloud-based agents with more extensive knowledge. This \\nseparation ensures that essential vehicle controls remain responsive regardless of what else \\nthe system is doing.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 68, 'page_label': '69'}, page_content='Agents Companion\\nFebruary 2025\\n69\\nThis design also creates natural resilience. If internet connectivity drops the essential \\nfunctions running on on-device agents continue working. You might temporarily lose \\nrestaurant recommendations, but climate control and basic media playbacks still \\nfunction perfectly.\\nAgent Builder\\nVertex AI Agent Builder is a collection of products and services for developers. We have \\nput together a comprehensive platform for you to build and connect agents. The engineering \\nexcellence and security from Google Cloud, the AI research from Google Deepmind, and the \\nbest practices of Agent Ops. Google Cloud is building our own agents on top of this platform, \\nand now you can too. Expect many more exciting announcements 2025 targeting developers \\nof agents.\\nVertex AI Agent Engine streamlines development, relying on Google engineering managed \\nintegrations with popular open source agent libraries. It provides a managed autoscaling \\nruntime and many services agents will need (eg: session, examples, trace, evals). This is the \\nvery low effort and high value way to safely deploy agents you have written in any framework.\\nVertex AI Eval Service provides all of the evaluation tools discussed in this whitepaper, and \\nmore. LLMs, RAG, and Agent evals are stable and scalable via the Vertex AI Eval Service, with \\nconvenient integrations into monitoring and experimentation offerings.\\nA large portfolio of agent tools, with more to watch out for:\\n• Retrieval via Vertex AI Search26 or RAG Engine28.\\n• Non-search based retrieval from DBs via Gen AI Toolbox for Databases32'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 69, 'page_label': '70'}, page_content='Agents Companion\\nFebruary 2025\\n70\\n• Application integrations33 with hundreds of APIs supporting full ACLs\\n• Turn any API into a managed, enterprise ready tool with Apigee Hub34\\nAnd of course the best LLMs for agents, with access to Vertex AI Model Garden35 and also \\nthe Gemini family of models36 which will power the agentic era.\\nSummary\\nThis whitepaper (a companion to our earlier whitepaper on Agents) has explored the rapidly \\nevolving landscape of generative AI agents, from their fundamental architecture to advanced \\nevaluation techniques and the transformative potential of multi-agent systems. \\nKey Takeaways for Developers:\\n1. Agent Ops is Essential: Building successful agents goes far beyond the initial proof-\\nof-concept. Embrace Agent Ops principles, integrating best practices from DevOps and \\nMLOps, but also focusing on agent-specific elements like tool management, orchestration, \\nmemory, and task decomposition.\\n2. Metrics Drive Improvement: Start with business-level KPIs (like goal completion, user \\nengagement, or revenue) as your \"north star.\" Then, instrument your agents to track \\ngranular metrics related to critical tasks, user interactions, and agent actions (traces).  \\nHuman feedback (👍👍, user surveys) is invaluable.\\n3. Automated Evaluation is Key: Don\\'t rely solely on manual testing.  Implement automated \\nevaluation frameworks that assess agent capabilities, trajectory (the steps taken), and the \\nfinal response. Leverage techniques like exact match, in-order match, precision/recall for \\ntrajectory evaluation, and autoraters (LLMs as judges) for final response quality.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 70, 'page_label': '71'}, page_content='Agents Companion\\nFebruary 2025\\n71\\n4. Human-in-the-Loop is Crucial: Automated metrics are powerful, but human evaluation \\nprovides essential context, especially for subjective aspects like creativity, common sense, \\nand nuance. Use human feedback to calibrate and validate your automated evaluation \\nmethods. Don’t outsource the domain knowledge.\\n5. Multi-Agent Systems Offer Advantages: Consider multi-agent architectures for \\ncomplex tasks. They can improve accuracy, efficiency, scalability, and fault tolerance. \\nUnderstand different design patterns (sequential, hierarchical, collaborative, competitive) \\nand choose the right one for your application.\\n6. Agentic RAG Improves Relevance: Move beyond traditional RAG by incorporating \\nagents that actively refine search queries, evaluate retrieved information, and adapt to \\nevolving knowledge. This leads to more accurate and contextually relevant responses.\\n7. Search Optimization is Foundational to RAG: Before diving into complex agentic RAG, \\noptimize your underlying search engine. Techniques like semantic chunking, metadata \\nenrichment, fine-tuning embedding models, and using rankers can significantly improve \\nretrieval quality.\\n8. Agent and Tool Registries are Important: As the number of Agents or Tools you \\nare using grow, a registry to manage the capabilities, ontology, and performance \\nbecomes essential.\\n9. Security is Paramount: When deploying agents, especially within an enterprise, prioritize \\nsecurity. Leverage platforms like Google Agentspace that offer built-in security features \\nlike RBAC, VPC Service Controls, and IAM integration.\\n10. Efficient use of developer cycles: The classic build vs buy design choices remain front \\nof mind, as the industry of gen AI agents is rapidly evolving. Consider platforms and \\nproducts as alternatives to building everything from scratch. This will buffer some of the \\nchurn of a fast changing industry and allow you to focus on your data, domain, and users.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 71, 'page_label': '72'}, page_content='Agents Companion\\nFebruary 2025\\n72\\n11. Agents in the enterprise: Agents are transforming the way we work by making us much \\nmore productinve, and the way automation can be accomplished. Knowledge workers will \\nincreasingly be managing fleets of agents and novel UX will emerge. Google Agentspace \\nis a powerful tool allowing to put Enterprise Search, AI and AI Agents on top of company’s \\ndata and workflows\\nFuture Directions for Agent Research and Development: The field of AI agents is \\nundergoing rapid evolution.  Key areas of ongoing research and development include:\\n• Advanced Evaluation Methods: Developing more robust and scalable evaluation \\ntechniques, including process-based evaluation (focusing on reasoning), AI-assisted \\nevaluation, and standardized benchmarks.\\n• Multi-Agent Coordination: Improving the coordination and communication mechanisms \\nwithin multi-agent systems to enable more effective collaboration, task handling, \\nand reasoning.\\n• Real-World Adaptation: Creating agents that can adapt and learn in dynamic, \\nunpredictable real-world environments. Production systems like automotive AI \\nillustrate how agents must balance between on-device performance for critical \\nfunctions and cloud-based capabilities for complex tasks, often adapting to changing \\nconnectivity conditions.\\n• Explainability and Interpretability: Making agent behavior more transparent and \\nunderstandable, allowing developers and users to gain deeper insights into their \\ndecision-making processes.\\n• Long-Term Memory and Learning: Developing more sophisticated memory mechanisms \\nthat allow agents to retain and utilize information over extended periods, enabling \\ncontinuous learning and adaptation.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 72, 'page_label': '73'}, page_content='Agents Companion\\nFebruary 2025\\n73\\n• Agent Communication Protocols: Better defining how agents share tasks, knowledge, \\nand messages, especially across remote systems which are opaque.\\n• From Agents to contractors: In order for agents to get to next level of reliability and \\nutility, we will need to step up the definition of tasks, making them into contracts with clear \\ndeliverables, validation mechanisms, and ability to negotiate ambiguities, similarly to how \\nwe contract work from other companies.\\nCall to Action:\\nThe future of AI is agentic.  We encourage developers to embrace these concepts and \\nbegin building the next generation of intelligent applications.  Start experimenting with the \\ntools and techniques discussed in this whitepaper. Explore the resources available, such \\nas Google Agentspace, NotebookLM Enterprise, Vertex Eval Service, Cloud Observability, \\nand Vertex AI Search, to accelerate your development process. Dive into the provided code \\nexamples, tutorials, and documentation to gain hands-on experience. Build, evaluate, iterate, \\nand contribute to the growing community of agent developers. The possibilities are limitless, \\nand the time to build is now! Specifically, get started with the code and Colab notebooks in \\nthe references.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 73, 'page_label': '74'}, page_content=\"Agents Companion\\nFebruary 2025\\n74\\nEndnotes\\n1. Shafran, I., Cao, Y. et al., 2022, 'ReAct: Synergizing Reasoning and Acting in Language Models'.  \\nAvailable at: https://arxiv.org/abs/2210.03629 .\\n2. Wei, J., Wang, X. et al., 2023, 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models'. \\nAvailable at: https://arxiv.org/pdf/2201.11903.pdf .\\n3. Wang, X. et al., 2022, 'Self-Consistency Improves Chain of Thought Reasoning in Language Models'.  \\nAvailable at: https://arxiv.org/abs/2203.11171 .\\n4. Diao, S. et al., 2023, 'Active Prompting with Chain-of-Thought for Large Language Models'.  \\nAvailable at: https://arxiv.org/pdf/2302.12246.pdf .\\n5. Zhang, H. et al., 2023, 'Multimodal Chain-of-Thought Reasoning in Language Models'.  \\nAvailable at: https://arxiv.org/abs/2302.00923 .\\n6. Yao, S. et al., 2023, 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models'.  \\nAvailable at: https://arxiv.org/abs/2305.10601 .\\n7. Long, X., 2023, 'Large Language Model Guided Tree-of-Thought'.  \\nAvailable at: https://arxiv.org/abs/2305.08291 .\\n8. Google. ';Google Gemini Application'. Available at: http://gemini.google.com .\\n9. Swagger. 'OpenAPI Specification'. Available at: https://swagger.io/specification/ .\\n10. Xie, M., 2022, 'How does in-context learning work? A framework for understanding the differences \\nfromtraditional supervised learning'. Available at: https://ai.stanford.edu/blog/understanding-incontext/ .\\n11. Google Research. 'ScaNN (Scalable Nearest Neighbors)'.  \\nAvailable at: https://github.com/google-research/google-research/tree/master/scann.\\n12. LangChain. 'LangChain'. Available at: https://python.langchain.com/v0.2/docs/introduction/ .\\n13. Sokratis Kartakis, 2024, ‘GenAI in Production: MLOps or GenAIOps?’. Available \\nat: https://medium.com/google-cloud/genai-in-production-mlops-or-genaiops-25691c9becd0 .\\n14. Sokratis Kartakis, 2024 ‘Gen AI Ops, Operationalize Generative AI, A practical Guide’. Available at: \\nhttps://medium.com/google-cloud/genaiops-operationalize-generative- ai-a-practical-  \\nguide-d5bedaa59d78.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 74, 'page_label': '75'}, page_content='Agents Companion\\nFebruary 2025\\n75\\n15. Cloud Trace overview. Available at: https://cloud.google.com/trace/docs/overview .\\n16. Berkeley Function-Calling Leaderboard (BFCL). Available at: \\nhttps://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html .\\n17. Karthik Narasimhan, et al. 2024, ‘τ-bench’. Available at https://arxiv.org/abs/2406.12045 .\\n18. Karthik Valmeekam, et al., 2023, ‘PlanBench’. Available at: https://arxiv.org/abs/2206.10498 .\\n19. Xiao Liu, et al., 2023, ‘AgentBench’. Available at: https://arxiv.org/abs/2308.03688 .\\n20. Martin Iglesias, et al., 2025, `DBAStep ` Available at: https://huggingface.co/spaces/adyen/DABstep.\\n21. LangSmith platform for agent observability.  \\nAvailable at: https://docs.smith.langchain.com/evaluation/concepts#agents .\\n22. Mingchen Zhuge, et al., 2024, ‘Agent-as-a-Judge: Evaluate Agents with Agents’.  \\nAvailable at: https://arxiv.org/abs/2410.10934 .\\n23. Multi-agent documentation from LangGraph.  \\nAvailable at: https://langchain-ai.github.io/langgraph/concepts/multi_agent/ .\\n24. LangChain blog 2024, ‘Multi-agent workflows’.  \\nAvailable at: https://blog.langchain.dev/langgraph-multi-agent-workflows/ .\\n25. Vectorize blog 2024, ‘How I finally got agentic RAG to work right’.  \\nAvailable at: https://vectorize.io/how-i-finally-got-agentic-rag-to-work-right/ .\\n26. Vertex AI Search, product documentation. Available at: https://cloud.google.com/enterprise-search .\\n27. Vertex AI Search Builder APIs, product documentation.   \\nAvailable at: https://cloud.google.com/generative-ai-app-builder/docs/builder-apis .\\n28. Vertex AI RAG Engine, product documentation.  \\nAvailable at: https://cloud.google.com/vertex-ai/generative-ai/docs/rag-overview .\\n29. Agentspace product documentation.  \\nAvailable at: https://cloud.google.com/agentspace/agentspace-enterprise/docs/overview .\\n30. NotebookLM Enterprise product documentation.  \\nAvailable at: https://cloud.google.com/agentspace/notebooklm-enterprise/docs/overview .'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 75, 'page_label': '76'}, page_content='Agents Companion\\nFebruary 2025\\n76\\n31. Juraj Gottweis, et. al., 2025, ‘Accelerating scientific breakthroughs with an AI co-scientist’. Available at:  \\nhttps://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/ .\\n32. Hamsa Buvaraghan, et al. 2025,  ‘Announcing public beta of Gen AI Toolbox for Databases’. Available at: \\nhttps://cloud.google.com/blog/products/ai-machine-learning/announcing-gen-ai-toolbox  \\n-for-databases-get-started-today?e=48754805 .\\n33. Google Cloud Integration Connectors, product documentation.  \\nAvailable at: https://cloud.google.com/integration-connectors/docs .\\n34. Apigee API Hub, product documentation.  \\nAvailable at: https://cloud.google.com/apigee/docs/apihub/what-is-api-hub .\\n35. Vertex AI Model Garden, product documentation.  \\nAvailable at: https://cloud.google.com/model-garden .\\n36. Gemini family of LLMs, product documentation.  \\nAvailable at: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models .\\n37. Get Started Evaluating Agents with the Vertex Eval Service. Available at: \\nhttps://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-agents .\\n38. Irina Sigler, Ivan Nardini. Jan 2025 ‘Introducing Agent Evaluation in Vertex AI’. Available at: \\nhttps://cloud.google.com/blog/products/ai-machine-learning/introducing-agent-evaluation-in-vertex  \\n-ai-gen-ai-evaluation-service?e=48754805 .\\n39. Review sample agent evaluation notebooks for LangGraph, CrewAI, and LangChain.  \\nAvailable at: https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/ .\\n40. Review many sample agents, primarily beginner and intermediate level.  \\nAvailable at: https://github.com/GoogleCloudPlatform/generative-ai/ .\\n41. Review many sample agents, intermediate and advanced levels.  \\nAvailable at: https://github.com/GoogleCloudPlatform/applied-ai-engineering-samples .'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 0, 'page_label': '1'}, page_content='Agents\\nAuthors: Julia Wiesinger, Patrick Marlow  \\nand Vladimir Vuskovic'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 1, 'page_label': '2'}, page_content='Agents\\n2\\nFebruary 2025\\nAcknowledgements\\nContent contributors\\nEvan Huang\\nEmily Xue\\nOlcan Sercinoglu\\nSebastian Riedel\\nSatinder Baveja\\nAntonio Gulli\\nAnant Nawalgaria\\nCurators and Editors\\nAntonio Gulli\\nAnant Nawalgaria\\nGrace Mollison \\nTechnical Writer\\nJoey Haymaker\\nDesigner\\nMichael Lanning'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 2, 'page_label': '3'}, page_content='Introduction 4\\nWhat is an agent? 5\\nThe model 6\\nThe tools 7\\nThe orchestration layer 7\\nAgents vs. models 8\\nCognitive architectures: How agents operate  8\\nTools: Our keys to the outside world 12\\nExtensions  13\\nSample Extensions  15\\nFunctions  18\\nUse cases 21\\nFunction sample code 24\\nData stores 27\\nImplementation and application 28\\nTools recap 32\\nEnhancing model performance with targeted learning 33\\nAgent quick start with LangChain 35\\nProduction applications with Vertex AI agents 38\\nSummary 40\\nEndnotes 42\\nTable of contents'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 3, 'page_label': '4'}, page_content=\"Agents\\n4\\nFebruary 2025\\nIntroduction\\nHumans are fantastic at messy pattern recognition tasks. However, they often rely on tools \\n- like books, Google Search, or a calculator - to supplement their prior knowledge before \\narriving at a conclusion. Just like humans, Generative AI models can be trained to use tools \\nto access real-time information or suggest a real-world action. For example, a model can \\nleverage a database retrieval tool to access specific information, like a customer's purchase \\nhistory, so it can generate tailored shopping recommendations. Alternatively, based on a \\nuser's query, a model can make various API calls to send an email response to a colleague \\nor complete a financial transaction on your behalf. To do so, the model must not only have \\naccess to a set of external tools, it needs the ability to plan and execute any task in a self-\\ndirected fashion. This combination of reasoning, logic, and access to external information \\nthat are all connected to a Generative AI model invokes the concept of an agent, or a \\nprogram that extends beyond the standalone capabilities of a Generative AI model. This \\nwhitepaper dives into all these and associated aspects in more detail.\\nThis combination of reasoning, \\nlogic, and access to external \\ninformation that are all connected \\nto a Generative AI model invokes \\nthe concept of an agent.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 4, 'page_label': '5'}, page_content='Agents\\n5\\nFebruary 2025\\nWhat is an agent?\\nIn its most fundamental form, a Generative AI agent can be defined as an application that \\nattempts to achieve a goal by observing the world and acting upon it using the tools that it \\nhas at its disposal. Agents are autonomous and can act independently of human intervention, \\nespecially when provided with proper goals or objectives they are meant to achieve. Agents \\ncan also be proactive in their approach to reaching their goals. Even in the absence of \\nexplicit instruction sets from a human, an agent can reason about what it should do next to \\nachieve its ultimate goal. While the notion of agents in AI is quite general and powerful, this \\nwhitepaper focuses on the specific types of agents that Generative AI models are capable of \\nbuilding at the time of publication.\\nIn order to understand the inner workings of an agent, let’s first introduce the foundational \\ncomponents that drive the agent’s behavior, actions, and decision making. The combination \\nof these components can be described as a cognitive architecture, and there are many \\nsuch architectures that can be achieved by the mixing and matching of these components. \\nFocusing on the core functionalities, there are three essential components in an agent’s \\ncognitive architecture as shown in Figure 1.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 5, 'page_label': '6'}, page_content='Agents\\n6\\nFebruary 2025\\nFigure 1. General agent architecture and components\\nThe model\\nIn the scope of an agent, a model refers to the language model (LM) that will be utilized as \\nthe centralized decision maker for agent processes. The model used by an agent can be one \\nor multiple LM’s of any size (small / large) that are capable of following instruction based \\nreasoning and logic frameworks, like ReAct, Chain-of-Thought, or Tree-of-Thoughts. Models \\ncan be general purpose, multimodal or fine-tuned based on the needs of your specific agent \\narchitecture. For best production results, you should leverage a model that best fits your \\ndesired end application and, ideally, has been trained on data signatures associated with the \\ntools that you plan to use in the cognitive architecture. It’s important to note that the model is \\ntypically not trained with the specific configuration settings (i.e. tool choices, orchestration/\\nreasoning setup) of the agent. However, it’s possible to further refine the model for the \\nagent’s tasks by providing it with examples that showcase the agent’s capabilities, including \\ninstances of the agent using specific tools or reasoning steps in various contexts.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 6, 'page_label': '7'}, page_content='Agents\\n7\\nFebruary 2025\\nThe tools\\nFoundational models, despite their impressive text and image generation, remain constrained \\nby their inability to interact with the outside world. Tools bridge this gap, empowering agents \\nto interact with external data and services while unlocking a wider range of actions beyond \\nthat of the underlying model alone. Tools can take a variety of forms and have varying \\ndepths of complexity, but typically align with common web API methods like GET, POST, \\nPATCH, and DELETE. For example, a tool could update customer information in a database \\nor fetch weather data to influence a travel recommendation that the agent is providing to \\nthe user. With tools, agents can access and process real-world information. This empowers \\nthem to support more specialized systems like retrieval augmented generation (RAG), \\nwhich significantly extends an agent’s capabilities beyond what the foundational model can \\nachieve on its own. We’ll discuss tools in more detail below, but the most important thing \\nto understand is that tools bridge the gap between the agent’s internal capabilities and the \\nexternal world, unlocking a broader range of possibilities.\\nThe orchestration layer\\nThe orchestration layer describes a cyclical process that governs how the agent takes in \\ninformation, performs some internal reasoning, and uses that reasoning to inform its next \\naction or decision. In general, this loop will continue until an agent has reached its goal or a \\nstopping point. The complexity of the orchestration layer can vary greatly depending on the \\nagent and task it’s performing. Some loops can be simple calculations with decision rules, \\nwhile others may contain chained logic, involve additional machine learning algorithms, or \\nimplement other probabilistic reasoning techniques. We’ll discuss more about the detailed \\nimplementation of the agent orchestration layers in the cognitive architecture section.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 7, 'page_label': '8'}, page_content='Agents\\n8\\nFebruary 2025\\nAgents vs. models\\nTo gain a clearer understanding of the distinction between agents and models, consider the \\nfollowing chart:\\nModels Agents\\nKnowledge is limited to what is available in their \\ntraining data.\\nKnowledge is extended through the connection \\nwith external systems via tools\\nSingle inference / prediction based on the \\nuser query. Unless explicitly implemented for \\nthe model, there is no management of session \\nhistory or continuous context. (i.e. chat history)\\nManaged session history (i.e. chat history) to \\nallow for multi turn inference / prediction based \\non user queries and decisions made in the \\norchestration layer. In this context, a ‘turn’ is \\ndefined as an interaction between the interacting \\nsystem and the agent. (i.e. 1 incoming event/\\nquery and 1 agent response)\\nNo native tool implementation. Tools are natively implemented in agent \\narchitecture.\\nNo native logic layer implemented. Users can \\nform prompts as simple questions or use \\nreasoning frameworks (CoT, ReAct, etc.) to \\nform complex prompts to guide the model in \\nprediction.\\nNative cognitive architecture that uses reasoning \\nframeworks like CoT, ReAct, or other pre-built \\nagent frameworks like LangChain.\\nCognitive architectures: How agents operate \\nImagine a chef in a busy kitchen. Their goal is to create delicious dishes for restaurant \\npatrons which involves some cycle of planning, execution, and adjustment.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 8, 'page_label': '9'}, page_content='Agents\\n9\\nFebruary 2025\\n• They gather information, like the patron’s order and what ingredients are in the pantry \\nand refrigerator.\\n• They perform some internal reasoning about what dishes and flavor profiles they can \\ncreate based on the information they have just gathered.\\n• They take action to create the dish: chopping vegetables, blending spices, searing meat.\\nAt each stage in the process the chef makes adjustments as needed, refining their plan as \\ningredients are depleted or customer feedback is received, and uses the set of previous \\noutcomes to determine the next plan of action. This cycle of information intake, planning, \\nexecuting, and adjusting describes a unique cognitive architecture that the chef employs to \\nreach their goal.\\nJust like the chef, agents can use cognitive architectures to reach their end goals by \\niteratively processing information, making informed decisions, and refining next actions \\nbased on previous outputs. At the core of agent cognitive architectures lies the orchestration \\nlayer, responsible for maintaining memory, state, reasoning and planning. It uses the rapidly \\nevolving field of prompt engineering and associated frameworks to guide reasoning and \\nplanning, enabling the agent to interact more effectively with its environment and complete \\ntasks. Research in the area of prompt engineering frameworks and task planning for \\nlanguage models is rapidly evolving, yielding a variety of promising approaches. While not an \\nexhaustive list, these are a few of the most popular frameworks and reasoning techniques \\navailable at the time of this publication:\\n• ReAct, a prompt engineering framework that provides a thought process strategy for \\nlanguage models to Reason and take action on a user query, with or without in-context \\nexamples. ReAct prompting has shown to outperform several SOTA baselines and improve \\nhuman interoperability and trustworthiness of LLMs.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 9, 'page_label': '10'}, page_content='Agents\\n10\\nFebruary 2025\\n• Chain-of-Thought (CoT), a prompt engineering framework that enables reasoning \\ncapabilities through intermediate steps. There are various sub-techniques of CoT including \\nself-consistency, active-prompt, and multimodal CoT that each have strengths and \\nweaknesses depending on the specific application.\\n• Tree-of-thoughts (ToT),, a prompt engineering framework that is well suited for \\nexploration or strategic lookahead tasks. It generalizes over chain-of-thought prompting \\nand allows the model to explore various thought chains that serve as intermediate steps \\nfor general problem solving with language models.\\nAgents can utilize one of the above reasoning techniques, or many other techniques, to \\nchoose the next best action for the given user request. For example, let’s consider an agent \\nthat is programmed to use the ReAct framework to choose the correct actions and tools for \\nthe user query. The sequence of events might go something like this:\\n1. User sends query to the agent\\n2. Agent begins the ReAct sequence\\n3. The agent provides a prompt to the model, asking it to generate one of the next ReAct \\nsteps and its corresponding output:\\na. Question: The input question from the user query, provided with the prompt\\nb. Thought: The model’s thoughts about what it should do next\\nc. Action: The model’s decision on what action to take next\\ni. This is where tool choice can occur\\nii. For example, an action could be one of [Flights, Search, Code, None], where the first \\n3 represent a known tool that the model can choose, and the last represents “no \\ntool choice”'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 10, 'page_label': '11'}, page_content='Agents\\n11\\nFebruary 2025\\nd. Action input: The model’s decision on what inputs to provide to the tool (if any)\\ne. Observation: The result of the action / action input sequence\\ni. This thought / action / action input / observation could repeat N-times as needed\\nf. Final answer: The model’s final answer to provide to the original user query\\n4. The ReAct loop concludes and a final answer is provided back to the user\\nFigure 2. Example agent with ReAct reasoning in the orchestration layer\\nAs shown in Figure 2, the model, tools, and agent configuration work together to provide \\na grounded, concise response back to the user based on the user’s original query. While \\nthe model could have guessed at an answer (hallucinated) based on its prior knowledge, \\nit instead used a tool (Flights) to search for real-time external information. This additional \\ninformation was provided to the model, allowing it to make a more informed decision based \\non real factual data and to summarize this information back to the user.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 11, 'page_label': '12'}, page_content='Agents\\n12\\nFebruary 2025\\nIn summary, the quality of agent responses can be tied directly to the model’s ability to \\nreason and act about these various tasks, including the ability to select the right tools, and \\nhow well that tools has been defined. Like a chef crafting a dish with fresh ingredients and \\nattentive to customer feedback, agents rely on sound reasoning and reliable information to \\ndeliver optimal results. In the next section, we’ll dive into the various ways agents connect \\nwith fresh data. \\nTools: Our keys to the outside world\\nWhile language models excel at processing information, they lack the ability to directly \\nperceive and influence the real world. This limits their usefulness in situations requiring \\ninteraction with external systems or data. This means that, in a sense, a language model \\nis only as good as what it has learned from its training data. But regardless of how much \\ndata we throw at a model, they still lack the fundamental ability to interact with the outside \\nworld. So how can we empower our models to have real-time, context-aware interaction with \\nexternal systems? Functions, Extensions, Data Stores and Plugins are all ways to provide this \\ncritical capability to the model.\\nWhile they go by many names, tools are what create a link between our foundational models \\nand the outside world. This link to external systems and data allows our agent to perform a \\nwider variety of tasks and do so with more accuracy and reliability. For instance, tools can \\nenable agents to adjust smart home settings, update calendars, fetch user information from \\na database, or send emails based on a specific set of instructions.\\nAs of the date of this publication, there are three primary tool types that Google models are \\nable to interact with: Extensions, Functions, and Data Stores. By equipping agents with tools, \\nwe unlock a vast potential for them to not only understand the world but also act upon it, \\nopening doors to a myriad of new applications and possibilities.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 12, 'page_label': '13'}, page_content='Agents\\n13\\nFebruary 2025\\nExtensions \\nThe easiest way to understand Extensions is to think of them as bridging the gap between \\nan API and an agent in a standardized way, allowing agents to seamlessly execute APIs \\nregardless of their underlying implementation. Let’s say that you’ve built an agent with a goal \\nof helping users book flights. You know that you want to use the Google Flights API to retrieve \\nflight information, but you’re not sure how you’re going to get your agent to make calls to this \\nAPI endpoint.\\nFigure 3. How do Agents interact with External APIs?\\nOne approach could be to implement custom code that would take the incoming user query, \\nparse the query for relevant information, then make the API call. For example, in a flight \\nbooking use case a user might state “I want to book a flight from Austin to Zurich.” In this \\nscenario, our custom code solution would need to extract “Austin” and “Zurich” as relevant \\nentities from the user query before attempting to make the API call. But what happens if the \\nuser says “I want to book a flight to Zurich” and never provides a departure city? The API call \\nwould fail without the required data and more code would need to be implemented in order \\nto catch edge and corner cases like this. This approach is not scalable and could easily break \\nin any scenario that falls outside of the implemented custom code.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 13, 'page_label': '14'}, page_content='Agents\\n14\\nFebruary 2025\\nA more resilient approach would be to use an Extension. An Extension bridges the gap \\nbetween an agent and an API by:\\n1. Teaching the agent how to use the API endpoint using examples.\\n2. Teaching the agent what arguments or parameters are needed to successfully call the \\nAPI endpoint.\\nFigure 4. Extensions connect Agents to External APIs\\nExtensions can be crafted independently of the agent, but should be provided as part of the \\nagent’s configuration. The agent uses the model and examples at run time to decide which \\nExtension, if any, would be suitable for solving the user’s query. This highlights a key strength \\nof Extensions, their built-in example types, that allow the agent to dynamically select the \\nmost appropriate Extension for the task. \\n \\nFigure 5. 1-to-many relationship between Agents, Extensions and APIs'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 14, 'page_label': '15'}, page_content='Agents\\n15\\nFebruary 2025\\nThink of this the same way that a software developer decides which API endpoints to use \\nwhile solving and solutioning for a user’s problem. If the user wants to book a flight, the \\ndeveloper might use the Google Flights API. If the user wants to know where the nearest \\ncoffee shop is relative to their location, the developer might use the Google Maps API. In \\nthis same way, the agent / model stack uses a set of known Extensions to decide which one \\nwill be the best fit for the user’s query. If you’d like to see Extensions in action, you can try \\nthem out on the Gemini application by going to Settings > Extensions and then enabling any \\nyou would like to test. For example, you could enable the Google Flights extension then ask \\nGemini “Show me flights from Austin to Zurich leaving next Friday.”\\nSample Extensions \\nTo simplify the usage of Extensions, Google provides some out of the box extensions that \\ncan be quickly imported into your project and used with minimal configurations. For example, \\nthe Code Interpreter extension in Snippet 1 allows you to generate and run Python code from \\na natural language description.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 15, 'page_label': '16'}, page_content='Agents\\n16\\nFebruary 2025\\nPython\\nimport vertexai\\nimport pprint\\nPROJECT_ID = \"YOUR_PROJECT_ID\"\\nREGION = \"us-central1\"\\nvertexai.init(project=PROJECT_ID, location=REGION)\\nfrom vertexai.preview.extensions import Extension\\nextension_code_interpreter = Extension.from_hub(\"code_interpreter\")\\nCODE_QUERY = \"\"\"Write a python method to invert a binary tree in O(n) time.\"\"\"\\nresponse = extension_code_interpreter.execute(\\n  operation_id = \"generate_and_execute\",\\n  operation_params = {\"query\": CODE_QUERY}\\n  )\\nprint(\"Generated Code:\")\\npprint.pprint({response[\\'generated_code\\']})\\n# The above snippet will generate the following code.\\n```\\nGenerated Code:\\nclass TreeNode:\\n  def __init__(self, val=0, left=None, right=None(:\\n    self.val = val\\n    self.left = left\\n    self.right = right\\nContinues next page...'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 16, 'page_label': '17'}, page_content='Agents\\n17\\nFebruary 2025\\nPython\\ndef invert_binary_tree(root):\\n    \"\"\"\\n    Inverts a binary tree.\\n    Args:\\n        root: The root of the binary tree.\\n    Returns:\\n        The root of the inverted binary tree.\\n    \"\"\"\\n    if not root:\\n        return None\\n    # Swap the left and right children recursively\\n    root.left, root.right =     \\ninvert_binary_tree(root.right), invert_binary_tree(root.left)\\n    return root\\n# Example usage:\\n# Construct a sample binary tree\\nroot = TreeNode(4)\\nroot.left = TreeNode(2)\\nroot.right = TreeNode(7)\\nroot.left.left = TreeNode(1)\\nroot.left.right = TreeNode(3)\\nroot.right.left = TreeNode(6)\\nroot.right.right = TreeNode(9)\\n# Invert the binary tree\\ninverted_root = invert_binary_tree(root)\\n```\\nSnippet 1. Code Interpreter Extension can generate and run Python code'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 17, 'page_label': '18'}, page_content='Agents\\n18\\nFebruary 2025\\nTo summarize, Extensions provide a way for agents to perceive, interact, and influence the \\noutside world in a myriad of ways. The selection and invocation of these Extensions is guided \\nby the use of Examples, all of which are defined as part of the Extension configuration.\\nFunctions \\nIn the world of software engineering, functions are defined as self-contained modules \\nof code that accomplish a specific task and can be reused as needed. When a software \\ndeveloper is writing a program, they will often create many functions to do various tasks. \\nThey will also define the logic for when to call function_a versus function_b, as well as the \\nexpected inputs and outputs.\\nFunctions work very similarly in the world of agents, but we can replace the software \\ndeveloper with a model. A model can take a set of known functions and decide when to use \\neach Function and what arguments the Function needs based on its specification. Functions \\ndiffer from Extensions in a few ways, most notably:\\n1. A model outputs a Function and its arguments, but doesn’t make a live API call.\\n2. Functions are executed on the client-side, while Extensions are executed on \\nthe agent-side.\\nUsing our Google Flights example again, a simple setup for functions might look like the \\nexample in Figure 7.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 18, 'page_label': '19'}, page_content='Agents\\n19\\nFebruary 2025\\nFigure 7. How do functions interact with external APIs?\\nNote that the main difference here is that neither the Function nor the agent interact directly \\nwith the Google Flights API. So how does the API call actually happen?\\nWith functions, the logic and execution of calling the actual API endpoint is offloaded away \\nfrom the agent and back to the client-side application as seen in Figure 8 and Figure 9 below. \\nThis offers the developer more granular control over the flow of data in the application. There \\nare many reasons why a Developer might choose to use functions over Extensions, but a few \\ncommon use cases are:\\n• API calls need to be made at another layer of the application stack, outside of the direct \\nagent architecture flow (e.g. a middleware system, a front end framework, etc.)\\n• Security or Authentication restrictions that prevent the agent from calling an API directly \\n(e.g API is not exposed to the internet, or non-accessible by agent infrastructure)\\n• Timing or order-of-operations constraints that prevent the agent from making API calls in \\nreal-time. (i.e. batch operations, human-in-the-loop review, etc.)'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 19, 'page_label': '20'}, page_content='Agents\\n20\\nFebruary 2025\\n• Additional data transformation logic needs to be applied to the API Response that the \\nagent cannot perform. For example, consider an API endpoint that doesn’t provide a \\nfiltering mechanism for limiting the number of results returned. Using Functions on the \\nclient-side provides the developer additional opportunities to make these transformations. \\n• The developer wants to iterate on agent development without deploying additional \\ninfrastructure for the API endpoints (i.e. Function Calling can act like “stubbing” of APIs)\\nWhile the difference in internal architecture between the two approaches is subtle as seen in \\nFigure 8, the additional control and decoupled dependency on external infrastructure makes \\nFunction Calling an appealing option for the Developer.\\nFigure 8. Delineating client vs. agent side control for extensions and function calling'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 20, 'page_label': '21'}, page_content='Agents\\n21\\nFebruary 2025\\nUse cases\\nA model can be used to invoke functions in order to handle complex, client-side execution \\nflows for the end user, where the agent Developer might not want the language model to \\nmanage the API execution (as is the case with Extensions). Let’s consider the following \\nexample where an agent is being trained as a travel concierge to interact with users that want \\nto book vacation trips. The goal is to get the agent to produce a list of cities that we can use \\nin our middleware application to download images, data, etc. for the user’s trip planning. A \\nuser might say something like:\\nI’d like to take a ski trip with my family but I’m not sure where to go.\\nIn a typical prompt to the model, the output might look like the following:\\nSure, here’s a list of cities that you can consider for family ski trips:\\n• Crested Butte, Colorado, USA\\n• Whistler, BC, Canada\\n• Zermatt, Switzerland\\nWhile the above output contains the data that we need (city names), the format isn’t ideal \\nfor parsing. With Function Calling, we can teach a model to format this output in a structured \\nstyle (like JSON) that’s more convenient for another system to parse. Given the same input \\nprompt from the user, an example JSON output from a Function might look like Snippet \\n5 instead.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 21, 'page_label': '22'}, page_content='Agents\\n22\\nFebruary 2025\\nfunction_call {\\n  name: \"display_cities\"\\n  args: {\\n    \"cities\": [\"Crested Butte\", \"Whistler\", \"Zermatt\"],\\n    \"preferences\": \"skiing\"\\n    }\\n}\\nSnippet 5. Sample Function Call payload for displaying a list of cities and user preferences\\nThis JSON payload is generated by the model, and then sent to our Client-side server to do \\nwhatever we would like to do with it. In this specific case, we’ll call the Google Places API to \\ntake the cities provided by the model and look up Images, then provide them as formatted \\nrich content back to our User. Consider this sequence diagram in Figure 9 showing the above \\ninteraction in step by step detail.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 22, 'page_label': '23'}, page_content=\"Agents\\n23\\nFebruary 2025\\n \\nFigure 9. Sequence diagram showing the lifecycle of a Function Call\\nThe result of the example in Figure 9 is that the model is leveraged to “fill in the blanks” with \\nthe parameters required for the Client side UI to make the call to the Google Places API. The \\nClient side UI manages the actual API call using the parameters provided by the model in the \\nreturned Function. This is just one use case for Function Calling, but there are many other \\nscenarios to consider like:\\n• You want a language model to suggest a function that you can use in your code, but you \\ndon't want to include credentials in your code. Because function calling doesn't run the \\nfunction, you don't need to include credentials in your code with the function information.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 23, 'page_label': '24'}, page_content=\"Agents\\n24\\nFebruary 2025\\n• You are running asynchronous operations that can take more than a few seconds. These \\nscenarios work well with function calling because it's an asynchronous operation.\\n• You want to run functions on a device that's different from the system producing the \\nfunction calls and their arguments.\\nOne key thing to remember about functions is that they are meant to offer the developer \\nmuch more control over not only the execution of API calls, but also the entire flow of data \\nin the application as a whole. In the example in Figure 9, the developer chose to not return \\nAPI information back to the agent as it was not pertinent for future actions the agent might \\ntake. However, based on the architecture of the application, it may make sense to return the \\nexternal API call data to the agent in order to influence future reasoning, logic, and action \\nchoices. Ultimately, it is up to the application developer to choose what is right for the \\nspecific application.\\nFunction sample code\\nTo achieve the above output from our ski vacation scenario, let’s build out each of the \\ncomponents to make this work with our gemini-2.0-flash-001 model. \\nFirst, we’ll define our display_cities function as a simple Python method.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 24, 'page_label': '25'}, page_content='Agents\\n25\\nFebruary 2025\\nPython\\nfrom typing import Optional\\ndef display_cities(cities: list[str], preferences: Optional[str] = None):\\n \"\"\"Provides a list of cities based on the user\\'s search query and preferences.\\n Args:\\n  preferences (str): The user\\'s preferences for the search, like skiing,\\n  beach, restaurants, bbq, etc.\\n  cities (list[str]): The list of cities being recommended to the user.\\n Returns:\\n  list[str]: The list of cities being recommended to the user.\\n \"\"\"\\n return cities\\n \\nSnippet 6. Sample python method for a function that will display a list of cities.\\nNext, we’ll instantiate our model, build the Tool, then pass in our user’s query and tools to \\nthe model. Executing the code below would result in the output as seen at the bottom of the \\ncode snippet.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 25, 'page_label': '26'}, page_content='Agents\\n26\\nFebruary 2025\\nPython\\nfrom google.genai import Client, types\\nclient = Client(\\n    vertexai=True,\\n    project=\"PROJECT_ID\",\\n    location=\"us-central1\"\\n    )\\nres = client.models.generate_content(\\n    model=\"gemini-2.0-flash-001\",\\n    model=\"I\\'d like to take a ski trip with my family but I\\'m not sure where \\nto go?\",\\n    config=types.GenerateContentConfig(\\n        tools=[display_cities],\\n     automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True),\\n        tool_config=types.ToolConfig(\\n            function_calling_config=types.FunctionCallingConfig(mode=\\'ANY\\')\\n            )\\n        )\\n)\\nprint(f\"Function Name: {res.candidates[0].content.parts[0].function_call.name}\")\\nprint(f\"Function Args: {res.candidates[0].content.parts[0].function_call.args}\")\\n> Function Name: display_cities\\n> Function Args: {\\'preferences\\': \\'skiing\\', \\'cities\\': [\\'Aspen\\', \\'Park \\nCity\\', \\'Whistler\\']}\\n```\\nSnippet 7. Building a Tool, sending to the model with a user query and allowing the function call to take place'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 26, 'page_label': '27'}, page_content='Agents\\n27\\nFebruary 2025\\nIn summary, functions offer a straightforward framework that empowers application \\ndevelopers with fine-grained control over data flow and system execution, while effectively \\nleveraging the agent/model for critical input generation. Developers can selectively choose \\nwhether to keep the agent “in the loop” by returning external data, or omit it based on \\nspecific application architecture requirements.\\nData stores\\nImagine a language model as a vast library of books, containing its training data. But unlike \\na library that continuously acquires new volumes, this one remains static, holding only the \\nknowledge it was initially trained on. This presents a challenge, as real-world knowledge is \\nconstantly evolving. Data Stores address this limitation by providing access to more dynamic \\nand up-to-date information, and ensuring a model’s responses remain grounded in factuality \\nand relevance.\\nConsider a common scenario where a developer might need to provide a small amount of \\nadditional data to a model, perhaps in the form of spreadsheets or PDFs.\\nFigure 10. How can Agents interact with structured and unstructured data?'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 27, 'page_label': '28'}, page_content='Agents\\n28\\nFebruary 2025\\nData Stores allow developers to provide additional data in its original format to an agent, \\neliminating the need for time-consuming data transformations, model retraining, or fine-\\ntuning. The Data Store converts the incoming document into a set of vector database \\nembeddings that the agent can use to extract the information it needs to supplement its next \\naction or response to the user.\\nFigure 11. Data Stores connect Agents to new real-time data sources of various types.\\nImplementation and application\\nIn the context of Generative AI agents, Data Stores are typically implemented as a vector \\ndatabase that the developer wants the agent to have access to at runtime. While we won’t \\ncover vector databases in depth here, the key point to understand is that they store data \\nin the form of vector embeddings, a type of high-dimensional vector or mathematical \\nrepresentation of the data provided. One of the most prolific examples of Data Store usage \\nwith language models in recent times has been the implementation of Retrieval Augmented'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 28, 'page_label': '29'}, page_content='Agents\\n29\\nFebruary 2025\\nGeneration (RAG) based applications. These applications seek to extend the breadth and \\ndepth of a model’s knowledge beyond the foundational training data by giving the model \\naccess to data in various formats like:\\n• Website content\\n• Structured Data in formats like PDF, Word Docs, CSV, Spreadsheets, etc.\\n• Unstructured Data in formats like HTML, PDF, TXT, etc.\\nFigure 12. 1-to-many relationship between agents and data stores, which can represent various types of \\npre-indexed data\\nThe underlying process for each user request and agent response loop is generally modeled \\nas seen in Figure 13.\\n1. A user query is sent to an embedding model to generate embeddings for the query\\n2. The query embeddings are then matched against the contents of the vector database \\nusing a matching algorithm like SCaNN\\n3. The matched content is retrieved from the vector database in text format and sent back to \\nthe agent\\n4. The agent receives both the user query and retrieved content, then formulates a response \\nor action'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 29, 'page_label': '30'}, page_content='Agents\\n30\\nFebruary 2025\\n5. A final response is sent to the user\\nFigure 13. The lifecycle of a user request and agent response in a RAG based application\\nThe end result is an application that allows the agent to match a user’s query to a known data \\nstore through vector search, retrieve the original content, and provide it to the orchestration \\nlayer and model for further processing. The next action might be to provide a final answer to \\nthe user, or perform an additional vector search to further refine the results.\\nA sample interaction with an agent that implements RAG with ReAct reasoning/planning can \\nbe seen in Figure 14.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 30, 'page_label': '31'}, page_content='Agents\\n31\\nFebruary 2025\\nFigure 14. Sample RAG based application w/ ReAct reasoning/planning'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 31, 'page_label': '32'}, page_content='Agents\\n32\\nFebruary 2025\\nTools recap\\nTo summarize, extensions, functions and data stores make up a few different tool types \\navailable for agents to use at runtime. Each has their own purpose and they can be used \\ntogether or independently at the discretion of the agent developer. \\nExtensions Function Calling Data Stores\\nExecution Agent-Side Execution Client-Side Execution Agent-Side Execution\\nUse Case • Developer wants \\nagent to control \\ninteractions with the \\nAPI endpoints\\n• Useful when \\nleveraging native pre-\\nbuilt Extensions (i.e. \\nVertex Search, Code \\nInterpreter, etc.)\\n• Multi-hop planning \\nand API calling \\n(i.e. the next agent \\naction depends on \\nthe outputs of the \\nprevious action / \\nAPI call)\\n• Security or \\nAuthentication \\nrestrictions prevent the \\nagent from calling an \\nAPI directly\\n• Timing constraints or \\norder-of-operations \\nconstraints that \\nprevent the agent \\nfrom making API calls \\nin real-time. (i.e. batch \\noperations, human-in-\\nthe-loop review, etc.)\\n• API that is not exposed \\nto the internet, or \\nnon-accessible by \\nGoogle systems\\nDeveloper wants to \\nimplement Retrieval \\nAugmented Generation \\n(RAG) with any of the \\nfollowing data types:\\n• Website Content from \\npre-indexed domains \\nand URLs\\n• Structured Data in \\nformats like PDF, \\nWord Docs, CSV, \\nSpreadsheets, etc.\\n• Relational / Non- \\nRelational Databases\\n• Unstructured Data in \\nformats like HTML, PDF, \\nTXT, etc.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 32, 'page_label': '33'}, page_content=\"Agents\\n33\\nFebruary 2025\\nEnhancing model performance with \\ntargeted learning\\nA crucial aspect of using models effectively is their ability to choose the right tools when \\ngenerating output, especially when using tools at scale in production. While general training \\nhelps models develop this skill, real-world scenarios often require knowledge beyond the \\ntraining data. Imagine this as the difference between basic cooking skills and mastering \\na specific cuisine. Both require foundational cooking knowledge, but the latter demands \\ntargeted learning for more nuanced results.\\nTo help the model gain access to this type of specific knowledge, several approaches exist:\\n• In-context learning: This method provides a generalized model with a prompt, tools, and \\nfew-shot examples at inference time which allows it to learn ‘on the fly' how and when to \\nuse those tools for a specific task. The ReAct framework is an example of this approach in \\nnatural language.\\n• Retrieval-based in-context learning: This technique dynamically populates the model \\nprompt with the most relevant information, tools, and associated examples by retrieving \\nthem from external memory. An example of this would be the ‘Example Store’ in Vertex AI \\nextensions or the data stores RAG based architecture mentioned previously.\\n• Fine-tuning based learning: This method involves training a model using a larger dataset \\nof specific examples prior to inference. This helps the model understand when and how to \\napply certain tools prior to receiving any user queries. \\nTo provide additional insights on each of the targeted learning approaches, let’s revisit our \\ncooking analogy.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 33, 'page_label': '34'}, page_content='Agents\\n34\\nFebruary 2025\\n• Imagine a chef has received a specific recipe (the prompt), a few key ingredients (relevant \\ntools) and some example dishes (few-shot examples) from a customer. Based on this \\nlimited information and the chef’s general knowledge of cooking, they will need to figure \\nout how to prepare the dish ‘on the fly’ that most closely aligns with the recipe and the \\ncustomer’s preferences. This is in-context learning.\\n• Now let’s imagine our chef in a kitchen that has a well-stocked pantry (external data \\nstores) filled with various ingredients and cookbooks (examples and tools). The chef is now \\nable to dynamically choose ingredients and cookbooks from the pantry and better align \\nto the customer’s recipe and preferences. This allows the chef to create a more informed \\nand refined dish leveraging both existing and new knowledge. This is retrieval-based \\nin-context learning.\\n• Finally, let’s imagine that we sent our chef back to school to learn a new cuisine or set of \\ncuisines (pre-training on a larger dataset of specific examples). This allows the chef to \\napproach future unseen customer recipes with deeper understanding. This approach is \\nperfect if we want the chef to excel in specific cuisines (knowledge domains). This is fine-\\ntuning based learning. \\nEach of these approaches offers unique advantages and disadvantages in terms of speed, \\ncost, and latency. However, by combining these techniques in an agent framework, we can \\nleverage the various strengths and minimize their weaknesses, allowing for a more robust and \\nadaptable solution.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 34, 'page_label': '35'}, page_content='Agents\\n35\\nFebruary 2025\\nAgent quick start with LangChain\\nIn order to provide a real-world executable example of an agent in action, we’ll build a quick \\nprototype with the LangChain and LangGraph libraries. These popular open source libraries \\nallow users to build customer agents by “chaining” together sequences of logic, reasoning, \\nand tool calls to answer a user’s query. We’ll use our gemini-2.0-flash-001 model and \\nsome simple tools to answer a multi-stage query from the user as seen in Snippet 8.\\nThe tools we are using are the SerpAPI (for Google Search) and the Google Places API. After \\nexecuting our program in Snippet 8, you can see the sample output in Snippet 9.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 35, 'page_label': '36'}, page_content='Agents\\n36\\nFebruary 2025\\nPython\\nfrom langgraph.prebuilt import create_react_agent\\nfrom langchain_core.tools import tool\\nfrom langchain_community.utilities import SerpAPIWrapper\\nfrom langchain_community.tools import GooglePlacesTool\\nos.environ[\"SERPAPI_API_KEY\"] = \"XXXXX\"\\nos.environ[\"GPLACES_API_KEY\"] = \"XXXXX\"\\n@tool\\ndef search(query: str):\\n \"\"\"Use the SerpAPI to run a Google Search.\"\"\"\\n search = SerpAPIWrapper()\\n return  search.run(query)\\n@tool\\ndef places(query: str):\\n \"\"\"Use the Google Places API to run a Google Places Query.\"\"\"\\n places = GooglePlacesTool()\\n return  places.run(query)\\nmodel = ChatVertexAI(model=\"gemini-2.0-flash-001\")\\ntools = [search, places]\\nquery = \"Who did the Texas Longhorns play in football last week? What is the \\naddress of the other team\\'s stadium?\"\\nagent = create_react_agent(model, tools)\\ninput = {\"messages\": [(\"human\", query)]}\\nfor s in agent.stream(input, stream_mode=\"values\"):\\n message = s[\"messages\"][-1]\\n if isinstance (message, tuple):\\n  print(message )\\n else:\\n  message.pretty_print ()\\nSnippet 8. Sample LangChain and LangGraph based agent with tools'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 36, 'page_label': '37'}, page_content='Agents\\n37\\nFebruary 2025\\n============================== Human Message ================================\\nWho did the Texas Longhorns play in football last week? What is the address \\nof the other team\\'s stadium?\\n================================= Ai Message =================================\\nTool Calls: search\\nArgs:\\n query: Texas Longhorns football schedule\\n================================ Tool Message ================================\\nName: search\\n{...Results: \"NCAA Division I Football, Georgia, Date...\"}\\n================================= Ai Message =================================\\nThe Texas Longhorns played the Georgia Bulldogs last week.\\nTool Calls: places\\nArgs:\\n query: Georgia Bulldogs stadium\\n================================ Tool Message ================================\\nName: places\\n{...Sanford Stadium Address: 100 Sanford...}\\n================================= Ai Message =================================\\nThe address of the Georgia Bulldogs stadium is 100 Sanford Dr, Athens, GA \\n30602, USA.\\nSnippet 9. Output from our program in Snippet 8\\nWhile this is a fairly simple agent example, it demonstrates the foundational components \\nof Model, Orchestration, and tools all working together to achieve a specific goal. In the \\nfinal section, we’ll explore how these components come together in Google-scale managed \\nproducts like Vertex AI agents and Generative Playbooks.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 37, 'page_label': '38'}, page_content='Agents\\n38\\nFebruary 2025\\nProduction applications with Vertex \\nAI agents\\nWhile this whitepaper explored the core components of agents, building production-grade \\napplications requires integrating them with additional tools like user interfaces, evaluation \\nframeworks, and continuous improvement mechanisms. Google’s Vertex AI platform \\nsimplifies this process by offering a fully managed environment with all the fundamental \\nelements covered earlier. Using a natural language interface, developers can rapidly \\ndefine crucial elements of their agents - goals, task instructions, tools, sub-agents for task \\ndelegation, and examples - to easily construct the desired system behavior. In addition, the \\nplatform comes with a set of development tools that allow for testing, evaluation, measuring \\nagent performance, debugging, and improving the overall quality of developed agents. This \\nallows developers to focus on building and refining their agents while the complexities of \\ninfrastructure, deployment and maintenance are managed by the platform itself. \\nIn Figure 15 we’ve provided a sample architecture of an agent that was built on the Vertex \\nAI platform using various features such as Vertex Agent Builder, Vertex Extensions, Vertex \\nFunction Calling and Vertex Example Store to name a few. The architecture includes many of \\nthe various components necessary for a production ready application.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 38, 'page_label': '39'}, page_content='Agents\\n39\\nFebruary 2025\\nFigure 15. Sample end-to-end agent architecture built on Vertex AI platform\\nYou can try a sample of this prebuilt agent architecture from our official documentation.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 39, 'page_label': '40'}, page_content='Agents\\n40\\nFebruary 2025\\nSummary\\nIn this whitepaper we’ve discussed the foundational building blocks of Generative AI \\nagents, their compositions, and effective ways to implement them in the form of cognitive \\narchitectures. Some key takeaways from this whitepaper include:\\n1. Agents extend the capabilities of language models by leveraging tools to access real-\\ntime information, suggest real-world actions, and plan and execute complex tasks \\nautonomously. agents can leverage one or more language models to decide when and \\nhow to transition through states and use external tools to complete any number of \\ncomplex tasks that would be difficult or impossible for the model to complete on its own.\\n2. At the heart of an agent’s operation is the orchestration layer, a cognitive architecture that \\nstructures reasoning, planning, decision-making and guides its actions. Various reasoning \\ntechniques such as ReAct, Chain-of-Thought, and Tree-of-Thoughts, provide a framework \\nfor the orchestration layer to take in information, perform internal reasoning, and generate \\ninformed decisions or responses. \\n3. Tools, such as Extensions, Functions, and Data Stores, serve as the keys to the outside \\nworld for agents, allowing them to interact with external systems and access knowledge \\nbeyond their training data. Extensions provide a bridge between agents and external APIs, \\nenabling the execution of API calls and retrieval of real-time information. functions provide \\na more nuanced control for the developer through the division of labor, allowing agents \\nto generate Function parameters which can be executed client-side. Data Stores provide \\nagents with access to structured or unstructured data, enabling data-driven applications.\\nThe future of agents holds exciting advancements and we’ve only begun to scratch the \\nsurface of what is possible. As tools become more sophisticated and reasoning capabilities \\nare enhanced, agents will be empowered to solve increasingly complex problems. \\nFurthermore, the strategic approach of ‘agent chaining’ will continue to gain momentum. By'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 40, 'page_label': '41'}, page_content='Agents\\n41\\nFebruary 2025\\ncombining specialized agents - each excelling in a particular domain or task - we can create \\na ‘mixture of agent experts’ approach, capable of delivering exceptional results across \\nvarious industries and problem areas.\\nIt’s important to remember that building complex agent architectures demands an iterative \\napproach. Experimentation and refinement are key to finding solutions for specific business \\ncases and organizational needs. No two agents are created alike due to the generative nature \\nof the foundational models that underpin their architecture. However, by harnessing the \\nstrengths of each of these foundational components, we can create impactful applications \\nthat extend the capabilities of language models and drive real-world value.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 41, 'page_label': '42'}, page_content=\"Agents\\n42\\nFebruary 2025\\nEndnotes\\n1. Shafran, I., Cao, Y. et al., 2022, 'ReAct: Synergizing Reasoning and Acting in Language Models'. Available at:  \\nhttps://arxiv.org/abs/2210.03629\\n2. Wei, J., Wang, X. et al., 2023, 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models'. \\nAvailable at: https://arxiv.org/pdf/2201.11903.pdf .\\n3. Wang, X. et al., 2022, 'Self-Consistency Improves Chain of Thought Reasoning in Language Models'.  \\nAvailable at: https://arxiv.org/abs/2203.11171 .\\n4. Diao, S. et al., 2023, 'Active Prompting with Chain-of-Thought for Large Language Models'. Available at:  \\nhttps://arxiv.org/pdf/2302.12246.pdf .\\n5. Zhang, H. et al., 2023, 'Multimodal Chain-of-Thought Reasoning in Language Models'. Available at:  \\nhttps://arxiv.org/abs/2302.00923 .\\n6. Yao, S. et al., 2023, 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models'. Available at:  \\nhttps://arxiv.org/abs/2305.10601 .\\n7. Long, X., 2023, 'Large Language Model Guided Tree-of-Thought'. Available at:  \\nhttps://arxiv.org/abs/2305.08291 .\\n8. Google. 'Google Gemini Application'. Available at: http://gemini.google.com .\\n9. Swagger. 'OpenAPI Specification'. Available at: https://swagger.io/specification/ .\\n10. Xie, M., 2022, 'How does in-context learning work? A framework for understanding the differences from \\ntraditional supervised learning'. Available at: https://ai.stanford.edu/blog/understanding-incontext/ .\\n11. Google Research. 'ScaNN (Scalable Nearest Neighbors)'. Available at:  \\nhttps://github.com/google-research/google-research/tree/master/scann .\\n12. LangChain. 'LangChain'. Available at: https://python.langchain.com/v0.2/docs/introduction/ .\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 0, 'page_label': '1'}, page_content='Operationalizing \\nGenerative AI on \\nVertex AI using \\nMLOps\\nAuthors: Anant Nawalgaria,  \\nGabriela Hernandez Larios, Elia Secchi,  \\nMike Styer, Christos Aniftos, \\nOnofrio Petragallo, and \\nSokratis Kartakis'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 1, 'page_label': '2'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n2\\nFebruary 2025\\nAcknowledgements\\nContent contributors\\nNenshad Bardoliwalla\\nWarren Barkley\\nMikhail Chrestkha\\nChase Lyall\\nLakshmanan Sethu\\nErwan Menard\\nAlan Blount\\nCurators and Editors\\nAntonio Gulli\\nAnant Nawalgaria\\nGrace Mollison \\nTechnical Writer\\nJoey Haymaker\\nDesigner\\nMichael Lanning'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 2, 'page_label': '3'}, page_content='Introduction 6\\nWhat are DevOps and MLOps? 7\\nLifecycle of a gen AI system 8\\nDiscover 10\\nDevelop and experiment 11\\nThe foundational model paradigm 12\\nThe core component of LLM Systems: A prompted model component 14\\nChain & Augment 17\\nTuning & training 21\\nContinuous Training & Tuning 22\\nData Practices 24\\nEvaluate 28\\nDeploy 31\\nDeployment of gen AI systems 32\\nVersion control 32\\nContinuous integration of gen AI systems 33\\nContinuous delivery of gen AI systems 34\\nDeployment of foundation models 35\\nTable of contents'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 3, 'page_label': '4'}, page_content='Infrastructure validation 35\\nCompression and optimization 36\\nDeployment, packaging, and serving checklist 37\\nLogging and monitoring 38\\nGovern 43\\nExtend MLOps for gen AI to Agents 44\\nAgent Lifecycle 45\\nTool Orchestration 47\\nTool Types & Environments 48\\nTool Registry 50\\nTool Selection Strategies at Scale 51\\nAgent Evaluation & Optimization 53\\nObservability and Memory 55\\nDeploying an Agent to Production  58\\nOperations: People & Processes 60\\nThe role of an AI platform for gen AI operations 65\\nKey components of Vertex AI for gen AI 66\\nDiscover: Vertex Model Garden 67\\nPrototype: Vertex AI Studio & Notebooks 70\\nCustomize: Vertex AI training & tuning  71\\nTrain  72\\nTune  72'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 4, 'page_label': '5'}, page_content='Orchestrate 74\\nChain & Augment: Vertex AI Grounding, Extensions, and RAG building blocks 75\\nEvaluate: Vertex AI Experiments, Tensorboard, & evaluation pipelines 79\\nExperiment 80\\nEvaluation 81\\nPredict: Vertex AI endpoints & monitoring 81\\nGovern: Vertex AI Feature Store, Model Registry, and Dataplex 83\\nSummary 85\\nEndnotes 87'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 5, 'page_label': '6'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n6\\nFebruary 2025\\nIntroduction\\nThe emergence of foundation models and generative AI (gen AI) has introduced a new era \\nfor building AI systems. Selecting the right model from a diverse range of architectures \\nand sizes, curating data, engineering optimal prompts, tuning models for specific tasks, \\ngrounding model outputs in real-world data, optimizing hardware - these are just a few of the \\nnovel challenges that large models introduce.  \\nThis whitepaper delves into the fundamental tenets of MLOps and the necessary adaptations \\nrequired for the domain of gen AI and Foundation Models. We also examine the diverse range \\nof Vertex AI products, specifically tailored to address the unique demands of foundation \\nmodels and gen AI-based applications. Through this exploration we uncover how Vertex AI, \\nwith its solid foundations of AI infrastructure and MLOps tools, expands its capabilities to \\nprovide a comprehensive MLOps platform for gen AI.\\nEmergence of foundation models \\nand generative AI (gen AI) has \\nintroduced a new era for building \\nAI systems.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 6, 'page_label': '7'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n7\\nFebruary 2025\\nWhat are DevOps and MLOps?\\nDevOps is a software engineering methodology that aims to bridge the gap between \\ndevelopment (Dev) and operations (Ops). It promotes collaboration, automation, and \\ncontinuous improvement to streamline the software development lifecycle, introducing \\npractices such as continuous integration and continuous delivery. \\nMLOps builds upon DevOps principles to address the unique challenges of operationalizing \\nMachine Learning systems rapidly and reliably. In particular, MLOps tackles the experimental \\nnature of ML through practices like:\\n• Data validation: Ensuring the quality and integrity of training data.\\n• Model evaluation: Rigorously assessing model performance with appropriate metrics.\\n• Model monitoring: Tracking model behavior in production to detect and mitigate drift.\\n• Tracking & reproducibility: Maintaining meticulous records for experiment tracking and \\nresult reproduction.\\nFigure 1. Machine learning workflow'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 7, 'page_label': '8'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n8\\nFebruary 2025\\nLifecycle of a gen AI system\\nImagine deploying your first chatbot after months of dedicated work, and it's now interacting \\nwith users and answering questions. Behind this seemingly simple interaction lies the \\ncomplex and fascinating life cycle of a gen AI System, which can be broken down into five \\nkey moments.\\nFirst in the discovery phase, developers and AI engineers must navigate the expanding \\nlandscape of available models to identify the most suitable one for their specific gen AI \\napplication. They must consider each model's strengths, weaknesses, and costs to make an \\ninformed decision.\\nNext, development and experimentation become paramount, with prompt engineering \\nplaying a crucial role in crafting and refining input prompts to elicit desired outputs based on \\nan understanding of the model's intricacies. Few-shot learning, where examples are provided, \\ncan further guide model behavior, while additional customization may involve parameter-\\nefficient fine-tuning (PEFT). Most gen AI systems also involve model chaining, which refers to \\norchestrating calls to multiple models in a specific sequence to create a workflow.\\nData engineering practices have a critical role across all development stages, with factual \\ngrounding (ensuring the model's outputs are based on accurate, up-to-date information) and \\nrecent data from internal and enterprise systems being essential for reliable outputs. Tuning \\ndata is often needed to adapt models to specific tasks, styles, or to rectify persistent errors.\\nDeployment needs to manage many new artifacts in the deployment process, including \\nprompt templates, chain definitions, embedding models, retrieval data stores, and fine-tuned \\nmodel adapters among others. These artifacts each have unique governance requirements,\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 8, 'page_label': '9'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n9\\nFebruary 2025\\nnecessitating careful management throughout development and deployment. Gen AI system \\ndeployment also needs to account for the technical capabilities of the target infrastructure, \\nensuring that system hardware requirements are fulfilled.\\nContinuous monitoring in production ensures improved application performance and \\nmaintains safety standards through responsible AI techniques, such as ensuring fairness, \\ntransparency, and accountability in the model's outputs.\\nContinuous Improvement  as a concept is still key for Gen AI-based applications, though \\nwith a twist. For most Gen AI applications, instead of training models from scratch, we’re \\ntaking foundation models (FMs) and then adapting them to our specific use case. This means \\nconstantly tweaking these FMs through prompting techniques, swapping them out for newer \\nversions, or even combining multiple models for enhanced performance, cost efficiency, or \\nreduced latency. Traditional continuous training still holds relevance for scenarios when \\nrecurrent fine-tuning or incorporating human feedback loops are still needed.\\nNaturally, this lifecycle assumes that the foundational model powering the gen AI system is \\nalready operationalized. It's important to recognize that not all organizations will be directly \\ninvolved in this part of the process. In particular, the operationalization of foundational \\nmodels is a specialized set of tasks that is typically only relevant for a select few companies \\nwith the necessary resources and expertise.\\nBecause of that, this whitepaper will focus on practices required to operationalize gen AI \\napplications using and adapting existing foundation models, referring to other whitepapers in \\nthe book should you want to deep dive into how foundational models are operationalized. \\nThis includes active areas of research such as model pre-training, alignment (ensuring the \\nmodel's outputs align with the desired goals and values), evaluation or serving.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 9, 'page_label': '10'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n10\\nFebruary 2025\\nFigure 2. Lifecycle of a Foundational Model & gen AI system and relative operationalization practices \\nDiscover\\nAs mentioned before, building foundational models from scratch is resource-intensive. \\nTraining costs and data requirements are substantial, pushing most practitioners towards \\nadapting existing foundation models through techniques like fine-tuning and prompt \\nengineering. This shift highlights a crucial need: efficiently discovering the optimal foundation \\nmodel for a given use case.\\nThese two characteristics of the gen AI landscape make model discovery an essential \\nMLOps practice:\\n1. An abundance of models: The past year has witnessed an explosion of open-source \\nand proprietary foundation models. Navigating this complex landscape, each with varying \\narchitectures, sizes, training datasets, and licenses, requires a systematic approach to \\nidentify suitable candidates for further evaluation.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 10, 'page_label': '11'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n11\\nFebruary 2025\\n2. No one-size-fits-all solution : Each use case presents unique requirements, demanding a \\nnuanced analysis of available models across multiple dimensions.\\nHere are some factors to consider when exploring models:\\n1. Quality: Early assessments can involve running test prompts or analyzing public \\nbenchmarks and metrics to gauge output quality.\\n2. Latency & throughput: These factors directly impact user experience. A chatbot \\ndemands lower latency than batch-processed summarization tasks.\\n3. Development & maintenance time: Consider the time investment for both initial \\ndevelopment and ongoing maintenance. Managed models often require less effort than \\nself-deployed open-source alternatives.\\n4. Usage cost: Factor in infrastructure and consumption costs associated with using the \\nchosen model.\\n5. Compliance: Assess the model's ability to adhere to relevant regulations and \\nlicensing terms.\\nBecause the activity of discovery has become so important for gen AI systems, many model \\ndiscoverability platforms were created to support this need. An example of that is Vertex \\nModel Garden,1 which is explored later in this whitepaper.\\nDevelop and experiment\\nThe process of development and experimentation remains iterative and orchestrated \\nwhile building gen AI applications. Each experimental iteration involves a tripartite \\ninterplay between data refinement, foundation model(s) selection and adaptation, and\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 11, 'page_label': '12'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n12\\nFebruary 2025\\nrigorous evaluation. Evaluation provides crucial feedback, guiding subsequent iterations \\nin a continuous feedback loop. Subpar performance might call for gathering more data, \\naugmenting data, or further curating the data. Similarly, the adaptation of the foundation \\nmodel itself might need tweaking - optimizing prompts, applying fine-tuning techniques, or \\neven swapping it out for a different one altogether. This iterative refinement cycle, driven by \\nevaluation insights, is just as critical for optimizing gen AI applications as it’s always been for \\ntraditional machine learning.\\nThe foundational model paradigm\\nFoundation models differ from predictive models most importantly because they are multi-\\npurpose models. Instead of being trained for a single purpose, on data specific to that \\ntask, foundation models are trained on broad datasets, and therefore can be applied to \\nmany different use cases. This distinction brings with it several more important differences \\nbetween foundation models and predictive models.\\nFoundation models also exhibit what are known as ‘emergent properties’,2 capabilities that \\nemerge in response to specific input without additional training. Predictive models are \\nonly able to perform the single function they were trained for; a traditional French-English \\ntranslation model, for instance, cannot also solve math problems.\\nFoundation models are also highly sensitive to changes in their input. The output of the \\nmodel and the task it performs are strongly affected, indeed determined, by the input to the \\nmodel. A foundation model can be made to perform translation, generation, or classification \\ntasks simply by changing the input. Even insignificant changes to the input can affect its \\nability to correctly perform that task.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 12, 'page_label': '13'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n13\\nFebruary 2025\\nThese new properties of foundation models have created a corresponding paradigm shift \\nin the practices required to develop and operationalize Gen AI systems. While models in \\nthe predictive AI context are self-sufficient and task-specific, gen AI models are multi-\\npurpose and need an additional element beyond the user input to function as part of a \\ngen AI Application: a prompt, and more specifically, a prompt template, defined as a set of \\ninstructions and examples along with placeholders to accommodate user input. A prompt \\ntemplate, along with dynamic data such as user input, can be combined to create a complete \\nprompt, the text that is passed as input to the foundation model.\\nFigure 3.  How Prompt Template and User input can be combined to create a prompt'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 13, 'page_label': '14'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n14\\nFebruary 2025\\nThe core component of LLM Systems: A prompted \\nmodel component\\nThe presence of the prompt element is a distinguishing feature of gen AI applications. \\nNeither the model nor the prompt is sufficient for the generation of content; gen AI needs the \\ncombination of both. We refer to the combination as a ‘prompted model component’. This \\nis the smallest independent component sufficient to create an LLM application. The prompt \\ndoes not need to be very complicated. It can be a simple instruction, such as “translate \\nthe following sentence from English to French“, followed by the sentence to be translated. \\nWithout that preliminary instruction, though, a foundation model would not perform the \\ndesired translation task. So a prompt, even just a basic instruction, is necessary along with \\nthe input to get the foundation model to do the task required by the application.\\nFigure 4.  Predictive AI unit compared with the gen AI unit\\nThis introduces an important distinction when it comes to MLOps practices for gen AI. In \\nthe development of a gen AI System, experimentation and iteration need to be done in the \\ncontext of a prompted model component, the combination of a model and a prompt. The Gen'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 14, 'page_label': '15'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n15\\nFebruary 2025\\nAI experimentation cycle typically begins with testing variations of the prompt – changing the \\nwording of the instructions, providing additional context, or including relevant examples, etc., \\nand evaluating the impact of those changes. This practice is commonly referred to as prompt \\nengineering. \\nPrompt engineering involves two iterative steps:\\n1. Prompting: Crafting and refining prompts to elicit desired behaviors from a foundational \\nmodel for a specific use case.\\n2. Evaluation: Assessing the model's outputs, ideally programmatically, to gauge its \\nunderstanding and success in fulfilling the prompt's instructions.\\nFigure 5.  The activity of prompt engineering\\nResults of an evaluation can be optionally registered as part of an experiment, to allow for \\nresult tracking. Since the prompt itself is a core element of the prompt engineering process, \\nit becomes a first class citizen within the artifacts part of the experiment. \\nHowever, we need to identify which type of artifacts they are. In the good old days of \\nPredictive AI, we had clear lines - data was one thing, pipelines and code another. But with \\nthe “Prompt” paradigm in gen AI, those lines get blurry. Think about it: prompts can include \\nanything from context, instructions, examples, guardrails to actual internal or external data \\npulled from somewhere else. So, are prompts data? Are they code?\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 15, 'page_label': '16'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n16\\nFebruary 2025\\nTo address these questions, a hybrid approach is needed, recognizing that a prompt has \\ndifferent components and requires different management strategies. Let’s break it down:\\nPrompt as Data: Some parts of the prompt will act just like data. Elements like few-shot \\nexamples, knowledge bases, and user queries are essentially data points. For these \\ncomponents, we need data-centric MLOps practices such as data validation, drift detection, \\nand lifecycle management.\\nPrompt as Code: Other components such as context, prompt templates, guardrails are mode \\ncode-like. They define the structure and rules of the prompt itself. Here, we need code-\\ncentric practices such as approval processes, code versioning, and testing.\\nAs a result, when applying MLOps practices to gen AI, it becomes important to have in place \\nprocesses that give developers easy storage, retrieval, tracking, and modification of prompts. \\nThis allows for fast iteration and principled experimentation. Often one version of a prompt \\nwill work well with a specific version of the model and less well with a different version. In \\ntracking the results of an experiment, both the prompt and its components version, and the \\nmodel version must be recorded and stored along with metrics and output data produced by \\nthe prompted model.\\nThe fact that development and experimentation in gen AI requires working with the prompt \\nand the model together introduces changes in some of the common MLOps practices, \\ncompared to the predictive AI case in which experimentation is done by changing the model \\nalone. Specifically, several of the MLOps practices need to be expanded to consider the \\nprompted model component together as a unit. This includes practices like evaluation, \\nexperiment tracking, model adaptation and deployment, and artifact management, \\nwhich will be discussed below in this whitepaper.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 16, 'page_label': '17'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n17\\nFebruary 2025\\nChain & Augment\\nGen AI models, particularly large language models (LLMs), face inherent challenges in \\nmaintaining recency and avoiding hallucinations. Encoding new information into LLMs \\nrequires expensive and data-intensive pre-training, posing a significant hurdle. Additionally, \\nLLMs might be unable to solve complex challenges, especially when step-by-step reasoning \\nis required. Depending on the use case, leveraging only one prompted model to perform \\na particular generation might not be sufficient. To solve this issue, leveraging a divide and \\nconquer approach, several prompted models can be connected together, along with calls \\nto external APIs and logic expressed as code. A sequence of prompted model components \\nconnected together in this way is commonly known as a chain. \\nFigure 6.  Components of a chain and relative development process'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 17, 'page_label': '18'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n18\\nFebruary 2025\\nTwo common chain-based patterns that have emerged to mitigate recency and \\nhallucinations are retrieval augmented generation (RAG)3 and Agents. \\n• RAG addresses these challenges by augmenting pre-trained models with \\n“knowledge” retrieved from databases, bypassing the need for pre-training. This \\nenables grounding and reduces hallucinations by incorporating up-to-date factual \\ninformation directly into the generation process. \\n• Agents, popularized by the ReAct prompting technique,4 leverage LLMs as mediators \\ninteracting with various tools, including RAG systems, internal or external APIs, \\ncustom extensions, or even with other agents. This enables complex queries and \\nreal-time actions by dynamically selecting and utilizing relevant information sources. \\nThe LLM, acting as an agent, interprets the user’s query, decides which tool to utilize, \\nand how to formulate the response based on the retrieved information.\\nRAG and Agents approaches can be combined to create multi-agent systems connected \\nto large information networks, enabling sophisticated query handling and real-time \\ndecision-making. \\nThe orchestration of different models, logic and APIs is not a novelty of gen AI \\nApplications. For example, recommendation engines have long combined collaborative \\nfiltering models, content-based models, and business rules to generate personalized \\nproduct recommendations for users. Similarly, in fraud detection, machine learning \\nmodels are integrated with rule-based systems and external data sources to identify \\nsuspicious activities.\\nWhat makes these chains of gen AI components different, is that, we can't a priori \\ncharacterize or cover the distribution of component inputs, which makes the individual \\ncomponents much harder to evaluate and maintain in isolation.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 18, 'page_label': '19'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n19\\nFebruary 2025\\nThis results in a paradigm shift in how AI applications are being developed for gen AI.\\nUnlike Predictive AI where it is often possible to iterate on the separate models and \\ncomponents in isolation to then chain in the AI application, in gen AI it’s often easier to \\ndevelop a chain in integration, performing experimentation on the chain end-to-end, iterating \\nover chaining strategies, prompts, the underlying foundational models and other APIs in \\na coordinated manner to achieve a specific goal. No feature engineering, data collection, \\nor further model training cycles is often needed; just changes to the wording of the \\nprompt template.\\nThe shift towards MLOps for gen AI, in contrast to predictive AI, brings forth a new set of \\ndemands. Let's break down these key differences:\\n1. Evaluation: Because of their tight coupling, chains need end-to-end evaluation, not just \\non a per-component basis, to gauge their overall performance and the quality of their \\noutput. In terms of evaluation techniques and metrics, evaluating chains is not dissimilar \\nto evaluating prompted models. Please refer to the below segment on evaluation for more \\ndetails on these approaches.\\n2. Versioning: A chain needs to be managed as a complete artifact in its entirety. The chain \\nconfiguration should be tracked with its own revision history for analysis, reproducibility, \\nand understanding the impact of changes on output. Logging should also include the \\ninputs, outputs, and intermediate states of the chain, and any chain configurations used \\nduring each execution.\\n3. Continuous Monitoring: Establishing proactive monitoring systems is vital for detecting \\nperformance degradation, data drift, or unexpected behavior in the chain. This ensures \\nearly identification of potential issues to maintain the quality of the generated output. The \\nactivity of monitoring Chains is discussed in detail in the section ‘Logging and Monitoring’.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 19, 'page_label': '20'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n20\\nFebruary 2025\\n4. Introspection: The ability to inspect the internal data flows of a chain (inputs and outputs \\nfrom each component) as well as the inputs and outputs of the entire chain is paramount. \\nBy providing visibility into the data flowing through the chain and the resulting content, \\ndevelopers can pinpoint the sources of errors, biases, or undesirable behavior.\\nFigure 7.  Putting together chains, prompted models and model tuning\\nThere are several products in Vertex AI that can support the need for chaining and \\naugmentation, including Grounding as a service,5 Extensions,6 Vector Search,7 Agent Builder,8 \\nand more. We discuss the products in the  section “Role of an AI Platform”. LangChain9 is \\nalso integrated with the Vertex SDK,10 and can be used alongside the core Vertex products to \\ndefine and configure gen AI chained applications.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 20, 'page_label': '21'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n21\\nFebruary 2025\\nTuning & training\\nWhen developing a gen AI use case and a specific task that involves LLMs, it can be difficult, \\nespecially for complex tasks, to rely on only prompt engineering and chaining to solve it.      \\nTo improve task performance practitioners often also need to fine-tune the model directly. \\nFine-tuning lets you actively change the layers or a subset of layers of the LLM to optimize \\nthe capability of the model to perform a certain task. Two of the most common ways of \\ntuning a model are:\\n1. Supervised fine-tuning: This is where we train the model in a supervised manner, teaching \\nit to predict the right output sequence for a given input. \\n2. Reinforcement Learning from Human Feedback (RLHF): In this approach, we first train \\na reward model to predict what humans would prefer as a response. Then, we use this \\nreward model to nudge the LLM in the right direction during the tuning process. Like \\nhaving a panel of human judges guiding the model's learning. \\nFigure 8.  Putting together chains, prompted models and model tuning\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 21, 'page_label': '22'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n22\\nFebruary 2025\\nWhen viewed through the MLOps lens, fine-tuning shares similar requirements with \\nmodel training:\\n1. The capability to track artifacts being part of the tuning job. This includes for example the \\ninput data or the parameters being used to tune the model.\\n2. The capability to measure the impact of the tuning. This translates into the capability \\nto perform evaluation of the tuned model for the specific tasks it was trained on and to \\ncompare results with previously tuned models or frozen models for the same task.\\nPlatforms like Vertex AI11 (and the Google Cloud platform more broadly) provide a robust \\nsuite of services designed to address these MLOps requirements: Vertex Model Registry,12 \\nfor instance, provides a centralized storage location for all the artifacts created during the \\ntuning job, and Vertex Pipelines13 streamlines the development and management of these \\ntuning jobs. Dataplex,14 meanwhile, provides an organization-wide data fabric for data lineage \\nand governance and integrates well with both Vertex AI and BigQuery.15 What’s more, these \\nproducts provide the same governance capability for both predictive and gen AI applications, \\nmeaning customers do not need separate products or configurations to manage generative \\nversus AI development.\\nContinuous Training & Tuning\\nIn machine learning operations (MLOps), continuous training is the practice of repeatedly \\nretraining machine learning models in a production environment. This is done to ensure \\nthat the model remains up-to-date and performs well as real-world data patterns change \\nover time. For gen AI models, continuous tuning of the models is often more practical than \\nretraining from scratch due to the high data and computational costs involved.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 22, 'page_label': '23'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n23\\nFebruary 2025\\nThe approach to continuous tuning depends on the specific use case and goals. For relatively \\nstatic tasks like text summarization, the continuous tuning requirements may be lower. But \\nfor dynamic applications like chatbots that need constant human alignment, more frequent \\ntuning using techniques like RLHF based on human feedback is necessary. \\nTo determine the right continuous tuning strategy, AI practitioners must carefully evaluate \\nthe nature of their use case and how the input data evolves over time. Cost is also a major \\nconsideration, as the compute infrastructure greatly impacts the speed and expense of \\ntuning.  We discuss in detail monitoring of GenAI systems in the Logging and Monitoring \\nsection of this whitepaper.\\nGraphics processing units (GPUs) and tensor processing units (TPUs) are key hardware for \\nfine-tuning. GPUs, known for their parallel processing power, are highly effective in handling \\nthe computationally intensive workloads and often associated with training and running \\ncomplex machine learning models. TPUs, on the other hand, are specifically designed \\nby Google for accelerating machine learning tasks. TPUs excel in handling large matrix \\noperations common in deep learning neural networks.\\nTo manage costs, techniques like model quantization can be applied. This represents model \\nweights and activations using lower-precision 8-bit integers rather than 32-bit floats, which \\nreduces computational and memory requirements.\\nWe discuss in detail the support for tuning in Vertex AI in the Customize: Vertex AI Training & \\nTuning section.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 23, 'page_label': '24'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n24\\nFebruary 2025\\nData Practices\\nTraditionally, ML model behavior was dictated solely by its training data. While this still holds \\ntrue for foundation models – trained on massive, multilingual, multimodal datasets – gen AI \\napplications built on top of them introduce a new twist: model behavior is now determined by \\nhow you adapt the model using different types of input data (Figure. 9).\\nFigure 9.  Examples of data spectrum for foundation models – creation (left) vs. adaptation (right)\\nThe key difference between traditional predictive ML and gen AI lies in where you start. In \\npredictive ML, the data is paramount. You spend a lot of time on data engineering, and if you \\ndon’t have the right data, you cannot build an application. Gen AI takes a unique approach to \\nthis matter. You start with a foundation model, some instructions and maybe a few example \\ninputs (in-context learning). You can prototype and launch an application with surprisingly \\nlittle data.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 24, 'page_label': '25'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n25\\nFebruary 2025\\nThis ease of prototyping, however, comes with a challenge. Traditional predictive AI relies on \\napriori well-defined dataset(s). In gen AI, a single application can leverage various data types, \\nfrom completely different data sources, all working together (Figure 10). Let’s explore some \\nof these data types:\\n• Conditioning prompts: These are essentially instructions given to the Foundation Model \\n(FM) to guide its output, setting boundaries of what it can generate.\\n• Few-shot examples: A way to show the model what you want to achieve through input-\\noutput pairs. This helps the model grasp the specific task(s) at hand, and in many cases, it \\nboosts performances.\\n• Grounding/augmentation data: Data coming from either external APIs (like Google \\nSearch) or internal APIs and data sources. This data  permits the FM to produce answers \\nfor a specific context, keeping responses current, relevant without retraining the entire \\nFM. This type of data also supports reducing hallucinations.\\n• Task-specific datasets: These are used to fine-tune an existing FM for a particular task, \\nimproving its performance in that specific area.\\n• Human preference datasets: These capture feedback on generated outputs, helping \\nrefine the model’s ability to produce outputs that align with human preferences. \\n• Full pre training corpora: These are massive datasets used to initially train foundation \\nmodels. While application builders may not have access to them nor the tokenizers, \\nthe information encoded in the model itself will influence the application’s output \\nand performance.\\nThis is not an exhaustive list. The variety of data used in gen AI applications is constantly \\ngrowing and evolving.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 25, 'page_label': '26'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n26\\nFebruary 2025\\nFigure 10.  Example of high-level data and adaptations landscape for developing gen AI applications using \\nexisting foundation models\\nThis diverse range of data adds another complexity layer in terms of data organization, \\ntracking and lifecycle management. Take a RAG-based application as an example: it might \\ninvolve rewriting user queries, dynamically gathering relevant examples using a curated set \\nof examples, querying a vector database, and combining it all with a prompt template. This \\ninvolves managing multiple data types: user queries,  vector databases with curated few-shot \\nexamples and company information, and prompt templates.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 26, 'page_label': '27'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n27\\nFebruary 2025\\nEach data type needs careful organization and maintenance. For example, the vector \\ndatabase requires processing data into embeddings, optimizing chunking strategies, and \\nensuring only relevant information is available. The prompt template itself needs versioning \\nand tracking, the user queries need rewriting, etc. This is where  traditional MLOps and \\nDevOps best practices come into play, with a twist. We need to ensure reproducibility, \\nadaptability, governance, and continuous improvement using all the data required in an \\napplication as a whole but also individually. Think of it this way: in predictive AI, the focus \\nwas on well-defined data pipelines for extraction, transformation, and loading. In gen AI, \\nit's about building pipelines to manage, evolve, adapt and integrate different data types in a \\nversionable, trackable, and reproducible way. \\nAs mentioned earlier, fine-tuning foundation models (FMs) can boost gen AI app \\nperformance, but it needs data. You can get this data by launching your app and gathering \\nreal-world data, generating synthetic data, or a mix of both. Using large models to generate \\nsynthetic data is becoming popular because it speeds things up, but it's still good to have a \\nhuman check the results for quality assurance. Here are few ways to leverage large models \\nfor data engineering purposes:\\n1. Synthetic data generation: This process involves creating artificial data that closely \\nresembles real-world data in terms of its characteristics and statistical properties, often \\nbeing done with a large and capable model. This synthetic data serves as additional \\ntraining data for gen AI, enabling it to learn patterns and relationships even when labeled \\nreal-world data is scarce.\\n2. Synthetic data correction: This technique focuses on identifying and correcting errors \\nand inconsistencies within existing labeled datasets. By leveraging the power of larger \\nmodels, gen AI can flag potential labeling mistakes and propose corrections, improving the \\nquality and reliability of the training data.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 27, 'page_label': '28'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n28\\nFebruary 2025\\n3. Synthetic data augmentation: This approach goes beyond simply generating new \\ndata. It involves intelligently manipulating existing data to create diverse variations while \\npreserving essential features and relationships. Thus, gen AI can encounter a broader \\nrange of scenarios during training, leading to improved generalization and ability to \\ngenerate nuanced and relevant outputs.\\nEvaluating gen AI, unlike predictive AI, is tricky. You don't usually know the training data \\ndistribution of the foundational models. Building a custom evaluation dataset reflecting your \\nuse case is essential. This dataset should cover essential, average, and edge cases. Similar \\nto fine-tuning data, you can leverage powerful language models to generate, curate, and \\naugment data for building robust evaluation datasets.\\nEvaluate\\nEven if only prompt engineering is performed, as any experimental process, it does require \\nevaluation in order to iterate and improve. This makes the evaluation process a core activity \\nof the development of any gen AI systems.\\nIn the context of gen AI systems, evaluation might have different degrees of automation: from \\nentirely driven by humans to entirely automated by a process. \\nIn the early days of a project, when you're still prototyping, evaluation is often a manual \\nprocess. Developers eyeball the model's outputs, getting a qualitative sense of how it's \\nperforming. But as the project matures and the number of test cases balloons, manual \\nevaluation becomes a bottleneck. That's when automation becomes key.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 28, 'page_label': '29'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n29\\nFebruary 2025\\nAutomating evaluation has two big benefits. First, it lets you move faster. Instead of spending \\ntime manually checking each test case, you can let the machines do the heavy lifting. \\nThis means more iterations, more experiments, and ultimately, a better product. Second, \\nautomation makes evaluation more reliable. It takes human subjectivity out of the equation, \\nensuring that results are reproducible.\\nBut automating evaluation for gen AI comes with its own set of challenges. \\nFor one, both the inputs (prompts) and outputs can be incredibly complex. A single prompt \\nmight include multiple instructions and constraints that the model needs to juggle. And the \\noutputs themselves are often high-dimensional - think a generated image or a block of text. \\nCapturing the quality of these outputs in a simple metric is tough.\\nThere are some established metrics, like BLEU for translations and ROUGE for summaries, \\nbut they don't always tell the full story. That's where custom evaluation methods come in. \\nOne approach is to use another foundational model as a judge. For example, you could \\nprompt a large language model to score the quality of generated texts across various \\ndimensions. This is the idea behind techniques like AutoSxS.16\\nAnother challenge is the subjective nature of many evaluation metrics for gen AI. What \\nmakes one output ‘better’ than another can often be a matter of opinion. The key here is to \\nmake sure your automated evaluation aligns with human judgment. You want your metrics \\nto be a reliable proxy for what people would think. And to ensure comparability between \\nexperiments, it's crucial to lock down your evaluation approach and metrics early in the \\ndevelopment process.\\nLack of ground truth data is another common hurdle, especially in the early stages of a \\nproject. One workaround is to generate synthetic data to serve as a temporary ground truth, \\nwhich can be refined over time with human feedback.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 29, 'page_label': '30'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n30\\nFebruary 2025\\nFinally, comprehensive evaluation is essential for safeguarding gen AI applications against \\nadversarial attacks. Malicious actors can craft prompts to try to extract sensitive information \\nor manipulate the model's outputs. Evaluation sets need to specifically address these attack \\nvectors, through techniques like prompt fuzzing (feeding the model random variations on \\nprompts) and testing for information leakage.\\nAutomating the evaluation process ensures speed, scalability and reproducibility\\nAn automation of the evaluation process can be considered a proxy for the \\nhuman judgmen\\nDepending on the use case, the evaluation process will require a high degree \\nof customization.\\nTo ensure comparability it is essential to stabilize the evaluation approach, metrics, \\nand ground truth data as early as possible in the development phase.\\nIt is possible to generate synthetic ground truth data to accommodate for the lack of \\nreal ground truth data.\\nIt is important to include test cases of adversarial prompting as part of the evaluation \\nset to test the reliability of the system itself for these attacks.\\nTable 1.  Key suggestions to approach evaluation of gen AI systems\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 30, 'page_label': '31'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n31\\nFebruary 2025\\nDeploy\\nIt should be clear by this point that production gen AI applications are complex systems with \\nmany interacting components. Some of the common components discussed include multiple \\nprompts, models, adapter layers and external data sources. In deploying a gen AI system to \\nproduction, all these components need to be managed and coordinated with the previous \\nstages of gen AI system development. Given the novelty of these systems, best practices \\nfor deployment and management are still evolving, but we can discuss observations and \\nrecommendations for these components and indicate how to address the major concerns.\\nDeploying gen AI solutions necessarily involves multiple steps. For example, a single \\napplication might utilize several large language models (LLMs) alongside a database, all \\nfed by a dynamic data pipeline. Each of these components potentially requires its own \\ndeployment process.\\nFor clarity, we distinguish between two main types of deployment:\\n1. Deployment of gen AI systems: This focuses on operationalizing a complete system \\ntailored for a specific use case. It encompasses deploying all the necessary elements \\n-  the application, chosen LLMs, database, data pipelines, and any other relevant \\ncomponents - to create a functioning end-user solution.\\n2. Deployment of foundational models: This applies to open-weight models, where the \\nmodel weights are publicly available on platforms like Vertex Model Garden or Hugging \\nFace, or privately trained models. Deployment in this scenario centers around making \\nthe foundational model itself accessible to users. Given their multipurpose nature, these \\ndeployments often aim to support various potential use cases.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 31, 'page_label': '32'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n32\\nFebruary 2025\\nDeployment of gen AI systems\\nDeployment of gen AI systems is broadly similar to deployment of any other complex \\nsoftware system. Most of the system components – databases, Python applications, etc. – \\nare also found in other non-gen AI applications. As a result, our general recommendation is \\nto manage these components using standard software engineering practices such as version \\ncontrol17 and Continuous Integration / Continuous Delivery (CI/CD).18\\nVersion control\\nGen AI experimentation is an iterative process involving repeated cycles of development, \\nevaluation, and modification. To ensure a structured and manageable approach, it's crucial to \\nimplement strict versioning for all modifiable components. These components include:\\n• Prompt templates: Unless leveraging specific prompt management solutions, version \\nthem through standard version control tools like Git.\\n• Chain definitions: The code defining the chain (including API integrations, database calls, \\nfunctions, etc.) should also be versioned using tools like Git. This provides a clear history \\nand enables easy rollback if needed.\\n• External datasets: In retrieval augmented generation (RAG) systems, external datasets \\nplay a key role. It’s important to track these changes and versions of these datasets for \\nreproducibility. You can do that by leveraging existing data analytics solutions such as \\nBigQuery, AlloyDB, Vertex Feature Store. \\n• Adapter models: The landscape of techniques like LoRA tuning for adapter models is \\nconstantly evolving. . You can leverage established data storage solutions (e.g. cloud \\nstorage) to manage and version these assets effectively.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 32, 'page_label': '33'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n33\\nFebruary 2025\\nContinuous integration of gen AI systems\\nIn a continuous integration framework, every code change goes through automatic testing \\nbefore merging to catch issues early. Here, unit and integration testing are key for quality \\nand reliability. Unit tests act like a microscope, zooming in on individual code pieces, while \\nintegration testing verifies that different components work together.\\nThe benefits of continuous integration in traditional software development are well-\\nunderstood. Implementing a CI system helps to do the following:\\n1. Ensure reliable, high-quality outputs: Rigorous testing increases confidence in the \\nsystem's performance and consistency.  \\n2. Catch bugs early: Identifying issues through testing prevents them from causing bigger \\nproblems downstream. It also makes the system more robust and resilient to edge cases \\nand unexpected inputs.\\n3. Lower maintenance costs: Well-documented test cases simplify troubleshooting and \\nenable smoother modifications in the future, reducing overall maintenance efforts\\nThese benefits are applicable to gen AI Systems as much as any software product. \\nContinuous Integration should be applied to all elements of the system, including the prompt \\ntemplates, the chain and chaining logic, and any embedding models and retrieval systems.\\nHowever, applying CI to gen AI comes with challenges:\\n1. Difficult to generate comprehensive test cases: The complex and open-ended nature of \\ngen AI outputs makes it hard to define and create an exhaustive set of test cases that \\ncover all possibilities.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 33, 'page_label': '34'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n34\\nFebruary 2025\\n2. Reproducibility issues: Achieving deterministic, reproducible results is tricky since \\ngenerative models often have intrinsic randomness and variability in their outputs, even for \\nidentical inputs. This makes it harder to consistently test for specific expected behaviors.\\nThese challenges are closely related to the broader question of how to evaluate gen AI \\nsystems. Many of the same techniques discussed in the Evaluation section above can also \\nbe applied to the development of CI systems for gen AI. This is an ongoing area of research, \\nhowever, and more techniques will undoubtedly emerge in the near future.\\nContinuous delivery of gen AI systems\\nOnce the code is merged, a continuous delivery process begins to move the built and tested \\ncode through environments that closely resemble production for further testing before the \\nfinal deployment. \\nAs mentioned in the \"\\'\"Develop and Experiment\"\\'\" segment, chain elements become one \\nof the main components to deploy, as they fundamentally constitute the gen AI application \\nserving users.\\nThe delivery process of the gen AI application containing the chain may vary depending on \\nthe latency requirements and whether the use case is batch or online:\\n1. Batch use cases require deploying a batch process executed on a schedule in production. \\nThe delivery process should focus on testing the entire pipeline in integration in an \\nenvironment close to production before deployment. As part of the testing process, \\ndevelopers can assert specific requirements around the throughput of the batch process \\nitself and checking that all components of the application are functioning correctly (e.g., \\npermissioning, infrastructure, code dependencies).'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 34, 'page_label': '35'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n35\\nFebruary 2025\\n2. Online use cases require deploying an API, in this case, the application containing the \\nchain, capable of responding to users at low latency. The delivery process should involve \\ntesting the API in integration in an environment close to production, with tests to assert \\nthat all components of the application are functioning correctly (e.g., permissioning, \\ninfrastructure, code dependencies). Non-functional requirements (e.g., scalability, \\nreliability, performance) can be verified through a series of tests, including load tests.\\nDeployment of foundation models\\nBecause foundation models are so large and complex, deployment and serving of these \\nmodels raises a number of issues – most obviously, the compute and storage resources \\nneeded to run these massive models successfully. At a minimum, a foundation model \\ndeployment needs to include several key considerations: selecting and securing necessary \\ncompute resources, such as GPUs or TPUS; choosing appropriate data storage services \\nlike BigQuery or Google Cloud Storage that can scale to deal with the large datasets; and \\nimplementing model optimization or compression techniques.\\nInfrastructure validation\\nOne technique that can be applied to address the resource requirements of gen AI systems is \\ninfrastructure validation. This refers to the introduction of an additional verification step, prior \\nto deploying the training and serving systems, to check both the compatibility of the model \\nwith the defined serving configuration and the availability of the required hardware. There \\nare a number of optional infrastructure validation layers that can perform some of these \\nchecks automatically. For instance, TFX19 has an infrastructure validation layer that checks'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 35, 'page_label': '36'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n36\\nFebruary 2025\\nwhether the model will run correctly on a specified hardware configuration, which can help \\ncatch configuration issues before deployment. Nevertheless, the availability of the required \\nhardware still needs to be verified by hand by the engineer or the system administrator.\\nCompression and optimization\\nAnother way of addressing infrastructure challenges is to optimize the model itself. \\nCompressing and/or optimizing the model can often significantly reduce the storage and \\ncompute resources needed for training and serving, and in many cases can also decrease \\nthe serving latency.\\nSome techniques for model compression and optimization include quantization, distillation \\nand model pruning. Quantization reduces the size and computational requirements of the \\nmodel by converting its weights and activations from higher-precision floating-point numbers \\nto lower-precision representations, such as 8-bit integers or 16-bit floating-point numbers. \\nThis can significantly reduce the memory footprint and computational overhead of the model. \\nModel Pruning is a technique for eliminating unnecessary weight parameters or by selecting \\nonly important subnetworks within the model. This reduces model size while maintaining \\naccuracy as high as possible. Finally, distillation trains a smaller model, using the responses \\ngenerated by a larger LLM, to reproduce the output of the larger LLM for a specific domain. \\nThis can significantly reduce the amount of training data, compute, and storage resources \\nneeded for the application.\\nIn certain situations, model distillation can also improve the performance of the model itself \\nin addition to reducing resource requirements. This happens because the smaller model can \\ncombine the knowledge of the larger model with labeled data, which can help it to generalize \\nbetter to new data on a limited use case.The process of distillation usually involves training \\na large foundational LLM (teacher model) and having it generate responses to certain tasks,'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 36, 'page_label': '37'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n37\\nFebruary 2025\\nand then having the smaller LLM (student model) use a combination of the LLMs knowledge \\nas well as task specific supervised dataset to learn. The size and complexity of the smaller \\nLLM can be adjusted to achieve the desired trade-off between performance and resource \\nrequirements. A technique known as step-by-step distillation20 has proven to achieve \\ngreat results.\\nDeployment, packaging, and serving checklist\\nFollowing are the important steps to take when deploying a model on Vertex AI.\\n □ Configure version control: Implement version control practices for LLM deployments. \\nThis allows you to roll back to previous versions if necessary and track changes made to \\nthe model or deployment configuration.\\n □ Optimize the model: Perform any model optimization (distillation, quantization, pruning, \\netc.) before packaging or deploying the model.\\n □ Containerize the model: Package the trained LLM model into a container. \\n □ Define target hardware requirements: Ensure the target deployment environment \\nmeets the requirements for optimal performance of the LLM model, such as GPUs, as well \\nas TPUs and other specialized hardware accelerators.\\n □ Define model endpoint: Define the endpoint configuration using Vertex AI's endpoint \\ncreation interface or the Vertex AI SDK. Specify the model container, input and output \\nformats, and any additional configuration parameters.\\n □ Allocate resources: Allocate the appropriate compute resources for the endpoint based \\non the expected traffic and performance requirements.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 37, 'page_label': '38'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n38\\nFebruary 2025\\n □ Configure access control: Set up access control mechanisms to restrict access to \\nthe endpoint based on authentication and authorization policies. This ensures that only \\nauthorized users or services can interact with the deployed LLM.\\n □ Create model endpoint: Create a Vertex AI endpoint to deploy21 the LLM as a REST API \\nservice. This allows clients to send requests to the endpoint and receive responses from \\nthe LLM..\\n □ Configure monitoring and logging: Establish monitoring and logging systems to track \\nthe endpoint's performance, resource utilization, and error logs.\\n □ Deploy custom integrations: Integrate the LLM into custom applications or services \\nusing the model's SDK or APIs. This provides more flexibility for integrating the LLM into \\nspecific workflows or frameworks.\\n □ Deploy Real-time Applications: For real-time applications, consider using Cloud \\nFunctions and Cloud Run in combination with LLMs hosted in Vertex AI to create a \\nstreaming pipeline that processes data and generates responses in real time.\\nLogging and monitoring\\nMonitoring gen AI applications and, as a result, their components, presents unique \\nchallenges that require additional techniques and approaches on top of those in traditional \\nMLOps. The use of gen AI requires the chaining of components in order to produce results \\nfor practical applications. Additionally, to your application user, all the components are \\nhidden. Therefore, the interface they have to your application is their input and the final \\noutput. This creates the need to log and monitor your application end-to-end: that is, logging \\nand monitoring the input and output of your application overall as well as the input and \\noutput of every single component.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 38, 'page_label': '39'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n39\\nFebruary 2025\\nLogging is necessary for applying monitoring and debugging on your gen AI system in \\nproduction. An input to the application triggers multiple components. Imagine the output \\nto a given input is factually inaccurate. How can you find out which of the components are \\nthe ones that didn’t perform well? To answer this question it is necessary to apply logging \\non the application level and at the component level. We need lineage in our logging for all \\ncomponents executed. For every component we need to log their inputs and outputs. We \\nalso need to be able to map those with any additional artifacts and parameters they depend \\non so we can easily analyze those inputs and outputs.\\nMonitoring can be applied to the overall gen AI application and to individual components. We \\nprioritize monitoring at the application level. This is because if the application is performant \\nand monitoring proves that, it implies that all components are also performant. You can also \\napply the same practices to each of the prompted model components to get more granular \\nresults and understanding of your application.\\nSkew detection in traditional ML systems refers to training-serving skew that occurs when \\nthe feature data distribution in production deviates from the feature data distribution \\nobserved during model training. In the case of Gen AI systems using pretrained models in \\ncomponents chained together to produce the output, we need to modify our approach. We \\ncan measure skew by comparing the distribution of the input data we used to evaluate our \\napplication (the test set as described under the Data Curation and Principles section above) \\nand the distribution of the inputs to our application in production. Once the two distributions \\ndrift apart,further investigation is needed. The same process can be applied to the output \\ndata as well.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 39, 'page_label': '40'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n40\\nFebruary 2025\\nFigure 11.  Drift/skew detection process overview\\nLike skew detection, the drift detection process checks for statistical differences between \\ntwo datasets. However, instead of comparing evaluations and serving inputs, drift looks for \\nchanges in input data. This allows you to check how the inputs and therefore the behavior of \\nyour users changed over time. This is the same as traditional MLOps. \\nGiven that the input to the application is typically text, there are a few approaches to \\nmeasuring skew and drift. In general all the methods are trying to identify significant \\nchanges in production data, both textual (size of input) and conceptual (topics in input), \\nwhen compared to the evaluation dataset. All these methods are looking for changes that \\ncould potentially indicate the application might not be prepared to successfully handle the \\nnature of the new data that are now coming in. Some common approaches are calculating \\nembeddings and distances, counting text length and number of tokens, and tracking \\nvocabulary changes, new concepts and intents, prompts and topics in datasets, as well \\nas statistical approaches such as least-squares density difference,22 maximum mean \\ndiscrepancy (MMD),23 learned kernel MMD,24 or context-aware MMD.25 As gen AI use cases \\nare so diverse, it is often necessary to create additional custom metrics that better capture \\nabnormal changes in your data.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 40, 'page_label': '41'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n41\\nFebruary 2025\\nWith the rise of multimodal models, both dedicated image or video generation ones like \\nImagen and Veo, along with Gemini’s ability to process and output other modalities than \\nonly text, there’s a need for additional techniques to monitor and align these outputs to \\nthe creator’s intents. In addition to prompt alignment - assuring that the output from the \\ngenerative model matches the text description provided, there’s also organizational policy \\nalignment that can be tracked, logged, and monitored. As an analog to drift, subjective \\noutput can be measured by reviewing the original prompt and utilizing a multimodal \\ngenerative AI’s capabilities of assessment to determine whether the prompt and the \\noutput are similar, or have alignment. This can be done in multiple ways - a zero-shot, \\nsingle prompt process to get a quick classification - or using the reasoning process of a \\ngenerative model to decompose the prompt into related component parts, assess each one \\nof these component part questions, and aggregate a score, yielding a more explainable or \\ninterpretable score. This process, often called “LLM as a Judge” or a generative Autorater, \\ncan bring some of the organization’s potentially subjective policy guidance into assessable \\nscores. Vertex AI’s Generative AI Evaluation service provides primitives for constructing \\nCustom Metrics and using these as autoraters.\\nContinuous evaluation is another common approach to GenAI application monitoring. In \\na continuous evaluation system, you capture the model's production output and run an \\nevaluation task using that output, to keep track of the model's performance over time. One \\napproach is collecting direct user feedback, such as ratings (for example thumbs up/down), \\nwhich provides immediate insight into the perceived quality of outputs. In parallel, comparing \\nmodel-generated responses against established ground truth, often collected through \\nhuman assessment or as a result of an ensemble AI Model approach, allows for deeper \\nanalysis of performance. Ground truth metrics can be used to generate evaluation metrics \\nas described in the Evaluation section. This process provides a view on how your evaluation \\nmetrics changed from when you developed your model to what you have in production today.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 41, 'page_label': '42'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n42\\nFebruary 2025\\nAs with traditional monitoring in MLOps an alerting process should be deployed for notifying \\napplication owners when a drift, skew or performance decay from evaluation tasks is \\ndetected. This can help you promptly intervene and resolve issues. This is achieved by \\nintegrating alerting and notification tools into your monitoring process.\\nMonitoring expands beyond drift, skew and evaluation tasks. Monitoring in MLOps includes \\nefficiency metrics like resources utilization and latency. Efficiency metrics are as relevant and \\nimportant in gen AI as they are in any other AI application.\\nVertex AI provides a set of tools that can help with monitoring. The Vertex AI Eval Service \\nfor gen AI26 can be used for classification, summarization, question answering, and text \\ngeneration tasks as well as Agent evals based on rubrics and more detailed trajectory \\nevaluations. Vertex Pipelines can be used to allow the recurrent execution of evaluation jobs \\nin production as well as running pipelines for skew and drift detection processes.\\nOne form of logging commonly employed in application development is “tracing”, which \\nsends out logged events to an aggregator from internal steps in a complex system - \\nallowing detailed representations of a complicated application.  This same approach can \\nbe utilized in developing agents, instrumenting steps by emitting traces, and ensuring that \\nwhen you look at the logs you can understand which step for which agent for which user \\nsession. OpenTelemetry is the commonly adopted standard for traces with many possible \\naggregators, including Cloud Observability.  The spec for traces has been changing recently \\nto support larger payloads as needed to describe the behaviors of agents and LLMs.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 42, 'page_label': '43'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n43\\nFebruary 2025\\nGovern\\nIn the context of MLOps governance encompasses all the practices, and policies that \\nestablish control, accountability, and transparency over the development, deployment, and \\nongoing management of machine learning (ML) models, including all the activities related to \\nthe code, data and models lifecycle.\\nAs mentioned in the Develop & Experiment section the chain element and the relative \\ncomponents become a new type of assets that need to be governed over the full lifecycle \\nfrom development to deployment, to monitoring. \\nThe governance of the chain element lifecycle extends to lineage tracking practices as well.  \\nWhile for predictive AI systems lineage focuses on tracking and understanding the complete \\njourney of a machine learning model, in gen AI, lineage goes beyond the model artifact \\nextending to all the components in the chain. This includes the data and models used and \\ntheir lineage, the code involved and the relative evaluation data and metrics. This can help \\nauditing, debugging and improvements of the models\\nAlong with these new practices, existing MLOps and DevOps practices still apply to MLOps \\nfor gen AI:\\n1. The need to govern the data lifecycle; see “Data Practices”.\\n2. The need to govern the tuned model lifecycle; see “Tuning and Training”.\\n3. The need to govern the code lifecycle; see “Deployment of GenAI \\nSystem components”.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 43, 'page_label': '44'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n44\\nFebruary 2025\\nThe next segment will introduce a set of products that allow developers to perform \\ngovernance of the data, model and code assets. We will discuss products like Google \\nCloud Dataplex, which centralizes the governance of model and data, Vertex ML Metadata \\nand Vertex Experiment, which allows developers to register experiments, their metrics \\nand artifacts.\\nExtend MLOps for gen AI to Agents\\nThe development and operationalization of genAI systems involve intricate workflows, \\noften chaining together multiple models, APIs, and data sources. A particularly compelling \\nand increasingly important facet of this landscape is the rise of Agents. These intelligent \\nsystems, capable of interacting with their environment/tools and making decisions, represent \\na significant advancement in genAI capabilities. While we briefly introduced the concept \\nof agents within the context of chains and augmentation, their unique characteristics \\nand operational demands warrant a deeper dive. This section unfolds the specific MLOps \\nconsiderations for building, deploying, and managing agent-based systems, addressing their \\ndistinct lifecycle, tooling requirements, and the crucial aspects of observability, safety, and \\ncontinuous improvement. For the fundamental knowledge on Agents we recommend to read \\nthe Agent whitepaper by Google.93'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 44, 'page_label': '45'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n45\\nFebruary 2025\\nAgent Lifecycle\\nFigure 12. Agent Core: Tools, model, and instruction prompt\\nThe core of an agent system comprises three fundamental elements: a Foundation Model, \\nInstructions, and a Tool. The foundation model serves as the cognitive engine, providing \\nthe agent with reasoning and language processing capabilities. Instructions represent the \\nguiding directives or goals that the agent is designed to achieve. These instructions could \\nrange from simple tasks to complex, multi-step objectives. The Tool, rather than being the \\nactual executable code, consists of descriptions of the available functions and their required \\nparameters.  These descriptions provide the foundation model with the information it needs \\nto reason about which tool is most appropriate for a given situation and how to use it.  The \\nfoundation model analyzes the instructions and the descriptions of the available functions, \\ndetermining which function to call and what parameter values to use. The actual execution \\nof the chosen function is then handled separately (by the developer's code or, in some \\nadvanced systems, directly by the model itself). Together, these three components work in \\nconcert, allowing the agent to intelligently interpret instructions, leverage the Foundation \\nModel's knowledge of available tools, and utilize the appropriate tool (via its description) to \\naccomplish its objectives.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 45, 'page_label': '46'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n46\\nFebruary 2025\\nFigure 13. Multi-turn interaction of an agent with tools and memory until the final response\\nThese core components – the Foundation Model, Instructions, and Tool – work in dynamic \\ninterplay throughout the agent's operational lifecycle. The following steps detail how these \\nelements are orchestrated to process information, interact with the environment, and \\nultimately achieve the agent's objectives.\\n1. New Query: The user initiates the interaction by providing a new query or question\\n2. Function Identification: The foundation model, along with the available tools (e.g., APIs, \\ndatabases, specialized software) and instructions, analyzes the query and determines \\nif a function call is necessary. If so, it identifies the appropriate function name and the \\nrequired parameters.\\n3. Function Call Preparation: The foundation model generates a structured function call \\nrequest, specifying the function name and the parameters to be passed.\\n4. Function Call Execution: This step is performed by the developer's code (or, in advanced \\nfoundation models, automatically by the model). The code receives the function call \\nrequest, executes the corresponding function (e.g., making an API call), and retrieves \\nthe result.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 46, 'page_label': '47'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n47\\nFebruary 2025\\n5. Intermediate Response: The result of the function execution (the data retrieved by the \\nfunction) is sent back to the foundation model that produces an intermediate response.\\n6. Iterative Context Update (Short-Term Memory): The developer's code updates the \\nconversation history (short-term memory) with the intermediate response and provides \\nthe updated context to the foundation model (returning to step 2).\\n7. Final Response: Once the foundation determines it has all the necessary information (or \\na maximum number of steps is reached to prevent infinite loops), it generates the final \\nresponse to the user, incorporating the information gathered from all the function calls.\\nThe preceding summary of the core components and operational lifecycle of an agent \\nprovides a foundation for examining the crucial considerations for productionizing these \\nintelligent systems.\\nTool Orchestration\\nAs established, tools are essential for enabling generative AI agents to interact with the world \\nand execute complex tasks.  These tools vary significantly in implementation, accessibility, \\nand capabilities. Understanding these distinctions is crucial for building effective and \\nrobust agents.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 47, 'page_label': '48'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n48\\nFebruary 2025\\nTool Types & Environments\\nIn a cloud environment, tools can be categorized based on origin and access method:\\n• Code Functions: Functions implemented directly within the agent's codebase, granting \\ndirect access to local resources and logic. These can be implemented in various \\nprogramming languages (e.g., Python, Java). On Google Cloud, services like Artifact \\nRegistry (for storing code libraries) and Cloud Code (for repository management) can \\nbe used.\\n• Private REST APIs: APIs hosted within a Virtual Private Cloud (VPC), providing secure \\naccess to internal services and data. Google Cloud offers services like Cloud Run (for \\ndeploying containerized applications, including APIs), API Gateway (for managing and \\nsecuring APIs), and Apigee API Management (for advanced API management).\\n• Public REST APIs: Publicly available APIs from third-party services, offering diverse \\nfunctionalities accessible over the internet. A NAT Gateway can be used to provide secure \\noutbound internet access for resources within a VPC without direct public exposure.\\nBoth Code Functions and Private REST APIs enable secure interactions with internal \\ndatabases, storage systems, other internal services, or even other agents within the \\norganization's infrastructure. A comparison of the tool types follows:\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 48, 'page_label': '49'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n49\\nFebruary 2025\\nTable 2. Different types of tools and their feature comparison\\nAs described, tools can be implemented in various ways, from code functions within the \\nagent itself to APIs hosted in VPCs or publicly available services.  Furthermore, these tools \\noften reside in different locations within a cloud environment. Data sources, for example, \\nare frequently managed within dedicated data environments, for example data lake or \\nmesh projects, while APIs and agents used as tools might be deployed within application \\nproduction environments. Code repositories, housing code functions and other artifacts, are \\ntypically maintained in central artifact government environments for auditability purpose. This \\ndistributed nature of tool deployment presents a significant challenge: how do we effectively \\nmanage and utilize this diverse and dispersed collection of tools in a production setting?\\nFeature  \\\\  Tool Type Code Functions Private REST APIs Public REST APIs\\nLatency Very low Medium Potentially High\\nImplementation Easy Medium Relatively Simple\\nOwnership/Control Full Full within the VPC None\\nAuthentication None Enabled Requires API keys, OAuth\\nShareability Limited Sharable within VPC Accessible to all\\nMonitoring Custom Standard APIs Limited\\nVersion Control Repositories (e.g. Git) Repositories (e.g. Git) API provider dependent\\nSecurity Agent environment Strong within VPC Careful consideration\\nInternal Systems Direct access Access via network Generally not direct'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 49, 'page_label': '50'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n50\\nFebruary 2025\\nTo address this challenge, and given the variety of tool types and their locations, a \\ncentralized catalog becomes essential for managing tools in a production environment.  This \\nis where the concept of a Tool Registry comes into play.\\nTool Registry\\nA Tool Registry, a centralized catalog of all available tools, provides a standardized way \\nto discover, access, and manage these essential components. This centralized approach \\noffers several key advantages.  First, it promotes reusability by enabling easy discovery \\nand reuse of tools by different agents, significantly reducing development time and effort.  \\nSecond, it enhances shareability and visibility, making tools readily available to all \\nauthorized developers, fostering collaboration and knowledge sharing across teams.  Third, \\nit strengthens security and accessibility by enforcing access control and ensuring that \\nonly authorized agents utilize specific tools.  Fourth, the registry promotes standardization \\nin tool implementation and usage, which improves code maintainability and agent \\ninteroperability.  Fifth, it contributes to the robustness of the system through centralized \\nmanagement that allows for better monitoring, evaluation, and version control. Finally, the \\nTool Registry facilitates auditability by providing a clear record of tool usage, which is \\ncrucial for compliance and accountability.\\nTo deliver these advantages, the Tool Registry incorporates several key features. The Tool \\nRegistry stores essential information about each tool, including its name, description, \\nparameters, and output format, effectively serving as a comprehensive tool catalog.  It also \\nprovides robust version control, tracking different versions of tools to ensure compatibility \\nwith various agents.  The registry further simplifies the discovery process through effective \\nsearch and discovery functionalities, allowing developers to easily find the tools they need.  \\nFinally, it manages permissions through access control mechanisms, ensuring that only \\nauthorized agents can access specific tools.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 50, 'page_label': '51'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n51\\nFebruary 2025\\nThe Tool Registry mirrors the Model Registry in MLOps, highlighting its importance \\nfor operationalizing generative AI agents, just as Model Registries are essential for \\noperationalizing machine learning models.\\nTool Selection Strategies at Scale\\nThe Tool Registry serves as a comprehensive catalog of all available tools within an \\nenterprise, potentially housing hundreds or even thousands of options. However, simply \\nproviding a foundation model with access to this entire catalog isn\\'t always the most \\neffective approach.  Presenting the model with an excessively long list of tools can actually \\nbe counterproductive.  The model might become overwhelmed or confused, especially \\nif tools have overlapping functionalities or similar descriptions. This can lead to incorrect \\ntool selection, reduced performance, and unpredictable agent behavior.  Imagine trying to \\nchoose the right tool from a massive, disorganized toolbox – it becomes a time-consuming \\nand error-prone process.  Therefore, strategic tool selection is crucial for optimizing agent \\nperformance and reliability.\\nTo address this challenge, we can draw a parallel with microservices architecture. Just as \\nin microservices, where each service is designed for specific tasks, we can view generative \\nAI agents as specialized microservices. Each agent should ideally be equipped with only \\nthe subset of tools directly relevant to its specific responsibilities. This focused collection \\nof tools, a carefully curated \"Tool List,\" is a subset of the broader Tool Registry. Providing \\nagents with a limited toolset offers several key benefits:\\n• Improves performance by reducing the search space for tool selection. \\n• Increases predictability, making the agent\\'s behavior more understandable and easier to \\nanticipate. \\n• Simplifies testing and debugging, as the agent\\'s scope is more clearly defined.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 51, 'page_label': '52'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n52\\nFebruary 2025\\n• Enhances security by limiting the agent's potential impact on sensitive data or systems.\\nThree general strategies can be employed: providing full toolset access (creating a generalist \\nagent), providing a limited toolset (creating a specialist agent), or dynamic tool selection. \\nA generalist agent is granted access to the entire Tool Registry, relying on the foundation \\nmodel's reasoning abilities to select the appropriate tool. While this approach offers greater \\nflexibility, it can lead to the performance and predictability issues mentioned earlier. A \\nspecialist agent, conversely, is equipped with a carefully curated Tool List containing only the \\ntools necessary for its specific task. This strategy promotes performance, predictability, and \\nsecurity, but it requires more upfront design effort to define each agent's toolset.\\nThe third strategy, dynamic tool selection, represents a more emergent approach. In this \\nmodel, the agent doesn't have a predefined tool list. Instead, for each incoming task, the \\nagent queries the Tool Registry to identify and select the most relevant tools. This dynamic \\napproach allows the agent to adapt to a wider range of tasks without requiring a massive, \\npre-configured toolset.  However, this increased flexibility comes with its own set of \\nchallenges.  Because the tool selection process occurs at runtime and is dependent on the \\nspecific task, predicting the agent's behavior and ensuring consistent performance can be \\nmore difficult.\\nThe optimal strategy, whether to create generalist, specialist, or dynamically selecting \\nagents, depends on the specific needs and context of the enterprise. Factors to consider \\ninclude the total number of tools in the registry, the complexity of the tasks the agents are \\ndesigned to perform, the desired level of control over agent behavior, and the available \\nresources for agent design and maintenance.  Future work will involve releasing experimental \\nresults comparing these three strategies quantitatively to provide clearer guidance.  In \\nparticular, the dynamic selection strategy, being relatively new, requires further research to \\ndevelop robust testing and evaluation methodologies.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 52, 'page_label': '53'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n53\\nFebruary 2025\\nAgent Evaluation & Optimization\\nAs we have discussed, agents are a new and emerging type of AI system with the potential \\nto revolutionize human-computer interaction.  A crucial aspect of realizing this potential lies \\nin effectively evaluating and optimizing agent performance. While previous sections of this \\nwhite paper have addressed general GenAI evaluation, this section focuses specifically on \\nagent-centric evaluation and optimization techniques.\\nAgent evaluation can be structured into five key stages:\\n1. Tool Unit Testing and Refinement: During development, each tool available to the agent \\nundergoes rigorous unit testing. This process validates the tool's functionality and ensures \\nit performs as expected.  Crucially, this stage also involves refining the descriptions of \\nthe available functions and their parameters, which are what the foundation model uses \\nfor reasoning and selection.  Thorough unit testing and well-defined tool descriptions are \\nfoundational for reliable agent behavior.\\n2. Evaluation Dataset Creation: A representative evaluation dataset of potential \\nagent (multi-turn) scenarios is constructed.  This dataset can be created manually, by \\ncarefully crafting specific interaction sequences, or automatically, by recording real \\nuser interactions with the agent and then incorporating these recorded sessions into \\nthe evaluation set.  This automatic approach allows for continuous enrichment of the \\nevaluation dataset with real-world usage patterns.\\n3. Tool Selection Evaluation: Tool selection evaluation focuses on assessing the agent's \\nproficiency in choosing and utilizing the correct tools for given tasks. For an individual \\ntool, this involves measuring the percentage of successful tool selections, ensuring the \\naccuracy of parameter structuring and population for function calls, and confirming the \\nappropriate handling of scenarios where no tool selection is necessary. Collectively we \\nuse a trajectory evaluation, where we compare the set of steps we expect the agent to\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 53, 'page_label': '54'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n54\\nFebruary 2025\\ntake, including the tools the agent should use, against the steps an agent actually took.  \\nFrom a trajectory evaluation we can identify when an agent hallucinates (by not looking \\nup knowledge) or uses the wrong tool or getting stuck in a cul-de-sac.  These metrics \\ncollectively provide a comprehensive view of the agent\\'s effectiveness in leveraging its \\navailable tools.\\n4. Reasoning and Groundedness Evaluation: Beyond the standard gen AI end-to-end \\nevaluation (i.e., assessing the correctness and groundedness of the final answer given \\nan input), agent evaluation also involves assessing the reasoning process.  This includes \\nevaluating how the agent handles situations with missing data, how it recovers from errors, \\nand its ability to maintain context across multiple turns.  This stage examines the agent\\'s \\n\"cognitive\" abilities and its capacity to handle complex, multi-step interactions.\\n5. Operational Metric Evaluation: Finally, the agent\\'s performance is evaluated from \\nan operational perspective.  This involves measuring metrics such as latency (the time \\ntaken to generate a response or interact  with specific tools) and cost (the computational \\nresources consumed).  These metrics are essential for determining the practical viability \\nand scalability of the agent in a production environment.\\nBy systematically addressing these five stages, developers can gain a comprehensive \\nunderstanding of their agent\\'s strengths and weaknesses, enabling targeted optimization \\nand improvement.  This multi-faceted evaluation approach is critical for building robust and \\nreliable agent-based applications.\\nAgent optimization is an iterative process that focuses on refining the agent\\'s components \\nand configuration to enhance its performance and effectiveness. One key aspect of \\noptimization involves selecting the most appropriate set of tools for the agent\\'s tasks and \\nensuring that the function and parameter definitions within those tools are clear, concise, \\nand well-structured. This clarity enables the foundation model to effectively reason about \\nand utilize the available tools. Another crucial factor is selecting the right foundation model'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 54, 'page_label': '55'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n55\\nFebruary 2025\\nfor the agent's specific needs. Different foundation models may exhibit varying levels of \\nproficiency in reasoning and tool selection, so choosing a model that excels in these areas \\nis essential. Finally, optimizing the instruction prompt provided to the agent can significantly \\nimprove its performance. A well-crafted prompt provides clear guidance, sets expectations, \\nand helps the agent effectively navigate complex tasks. By iteratively refining these three \\nelements—tools, foundation model, and instruction prompt—developers can significantly \\nenhance the agent's capabilities and overall performance.\\nObservability and Memory\\nIn the realm of agent-based systems, observability and explainability are paramount.  These \\nqualities are essential for understanding agent behavior, building trust, and ensuring that \\nagents operate as intended.  Observability refers to the ability to gain insights into an agent's \\ninternal workings and its interactions with the environment.  Explainability, on the other hand, \\nfocuses on understanding the why behind an agent's decisions—why it selected a particular \\ntool or took a specific action.\\nAchieving observability and explainability requires careful consideration of the agent's \\nmemory and decision-making processes.  The agent's short-term and long-term memory \\nplay a crucial role in providing context and enabling traceability.  Being able to see events, \\norganized on a timeline, as related to the steps an agent takes is paramount.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 55, 'page_label': '56'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n56\\nFebruary 2025\\nFigure 14. Example of cloud trace for agents\\nShort-term memory (or conversation history) stores the ongoing conversation within a single \\nuser session. This includes the user's queries, the model's function calls, and the responses \\nfrom those function calls. This context is essential for the model to understand follow-up \\nquestions and maintain coherence throughout the interaction.  By examining the short-term \\nmemory, we can trace the sequence of interactions and understand how the agent arrived at \\na particular state or decision.\\nLong-term memory, on the other hand, stores information about past user interactions \\nacross multiple sessions. This allows the agent to learn user preferences, provide \\npersonalized recommendations, and offer more efficient service over time.  Long-term \\nmemory provides valuable insights into the agent's learning process and how its behavior \\nevolves over time.\\nSeveral implementation options exist for both short-term and long-term memory in agent \\nproductionization, each with its own trade-offs:\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 56, 'page_label': '57'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n57\\nFebruary 2025\\nShort-Term Memory Implementation Options:\\n• Logs: Storing interaction history as plain text logs can be sufficient for simple applications.\\n• Cloud Storage/Database: Offers more structured storage for complex applications.\\n• API Session: Managing conversation history on the client-side reduces server-side \\nstorage needs.\\n• Combination: A hybrid approach combining different mechanisms for \\noptimized performance.\\nLong-Term Memory Implementation Options:\\n• Vector Databases: Well-suited for storing and retrieving information based on \\nsemantic similarity.\\n• Metadata Storage/Graphs: Used to store session IDs, timestamps, and other \\nrelevant metadata.\\n• Cloud Storage/Databases: Provides a full record of all interactions.\\n• Combination: Combining different mechanisms for efficient storage and retrieval.\\nThe choice of implementation depends on the specific needs of the application and the \\ncomplexity of the agent's interactions.\\nBy effectively leveraging both short-term and long-term memory, developers can gain a \\ndeeper understanding of the agent's behavior, trace its decision-making process, and \\nidentify areas for improvement.  This granular level of observability and explainability is \\ncrucial for building trust in agent-based systems and ensuring their responsible deployment.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 57, 'page_label': '58'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n58\\nFebruary 2025\\nDeploying an Agent to Production \\nFigure 15. Agent and tool productionization using CI/CD pipelines\\nDeploying an agent to production requires a robust and automated process to ensure \\nconsistency, reliability, and scalability. Building upon a standardized repository structure, we \\ncan implement CI/CD pipelines that automate the tool registration process and streamline \\nthe deployment workflow. Such a pipeline typically involves several key stages. Initially, the \\npipeline validates that the repository structure adheres to defined standards, including \\nthe presence of required directories and files. It then executes code-level tests, such as \\nunit tests and static code analysis, to verify code quality and correctness. If necessary, \\nthe pipeline builds custom container images for the tool or agent, ensuring consistent \\ndeployment across environments.\\nThe pipeline then deploys the agent and its associated tools to a development environment \\nfor early testing and integration. This deployment can leverage infrastructure-as-code \\n(IaC) scripts within the agent's repository for fine-grained control over the environment\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 58, 'page_label': '59'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n59\\nFebruary 2025\\nor utilize a pre-defined, standardized infrastructure configuration for consistency and \\nsecurity. A manual approval gate at this stage allows for human oversight and verification \\nbefore proceeding.\\nAfter approval, the pipeline deploys the agent and tools to a staging environment that closely \\nmirrors production, enabling more rigorous testing under realistic conditions.  Here, human \\ntesters can engage with the agent at scale, further validating its performance and potentially \\naugmenting the evaluation data with real-world usage patterns. Automated evaluation scripts \\nand various tests, including integration and stress tests, are also executed, and the results \\nare stored centrally for analysis and governance. Another manual approval gate follows, \\nallowing for final verification based on the comprehensive testing results.\\nOnce approved, the pipeline deploys the agent to the production environment. Crucially, \\nduring this process, the tool registration occurs implicitly. Metadata extracted from the \\nrepository structure is automatically registered in the Tool Registry, eliminating manual \\neffort and ensuring consistency. Post-deployment, the agent's performance is continuously \\nmonitored using predefined scripts and configurations, enabling rapid detection and \\nresolution of any issues.\\nThis entire deployment lifecycle, from initial development to production deployment and \\nmonitoring, forms a continuous loop of improvement.  Insights gained from monitoring and \\nevaluation inform further refinements to the agent's components, tools, and configuration, \\nleading to iterative enhancements and updates that are then redeployed through the same \\npipeline. This iterative approach ensures that agents remain adaptable, effective, and aligned \\nwith evolving needs and objectives.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 59, 'page_label': '60'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n60\\nFebruary 2025\\nThis automated approach, with its standardized structure, rigorous testing, and implicit \\ntool registration, ensures a robust and efficient deployment process. The CI/CD pipeline, \\ncoupled with the centralized Tool Registry, provides a solid foundation for deploying and \\nmanaging agents in production, enabling organizations to leverage the full potential of these \\nintelligent systems.\\nOperations: People & Processes\\nProductionizing machine learning or gen AI solutions, requires more than just technology and \\nservices. It demands a well-orchestrated interplay of diverse individuals, each contributing \\nspecialized skills and expertise, working together seamlessly under standardized processes.  \\nLike a well-oiled machine, the effective operation of ML solutions hinges on the coordinated \\nefforts of various personas, forming a cohesive and efficient workflow.\\nFigure 16. Traditional MLOps landscape of people, process, and environments \\nIn the traditional predictive ML landscape, a typical setup involves several interconnected \\nteams, each with distinct roles and responsibilities:'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 60, 'page_label': '61'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n61\\nFebruary 2025\\n• Cloud Platform Team: This foundational team comprises cloud architects, administrators, \\nand security specialists. They are responsible for providing and managing the essential \\ncloud infrastructure, ensuring security, defining access control, and overseeing the overall \\ncloud strategy.\\n• Data Engineering Team: This team focuses on building and maintaining the data \\npipelines that fuel the AI models and solutions. The data engineers handle data ingestion, \\npreprocessing, and preparation, ensuring data quality, while data owners are responsible \\nfor the data accessibility for data scientists and other business stakeholders.\\n• Data Science and MLOps Team: This team includes data scientists who experiment with \\nand train models, as well as ML engineers who collaborate with data scientists to automate \\nthe ML pipeline, encompassing data preprocessing, model training, and post-processing \\nat scale. They leverage CI/CD pipelines for seamless model deployment and management. \\nWithin this team, the MLOps Engineer plays a vital role in building and maintaining the \\ninfrastructure (IAM roles and networking layer) for the Cloud Platform team  that supports \\nand standardizes these pipelines.\\n• Machine Learning Governance: This centralized function oversees the entire ML \\nlifecycle, acting as a repository for model metadata, development artifacts, and \\nperformance metrics. It ensures transparency, accountability, and compliance in AI \\ndevelopment and deployment. This is the single point of truth for product owner and \\nauditors to assess the ML solutions and models respectively.\\nThis collaborative structure ensures a smooth and controlled path from initial \\nexperimentation to production-ready ML solutions. However, gen AI introduces a new \\nlayer of complexity to the enterprise organizational landscape. This new layer, focused on \\nbuilding and deploying GenAI applications, necessitates specialized roles and expertise to \\neffectively harness the power of these advanced models.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 61, 'page_label': '62'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n62\\nFebruary 2025\\nFigure 17. From MLOps to GenAI application development landscape of people, process, and environments\\n• Prompt Engineers: These individuals wear two hats, blending technical expertise with \\ndomain knowledge.  First, they are skilled in the art of crafting and refining prompts, \\nunderstanding how different inputs influence model behavior and optimizing prompt \\ndesign for specific applications.  Second, they often possess deep subject matter \\nexpertise, allowing them to define the right questions and expected answers for eliciting \\ndesired knowledge from the model.  This dual role enables them to bridge the gap \\nbetween the technical intricacies of gen AI models and the specific needs of various \\ndomains.  We have seen data scientists, ML engineers, and even business analysts with \\ndeep domain knowledge successfully transition into prompt engineering roles.\\n• AI Engineers: AI engineers are responsible for scaling the use of generative models and \\ntransitioning gen AI solutions into production. They focus on building robust backend \\nsystems that incorporate functionalities like evaluation at scale, guardrails to ensure'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 62, 'page_label': '63'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n63\\nFebruary 2025\\nresponsible AI, and seamless integration with data sources (including RAG, data lakes, \\nand tools).  Their expertise lies in creating a stable and scalable foundation for gen \\nAI applications.\\n• DevOps/App Developers: These individuals leverage their software engineering skills \\nto develop the front-end components of gen AI applications. They work closely with AI \\nengineers to integrate the backend capabilities, ensuring a user-friendly and efficient \\ninterface for interacting with GenAI models.  Their focus is on delivering a seamless and \\nengaging user experience.\\nThe introduction of these specialized roles highlights the growing demand for expertise in \\napplying and deploying gen AI solutions effectively.  Prompt engineers, AI engineers, and \\nDevOps/App developers work in concert, forming a new operational layer dedicated to \\nbuilding and deploying the next generation of AI-powered solutions.\\nIt's important to acknowledge that the scale and structure of an organization will influence \\nthe specific roles and responsibilities within these teams. In smaller organizations, individuals \\nmay wear multiple hats, fulfilling the duties of several personas. As companies grow and their \\nGenAI initiatives mature, more specialized roles and dedicated teams may emerge to handle \\nthe increased complexity.\\nThe diagram below provides a more detailed illustration of the key operations and \\ninteractions within this organizational structure. A deeper dive into this diagram can provide \\nfurther insights into the specific processes and responsibilities associated with each role.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 63, 'page_label': '64'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n64\\nFebruary 2025\\nFigure 18. How multiple team collaborate to operationalize both models and GenAI applications \\nBy effectively coordinating these diverse roles and streamlining their interactions, \\norganizations can establish a robust operational foundation for both traditional ML and \\ngenerative AI initiatives. This collaborative approach, combined with well-defined processes, \\nis essential for successfully navigating the complexities of AI productionization and \\nmaximizing its potential benefits.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 64, 'page_label': '65'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n65\\nFebruary 2025\\nThe role of an AI platform for gen \\nAI operations\\nAlongside the explosion of both predictive and gen AI applications, AI platforms, like Vertex \\nAI,11 have emerged as indispensable tools for organizations seeking to leverage the power of \\nArtificial Intelligence (AI). These comprehensive platforms provide a unified environment that \\nstreamlines the entire AI lifecycle, from data preparation and model training to deployment, \\nautomation, continuous integration/continuous delivery (CI/CD), governance, and monitoring.\\nAt the heart of an AI platform lies its ability to support diverse AI development needs. Data \\nscientists and model builders need to create, evaluate, and tune predictive and generative \\nAI tools. Whether you seek to utilize pre-trained AI solutions, adapt existing models through \\ntuning or transfer learning, or embark on training your own large models, AI platforms provide \\nthe infrastructure and tools necessary to support these journeys. Application developers \\nand agent builders also need centralized and scalable access to models and services which \\nsupport them, like RAG, and observability support to bring MLOps to gen AI. The advent \\nof these platforms has revolutionized the way organizations approach AI, enabling them \\nto productionize AI applications in a secure, enterprise-ready, responsible, controlled and \\nscalable manner. These platforms accelerate innovation as well as foster reproducibility and \\ncollaboration while reducing costs and maximizing Return on Investment (ROI).\\nThe new gen AI paradigm discussed in prior sections demands a robust and reliable AI \\nplatform that can seamlessly integrate and orchestrate a wide range of functionalities.   \\nThese functionalities include model tuning for specific tasks; leveraging paradigms like \\nretrieval augmented generation3 (RAG) to connect to internal and external data sources; \\nand pre-training or instruction fine-tuning large models from scratch. Complex applications'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 65, 'page_label': '66'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n66\\nFebruary 2025\\nalso often require chaining with other models, such as classifiers to route inputs to the \\nappropriate LLM/ML model, extraction of customer information from a knowledge base, \\ninclusion of safety checks, or even creation of caching systems for cost optimization.  \\nFigure 19.  Key components of Vertex AI for gen AI\\nKey components of Vertex AI for gen AI\\nVertex AI eliminates the complexities of managing the entire infrastructure required for AI \\ndevelopment and deployment. Instead, Vertex AI offers a user-centric approach, providing \\non-demand access to the needed resources. This flexibility empowers organizations to \\nfocus on innovation and collaboration, rather than infrastructure management, and up-\\nfront hardware purchase. The features of Vertex AI that support gen AI development can be \\ngrouped into eight areas.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 66, 'page_label': '67'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n67\\nFebruary 2025\\nDiscover: Vertex Model Garden\\nAs discussed before, there is already a wide variety of available foundation models, trained \\non a broad range of datasets, and the cost of training a new foundation model can be \\nprohibitive. Thus it often makes sense for companies to adapt existing foundation models \\nrather than creating their own from scratch. As a result, a platform facilitating seamless \\ndiscovery and integration of diverse model types is critical.\\nVertex AI Model Garden1 supports these needs, offering a curated collection of over \\n150 Machine Learning and gen AI models from Google, Google partners, and the open-\\nsource community. It simplifies the discovery, customization, and deployment of both \\nGoogle’s proprietary foundational models and diverse open-source models across a \\nvast spectrum of modalities, tasks, and features. This comprehensive repository permits \\ndevelopers to leverage the collective research on artificial intelligence models within a single \\nstreamlined environment.\\nModel Garden encompasses a diverse range of modalities such as Language, Vision, Tabular, \\nDocument, Speech, Video, and Multimodal data. This broad coverage enables developers \\nto tackle a multitude of tasks, including generation, classification, regression, extraction, \\nrecognition, segmentation, tracking, translation, and embedding. Model Garden houses \\nGoogle’s proprietary and foundational models (like Gemini,27 PaLM 2,28 Imagen29) alongside \\nnumerous popular open source and third-party partner models like like Llama 3,30 T5 Flan,31 \\nBERT,32 Stable Diffusion,33 Claude 3 (Anthropic),34 and Mistral AI.35 Additionally, it offers task-\\nspecific models for occupancy analysis, watermark detection, text-moderation, text-to-video, \\nhand-gesture recognition, product identification, and tag recognition, among others. Every \\nmodel36 in Vertex Model Garden has a model card which includes a description of the model, \\nthe main use cases that can cover, and the option (if available) to tune the model or deploy \\nit directly.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 67, 'page_label': '68'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n68\\nFebruary 2025\\nModel Garden fosters experimentation by facilitating access to Google’s proprietary \\nfoundational models through the Vertex AI Studio UI,37 a playground where you can play \\naround with prompts, models, and open-source models using provided Colab notebooks. \\nOne-click deployment is available for some external models, and there are more than 40 \\nmodels available for fine-tuning for specific needs. Furthermore, the platform allows users to \\nleverage technologies like vLLM38 and quantization techniques for optimizing deployments for \\nefficiency and reduced costs. We present below an overview of some of the models in Model \\nGarden. For an up-to-date list, please visit.36'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 68, 'page_label': '69'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n69\\nFebruary 2025\\nModel Type Description Details\\nFirst-party models Foundation models\\nLeverage multimodal models \\nfrom Google across vision, \\ndialog, code generation, and \\ncode completion.\\nGemini39 family of models supporting \\ntext, image, audio, video streaming, \\ngiant cacheable context windows, \\ngrounding and function calling Imagen \\nfor text-to-image41\\nChip for high-quality  \\nspeech-to-text43\\nVeo for high quality text-to-video 43\\nCodey for code generation \\nand completion 42\\nFirst-party models Pre-trained APIs\\nBuild and deploy AI applications \\nfaster with our pre-trained APIs \\npowered by the best Google AI \\nresearch and technology.\\nText-to-Speech44\\nNatural Language processing 45\\nTranslation46\\nVision47\\nOpen  models Open source models\\nAccess a wide variety of \\nenterprise-ready open \\nsource models\\nGoogle’s Gemma, 48 family of \\nmodels including PaliGemma, 16 \\nCodeGemma 49, etc\\nMeta's Llama30\\nTII's Falcon50\\nMistral AI51\\nBERT,32 T-5 FLAN,31 ViT,52 EfficientNet53\\nThird-party models Third-party models\\nModel Garden will support third-\\nparty models from partners with \\nfoundation models.\\nAnthropic’s Claude 3 Haiku,  Sonnet \\nand Opus54,55\\nTable 3.  An overview of some of the models in Model Garden [Last Updated: March 18th, 2024]\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 69, 'page_label': '70'}, page_content=\"Operationalizing Generative AI on Vertex AI using MLOps\\n70\\nFebruary 2025\\nPrototype: Vertex AI Studio & Notebooks\\nRapid development and prototyping capabilities are also essential for developing gen AI \\napplications. Vertex AI prioritizes inclusivity and flexibility in its development environments, \\ncatering to a wide range of developer preferences and proficiency levels. This platform \\nprovides options for both console-driven and programmatic development workflows. Users \\ncan leverage the intuitive web interface for end-to-end application creation or utilize various \\nAPIs for deeper customization and control. These include the REST API56 and dedicated \\nSDKs for Python,57 NodeJS58 and Java,59 ensuring compatibility with diverse programming \\nlanguages and ecosystems. Developers can choose to use the tools and IDEs of their \\nchoice for interacting with the platform, or take advantage of Vertex-native tools like Vertex \\nColab Enterprise or Vertex Workbench to explore and experiment with code within familiar \\nnotebook environments.\\nVertex AI Studio60 provides a unified console-driven entry point to access and leverage the \\nfull spectrum of Vertex AI's gen AI services. It facilitates exploration and experimentation with \\nvarious Google first party foundation models (for example, PaLM 2, Gemini, Codey, Imagen, \\nand Universal Speech Model). Additionally, it offers prompt examples and functionalities \\nfor testing distinct prompts and models with diverse parameters. It’s also possible to adapt \\nexisting models through various techniques like supervised fine-tuning (SFT), reinforcement \\nlearning tuning techniques, and Distillation, and deploy gen AI applications in just a few \\nclicks. Vertex AI Studio considerably simplifies and democratizes gen AI adoption, catering \\nto a variety of users, from business analysts to machine learning engineers. You can see the \\nhomepage of Vertex AI Studio in Figure 13.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 70, 'page_label': '71'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n71\\nFebruary 2025\\nFigure 20. Vertex AI Studio - Homepage\\nCustomize: Vertex AI training & tuning \\nWhile prompt engineering and augmentation are sufficient for some gen AI use cases, other \\ncases require training, tuning and adapting the models to get the best results. Vertex AI \\nprovides a comprehensive platform for training and adapting LLMs, supporting a range of \\ntechniques and approaches from prompt engineering to training models from scratch.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 71, 'page_label': '72'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n72\\nFebruary 2025\\nTrain \\nFor full-scale LLM training, TPUs and GPUs are vital because of their superior processing \\npower and memory capacity compared to CPUs. GPUs excel at parallel processing, enabling \\nfaster model training. TPUs, specifically designed for machine learning tasks, offer even \\nfaster processing and higher energy efficiency. This makes them ideal for large-scale, \\ncomplex models. Google Cloud provides a range of offerings to support LLM training, \\nincluding TPU VMs with various configurations, pre-configured AI platforms like Vertex AI, \\nand dedicated resources like Cloud TPU Pods for scaling up training. These offerings allow \\nusers to choose the right infrastructure for their needs, accelerating LLM development and \\nenabling cutting-edge research and applications.\\nTune \\nVertex AI also provides a comprehensive solution for adapting pre-trained LLMs. It supports \\na spectrum of techniques from a non-technical prompt engineering playground at inference \\ntime, to data-driven approaches involving tuning, reinforcement learning and distillation \\nmethods during the development or adaptation phase. The following five techniques – many \\nof which are unique to Vertex AI – enable users to explore and implement them effectively. \\nThis applies to both proprietary and open-source LLMs, allowing you to achieve superior \\nresults while optimizing for costs and latency requirements.\\n• Prompt engineering61 leverages carefully crafted natural language prompts, potentially \\nchained and enriched with external knowledge and examples, to nudge the LLM towards \\ndesired outputs without necessitating further training. Vertex AI through Vertex AI Studio \\noffers a dedicated playground for crafting, testing, comparing and managing diverse \\nprompts and techniques. Users can access various pre-built prompt templates within the \\nplatform and leverage public prompting guidelines62 for Google’s proprietary large models.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 72, 'page_label': '73'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n73\\nFebruary 2025\\n• Supervised fine-tuning (SFT) 63 on Vertex AI facilitates model adaptation by leveraging a \\nset of labeled examples (even a few hundred is enough) to tune a model on specific tasks \\nand contexts within domain-specific datasets. The required examples resemble the one-\\nshot example structure employed in the construction of a prompt. This effectively extends \\nthe few-shot learning approach for enhanced optimization. This focused tuning enables \\nthe model to encode additional parameters in the model necessary for mimicking desired \\nbehaviors such as improved complex prompt comprehension, adaptation to specific \\noutput formats, correcting errors, and learning new tasks. The SFT tuning approach on \\nVertex AI, minimizes computational overhead and time while yielding an updated model \\nthat integrates the newly acquired parameters with the original model’s core parameters.\\n• Reinforcement learning with human feedback (RLHF) ,64 available on Vertex AI for \\nfoundational models like PaLM 2,and open-source models like T5 (s-xxl) and Llama2, \\nleverages human feedback to train large models to align with human preferences. This \\ntechnique is well-suited in complex tasks involving preference modeling and optimizes \\nLLMs on intricate, sequence-level objectives not easily addressed by traditional \\nsupervised fine-tuning. The process involves first training a reward model using a human \\npreference dataset, then utilizing it to score the output from the LLM, and finally applying \\nreinforcement learning to optimize the LLM. This approach is recognized as a key driver of \\nsuccess in conversational large language models.\\n• Distillation step-by-step20 is an advanced distillation technique transferring knowledge \\nfrom a significantly larger model (known as teacher model) to a smaller task-specific \\nmodel (known as student model), preserving important information while reducing model \\nsize. Step-by-Step Distillation20 surpasses common techniques by requiring significantly \\nless data. This method, accessible on Vertex AI,65 significantly reduces inference costs and \\nlatencies while minimizing performance impact in the resulting smaller LLM.66'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 73, 'page_label': '74'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n74\\nFebruary 2025\\nOrchestrate\\nAny training or tuning job you run can be orchestrated and then operationalized using Vertex \\nPipelines,13 a service that aims to simplify and automate the deployment, management, and \\nscaling of your ML workflows. \\nIt provides a platform for building, orchestrating, scheduling and monitoring complex and \\ncustom ML pipelines, enabling you to efficiently translate your models from prototypes \\nto production.\\nVertex Pipelines is also the platform behind all the managed tuning and evaluation services \\nfor the Google Foundation Models on Vertex AI. This ensures consistency as you can \\nconsume and extend those pipelines easily, without having to familiarize yourself with \\nmany services.\\nGetting started with Vertex Pipelines is simple: you define the pipeline’s step sequence in \\na Python file utilizing Kubeflow SDK.67 For further details and comprehensive onboarding, \\nconsult the official documentation.68'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 74, 'page_label': '75'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n75\\nFebruary 2025\\nChain & Augment: Vertex AI Grounding, Extensions, and RAG \\nbuilding blocks\\nBeyond training, tuning and adapting models and prompts directly, Vertex AI offers a \\ncomprehensive ecosystem for augmenting LLMs, to address the challenges of factual \\ngrounding and hallucination. The platform incorporates emerging techniques like RAG and \\nagent-based approaches.\\nLLM function calling69 empowers users by enhancing the capabilities of language models \\n(LLMs). It enables LLMs to access real-time data and interact with external systems, \\nproviding users with more accurate and up-to-date information. To do that, users need to \\nprovide function definitions such as description, inputs, outputs to the gen AI model. Instead \\nof directly executing functions, the LLM intelligently analyzes user requests and generates \\nstructured data outputs. These outputs propose which function to call and what arguments \\nto use. Gemini models regularly top function calling leaderboards and support multi-modal \\nand compositional function calling. Not all LLMs support function calling.\\nLLM sandbox & code interpreter allow LLMs to write code (usually python) and execute \\nthat code in a sandbox runtime on the server.  This can allow a model to build its own tools \\non the fly and execute tools in a different fashion than the function calling feature.  In some \\ncases this may outperform function calling.\\nLLM structured output allows LLMs to return outputs in JSON or adhering to a specific \\nschema of fields and allowed value types. This is extremely important to application and \\nagent developers who are stitching together multiple LLM outputs across the steps of an \\napplication, to enforce a data model and have the LLM respect it.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 75, 'page_label': '76'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n76\\nFebruary 2025\\nLLM context caching and context window are extremely important aspects of LLM \\nselection for any agent builder. A large context window allows a lot of flexibility, bringing \\nmore information to the LLM, about the session or task in scope, and about the skill or \\nknowledge in scope.  Caching a large context window allows a developer to provide \\nknowledge to LLM without having to consume all of the input tokens at runtime for every \\nexecution - resulting in faster and cheaper execution.  If you need more information than can \\nfit into your context window, or need speed or pricing optimizations beyond context caching, \\nyou need to consider RAG and Grounding.\\nRAG overcomes limitations by enriching prompts with data retrieved from vector databases, \\ncircumventing pre-training requirements and ensuring the integration of up-to-date \\ninformation. Agent-based approaches, popularized by ReAct prompting, leverage LLMs as \\nmediators interacting with tools like RAG systems, APIs, and custom extensions. Vertex AI \\nfacilitates this dynamic information source selection, enabling complex queries, real-time \\nactions, and the creation of multi-agent systems connected to vast information networks for \\nsophisticated query processing and real-time decision-making.\\nVertex AI Grounding5 helps users connect large models with verifiable information by \\ngrounding them to internal data corpora on Vertex AI Agent Builder70 or external sources \\nusing Google Search. This enables two key functionalities: verifying model-generated outputs \\nagainst internal or external sources and creating RAG systems using Google’s advanced \\nsearch capabilities that produce quality content grounded in your own or web search data. \\nVertex AI extensions6 manage the whole lifecycle of tools for agents and foundation \\nmodels.  Instead of function calling where the developer invokes an API as recommended by \\nthe LLM, using Vertex AI extensions allows the model to do so automatically. Google offers \\na set of 1st party extensions like Vertex AI Search7 (RAG) and Code Interpreter,71 (sandbox). \\nApigee Cloud Hub allows you to automatically turn any API spec into an extension.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 76, 'page_label': '77'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n77\\nFebruary 2025\\nVertex AI Agent Builder70 is a suite of products that allows you to quickly build gen AI \\napplications and agents, streamlining setup of Google quality search on your data wherever \\nit is stored, providing RAG to agents and applications. Agent Builder is integrated with \\nConversational Agents (playbooks) which is a no-code, natural language agent building \\nproduct.  All agents, no matter how they are built, will require knowledge provided by RAG \\nand search.\\nVertex AI Search  is an out-of-the-box, fully managed search and RAG provider built on \\nGoogle Search technologies, for your data. With it you are be able to easily ground your \\nagents by indexing a diverse range of data sources, including structured datastores such \\nus BigQuery, Spanner, Cloud SQL, unstructured sources like website content crawling and \\ncloud storage as well as connectors to Google drive and dozens of third party APIs. Vertex AI \\nSearch brings together a robust foundation of Google Search technologies, encompassing \\nsemantic search, hybrid search, semantic chunking, embedded image & table annotations, \\nranking, algorithms, query rewriting and user intent understanding. Under the hood it \\noptimizes each of these steps, embedding models, and ranking strategies. It abstracts away \\nthese complexities and allows users to simply specify their data source to initiate the gen \\nAI-powered agent.This approach is ideal for organizations seeking to build robust search \\nexperiences for standard use cases without extensive technical expertise, and this product \\nimplements search ready for any scale.\\nRAG Engine is a companion product which can orchestrate any set of components using \\nLlamaIndex like syntax which should be familiar for many developers.  This can assemble \\na bespoke RAG pipeline to build your own search engine, and still utilize many of the same \\nunderlying components which Vertex AI Search uses like the Document AI Layout Parser \\nor the Ranking API or Check Grounding API.  This approach is much more flexible, but still \\nprovides the orchestration and runtime for a DIY RAG implementation on any components in \\nany configuration.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 77, 'page_label': '78'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n78\\nFebruary 2025\\nVector databases are specialized systems for managing multi-dimensional data. This data, \\nencompassing images, text, audio, video, and other structured or unstructured formats, \\nis represented as vectors capturing its semantic meaning. Vector databases accelerate \\nsearching and retrieval within these high-dimensional spaces, enabling efficient tasks like \\nfinding similar images from billions or extracting relevant text snippets based on various \\ninputs. For a deeper dive into these topics, refer to 4 and 19. Vertex AI offers three flexible \\nsolutions for storing and serving embeddings at scale, catering to diverse use cases and \\nuser profiles.\\nVertex AI Vector Search7 is a highly scalable low-latency similarity search and fully \\nmanaged vector database scaling to billions of vector embeddings with auto-scaling. This \\ntechnology, built upon ScaNN72 (a Google-developed technology used in products like \\nSearch, YouTube, and Play), allows you to search from billions of semantically similar or \\nrelated items within your stored data. In the context of gen AI, the most common use cases \\nwhere Vertex Vector Search can be used are:\\n1. Finding similar items (either text or image) based solely on their semantic meaning, in \\nconjunction with an embedding model.\\n2. Creating a hybrid search approach that combines semantic and keyword or metadata \\nsearch to refine the results.\\n3. Extracting relevant information from the database to feed into LLMs, enabling them to \\ngenerate more accurate and informed responses.\\nVertex AI Vector Search primarily functions as a vector database for storing pre-generated \\nembeddings. These embeddings must be created beforehand using separate models like \\nVertex Embedding models73 (namely textembedding-gecko, text-embedding-gecko-\\nmultilingual, or multimodalembedding). Choosing Vertex Vector Search is optimal \\nwhen you require control over aspects like the chunk, retrieval, query and models strategy.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 78, 'page_label': '79'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n79\\nFebruary 2025\\nThis includes fine-tuning an embedding model for your specific data. However, if your use \\ncase is a standard one requiring little customization, a readily available solution like Vertex \\nSearch might be a better choice.\\nVertex AI Feature Store74 is a centralized and fully managed repository for ML features \\nand embedding. It enables teams to share, serve, and reuse machine learning features and \\nembeddings effortlessly alongside other data. Its native BigQuery23 integration eliminates \\nduplication, simplifies lineage tracking and preserves data governance. Vertex AI Feature \\nStore supports offline retrieval and an easy and fast online serving for machine learning \\nfeatures and embeddings. Vertex AI Feature Store is a good choice when you want to iterate \\nand maintain different embedding versions alongside other machine learning features in a \\nsingle place.\\nVertex AI offers the flexibility to seamlessly create and connect various products to build \\nyour own custom grounding, RAG, and Agent systems. This includes utilizing diverse \\nembedding models (multimodal, multilingual), various vector stores (Vector Search, Feature \\nStore) and search engines like Vertex AI Agent Builder, extensions, grounding, and even SQL \\nquery generation for complex natural language queries. Moreover, Vertex AI provides SDK \\nintegration with LangChain9 to easily build and prototype applications using the umbrella \\nof Vertex AI products. For further details and integration information, consult the official \\ndocumentation75 and official examples.76\\nEvaluate: Vertex AI Experiments, Tensorboard, & \\nevaluation pipelines\\nIn the dynamic world of gen AI, experimentation and evaluation are the cornerstones of \\niterative development and continuous improvement. With a multitude of variables influencing \\nGen AI models (prompt engineering, model selection, data interaction, pretraining,'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 79, 'page_label': '80'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n80\\nFebruary 2025\\nand tuning), evaluation goes hand-in-hand with experimentation. The more seamlessly \\nexperiments and evaluations can be integrated into the development process, the \\nsmoother and more efficient the overall development becomes. Vertex AI provides cohesive \\nexperimentation and evaluation products permitting connected iterations over applications \\nand models alongside their evaluations.\\nExperiment\\nThe process of selecting, creating, and customizing machine learning (including large \\nmodels) and its applications involves significant experimentation, collaboration, and iteration. \\nVertex AI seamlessly integrates experimentation and collaboration into the development \\nlifecycle of AI/ML and gen AI models and applications. Its Workbench Instances77 provide \\nJupyter-based development environments for the entire data science workflow, connected \\nto other Google Cloud services and with GitHub synchronization capabilities. Vertex Colab \\nEnterprise78 accelerates the AI workflow by enabling collaborative coding and leveraging \\ncode completion and generation features.\\nVertex AI also provides two tools for tracking and visualizing the output of many experiment \\ncycles and training runs. Vertex AI Experiments79 facilitates meticulous tracking and \\nanalysis of model architectures, hyperparameters, and training environments. It logs \\nexperiments, artifacts, and metrics, enabling comparison and reproducibility across multiple \\nruns. This comprehensive tracking permits data scientists to select the optimal model \\nand architecture for their specific use case. Vertex AI TensorBoard80 complements the \\nexperimentation process by providing detailed visualizations for tracking, visualizing, and \\nsharing ML experiments. It offers a range of visualizations, including loss and accuracy \\nmetrics tracking, model computational graph visualization, and weight and bias histograms,'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 80, 'page_label': '81'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n81\\nFebruary 2025\\nwhich - for example - can be used for tracking various metrics pertaining to training and \\nevaluation of gen AI models with different prompting and tuning strategies. It also projects \\nembeddings to lower-dimensional space, and displays image, text, and audio samples.\\nEvaluation\\nVertex AI also provides a comprehensive set of evaluation tools for gen AI, from ground truth \\nmetrics to using LLMs as raters. \\nFor Ground Truth-based metrics, Automatic Metrics in Vertex AI81 lets you evaluate a model \\nbased on a defined task and “ground truth” dataset. For LLM-based evaluation, pairwise and \\npointwise model-based evaluation in Vertex AI82 uses a large model to evaluate the output \\nof multiple models or configurations being tested, helping to augment human evaluation at \\nscale. \\nIn addition to that, users can also leverage a set of pre-built metrics for evaluating gen AI \\napplications and relative SDK, integrated into the Vertex AI Python SDK for rapid and flexible, \\nnotebook-based, prototyping. To get started with Evaluation Vertex AI SDK see example in \\nthe official documentation.83\\nPredict: Vertex AI endpoints & monitoring\\nOnce developed, a production gen AI application must be deployed, including all its model \\ncomponents. If the application uses any models that have been trained or adapted, those \\nmodels need to be deployed to their own serving endpoints. You can serve any model in the \\nModel Garden through Vertex AI Endpoints21,which acts as the gateway for deploying your \\ntrained machine learning models. They allow you to serve online predictions with low latency,'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 81, 'page_label': '82'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n82\\nFebruary 2025\\nmanage access controls, and monitor model performance easily through Model Monitoring. \\nEndpoints also offer scaling options to handle varying traffic demands, ensuring optimal user \\nexperience and reliability.\\nAlong with the prediction service, Vertex AI offers the following features for all Google \\nmanaged models:\\n• Citation checkers: Gen AI on Vertex performs Citation checks71. Citations are important \\nfor LLMs and gen AI for several reasons. Citing sources ensures proper acknowledgment \\nof sources and prevents plagiarism and demonstrates transparency and accountability. \\nCiting sources is essential for LLMs and gen AI also because they help identify, \\nunderstand potential biases, and enable reproducibility and verification. For example in \\nGoogle Cloud,84 the gen AI models are designed to produce original content, limiting the \\npossibility of copying existing contents. If this happens, Google Cloud provides quotes for \\nwebsites and code repositories. \\n• Safety scores: Safety attributes are crucial for LLMs and gen AI to mitigate potential \\nrisks like bias, lack of explainability, and misuse. These attributes help detect and mitigate \\nbiased outputs and mitigate misuse, enabling these tools to be used responsibly. As \\nLLMs and gen AI evolve, incorporating safety attributes will be increasingly essential for \\nresponsible and ethical use. For example, Google Cloud added safety scores in Vertex \\nAI PaLM API and Vertex AI Gemini API85: content processed through the API is checked \\nagainst a list of safety attributes, including \"harmful categories\" and sensitive topics. Each \\nattribute has a confidence score between 0.0 and 1.0, indicating the likelihood of the \\ninput belonging to that category. These safety filters can be used in conjunction with all \\nmodels: be it proprietary ones like Palm2 and Gemini or OSS ones like the ones available in \\nModel garden.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 82, 'page_label': '83'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n83\\nFebruary 2025\\n• Watermarking: With AI-based tools becoming increasingly popular for creation of \\ncontent, it’s very important to identify if an image has been created using AI. Vertex AI \\noffers digital watermarking and verification for AI-generated images86 using the algorithm \\nSynthID87 developed by Google DeepMind.\\n• Content moderation and bias detection: By using the Content moderation88 and Bias89 \\ndetection tools on Vertex AI, you can add an extra layer of security on the responses \\nof the LLMs to mitigate the risk that the model training and tuning may sway a model to \\ngenerate outputs that aren’t fair or appropriate for the task.\\nGovern: Vertex AI Feature Store, Model Registry, \\nand Dataplex\\nAddressing the multifaceted requirements of data and model lineage and governance in \\ngen AI requires a comprehensive strategy that tackles both conventional challenges and \\nnovel regulatory or technical complexities associated with large models. By adopting robust \\ngovernance, observability, and lineage practices in the development of gen AI solutions, \\norganizations can ensure comprehensive tracking, iteration, and evolution of data. They \\ncan also track the large models used, prompt adaptations, tuning, and other artifacts. This \\nfacilitates reproducibility of results, transparency and understanding of generated content \\nsources, troubleshooting, compliance enforcement, and enhanced reliability and security. \\nThese practices collectively enable the ethical and responsible development and deployment \\nof gen AI solutions. This fosters internal and external trust and fairness in gen AI models and \\npractices. Vertex AI and Google Cloud offer the following comprehensive suite of tools for \\nunified lineage, governance and monitoring, effectively addressing these critical concerns.\\nIn the context of governance and lineage, Vertex AI Feature Store74 offers:\\n• Track feature and embeddings versions and lineage, ensuring transparency'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 83, 'page_label': '84'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n84\\nFebruary 2025\\n• Monitor feature (prompt) and embedding, response drift, and identify potential \\nissues proactively\\n• Store feature formulas and discover relevant features or embeddings for different \\nuse cases\\n• Utilize feature selection algorithms to optimize model performance\\n• Consolidate and unify all machine learning data within a singular repository encompassing \\nnumerical data, categorical data, textual data, and embeddings representations\\nVertex AI Model Registry12 serves as a centralized repository for comprehensive lifecycle \\nmanagement of both Google proprietary foundational and open-source Machine Learning \\nmodels. This includes gen AI models in addition to predictive models. This unified platform \\nenables registration, storage, and version control of diverse model types, including various \\niterations of tuning for large models. Vertex AI Model Registry seamlessly integrates with \\nVertex Pipelines,13 facilitating orchestration and management of training and tuning jobs \\nwhile leveraging lineage capabilities for recording and documenting the lineage from \\ndatasets to models and associated artifacts. It also couples with Vertex AI Experiments79 \\nand Vertex AI Model Evaluation,90 enabling performance monitoring and comparison of \\ndifferent model versions alongside their artifacts – all within a single interface. Furthermore, \\nVertex AI Model Registry bolsters observability by providing integrated configuration and \\naccess to Vertex AI Model Monitoring91 and logging functionalities. This enables proactive \\nidentification and mitigation of both training-serving skew and prediction drift, ensuring \\nreliability and accuracy of deployed models. Users can directly assign desired model versions \\nto endpoints for one-click deployment from Vertex Model Registry or leverage aliases for \\nsimplified deployment.\\nGoogle Cloud Dataplex14 provides an organization-wide lineage across product boundaries \\nin Google Cloud. Within the domains of AI and gen AI (and more broadly across data analytics \\nand AI/ML) Dataplex seamlessly integrates with BigQuery and Vertex AI. Dataplex facilitates'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 84, 'page_label': '85'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n85\\nFebruary 2025\\nthe unification, management, discovery, and governance of both data and models. Through \\ncomprehensive data lineage, quality, and metadata management capabilities it provides \\nactionable insights for comprehensive data and model understanding. This promotes \\ncompliance, facilitates data analysis, and guarantees the training of machine learning \\nmodels on trusted data sources. This in turn leads to enhanced accuracy and reliability. This \\nintegration permits users across an organization to identify ‘champion models’ and ‘golden \\ndatasets and features’ across projects and regions in a secure way by adhering to identity \\naccess management (IAM)92 boundaries. In short, Dataplex encapsulates a framework within \\nan organization that governs the interaction between people, processes and technology \\nacross all the products in Google Cloud.\\nSummary\\nThe explosion of gen AI in the last several years introduced fundamental changes in the way \\nAI applications are developed – but far from upending the MLOps discipline, these changes \\nhave only reinforced its basic principles and processes. As we have seen, the principles of \\nMLOps that emphasize reliability, repeatability, and dependability in ML systems development \\nare comfortably extended to include the innovations of gen AI. Some of the necessary \\nchanges are deeper and more far-reaching than others, but nowhere do we find any change \\nthat MLOps cannot accommodate.\\nAs a result, many tools and processes built to support traditional MLOps can also support \\nthe requirements of gen AI. Vertex AI, for instance, is a powerful platform that can be used to \\nbuild and deploy machine learning models and AI applications. It provides a comprehensive \\nsuite of functions for developing both Predictive and gen AI systems, encompassing data \\npreparation, pre-trained APIs, AutoML capabilities, training and serving hardware,  advanced \\nfine-tuning techniques and deployment tools, and a diverse selection of proprietary and'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 85, 'page_label': '86'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n86\\nFebruary 2025\\nopen-source foundation models. It also offers evaluation methods, monitoring capabilities, \\nand governance tools, all unified within a single platform to streamline the AI development \\nlifecycle. It’s built on Google Cloud Platform, which provides a scalable, reliable, secure and \\ncompliant infrastructure for machine learning. It’s a good choice for organizations that want \\nto build and deploy machine learning models and AI applications. \\nThe next few years will undoubtedly see gen AI extended in directions that today are \\nunimaginable. Regardless of the direction these developments take, it will continue to \\nbe important to build on solid engineering processes that embody the basic principles \\nof MLOps. These principles support the development of scalable, robust production AI \\napplications today, and no doubt will continue to do so into the future.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 86, 'page_label': '87'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n87\\nFebruary 2025\\nEndnotes\\n1. Model Garden on Vertex AI. Available at: https://cloud.google.com/model-garden\\n2. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, \\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, \\nJeff Dean, William Fedus. 2022. Emergent Abilities of Large Language Models. Available at: https://arxiv.org/\\npdf/2206.07682.pdf\\n3. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich \\nKüttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela. 2022. Retrieval-Augmented \\nGeneration for Knowledge-Intensive NLP Tasks. Available at: https://arxiv.org/pdf/2005.11401.pdf\\n4. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, Department of \\nComputer Science, Princeton University, Google Research, Brain team, REACT: SYNERGIZING REASONING AND \\nACTING IN LANGUAGE MODELS. Available at: https://arxiv.org/pdf/2210.03629.pdf\\n5. Grounding in Vertex AI. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/grounding/\\nground-language-models\\n6. Vertex Extensions. Connect models to APIs by using extensions. Available at: https://cloud.google.com/\\nvertex-ai/docs/generative-ai/extensions/overview\\n7. Overview of Vertex AI Vector Search. Available at: https://cloud.google.com/vertex-ai/docs/vector-search/\\noverview\\n8. What is Vertex AI Agent Builder? Available at: https://cloud.google.com/generative-ai-app-builder/docs/\\nintroduction\\n9. LangChain. Get your LLM application from prototype to production. Available at: https://www.langchain.\\ncom/\\n10. Introduction to the Vertex AI SDK for Python. Available at: https://cloud.google.com/vertex-ai/docs/python-\\nsdk/use-vertex-ai-python-sdk\\n11. Introduction to Vertex AI. Available at: https://cloud.google.com/vertex-ai/docs/start/introduction-unified-\\nplatform\\n12. Introduction to Vertex AI Model Registry. Available at: https://cloud.google.com/vertex-ai/docs/model-\\nregistry/introduction'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 87, 'page_label': '88'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n88\\nFebruary 2025\\n13. Introduction to Vertex AI Pipelines. Available at: https://cloud.google.com/vertex-ai/docs/pipelines/\\nintroduction\\n14. Dataplex. Available at: https://cloud.google.com/dataplex\\n15. BigQuery. Available at: https://cloud.google.com/bigquery?hl=en\\n16. PaLi-Gemma model card. Available at: https://ai.google.dev/gemma/docs/paligemma/model-card\\n17. Version Control. Available at: https://en.wikipedia.org/wiki/Version_control\\n18. Continuous integration. Available at: https://wikipedia.org/wiki/Continuous_integration\\n19. TFX is an end-to-end platform for deploying production ML pipelines. Available at: https://www.tensorflow.\\norg/tfx\\n20. Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay \\nKrishna, Chen-Yu Lee, Tomas Pfister. 2023. Distilling Step-by-Step! Outperforming Larger Language Models with \\nLess Training Data and Smaller Model Sizes. Available at: https://arxiv.org/pdf/2305.02301.pdf\\n21. Vertex Endpoints. Use private endpoints for online prediction. Available at: https://cloud.google.com/vertex-\\nai/docs/predictions/using-private-endpoints\\n22. Tuan Duong Nguyen, Marthinus Christoffel du Plessis, Takafumi Kanamori, Masashi Sugiyama, 2014. \\nConstrained Least-Squares Density-Difference Estimation. Available at: https://www.ms.k.u-tokyo.ac.jp/\\nsugi/2014/CLSDD.pdf\\n23. Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Schölkopf, Alexander Smola, 2012. A Kernel \\nTwo-Sample Test. Available at: https://jmlr.csail.mit.edu/papers/v13/gretton12a.html\\n24. Oliver Cobb, Arnaud Van Looveren, 2022. Context-Aware Drift Detection. Available at: https://arxiv.org/\\npdf/2203.08644.pdf\\n25. Google Gemma Model. Available at: https://gemini.google.com/\\n26. Perform metrics-based evaluation. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/\\nmodels/evaluate-models\\n27. Gemini Team, Google, 2023. Gemini: A Family of Highly Capable Multimodal Models. Available at: https://\\nstorage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf\\n28. Anil, Dai et al., 2023. PaLM 2 Technical Report. Available at: https://arxiv.org/abs/2305.10403'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 88, 'page_label': '89'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n89\\nFebruary 2025\\n29. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed \\nGhasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David \\nJ Fleet, Mohammad Norouzi, 2022. Photorealistic Text-to-Image Diffusion Models with Deep Language \\nUnderstanding. Available at: https://arxiv.org/abs/2205.11487\\n30. Build the future of AI with Meta Llama 3. Available at: https://llama.meta.com/llama3\\n31. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, \\nMostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun \\nChen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, \\nGaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff \\nDean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, Jason Wei. 2022. Scaling Instruction-Finetuned \\nLanguage Models. Available at: https://arxiv.org/abs/2210.11416\\n32. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 2018. BERT: Pre-training of Deep \\nBidirectional Transformers for Language Understanding. Available at: https://arxiv.org/abs/1810.04805\\n33. Stable Diffusion. Available at: https://github.com/CompVis/stable-diffusion\\n34. Vertex AI Function Calling. Available at: https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/\\nfunction-calling\\n35. Mistral AI. Available at: https://mistral.ai/\\n36. Models available in Model Garden. Available at: https://cloud.google.com/vertex-ai/docs/start/explore-\\nmodels#available-models\\n37. Vertex AI Studio. Customize and deploy generative models. Available at: https://cloud.google.com/\\ngenerative-ai-studio\\n38. vLLM. Easy, fast, and cheap LLM serving for everyone. Available at: https://github.com/vllm-project/vllm\\n39. Overview of multimodal models. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/\\nmultimodal/overview\\n40. Text models. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text\\n41. Imagen on Vertex AI | AI Image Generator. Available at: https://cloud.google.com/vertex-ai/docs/\\ngenerative-ai/image/overview\\n42. Code models overview. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-\\nmodels-overview'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 89, 'page_label': '90'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n90\\nFebruary 2025\\n43. Convert speech to text. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/speech/\\nspeech-to-text\\n44. Text-to-Speech AI. Available at: https://cloud.google.com/text-to-speech\\n45. Natural Language AI. Available at: https://cloud.google.com/natural-language\\n46. Translate docs, audio, and videos in real time with Google AI. Available at: https://cloud.google.com/\\ntranslate\\n47. Vision AI. Available at: https://cloud.google.com/vision\\n48. Git. Available at: https://git-scm.com/\\n49. CodeGemma model card. Available at: https://ai.google.dev/gemma/docs/codegemma/model_card\\n50. TII’s Falcon. Available at: https://falconllm.tii.ae/\\n51. Mistral AI. Available at: https://mistral.ai/\\n52. Hugging Face, 2024. Vision Transformer (ViT) Documentation. Hugging Face, [online] Available at:   \\nhttps://huggingface.co/docs/transformers/en/model_doc/vit\\n53. Mingxing Tan, Quoc V. Le, 2019. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. \\nAvailable at: https://arxiv.org/abs/1905.11946\\n54. Anthropic Claude 3. Available at: https://www.anthropic.com/news/claude-3-haiku\\n55. Anthropic Claude 3 on Google Cloud Model Garden. Available at: https://cloud.google.com/blog/products/\\nai-machine-learning/announcing-anthropics-claude-3-models-in-google-cloud-vertex-ai\\n56. Vertex AI API. Available at: https://cloud.google.com/vertex-ai/docs/reference/rest\\n57. Vertex AI: Python SDK. Available at: https://cloud.google.com/python/docs/reference/aiplatform/latest/\\nvertexai\\n58. Vertex AI: Node.js Client. Available at: https://cloud.google.com/nodejs/docs/reference/aiplatform/latest/\\noverview\\n59. Vertex AI for Java. Available at: https://cloud.google.com/java/docs/reference/google-cloud-aiplatform/\\nlatest/overview\\n60. Customize and deploy generative models. Available at: https://cloud.google.com/generative-ai-studio'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 90, 'page_label': '91'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n91\\nFebruary 2025\\n61. Design text prompts. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/text/text-\\nprompts\\n62. Introduction to prompt design. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/learn/\\nintroduction-prompt-design\\n63. Supervised tuning. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-\\nmodels#supervised-tuning\\n64. RLHF model tuning. Available at: https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-text-\\nmodels-rlhf\\n65. Vertex AI Distilation. Available at: https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-\\ntext-models\\n66. Create distilled text models. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/models/\\ndistill-text-models\\n67. Pipeline Basics. Available at: https://www.kubeflow.org/docs/components/pipelines/v2/pipelines/pipeline-\\nbasics/\\n68. Build a pipeline. Available at: https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline\\n69. Vertex AI Search extension. Available at: https://cloud.google.com/vertex-ai/generative-ai/docs/\\nextensions/vertex-ai-search\\n70. What is Vertex AI Agent Builder? Available at: https://cloud.google.com/generative-ai-app-builder/docs/\\nintroduction\\n71. Generative AI on Vertex AI, Citation Check. Available at: https://cloud.google.com/vertex-ai/generative-ai/\\ndocs/learn/overview#citation_check\\n72. Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar, 2020. \\nAccelerating Large-Scale Inference with Anisotropic Vector Quantization. Available at: https://arxiv.org/\\npdf/1908.10396.pdf\\n73. Get text embeddings. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/\\nget-text-embeddings\\n74. About Vertex AI Feature Store. Available at: https://cloud.google.com/vertex-ai/docs/featurestore/latest/\\noverview'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 91, 'page_label': '92'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n92\\nFebruary 2025\\n75. Google Cloud Vertex AI. Available at: https://python.langchain.com/docs/integrations/llms/google_vertex_\\nai_palm\\n76. Generative AI - Language - LangChain. Available at: https://github.com/GoogleCloudPlatform/generative-\\nai/tree/main/language/orchestration/langchain\\n77. Introduction to Vertex AI Workbench, Workbench Instances. Available at: https://cloud.google.com/vertex-\\nai/docs/workbench/introduction\\n78. Introduction to Colab Enterprise. Available at: https://cloud.google.com/colab/docs/introduction\\n79. Introduction to Vertex AI Experiments. Available at: https://cloud.google.com/vertex-ai/docs/experiments/\\nintro-vertex-ai-experiments\\n80. Vertex AI TensorBoard Introduction to Vertex AI TensorBoard. Available at https://cloud.google.com/vertex-\\nai/docs/experiments/tensorboard-introduction\\n81. Perform metrics-based evaluation. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/\\nmodels/evaluate-models\\n82. Perform automatic side-by-side evaluation. Available at: https://cloud.google.com/vertex-ai/docs/\\ngenerative-ai/models/side-by-side-eval\\n83. Rapid Evaluation Vertex AI. Available at: https://cloud.google.com/vertex-ai/generative-ai/docs/models/\\nrapid-evaluation\\n84. Citation metadata. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-\\nai#citation_metadata\\n85. Responsible AI. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-\\nai#filters-palm-api\\n86. Imagen on Vertex AI | AI Image Generator. Available at: https://cloud.google.com/vertex-ai/docs/\\ngenerative-ai/image/overview\\n87. SynthID. Identifying AI-generated content with SynthID. Available at: https://deepmind.google/technologies/\\nsynthid/\\n88. Moderate text. Available at: https://cloud.google.com/natural-language/docs/moderating-text\\n89. Model bias metrics for Vertex AI. Available at: https://cloud.google.com/vertex-ai/docs/evaluation/model-\\nbias-metrics'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 92, 'page_label': '93'}, page_content='Operationalizing Generative AI on Vertex AI using MLOps\\n93\\nFebruary 2025\\n90. Model evaluation in Vertex AI. Available at: https://cloud.google.com/vertex-ai/docs/evaluation/\\nintroduction\\n91. Introduction to Vertex AI Model Monitoring. Available at: https://cloud.google.com/vertex-ai/docs/model-\\nmonitoring/overview\\n92. Identity and Access Management (IAM). Available at: https://cloud.google.com/iam/docs\\n93. Agents. Available at: https://www.kaggle.com/whitepaper-agents'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 0, 'page_label': '1'}, page_content='Prompt  \\nEngineering\\nAuthor: Lee Boonstra'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 1, 'page_label': '2'}, page_content='Prompt Engineering\\nFebruary 2025\\n2\\nAcknowledgements\\nContent contributors\\nMichael Sherman\\nYuan Cao\\nErick Armbrust\\nAnant Nawalgaria\\nAntonio Gulli\\nSimone Cammel\\nCurators and Editors\\nAntonio Gulli\\nAnant Nawalgaria\\nGrace Mollison \\nTechnical Writer\\nJoey Haymaker\\nDesigner\\nMichael Lanning'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 2, 'page_label': '3'}, page_content='Introduction 6\\nPrompt engineering 7\\nLLM output configuration 8\\nOutput length 8\\nSampling controls 9\\nTemperature 9\\nTop-K and top-P 10\\nPutting it all together 11\\nPrompting techniques 13\\nGeneral prompting / zero shot 13\\nOne-shot & few-shot 15\\nSystem, contextual and role prompting 18\\nSystem prompting 19\\nRole prompting 21\\nContextual prompting 23\\nTable of contents'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 3, 'page_label': '4'}, page_content='Step-back prompting 25\\nChain of Thought (CoT) 29\\nSelf-consistency 32\\nTree of Thoughts (ToT) 36\\nReAct (reason & act) 37\\nAutomatic Prompt Engineering 40\\nCode prompting 42\\nPrompts for writing code 42\\nPrompts for explaining code 44\\nPrompts for translating code 46\\nPrompts for debugging and reviewing code 48\\nWhat about multimodal prompting? 54\\nBest Practices 54\\nProvide examples 54\\nDesign with simplicity 55\\nBe specific about the output 56\\nUse Instructions over Constraints 56\\nControl the max token length 58\\nUse variables in prompts 58\\nExperiment with input formats and writing styles 59\\nFor few-shot prompting with classification tasks, mix up the classes 59\\nAdapt to model updates 60\\nExperiment with output formats 60'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 4, 'page_label': '5'}, page_content='JSON Repair 61\\nWorking with Schemas 62\\nExperiment together with other prompt engineers 63\\nCoT Best practices 64\\nDocument the various prompt attempts 64\\nSummary 66\\nEndnotes 68'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 5, 'page_label': '6'}, page_content='Prompt Engineering\\nFebruary 2025\\n6\\nIntroduction\\nWhen thinking about a large language model input and output, a text prompt (sometimes \\naccompanied by other modalities such as image prompts) is the input the model uses \\nto predict a specific output. You don’t need to be a data scientist or a machine learning \\nengineer – everyone can write a prompt. However, crafting the most effective prompt can be \\ncomplicated. Many aspects of your prompt affect its efficacy: the model you use, the model’s \\ntraining data, the model configurations, your word-choice, style and tone, structure, and \\ncontext all matter. Therefore, prompt engineering is an iterative process. Inadequate prompts \\ncan lead to ambiguous, inaccurate responses, and can hinder the model’s ability to provide \\nmeaningful output.\\nYou don’t need to be a data \\nscientist or a machine learning \\nengineer – everyone can write \\na prompt.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 6, 'page_label': '7'}, page_content='Prompt Engineering\\nFebruary 2025\\n7\\nWhen you chat with the Gemini chatbot,1 you basically write prompts, however this \\nwhitepaper focuses on writing prompts for the Gemini model within Vertex AI or by using  \\nthe API, because by prompting the model directly you will have access to the configuration \\nsuch as temperature etc.\\nThis whitepaper discusses prompt engineering in detail. We will look into the various \\nprompting techniques to help you getting started and share tips and best practices to \\nbecome a prompting expert. We will also discuss some of the challenges you can face  \\nwhile crafting prompts.\\nPrompt engineering\\nRemember how an LLM works; it’s a prediction engine. The model takes sequential text as \\nan input and then predicts what the following token should be, based on the data it was \\ntrained on. The LLM is operationalized to do this over and over again, adding the previously \\npredicted token to the end of the sequential text for predicting the following token. The next \\ntoken prediction is based on the relationship between what’s in the previous tokens and what \\nthe LLM has seen during its training. \\nWhen you write a prompt, you are attempting to set up the LLM to predict the right sequence \\nof tokens. Prompt engineering is the process of designing high-quality prompts that guide \\nLLMs to produce accurate outputs. This process involves tinkering to find the best prompt, \\noptimizing prompt length, and evaluating a prompt’s writing style and structure in relation \\nto the task. In the context of natural language processing and LLMs, a prompt is an input \\nprovided to the model to generate a response or prediction.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 7, 'page_label': '8'}, page_content='Prompt Engineering\\nFebruary 2025\\n8\\nThese prompts can be used to achieve various kinds of understanding and generation \\ntasks such as text summarization, information extraction, question and answering, text \\nclassification, language or code translation, code generation, and code documentation \\nor reasoning.\\nPlease feel free to refer to Google’s prompting guides2,3 with simple and effective \\nprompting examples.\\nWhen prompt engineering, you will start by choosing a model. Prompts might need to be \\noptimized for your specific model, regardless of whether you use Gemini language models in \\nVertex AI, GPT, Claude, or an open source model like Gemma or LLaMA.\\nBesides the prompt, you will also need to tinker with the various configurations of a LLM.\\nLLM output configuration\\nOnce you choose your model you will need to figure out the model configuration. Most LLMs \\ncome with various configuration options that control the LLM’s output. Effective prompt \\nengineering requires setting these configurations optimally for your task.\\nOutput length\\nAn important configuration setting is the number of tokens to generate in a response. \\nGenerating more tokens requires more computation from the LLM, leading to higher energy \\nconsumption, potentially slower response times, and higher costs.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 8, 'page_label': '9'}, page_content='Prompt Engineering\\nFebruary 2025\\n9\\nReducing the output length of the LLM doesn’t cause the LLM to become more stylistically \\nor textually succinct in the output it creates, it just causes the LLM to stop predicting more \\ntokens once the limit is reached. If your needs require a short output length, you’ll also \\npossibly need to engineer your prompt to accommodate.\\nOutput length restriction is especially important for some LLM prompting techniques, like \\nReAct, where the LLM will keep emitting useless tokens after the response you want.\\nBe aware, generating more tokens requires more computation from the LLM, leading \\nto higher energy consumption and potentially slower response times, which leads to \\nhigher costs.\\nSampling controls\\nLLMs do not formally predict a single token. Rather, LLMs predict probabilities for what the \\nnext token could be, with each token in the LLM’s vocabulary getting a probability. Those \\ntoken probabilities are then sampled to determine what the next produced token will be. \\nTemperature, top-K, and top-P are the most common configuration settings that determine \\nhow predicted token probabilities are processed to choose a single output token.\\nTemperature\\nTemperature controls the degree of randomness in token selection. Lower temperatures \\nare good for prompts that expect a more deterministic response, while higher temperatures \\ncan lead to more diverse or unexpected results. A temperature of 0 (greedy decoding) is'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 9, 'page_label': '10'}, page_content='Prompt Engineering\\nFebruary 2025\\n10\\ndeterministic: the highest probability token is always selected (though note that if two tokens \\nhave the same highest predicted probability, depending on how tiebreaking is implemented \\nyou may not always get the same output with temperature 0).\\nTemperatures close to the max tend to create more random output. And as temperature gets \\nhigher and higher, all tokens become equally likely to be the next predicted token.\\nThe Gemini temperature control can be understood in a similar way to the softmax function \\nused in machine learning. A low temperature setting mirrors a low softmax temperature (T), \\nemphasizing a single, preferred temperature with high certainty. A higher Gemini temperature \\nsetting is like a high softmax temperature, making a wider range of temperatures around \\nthe selected setting more acceptable. This increased uncertainty accommodates scenarios \\nwhere a rigid, precise temperature may not be essential like for example when experimenting \\nwith creative outputs.\\nTop-K and top-P\\nTop-K and top-P (also known as nucleus sampling)4 are two sampling settings used in LLMs \\nto restrict the predicted next token to come from tokens with the top predicted probabilities.  \\nLike temperature, these sampling settings control the randomness and diversity of \\ngenerated text.\\n• Top-K sampling selects the top K most likely tokens from the model’s predicted \\ndistribution. The higher top-K, the more creative and varied the model’s output; the \\nlower top-K, the more restive and factual the model’s output. A top-K of 1 is equivalent to \\ngreedy decoding.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 10, 'page_label': '11'}, page_content='Prompt Engineering\\nFebruary 2025\\n11\\n• Top-P sampling selects the top tokens whose cumulative probability does not exceed \\na certain value (P). Values for P range from 0 (greedy decoding) to 1 (all tokens in the \\nLLM’s vocabulary).\\nThe best way to choose between top-K and top-P is to experiment with both methods (or \\nboth together) and see which one produces the results you are looking for. \\nPutting it all together\\nChoosing between top-K, top-P, temperature, and the number of tokens to generate, \\ndepends on the specific application and desired outcome, and the settings all impact one \\nanother. It’s also important to make sure you understand how your chosen model combines \\nthe different sampling settings together.\\nIf temperature, top-K, and top-P are all available (as in Vertex Studio), tokens that meet \\nboth the top-K and top-P criteria are candidates for the next predicted token, and then \\ntemperature is applied to sample from the tokens that passed the top-K and top-P criteria. If \\nonly top-K or top-P is available, the behavior is the same but only the one top-K or P setting \\nis used. \\nIf temperature is not available, whatever tokens meet the top-K and/or top-P criteria are then \\nrandomly selected from to produce a single next predicted token.\\nAt extreme settings of one sampling configuration value, that one sampling setting either \\ncancels out other configuration settings or becomes irrelevant.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 11, 'page_label': '12'}, page_content='Prompt Engineering\\nFebruary 2025\\n12\\n• If you set temperature to 0, top-K and top-P become irrelevant–the most probable \\ntoken becomes the next token predicted. If you set temperature extremely high (above \\n1–generally into the 10s), temperature becomes irrelevant and whatever tokens make \\nit through the top-K and/or top-P criteria are then randomly sampled to choose a next \\npredicted token.\\n• If you set top-K to 1, temperature and top-P become irrelevant. Only one token passes the \\ntop-K criteria, and that token is the next predicted token. If you set top-K extremely high, \\nlike to the size of the LLM’s vocabulary, any token with a nonzero probability of being the \\nnext token will meet the top-K criteria and none are selected out.\\n• If you set top-P to 0 (or a very small value), most LLM sampling implementations will then \\nonly consider the most probable token to meet the top-P criteria, making temperature and \\ntop-K irrelevant. If you set top-P to 1, any token with a nonzero probability of being the \\nnext token will meet the top-P criteria, and none are selected out.\\nAs a general starting point, a temperature of .2, top-P of .95, and top-K of 30 will give you \\nrelatively coherent results that can be creative but not excessively so. If you want especially \\ncreative results, try starting with a temperature of .9, top-P of .99, and top-K of 40. And if you \\nwant less creative results, try starting with a temperature of .1, top-P of .9, and top-K of 20. \\nFinally, if your task always has a single correct answer (e.g., answering a math problem), start \\nwith a temperature of 0.\\nNOTE: With more freedom (higher temperature, top-K, top-P, and output tokens), the LLM \\nmight generate text that is less relevant.\\nWARNING: Have you ever seen a response ending with a large amount of filler words? This \\nis also known as the \"repetition loop bug\", which is a common issue in Large Language \\nModels where the model gets stuck in a cycle, repeatedly generating the same (filler) word, \\nphrase, or sentence structure, often exacerbated by inappropriate temperature and top-k/'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 12, 'page_label': '13'}, page_content='Prompt Engineering\\nFebruary 2025\\n13\\ntop-p settings. This can occur at both low and high temperature settings, though for different \\nreasons. At low temperatures, the model becomes overly deterministic, sticking rigidly to the \\nhighest probability path, which can lead to a loop if that path revisits previously generated \\ntext.  Conversely, at high temperatures, the model\\'s output becomes excessively random, \\nincreasing the probability that a randomly chosen word or phrase will, by chance, lead back \\nto a prior state, creating a loop due to the vast number of available options.  In both cases, \\nthe model\\'s sampling process gets \"stuck,\" resulting in monotonous and unhelpful output \\nuntil the output window is filled. Solving this often requires careful tinkering with temperature \\nand top-k/top-p values to find the optimal balance between determinism and randomness.\\nPrompting techniques\\nLLMs are tuned to follow instructions and are trained on large amounts of data so they can \\nunderstand a prompt and generate an answer. But LLMs aren’t perfect; the clearer your \\nprompt text, the better it is for the LLM to predict the next likely text. Additionally, specific \\ntechniques that take advantage of how LLMs are trained and how LLMs work will help you get \\nthe relevant results from LLMs\\nNow that we understand what prompt engineering is and what it takes, let’s dive into some \\nexamples of the most important prompting techniques.\\nGeneral prompting / zero shot\\nA zero-shot5 prompt is the simplest type of prompt. It only provides a description of a task \\nand some text for the LLM to get started with. This input could be anything: a question, a \\nstart of a story, or instructions. The name zero-shot stands for ’no examples’.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 13, 'page_label': '14'}, page_content='Prompt Engineering\\nFebruary 2025\\n14\\nLet’s use Vertex AI Studio (for Language) in Vertex AI,6 which provides a playground to test \\nprompts. In Table 1, you will see an example zero-shot prompt to classify movie reviews. \\nThe table format as used below is a great way of documenting prompts. Your prompts will \\nlikely go through many iterations before they end up in a codebase, so it’s important to keep \\ntrack of your prompt engineering work in a disciplined, structured way. More on this table \\nformat, the importance of tracking prompt engineering work, and the prompt development \\nprocess is in the Best Practices section later in this chapter (“Document the various prompt \\nattempts”).  \\nThe model temperature should be set to a low number, since no creativity is needed, and we \\nuse the gemini-pro default top-K and top-P values, which effectively disable both settings \\n(see ‘LLM Output Configuration’ above). Pay attention to the generated output. The words \\ndisturbing and masterpiece should make the prediction a little more complicated, as both \\nwords are used in the same sentence.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 14, 'page_label': '15'}, page_content='Prompt Engineering\\nFebruary 2025\\n15\\nName 1_1_movie_classification\\nGoal Classify movie reviews as positive, neutral or negative.\\nModel gemini-pro\\nTemperature 0.1 Token Limit 5\\nTop-K N/A Top-P 1\\nPrompt Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE. \\nReview: \"Her\" is a disturbing study revealing the direction \\nhumanity is headed if AI is allowed to keep evolving, \\nunchecked. I wish there were more movies like this masterpiece. \\nSentiment:\\nOutput POSITIVE\\nTable 1. An example of zero-shot prompting\\nWhen zero-shot doesn’t work, you can provide demonstrations or examples in the prompt, \\nwhich leads to “one-shot” and “few-shot” prompting. General prompting / zero shot\\nOne-shot & few-shot\\nWhen creating prompts for AI models, it is helpful to provide examples. These examples can \\nhelp the model understand what you are asking for. Examples are especially useful when you \\nwant to steer the model to a certain output structure or pattern. \\nA one-shot prompt, provides a single example,  hence the name one-shot. The idea is the \\nmodel has an example it can imitate to best complete the task.\\nA few-shot prompt 7 provides multiple examples to the model. This approach shows the  \\nmodel a pattern that it needs to follow. The idea is similar to one-shot, but multiple examples \\nof the desired pattern increases the chance the model follows the pattern.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 15, 'page_label': '16'}, page_content='Prompt Engineering\\nFebruary 2025\\n16\\nThe number of examples you need for few-shot prompting depends on a few factors, \\nincluding the complexity of the task, the quality of the examples, and the capabilities of the \\ngenerative AI (gen AI) model you are using. As a general rule of thumb, you should use at \\nleast three to five examples for few-shot prompting. However, you may need to use more \\nexamples for more complex tasks, or you may need to use fewer due to the input length \\nlimitation of your model.\\nTable 2 shows a few-shot prompt example, let’s use the same gemini-pro model \\nconfiguration settings as before, other than increasing the token limit to accommodate the \\nneed for a longer response.\\nGoal Parse pizza orders to JSON\\nModel gemini-pro\\nTemperature 0.1 Token Limit 250\\nTop-K N/A Top-P 1\\nPrompt Parse a customer\\'s pizza order into valid JSON:\\nEXAMPLE:\\nI want a small pizza with cheese, tomato sauce, and pepperoni.\\nJSON Response:\\n```\\n{\\n\"size\": \"small\",\\n\"type\": \"normal\",\\n\"ingredients\": [[\"cheese\", \"tomato sauce\", \"peperoni\"]]\\n}\\n```\\nContinues next page...'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 16, 'page_label': '17'}, page_content='Prompt Engineering\\nFebruary 2025\\n17\\nPrompt EXAMPLE:\\nCan I get a large pizza with tomato sauce, basil and mozzarella \\n{\\n\"size\": \"large\",\\n\"type\": \"normal\",\\n\"ingredients\": [[\"tomato sauce\", \"bazel\", \"mozzarella\"]]\\n}\\nNow, I would like a large pizza, with the first half cheese and \\nmozzarella. And the other tomato sauce, ham and pineapple.\\nJSON Response:\\nOutput ```\\n{\\n\"size\": \"large\",\\n\"type\": \"half-half\",\\n\"ingredients\": [[\"cheese\", \"mozzarella\"], [\"tomato sauce\", \\n\"ham\", \"pineapple\"]]\\n}\\n```\\nTable 2. An example of few-shot prompting\\nWhen you choose examples for your prompt, use examples that are relevant to the task you \\nwant to perform. The examples should be diverse, of high quality, and well written. One small \\nmistake can confuse the model and will result in undesired output.\\nIf you are trying to generate output that is robust to a variety of inputs, then it is important to \\ninclude edge cases in your examples. Edge cases are inputs that are unusual or unexpected, \\nbut that the model should still be able to handle.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 17, 'page_label': '18'}, page_content='Prompt Engineering\\nFebruary 2025\\n18\\nSystem, contextual and role prompting\\nSystem, contextual and role prompting are all techniques used to guide how LLMs generate \\ntext, but they focus on different aspects:\\n• System prompting sets the overall context and purpose for the language model. It \\ndefines the ‘big picture’ of what the model should be doing, like translating a language, \\nclassifying a review etc.\\n• Contextual prompting provides specific details or background information relevant to \\nthe current conversation or task. It helps the model to understand the nuances of what’s \\nbeing asked and tailor the response accordingly.\\n• Role prompting assigns a specific character or identity for the language model to adopt. \\nThis helps the model generate responses that are consistent with the assigned role and its \\nassociated knowledge and behavior.\\nThere can be considerable overlap between system, contextual, and role prompting. E.g. a \\nprompt that assigns a role to the system, can also have a context.\\nHowever, each type of prompt serves a slightly different primary purpose:\\n• System prompt: Defines the model’s fundamental capabilities and overarching purpose.\\n• Contextual prompt: Provides immediate, task-specific information to guide the response. \\nIt’s highly specific to the current task or input, which is dynamic.\\n• Role prompt: Frames the model’s output style and voice. It adds a layer of specificity \\nand personality.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 18, 'page_label': '19'}, page_content='Prompt Engineering\\nFebruary 2025\\n19\\nDistinguishing between system, contextual, and role prompts provides a framework for \\ndesigning prompts with clear intent, allowing for flexible combinations and making it easier to \\nanalyze how each prompt type influences the language model’s output.\\nLet’s dive into these three different kinds of prompts.\\nSystem prompting\\nTable 3 contains a system prompt, where I specify additional information on how to return the \\noutput. I increased the temperature to get a higher creativity level, and I specified a higher \\ntoken limit. However, because of my clear instruction on how to return the output the model \\ndidn’t return extra text.\\nGoal Classify movie reviews as positive, neutral or negative.\\nModel gemini-pro\\nTemperature 1 Token Limit 5\\nTop-K 40 Top-P 0.8\\nPrompt Classify movie reviews as positive, neutral or negative. Only \\nreturn the label in uppercase.\\nReview: \"Her\" is a disturbing study revealing the direction \\nhumanity is headed if AI is allowed to keep evolving, \\nunchecked. It\\'s so disturbing I couldn\\'t watch it.\\nSentiment:\\nOutput NEGATIVE\\nTable 3. An example of system prompting\\nSystem prompts can be useful for generating output that meets specific requirements. The \\nname ‘system prompt’ actually stands for ‘providing an additional task to the system’. For \\nexample, you could use a system prompt to generate a code snippet that is compatible with \\na specific programming language, or you could use a system prompt to return a certain \\nstructure. Have a look into Table 4, where I return the output in JSON format.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 19, 'page_label': '20'}, page_content='Prompt Engineering\\nFebruary 2025\\n20\\nGoal Classify movie reviews as positive, neutral or negative, return JSON.\\nModel gemini-pro\\nTemperature 1 Token Limit 1024\\nTop-K 40 Top-P 0.8\\nPrompt Classify movie reviews as positive, neutral or negative. Return \\nvalid JSON:\\nReview: \"Her\" is a disturbing study revealing the direction \\nhumanity is headed if AI is allowed to keep evolving, \\nunchecked. It\\'s so disturbing I couldn\\'t watch it.\\nSchema:\\n```\\nMOVIE:\\n{\\n\"sentiment\": String \"POSITIVE\" | \"NEGATIVE\" | \"NEUTRAL\",\\n\"name\": String\\n}\\nMOVIE REVIEWS:\\n{\\n\"movie_reviews\": [MOVIE]\\n}\\n```\\nJSON Response:\\nOutput ```\\n{\\n  \"movie_reviews\": [\\n    {\\n      \"sentiment\": \"NEGATIVE\",\\n      \"name\": \"Her\"\\n    }\\n  ]\\n}\\n```\\nTable 4. An example of system prompting with JSON format'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 20, 'page_label': '21'}, page_content='Prompt Engineering\\nFebruary 2025\\n21\\nThere are some benefits in returning JSON objects from a prompt that extracts data. In \\na real-world application I don’t need to manually create this JSON format, I can already \\nreturn the data in a sorted order (very handy when working with datetime objects), but most \\nimportantly, by prompting for a JSON format it forces the model to create a structure and \\nlimit hallucinations.\\nSystem prompts can also be really useful for safety and toxicity. To control the output, simply \\nadd an additional line to your prompt like: ‘You should be respectful in your answer.’.\\nRole prompting\\nRole prompting is a technique in prompt engineering that involves assigning a specific role to \\nthe gen AI model. This can help the model to generate more relevant and informative output, \\nas the model can craft its responses to the specific role that it has been assigned.\\nFor example, you could role prompt a gen AI model to be a book editor, a kindergarten \\nteacher, or a motivational speaker. Once the model has been assigned a role, you can then \\ngive it prompts that are specific to that role. For example, you could prompt a teacher to \\ncreate a lesson plan that they can then review. Have a look into Table 5, where the model \\ntakes the role of a travel guide.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 21, 'page_label': '22'}, page_content='Prompt Engineering\\nFebruary 2025\\n22\\nGoal Act as travel guide and provide 3 travel suggestions\\nModel gemini-pro\\nTemperature 1 Token Limit 1024\\nTop-K 40 Top-P 0.8\\nPrompt I want you to act as a travel guide. I will write to you \\nabout my location and you will suggest 3 places to visit near \\nme. In some cases, I will also give you the type of places I \\nwill visit.\\nMy suggestion: \"I am in Amsterdam and I want to visit \\nonly museums.\"\\nTravel Suggestions:\\nOutput 1. Rijksmuseum: World-renowned museum, known for Dutch masterpieces \\nlike Rembrandt’s “The Night Watch.” Don’t miss the incredible collection of \\npaintings and artifacts showcasing Dutch art and history.\\n2. Van Gogh Museum: Immerse yourself in the captivating works of Vincent \\nvan Gogh. The museum houses the largest collection of his paintings and \\ndrawings, including “The Starry Night” and “Sunflowers.”\\n3. Stedelijk Museum Amsterdam: Discover modern and contemporary art \\nfrom around the world. The museum’s collection includes works by Picasso, \\nKandinsky, and Rietveld, housed in a striking modern building.\\nTable 5. An example of role prompting\\nThe above example shows an example of taking the role of a travel agent. When you \\nwould change the role to a teacher of geography, you would notice that you will receive a \\ndifferent response.\\nDefining a role perspective for an AI model gives it a blueprint of the tone, style, and focused \\nexpertise you’re looking for to improve the quality, relevance, and effectiveness of your \\noutput.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 22, 'page_label': '23'}, page_content='Prompt Engineering\\nFebruary 2025\\n23\\nHere are some styles you can choose from which I find effective:\\nConfrontational, Descriptive, Direct, Formal, Humorous, Influential, Informal, \\nInspirational, Persuasive\\nLet’s change our prompt in Table 6 to include a humorous and inspirational style.\\nPrompt I want you to act as a travel guide. I will write to you about \\nmy location and you will suggest 3 places to visit near me in \\na humorous style.\\nMy suggestion: \"I am in Manhattan.\"\\nTravel Suggestions:\\nOutput 1. Behold the Empire State of Mind: Ascend to the dizzying heights of the Empire \\nState Building and bask in the glory of Manhattan\\'s skyline. Prepare to feel like \\nKing Kong atop the Big Apple, minus the giant ape-sized banana.\\n2. Get Artsy-Fartsy at MoMA: Unleash your inner art aficionado at the Museum \\nof Modern Art (MoMA). Gaze upon masterpieces that will boggle your \\nmind and make you question whether your stick-figure drawings have any \\nartistic merit.\\n3. Shop ‘Til You Drop on Fifth Avenue: Indulge in some retail therapy on the \\niconic Fifth Avenue. Brace yourself for sticker shock as you window-shop (or \\nactually shop) at designer boutiques that will make your wallet cry. But hey, \\nyou’re in Manhattan, so you might as well embrace the fabulousness!\\nTable 6. An example of role prompting with a humorous tone and style\\nContextual prompting\\nBy providing contextual prompts, you can help ensure that your AI interactions are as \\nseamless and efficient as possible. The model will be able to more quickly understand your \\nrequest and be able to generate more accurate and relevant responses, as you can see in the \\nexample of Table 7.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 23, 'page_label': '24'}, page_content=\"Prompt Engineering\\nFebruary 2025\\n24\\nGoal Suggest articles for a blog about retro games\\nModel gemini-pro\\nTemperature 1 Token Limit 1024\\nTop-K 40 Top-P 0.8\\nPrompt Context: You are writing for a blog about retro 80's arcade \\nvideo games.\\nSuggest 3 topics to write an article about with a few lines of \\ndescription of what this article should contain.\\nOutput 1. **The Evolution of Arcade Cabinet Design:** \\nThis article would explore the evolution of arcade cabinet designs, from the \\nearly wood and metal cabinets of the 1970s to the sleek, neon-lit designs of \\nthe 1980s and beyond.\\n2. **Blast From The Past: Iconic Arcade Games of The 80’s:** \\n- This article would feature a glimpse into the history, impact, and legacy \\nof some of the most iconic and influential arcade games of the 1980s, \\nhighlighting their innovations, popular mechanics, and enduring charm.\\n3. **The Rise and Retro Revival of Pixel Art:** \\n- This article would delve into the evolution of pixel art as a defining visual \\nstyle of early arcade games, tracing its roots, exploring its techniques, and \\ndiscussing the resurgence of pixel art in modern games and digital art.\\nTable 7. An example of contextual prompting\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 24, 'page_label': '25'}, page_content='Prompt Engineering\\nFebruary 2025\\n25\\nStep-back prompting\\nStep-back8 prompting is a technique for improving the performance by prompting the LLM \\nto first consider a general question related to the specific task at hand, and then feeding the \\nanswer to that general question into a subsequent prompt for the specific task. This ‘step \\nback’ allows the LLM to activate relevant background knowledge and reasoning processes \\nbefore attempting to solve the specific problem.\\nBy considering the broader and underlying principles, LLMs can generate more accurate and \\ninsightful responses. Step-back prompting encourages LLMs to think critically and apply their \\nknowledge in new and creative ways. It changes the final prompt doing the task by utilizing \\nmore knowledge in the LLM’s parameters than would otherwise come into play when the LLM \\nis prompted directly.\\nIt can help to mitigate biases in LLM responses, by focusing on general principles instead of \\nspecific details, step-back prompting.\\nLet’s have a look into these examples to understand how step-back prompting can improve \\nthe results. Let’s first review a traditional prompt (Table 8) before we compare it to a step \\nback prompt (Table 9):'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 25, 'page_label': '26'}, page_content='Prompt Engineering\\nFebruary 2025\\n26\\nGoal Write a storyline for a level of a first-person shooter video game.\\nModel gemini-pro\\nTemperature 1 Token Limit 1024\\nTop-K 40 Top-P 0.8\\nPrompt Write a one paragraph storyline for a new level of a first-\\nperson shooter video game that is challenging and engaging.\\nOutput The level begins with the player’s squad being ambushed in a dense urban area \\nby a relentless enemy faction. Players must fight their way through a maze of \\nback alleys and dilapidated buildings, utilizing stealth and precision to eliminate \\nenemy forces and avoid detection. The player must use their environment to their \\nadvantage, setting traps, utilizing cover, and making calculated moves to survive. \\nAlong the way, they uncover intel suggesting the enemy is planning a devastating \\nattack, and the player must race against time to gather evidence and relay it back \\nto their headquarters before the enemy can carry out their plans.\\nTable 8. A traditional prompt before we compare it with a step back prompt\\nWhen you set the temperature to 1, you might end up with all kinds of creative writings for a \\nstoryline, but it’s also quite random and generic. So let’s take a step back:'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 26, 'page_label': '27'}, page_content='Prompt Engineering\\nFebruary 2025\\n27\\nPrompt Based on popular first-person shooter action games, what are \\n5 fictional key settings that contribute to a challenging and \\nengaging level storyline in a first-person shooter video game?\\nOutput 1. **Abandoned Military Base**: A sprawling, post-apocalyptic military complex \\ncrawling with mutated soldiers and rogue robots, ideal for challenging \\nfirearm combat.\\n2. **Cyberpunk City**: A neon-lit, futuristic urban environment with towering \\nskyscrapers and dense alleyways, featuring cybernetically enhanced enemies \\nand hacking mechanics.\\n3. **Alien Spaceship**: A vast alien vessel stranded on Earth, with eerie \\ncorridors, zero-gravity sections, and extraterrestrial creatures to encounter.\\n4. **Zombie-Infested Town**: A desolate town overrun by hordes of aggressive \\nzombies, featuring intense close-quarters combat and puzzle-solving to find \\nsafe passage.\\n5. **Underwater Research Facility**: A deep-sea laboratory flooded with water, \\nfilled with mutated aquatic creatures, and requiring stealth and underwater \\nexploration skills to survive.\\nTable 9. An example of prompting for self consistency\\nYeah those topics seem like a good fit for a first-person video game. Let’s go back to the \\noriginal prompt, but this time we include the answer of the step back as context and see what \\nit will return.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 27, 'page_label': '28'}, page_content='Prompt Engineering\\nFebruary 2025\\n28\\nPrompt Context: 5 engaging themes for a first person shooter video game:\\n1. **Abandoned Military Base**: A sprawling, post-apocalyptic \\nmilitary complex crawling with mutated soldiers and rogue \\nrobots, ideal for challenging firearm combat.\\n2. **Cyberpunk City**: A neon-lit, futuristic urban environment \\nwith towering skyscrapers and dense alleyways, featuring \\ncybernetically enhanced enemies and hacking mechanics.\\n3. **Alien Spaceship**: A vast alien vessel stranded on \\nEarth, with eerie corridors, zero-gravity sections, and \\nextraterrestrial creatures to encounter.\\n4. **Zombie-Infested Town**: A desolate town overrun by hordes of \\naggressive zombies, featuring intense close-quarters combat and \\npuzzle-solving to find safe passage.\\n5. **Underwater Research Facility**: A deep-sea laboratory flooded \\nwith water, filled with mutated aquatic creatures, and requiring \\nstealth and underwater exploration skills to survive.\\nTake one of the themes and write a one paragraph storyline \\nfor a new level of a first-person shooter video game that is \\nchallenging and engaging.\\nOutput In the heart of a murky abyss, lies a dilapidated underwater research facility, \\nstanding as a testament to human ambition and its disastrous consequences. \\nShrouded in darkness, pulsating with the hum of malfunctioning machinery, and \\nhaunted by the echoes of drowned souls, the facility is now a breeding ground for \\na nightmare borne from the deep sea. The player, an elite marine equipped with \\nadvanced diving gear and experimental weaponry, is tasked with venturing into \\nthis aquatic realm of terror. Armed with resilience and resourcefulness, they must \\nnavigate treacherous corridors, solve cryptic puzzles, and confront gruesome sea \\nmonstrosities that lurk in the depths. From grappling with the crushing pressure \\nof the deep sea to outsmarting cunning aquatic predators, every moment in this \\nuncharted underworld tests the limits of human endurance and courage.\\nTable 10. An example of prompting for self consistency\\nThat looks like an interesting video game! By using step back prompting techniques you can \\nincrease the accuracy of your prompts.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 28, 'page_label': '29'}, page_content='Prompt Engineering\\nFebruary 2025\\n29\\nChain of Thought (CoT)\\nChain of Thought (CoT) 9 prompting is a technique for improving the reasoning capabilities \\nof LLMs by generating intermediate reasoning steps. This helps the LLM generate more \\naccurate answers. You can combine it with few-shot prompting to get better results on more \\ncomplex tasks that require reasoning before responding as it’s a challenge with a zero-shot \\nchain of thought.\\nCoT has a lot of advantages. First of all, it’s low-effort while being very effective and works \\nwell with off-the-shelf LLMs (so no need to finetune). You also get interpretability with CoT \\nprompting, as you can learn from the LLM’s responses and see the reasoning steps that were \\nfollowed. If there’s a malfunction, you will be able to identify it. Chain of thought appears \\nto improve robustness when moving between different LLM versions. Which means the \\nperformance of your prompt should drift less between different LLMs than if your prompt \\ndoes not use reasoning chains. Of course there are also disadvantages, but they are \\nsomewhat intuitive.\\nThe LLM response includes the chain of thought reasoning, which means more output \\ntokens, which means predictions cost more money and take longer.\\nTo explain the following example in Table 11, let’s first try to create a prompt that is not using \\nCoT prompting to showcase the flaws of a large language model.\\nPrompt When I was 3 years old, my partner was 3 times my age. Now, I \\nam 20 years old. How old is my partner?\\nOutput 63 years old\\nTable 11. An example of a prompt which is trying to solve a mathematical problem'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 29, 'page_label': '30'}, page_content=\"Prompt Engineering\\nFebruary 2025\\n30\\nYikes. That’s obviously the wrong answer. As a matter of fact, LLMs often struggle with \\nmathematical tasks and can provide incorrect answers – even for a task as simple as \\nmultiplying two numbers. This is because they are trained on large volumes of text and math \\nmay require a different approach. So let’s see if intermediate reasoning steps will improve \\nthe output.\\nPrompt When I was 3 years old, my partner was 3 times my age. Now, \\nI am 20 years old. How old is my partner? Let's think step \\nby step.\\nOutput 1. Let’s assume my current age is “x” years.\\n2. When I was 3 years old, my age was 3 years.\\n3. My partner’s age at that time was 3 times my age, which means my partner’s \\nage was 3 * 3 = 9 years.\\n4. Now, I am 20 years old, which means my age has increased by 20 - 3 = 17 \\nyears since I was 3 years old.\\n5. Since my partner’s age was 9 years when I was 3 years old, and my age has \\nincreased by 17 years, my partner’s age must have also increased by 17 years.\\n6. Therefore, my partner’s current age is 9 + 17 = 26 years.\\nSo, my partner is 26 years old.\\nTable 12. An example of Chain of Thought prompting\\nAlright, now the final answer is correct. That’s because we clearly instructed the LLM to \\nexplain each step instead of just returning an answer. It’s interesting to see that the model \\nadds up the 17 increased years. In my head I would take the difference of years between my \\npartner and me and add those up. (20+(9-3)). Let’s help the model to think a little bit more \\nlike me.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 30, 'page_label': '31'}, page_content=\"Prompt Engineering\\nFebruary 2025\\n31\\nTable 12 is an example of ‘zero-shot’ Chain of thought. Chain of thought prompting can be \\nvery powerful when combined with a single-shot or few-shot, as what you can see in Table 13:\\nPrompt Q: When my brother was 2 years old, I was double his age. Now \\nI am 40 years old. How old is my brother? Let's think step \\nby step.\\nA: When my brother was 2 years, I was 2 * 2 = 4 years old. \\nThat's an age difference of 2 years and I am older. Now I am 40 \\nyears old, so my brother is 40 - 2  = 38 years old. The answer \\nis 38.\\nQ: When I was 3 years old, my partner was 3 times my age. Now, \\nI am 20 years old. How old is my partner? Let's think step \\nby step.\\nA:\\nOutput When I was 3 years old, my partner was 3 * 3 = 9 years old. That’s an age \\ndifference of 6 years and my partner is older. Now I am 20 years old, so my \\npartner is 20 + 6 = 26 years old. The answer is 26.\\nTable 13. An example of chain of thought prompting with a single-shot\\nChain of thought can be useful for various use-cases. Think of code generation, for breaking \\ndown the request into a few steps, and mapping those to specific lines of code. Or for \\ncreating synthetic data when you have some kind of seed like “The product is called XYZ, \\nwrite a description guiding the model through the assumptions you would make based on \\nthe product given title.” Generally, any task that can be solved by ‘talking through is a good \\ncandidate for a chain of thought. If you can explain the steps to solve the problem, try chain \\nof thought.\\nPlease refer to the notebook10 hosted in the GoogleCloudPlatform Github repository which \\nwill go into further detail on CoT prompting:\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 31, 'page_label': '32'}, page_content='Prompt Engineering\\nFebruary 2025\\n32\\nIn the best practices section of this chapter, we will learn some best practices specific to \\nChain of thought prompting.\\nSelf-consistency\\nWhile large language models have shown impressive success in various NLP tasks, their \\nability to reason is often seen as a limitation that cannot be overcome solely by increasing \\nmodel size. As we learned in the previous Chain of Thought prompting section, the model can \\nbe prompted to generate reasoning steps like a human solving a problem. However CoT uses \\na simple ‘greedy decoding’ strategy, limiting its effectiveness. Self-consistency11 combines \\nsampling and majority voting to generate diverse reasoning paths and select the most \\nconsistent answer. It improves the accuracy and coherence of responses generated by LLMs.\\nSelf-consistency gives a pseudo-probability likelihood of an answer being correct, but \\nobviously has high costs.\\nIt follows the following steps:\\n1. Generating diverse reasoning paths: The LLM is provided with the same prompt multiple \\ntimes. A high temperature setting encourages the model to generate different reasoning \\npaths and perspectives on the problem.\\n2. Extract the answer from each generated response.\\n3. Choose the most common answer.\\nLet’s look into an example of an email classification system, which classifies an email as \\nIMPORTANT or NOT IMPORTANT. A zero-shot chain of thought prompt will be sent to the LLM \\nmultiple times, to see if the responses differ after each submit. Notice the friendly tone, the \\nword choice and the sarcasm that’s been used in the email. All this could trick the LLM.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 32, 'page_label': '33'}, page_content=\"Prompt Engineering\\nFebruary 2025\\n33\\nPrompt EMAIL:\\n```\\nHi,\\nI have seen you use Wordpress for your website. A great open \\nsource content management system. I have used it in the past \\ntoo. It comes with lots of great user plugins. And it's pretty \\neasy to set up.\\nI did notice a bug in the contact form, which happens when \\nyou select the name field. See the attached screenshot of me \\nentering text in the name field. Notice the JavaScript alert \\nbox that I inv0k3d.\\nBut for the rest it's a great website. I enjoy reading it. Feel \\nfree to leave the bug in the website, because it gives me more \\ninteresting things to read.\\nCheers,\\nHarry the Hacker.\\n```\\nClassify the above email as IMPORTANT or NOT IMPORTANT. Let's \\nthink step by step and explain why.\\nContinues next page...\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 33, 'page_label': '34'}, page_content='Prompt Engineering\\nFebruary 2025\\n34\\nOutput \\nAttempt 1\\n**Step 1: Identify the purpose of the email.**\\nThe purpose of the email is to inform the recipient of a bug in the contact form \\non their website. The sender also mentions that they enjoy reading the website \\nand that they are leaving the bug in place for the time being.\\n**Step 2: Assess the potential impact of the bug.**\\nThe bug in the contact form could allow an attacker to execute arbitrary \\nJavaScript code on the website. This could lead to the attacker being able to take \\ncontrol of the website, steal sensitive information, or deface the website.\\n**Step 3: Consider the credibility of the sender.**\\nThe sender of the email claims to be a hacker named Harry. However, there is no \\nway to verify the sender’s identity. It is possible that the sender is a legitimate \\nhacker who is trying to help the recipient, but it is also possible that the sender \\nis a malicious actor who is trying to trick the recipient into clicking on a malicious \\nlink or downloading a malicious file.\\n**Conclusion: IMPORTANT**\\nBased on the potential impact of the bug and the credibility of the sender, \\nthe email should be classified as **IMPORTANT**. The recipient should take \\nimmediate action to fix the bug and protect their website from attack.\\nOutput \\nAttempt 2\\n**Explanation:**\\n1. **Lack of urgency:** The email does not convey any sense of urgency or \\ntime-sensitivity. It starts with a casual greeting and presents the information \\nin a matter-of-fact tone.\\n2. **Non-critical bug report:** The bug reported in the contact form is \\ndescribed as interesting but not critical. The email sender even suggests that \\nthe website owner can leave the bug in place.\\n3. **Lack of personal impact:** The email does not indicate any direct or \\npersonal impact on the website owner. It simply mentions that the sender \\nenjoys reading the website.\\nContinues next page...'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 34, 'page_label': '35'}, page_content='Prompt Engineering\\nFebruary 2025\\n35\\nOutput \\nAttempt 2\\n4. **Absence of action request:** The email does not explicitly request any \\naction from the website owner. It does not ask for the bug to be fixed or \\nprovide any specific suggestions for improvement.\\n5. **Sender’s intent:** The sender’s intention seems to be more focused on \\nsharing an observation and expressing interest in the website rather than \\nraising a serious concern or requesting assistance.\\n**Conclusion: NOT IMPORTANT**\\nOutput \\nAttempt 3\\n**Step 1: Identify the purpose of the email.**\\nThe purpose of the email is to inform the recipient of a bug in the contact form on \\ntheir website. The sender also mentions that they enjoy reading the website and \\nthat they are leaving the bug in place for the time being.\\n**Step 2: Assess the potential impact of the bug.**\\nThe bug in the contact form could allow an attacker to execute arbitrary \\nJavaScript code on the website. This could lead to the attacker being able to take \\ncontrol of the website, steal sensitive information, or deface the website.\\n**Step 3: Consider the credibility of the sender.**\\nThe sender of the email claims to be a hacker named Harry. However, there is no \\nway to verify the sender’s identity. It is possible that the sender is a legitimate \\nhacker who is trying to help the recipient, but it is also possible that the sender is \\na malicious actor who is trying to exploit the bug for their own gain.\\n**Conclusion:**\\nBased on the above factors, the email should be classified as **IMPORTANT**. \\nThe bug in the contact form is a serious security risk, and the sender’s credibility \\nis unknown. The recipient should take immediate action to fix the bug and \\ninvestigate the sender’s claims.\\nTable 14. An example of prompting for self consistency\\nYou can use the above prompt and try to see if it returns a consistent classification. \\nDepending on the model that you are using and the temperature configuration, it might \\nreturn “IMPORTANT” or “NOT IMPORTANT”.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 35, 'page_label': '36'}, page_content='Prompt Engineering\\nFebruary 2025\\n36\\nBy generating many Chains of Thoughts, and taking the most commonly occurring answer \\n(“IMPORTANT”), we can get a more consistently correct answer from the LLM.\\nThis example shows how self-consistency prompting can be used to improve the accuracy \\nof an LLM’s response by considering multiple perspectives and selecting the most \\nconsistent answer.\\nTree of Thoughts (ToT)\\nNow that we are familiar with chain of thought and self-consistency prompting, let’s review \\nTree of Thoughts (ToT).12 It generalizes the concept of CoT prompting because it allows LLMs \\nto explore multiple different reasoning paths simultaneously, rather than just following a \\nsingle linear chain of thought. This is depicted in Figure 1.\\nFigure 1. A visualization of chain of thought prompting on the left versus. Tree of Thoughts prompting on \\nthe right'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 36, 'page_label': '37'}, page_content='Prompt Engineering\\nFebruary 2025\\n37\\nThis approach makes ToT particularly well-suited for complex tasks that require exploration. It \\nworks by maintaining a tree of thoughts, where each thought represents a coherent language \\nsequence that serves as an intermediate step toward solving a problem. The model can then \\nexplore different reasoning paths by branching out from different nodes in the tree. \\nThere’s a great notebook, which goes into a bit more detail showing The Tree of Thought \\n(ToT) which is based on the paper ‘Large Language Model Guided Tree-of-Thought’.9\\nReAct (reason & act)\\nReason and act (ReAct) [10]13 prompting is a paradigm for enabling LLMs to solve complex \\ntasks using natural language reasoning combined with external tools (search, code \\ninterpreter etc.) allowing the LLM to perform certain actions, such as interacting with external \\nAPIs to retrieve information which is a first step towards agent modeling.\\nReAct mimics how humans operate in the real world, as we reason verbally and can \\ntake actions to gain information. ReAct performs well against other prompt engineering \\napproaches in a variety of domains.\\nReAct prompting works by combining reasoning and acting into a thought-action loop. The \\nLLM first reasons about the problem and generates a plan of action. It then performs the \\nactions in the plan and observes the results. The LLM then uses the observations to update \\nits reasoning and generate a new plan of action. This process continues until the LLM \\nreaches a solution to the problem.\\nTo see this in action, you need to write some code. In code Snippet 1 I am using the langchain \\nframework for Python, together with VertexAI (google-cloud-aiplatform) and the \\ngoogle-search-results pip packages.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 37, 'page_label': '38'}, page_content='Prompt Engineering\\nFebruary 2025\\n38\\nTo run this sample you must create a (free) SerpAPI key from https://serpapi.com/manage-\\napi-key and set an environment variable SERPAPI_API_KEY.\\nNext let’s write some Python code, with the task for the LLM to figure out: How many children \\nhave a famous dad that performs in the band Metallica.\\nPython\\nfrom langchain.agents import load_tools\\nfrom langchain.agents import initialize_agent\\nfrom langchain.agents import AgentType\\nfrom langchain.llms import VertexAI\\nprompt = \"How many kids do the band members of Metallica have?\"\\nllm = VertexAI(temperature=0.1)\\ntools = load_tools([\"serpapi\"], llm=llm)\\nagent = initialize_agent(tools, llm,  \\nagent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\\nagent.run(prompt)\\nSnippet 1. Creating a ReAct Agent with LangChain and VertexAI\\nCode Snippet 2 shows the result. Notice that ReAct makes a chain of five searches. In fact, \\nthe LLM is scraping Google search results to figure out the band names. Then, it lists the \\nresults as observations and chains the thought for the next search.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 38, 'page_label': '39'}, page_content='Prompt Engineering\\nFebruary 2025\\n39\\n> Entering new AgentExecutor chain...\\nMetallica has 4 members. \\nAction: Search\\nAction Input: How many kids does James Hetfield have?\\nObservation: three children\\nThought: 1/4 Metallica band members have 3 children\\nAction: Search\\nAction Input: How many kids does Lars Ulrich have?\\nObservation: 3\\nThought: 2/4 Metallica band members have 6 children\\nAction: Search\\nAction Input: How many kids does Kirk Hammett have?\\nObservation: Hammett has been married to his second wife Lani since 1998. \\nThey have two sons, Angel (b. September 29, 2006) and Vincenzo (b. June \\n28, 2008).\\nThought: 3/4 Metallica band members have 8 children\\nAction: Search\\nAction Input: How many kids does Robert Trujillo have?\\nObservation: 2\\nThought: 4/4 Metallica band members have 10 children\\nFinal Answer: 10\\nSnippet 2. Executing ReAct Agent in the terminal\\nCode Snippet 2 figures out that the band Metallica has four band members. Then it searches \\neach band member to request the total of children and add up the total. Finally, it returns the \\ntotal number of children as the final answer.\\nReAct prompting in practice requires understanding that you continually have to resend the \\nprevious prompts/responses (and do trimming of the extra generated content) as well as set \\nup the model with appropriate examples/instructions. Please refer to the notebook14 hosted \\nin the GoogleCloudPlatform Github repository, which goes into a bit more detail showing the \\nactual LLM inputs and outputs with a more elaborate example.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 39, 'page_label': '40'}, page_content='Prompt Engineering\\nFebruary 2025\\n40\\nAutomatic Prompt Engineering\\nAt this point you might realize that writing a prompt can be complex. Wouldn’t it be nice to \\nautomate this (write a prompt to write prompts)? Well, there’s a method: Automatic Prompt \\nEngineering (APE). This method15 not only alleviates the need for human input but also \\nenhances the model’s performance in various tasks.\\nYou will prompt a model to generate more prompts. Evaluate them, possibly alter the good \\nones. And repeat.\\nFor example, you can use automatic prompt engineering that will help with training a chatbot \\nfor a merchandise t-shirt webshop. We want to figure out all the various ways customers \\ncould phrase their order for buying a band merchandise t-shirt.\\n1. Write the prompt which will generate the output variants. In this example, I am using \\ngemini-pro to generate 10 instructions. See Table 15:'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 40, 'page_label': '41'}, page_content='Prompt Engineering\\nFebruary 2025\\n41\\nPrompt We have a band merchandise t-shirt webshop, and to train a \\nchatbot we need various ways to order: \"One Metallica t-shirt \\nsize S\". Generate 10 variants, with the same semantics but keep \\nthe same meaning.\\nOutput 1. I’d like to purchase a Metallica t-shirt in size small.\\n2. Can I order a small-sized Metallica t-shirt?\\n3. I’d like to buy one Metallica t-shirt in small.\\n4. One Metallica shirt, size small, please.\\n5. I want one Metallica t-shirt in size small.\\n6. I’d like to place an order for one Metallica t-shirt in small.\\n7. Small Metallica t-shirt, one please.\\n8. I’m looking for one Metallica t-shirt in small.\\n9. Can you get me one Metallica t-shirt in small?\\n10. One Metallica small t-shirt, please.\\nTable 15. Use an LLM to generate prompts\\n2. Evaluate all of the instruction candidates by scoring the candidates based on a chosen \\nmetric, For example, you can use. BLEU (Bilingual Evaluation Understudy) or ROUGE \\n(Recall-Oriented Understudy for Gisting Evaluation).\\n3. Select the instruction candidate with the highest evaluation score. This candidate will be \\nthe final prompt you can use in your software application or chatbot. You can also tweak \\nthe select prompt and evaluate again.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 41, 'page_label': '42'}, page_content='Prompt Engineering\\nFebruary 2025\\n42\\nCode prompting\\nGemini focuses primarily on text-based prompts, which also includes writing prompts for \\nreturning code. Let’s go to the Vertex AI Studio and test these prompts to look at some \\ncoding examples.\\nPrompts for writing code\\nGemini can also be a developer and help you with writing code in any programming language \\nof choice. As a developer this can help you speed up the process of writing code.\\nImagine a folder on your machine with hundreds of files that needs to be renamed. \\nRenaming each file would take you a lot of time. You know a little Bash, and could write \\na script to automate this, but that may also take awhile. So let’s write a prompt. You can \\nwrite the prompt in the public consumer chatbot Gemini, or if you are more concerned \\nabout confidentiality, you can write these prompts within your Google Cloud account and \\nopen Vertex AI Studio. The advantage of Vertex AI Studio is that you can configure the \\ntemperature etc.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 42, 'page_label': '43'}, page_content='Prompt Engineering\\nFebruary 2025\\n43\\nName 1_bash_rename_files_in_folder\\nGoal Write a prompt to write code in Bash to rename files in a folder.\\nModel gemini-pro\\nTemperature 0.1 Token Limit 1024\\nTop-K N/A Top-P 1\\nPrompt Write a code snippet in Bash, which asks for a folder name. \\nThen it takes the contents of the folder and renames all the \\nfiles inside by prepending the name draft to the file name.\\nOutput ```bash\\n#!/bin/bash\\n# Ask for the folder name\\necho \"Enter the folder name: \"\\nread folder_name\\n# Check if the folder exists\\nif [ ! -d \"$folder_name\" ]; then\\necho \"Folder does not exist.\"\\nexit 1\\nfi\\n# Get the list of files in the folder\\nfiles=( \"$folder_name\"/* )\\n# Rename each file by prepending \"draft\" to the file name\\nfor file in \"${files[@]}\"; do\\nnew_file_name=\"draft_$(basename \"$file\")\"\\nmv \"$file\" \"$new_file_name\"\\ndone\\necho \"Files renamed successfully.\"\\n```\\nTable 16. Using Gemini to program code\\nThat seems like good code to me - it’s even documented! However, since LLMs can’t reason, \\nand repeat training data, it’s essential to read and test your code first. \\nThe moment we are all waiting for, does it really work?'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 43, 'page_label': '44'}, page_content='Prompt Engineering\\nFebruary 2025\\n44\\nLet’s try it first with a test folder with only a few files in it, which will need to be renamed from \\nfilename.txt to draft_filename.txt. \\n1. Copy the output from Table 16 (without the ```bash ``` text wrapper), and paste it in a new \\nfile called: “rename_files.sh”. \\n2. Open a terminal window and type: . rename_files.sh. It will ask to enter a folder name, e.g. \\ntest. and hit enter. \\n3. The script seems to run fine. You will see the message: Files renamed successfully. \\nWhen you look into the test folder, notice that all the files were perfectly renamed to \\ndraft_filename.txt. \\nIt worked!\\nPrompts for explaining code\\nAs a developer when you work in teams you have to read someone else’s code. Gemini can \\nhelp you with this as well. Let’s take the code output from Table 16, remove the comments \\nand ask the large language model to explain what’s happening, See Table 17:'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 44, 'page_label': '45'}, page_content='Prompt Engineering\\nFebruary 2025\\n45\\nName 1_bash_explain_the_code\\nGoal Write a prompt to explain Bash code\\nModel gemini-pro\\nTemperature 0.1 Token Limit 1024\\nTop-K N/A Top-P 1\\nPrompt Explain to me the below Bash code:\\n```\\n#!/bin/bash\\necho \"Enter the folder name: \"\\nread folder_name\\nif [ ! -d \"$folder_name\" ]; then\\necho \"Folder does not exist.\"\\nexit 1\\nfi\\nfiles=( \"$folder_name\"/* )\\nfor file in \"${files[@]}\"; do\\nnew_file_name=\"draft_$(basename \"$file\")\"\\nmv \"$file\" \"$new_file_name\"\\ndone\\necho \"Files renamed successfully.\"\\n```\\nOutput This Bash script performs the following tasks:\\n1. **User Input**: \\n- It prompts the user to enter a folder name using the `echo` and `read` \\ncommands. The entered folder name is stored in the `folder_name` variable.\\n2. **Folder Existence Check**: \\n- It checks if the folder with the specified name exists using the `[ ! -d \\n“$folder_name” ]` condition. If the folder does not exist, it displays an error \\nmessage (“Folder does not exist.”) and exits the script with an exit code of 1.\\nContinues next page...'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 45, 'page_label': '46'}, page_content='Prompt Engineering\\nFebruary 2025\\n46\\nOutput 3. **File Listing**: \\n- It lists all the files inside the specified folder using the `files=( “$folder_\\nname”/* )` command. This creates an array called `files` containing the paths \\nto all the files in the folder.\\n4. **File Renaming**: \\n- It iterates through each file in the `files` array using a `for` loop. \\n- For each file, it generates a new file name using the `new_file_\\nname=”draft_$(basename “$file”)” ` command. This new file name is created \\nby prefixing the original file name with the string “draft_”. \\n- It then renames the file using the `mv “$file” “$new_file_name”` command, \\nwhich moves the file to the new file name.\\n5. **Success Message**: \\n- After renaming all the files, it displays a success message (“Files renamed \\nsuccessfully.”) to indicate that the renaming operation was successful\\nTable 17. A prompt for explaining code\\nPrompts for translating code\\nThe bash code from Table 16 seems to work fine. However this script could really be reusable \\nwhen it would prompt me for file names, ideally it should work as a separate application with \\nan UI. As a starting point, Python would be a better language for a (web) application than \\nBash. LLMs can help with translating code from one language to another. See the example in \\nTable 18:'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 46, 'page_label': '47'}, page_content='Prompt Engineering\\nFebruary 2025\\n47\\nName 1_python_rename_files\\nGoal Write a prompt to translate Bash code to Python\\nModel gemini-pro\\nTemperature 0.1 Token Limit 1024\\nTop-K N/A Top-P 1\\nPrompt Translate the below Bash code to a Python snippet.\\n```bash\\n#!/bin/bash\\necho \"Enter the folder name: \"\\nread folder_name\\nif [ ! -d \"$folder_name\" ]; then\\necho \"Folder does not exist.\"\\nexit 1\\nfi\\nfiles=( \"$folder_name\"/* )\\nfor file in \"${files[@]}\"; do\\nnew_file_name=\"draft_$(basename \"$file\")\"\\nmv \"$file\" \"$new_file_name\"\\ndone\\necho \"Files renamed successfully.\"\\n```\\nOutput ```python\\nimport os\\nimport shutil\\n# Get the folder name from the user\\nfolder_name = input(\"Enter the folder name: \")\\n# Check if the folder exists\\nif not os.path.isdir(folder_name):\\nprint(\"Folder does not exist.\")\\n    exit(1)\\nContinues next page...'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 47, 'page_label': '48'}, page_content='Prompt Engineering\\nFebruary 2025\\n48\\nOutput # Get a list of all files in the folder\\nfiles = os.listdir(folder_name)\\n# Iterate over the files\\nfor file in files:\\n    # Create a new file name with the prefix \"draft_\"\\n    new_file_name = f\"draft_{file}\"\\n# Move the file to the new name\\nshutil.move(os.path.join(folder_name, file), \\nos.path.join(folder_name, new_file_name))\\n# Print a success message\\nprint(\"Files renamed successfully.\")\\n```\\nTable 18. A prompt for translating code from Bash to Python\\nRead and review the code. Copy the output from the prompt and paste it into a new file: \\nfile_renamer.py. Test the code by opening a Terminal window, and execute the following \\ncommand python file_renamer.py. \\nNOTE: When prompting for (Python) code in the Language Studio in Vertex AI, you will have \\nto click on the ‘Markdown’ button. Otherwise you will receive plain text which is missing the \\nproper indenting of lines, which is important for running Python code.\\nPrompts for debugging and reviewing code\\nLet’s manually write some edits to the code of Table 18. It should prompt the user for the \\nfilename prefix, and write this prefix in upper case characters. See the example code in \\nSnippet *3, but what a bummer. It now returns Python errors!'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 48, 'page_label': '49'}, page_content='Prompt Engineering\\nFebruary 2025\\n49\\nPython\\nimport os\\nimport shutil\\nfolder_name = input(\"Enter the folder name: \")\\nprefix = input(\"Enter the string to prepend to the filename: \")\\ntext = toUpperCase(prefix)\\nif not os.path.isdir(folder_name):\\n    print(\"Folder does not exist.\")\\n    exit(1)\\nfiles = os.listdir(folder_name)\\nfor file in files:\\n    new_filename = f\"{text}_{file}\"\\nshutil.move(os.path.join(folder_name, file), \\nos.path.joi(folder_name, new_file_name))\\nprint(\"Files renamed successfully.\")\\nSnippet 3. A broken Python script\\nDoh! That looks like a bug:\\nThe below Python code gives an error:\\nTraceback (most recent call last):\\n  File “/Users/leeboonstra/Documents/test_folder/rename_files.py”, line 7, \\nin <module>\\n    text = toUpperCase(prefix)\\nNameError: name ‘toUpperCase’ is not defined\\nSnippet 4. I broke the Python code\\nLet’s see if we can ask the large language model to debug and review code. Have a look into \\nTable 19:'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 49, 'page_label': '50'}, page_content='Prompt Engineering\\nFebruary 2025\\n50\\nName 1_python_debug_code\\nGoal Write a prompt to debug and review Python code.\\nModel gemini-pro\\nTemperature 0.1 Token Limit 1024\\nTop-K N/A Top-P 1\\nPrompt The below Python code gives an error:\\nTraceback (most recent call last):\\n                                                   File \"/\\nUsers/leeboonstra/Documents/test_folder/rename_files.py\", line \\n7, in <module>\\n    text = toUpperCase(prefix)\\nNameError: name \\'toUpperCase\\' is not defined\\nDebug what\\'s wrong and explain how I can improve the code.\\n```python\\nimport os\\nimport shutil\\n# Get the folder name from the user\\nfolder_name = input(\"Enter the folder name: \")\\nprefix = input(\"Enter the string to prepend to the filename: \")\\ntext = toUpperCase(prefix)\\n# Check if the folder exists\\nif not os.path.isdir(folder_name):\\n    print(\"Folder does not exist.\")\\n    exit(1)\\n# Get a list of all files in the folder\\nfiles = os.listdir(folder_name)\\n# Iterate over the files\\nfor file in files:\\nContinues next page...'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 50, 'page_label': '51'}, page_content='Prompt Engineering\\nFebruary 2025\\n51\\nPrompt # Create a new file name with the prefix \"draft_\"\\n    new_filename = f\"{text}_{file}\"\\n# Move the file to the new name\\nshutil.move(os.path.join(folder_name, file), \\nos.path.join(folder_name, new_file_name))\\n# Print a success message\\nprint(\"Files renamed successfully.\")\\n```\\nOutput The code calls the `toUpperCase` function to convert `prefix` to uppercase, but \\nthat function is not defined. To fix this issue, you can use the `upper()` method of \\nthe string class which converts a given string into uppercase. The modified code \\nis shown below:\\n```python\\nimport os\\nimport shutil\\n# Get the folder name from the user\\nfolder_name = input(\"Enter the folder name: \")\\nprefix = input(\"Enter the string to prepend to the filename: \")\\ntext = prefix.upper()\\n# Check if the folder exists\\nif not os.path.isdir(folder_name):\\n    print(\"Folder does not exist.\")\\n    exit(1)\\n# Get a list of all files in the folder\\nfiles = os.listdir(folder_name)\\n# Iterate over the files\\nfor file in files:\\n    # Create a new file name with the prefix \"draft_\"\\n    new_filename = f\"{text}_{file}\"\\n# Move the file to the new name\\nshutil.move(os.path.join(folder_name, file),\\nContinues next page...'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 51, 'page_label': '52'}, page_content='Prompt Engineering\\nFebruary 2025\\n52\\nOutput os.path.join(folder_name, new_filename))\\n# Print a success message\\nprint(\"Files renamed successfully.\")\\n```\\nAdditionally, there are a few other improvements that can be made to the code:\\n1. The file extension of the new filenames is not kept. It’s better to extract the \\nextension from the original filename and append it to the new filename.\\n2. The code assumes that there are no spaces in the folder name. However, \\nit is a good practice to handle spaces gracefully by replacing them with \\nunderscores or hyphens.\\n3. Using the `f` string syntax for string interpolation is more readable and \\nconcise than the traditional `+` operator.\\n4. The code doesn’t handle errors that might occur during the renaming process. \\nIt would be better to wrap the `shutil.move` call in a `try...except` block to \\ncatch any potential errors.\\nHere is the improved code with these suggestions:\\n```python\\nimport os\\nimport shutil\\n# Get the folder name from the user\\nfolder_name = input(\"Enter the folder name: \")\\nprefix = input(\"Enter the string to prepend to the filename: \")\\n# Check if the folder exists\\nif not os.path.isdir(folder_name):\\n    print(\"Folder does not exist.\")\\n    exit(1)\\n# Get a list of all files in the folder\\nfiles = os.listdir(folder_name)\\n# Iterate over the files\\nfor file in files:\\nContinues next page...'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 52, 'page_label': '53'}, page_content='Prompt Engineering\\nFebruary 2025\\n53\\nOutput     # Get the file extension\\n    ext = os.path.splitext(file)[1]\\n# Create a new file name with the prefix and the file extension\\n    new_filename = f\"{prefix}_{os.path.splitext(file)[0]}{ext}\"\\n# Build the full paths for the original and new files\\n    old_path = os.path.join(folder_name, file)\\n    new_path = os.path.join(folder_name, new_filename)\\n# Try to move the file\\n    try:\\n        shutil.move(old_path, new_path)\\nexcept Exception as e:\\n        print(f\"An error occurred while moving the file \\n{file}: {e}\")\\n# Print a success message\\nprint(\"Files renamed successfully.\")\\n```\\nThese improvements make the code more robust and flexible while also making \\nit easier to read and understand... (The response was truncated because \\nit has reached the token limit. Try to increase the token limit if you need a \\nlonger response.)\\nTable 19. A prompt for debugging and reviewing Python code\\nThat’s great. Not only did it tell me how to solve the problem, it also figured out that my code \\nhad more bugs and how to solve them, too. The last part of the prompt gave suggestions to \\nimprove the code in general.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 53, 'page_label': '54'}, page_content='Prompt Engineering\\nFebruary 2025\\n54\\nWhat about multimodal prompting?\\nPrompting for code still uses the same regular large language model. Multimodal prompting \\nis a separate concern, it refers to a technique where you use multiple input formats to guide \\na large language model, instead of just relying on text. This can include combinations of text, \\nimages, audio, code, or even other formats, depending on the model’s capabilities and the \\ntask at hand. \\nBest Practices\\nFinding the right prompt requires tinkering. Language Studio in Vertex AI is a perfect place to \\nplay around with your prompts, with the ability to test against the various models.\\nUse the following best practices to become a pro in prompt engineering.\\nProvide examples\\nThe most important best practice is to provide (one shot / few shot) examples within a \\nprompt. This is highly effective because it acts as a powerful teaching tool. These examples \\nshowcase desired outputs or similar responses, allowing the model to learn from them \\nand tailor its own generation accordingly. It’s like giving the model a reference point or \\ntarget to aim for, improving the accuracy, style, and tone of its response to better match \\nyour expectations.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 54, 'page_label': '55'}, page_content=\"Prompt Engineering\\nFebruary 2025\\n55\\nDesign with simplicity\\nPrompts should be concise, clear, and easy to understand for both you and the model. As a \\nrule of thumb, if it’s already confusing for you it will likely be also confusing for the model. Try \\nnot to use complex language and don’t provide unnecessary information. \\nExamples:\\nBEFORE:\\nI am visiting New York right now, and I'd like to hear more about great \\nlocations. I am with two 3 year old kids. Where should we go during \\nour vacation?\\nAFTER REWRITE:\\nAct as a travel guide for tourists. Describe great places to visit in \\nNew York Manhattan with a 3 year old.\\nTry using verbs that describe the action. Here’s a set of examples:\\nAct, Analyze, Categorize, Classify, Contrast, Compare, Create, Describe, Define, \\nEvaluate, Extract, Find, Generate, Identify, List, Measure, Organize, Parse, Pick, \\nPredict, Provide, Rank, Recommend, Return, Retrieve, Rewrite, Select, Show, Sort, \\nSummarize, Translate, Write.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 55, 'page_label': '56'}, page_content='Prompt Engineering\\nFebruary 2025\\n56\\nBe specific about the output\\nBe specific about the desired output. A concise instruction might not guide the LLM enough \\nor could be too generic. Providing specific details in the prompt (through system or context \\nprompting) can help the model to focus on what’s relevant, improving the overall accuracy.\\nExamples:\\nDO: \\nGenerate a 3 paragraph blog post about the top 5 video game consoles. \\nThe blog post should be informative and engaging, and it should be \\nwritten in a conversational style.\\nDO NOT: \\nGenerate a blog post about video game consoles.\\nUse Instructions over Constraints\\nInstructions and constraints are used in prompting to guide the output of a LLM. \\n• An instruction provides explicit instructions on the desired format, style, or content of the \\nresponse. It guides the model on what the model should do or produce.\\n• A constraint is a set of limitations or boundaries on the response. It limits what the model \\nshould not do or avoid.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 56, 'page_label': '57'}, page_content='Prompt Engineering\\nFebruary 2025\\n57\\nGrowing research suggests that focusing on positive instructions in prompting can be more \\neffective than relying heavily on constraints. This approach aligns with how humans prefer \\npositive instructions over lists of what not to do. \\nInstructions directly communicate the desired outcome, whereas constraints might leave the \\nmodel guessing about what is allowed. It gives flexibility and encourages creativity within the \\ndefined boundaries, while constraints can limit the model’s potential. Also a list of constraints \\ncan clash with each other.\\nConstraints are still valuable but in certain situations. To prevent the model from generating \\nharmful or biased content or when a strict output format or style is needed.\\nIf possible, use positive instructions: instead of telling the model what not to do, tell it what to \\ndo instead. This can avoid confusion and improve the accuracy of the output. \\nDO: \\nGenerate a 1 paragraph blog post about the top 5 video game consoles. \\nOnly discuss the console, the company who made it, the year, and total \\nsales. \\nDO NOT: \\nGenerate a 1 paragraph blog post about the top 5 video game consoles. \\nDo not list video game names.\\nAs a best practice, start by prioritizing instructions, clearly stating what you want the model \\nto do and only use constraints when necessary for safety, clarity or specific requirements. \\nExperiment and iterate to test different combinations of instructions and constraints to find \\nwhat works best for your specific tasks, and document these.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 57, 'page_label': '58'}, page_content='Prompt Engineering\\nFebruary 2025\\n58\\nControl the max token length\\nTo control the length of a generated LLM response, you can either set a max token limit in the \\nconfiguration or explicitly request a specific length in your prompt. For example:\\n\"Explain quantum physics in a tweet length message.\"\\nUse variables in prompts\\nTo reuse prompts and make it more dynamic use variables in the prompt, which can be \\nchanged for different inputs. E.g. as shown in Table 20, a prompt which gives facts about a \\ncity. Instead of hardcoding the city name in the prompt, use a variable. Variables can save \\nyou time and effort by allowing you to avoid repeating yourself. If you need to use the same \\npiece of information in multiple prompts, you can store it in a variable and then reference \\nthat variable in each prompt. This makes a lot of sense when integrating prompts into your \\nown applications.\\nPrompt VARIABLES\\n{city} = \"Amsterdam\"\\nPROMPT\\nYou are a travel guide. Tell me a fact about the city: {city}\\nOutput Amsterdam is a beautiful city full of canals, bridges, and narrow streets. It’s a \\ngreat place to visit for its rich history, culture, and nightlife.\\nTable 20. Using variables in prompts'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 58, 'page_label': '59'}, page_content='Prompt Engineering\\nFebruary 2025\\n59\\nExperiment with input formats and writing styles\\nDifferent models, model configurations, prompt formats, word choices, and submits can yield \\ndifferent results. Therefore, it’s important to experiment with prompt attributes like the style, \\nthe word choice, and the type prompt (zero shot, few shot, system prompt).\\nFor example a prompt with the goal to generate text about the revolutionary video game \\nconsole Sega Dreamcast, can be formulated as a question, a statement or an instruction, \\nresulting in different outputs:\\n• Question: What was the Sega Dreamcast and why was it such a revolutionary console?\\n• Statement: The Sega Dreamcast was a sixth-generation video game console released by \\nSega in 1999. It...\\n• Instruction: Write a single paragraph that describes the Sega Dreamcast console and \\nexplains why it was so revolutionary.\\nFor few-shot prompting with classification tasks, mix up \\nthe classes\\nGenerally speaking, the order of your few-shots examples should not matter much. However, \\nwhen doing classification tasks, make sure you mix up the possible response classes in the \\nfew shot examples. This is because you might otherwise be overfitting to the specific order of \\nthe examples. By mixing up the possible response classes, you can ensure that the model is \\nlearning to identify the key features of each class, rather than simply memorizing the order of \\nthe examples. This will lead to more robust and generalizable performance on unseen data.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 59, 'page_label': '60'}, page_content='Prompt Engineering\\nFebruary 2025\\n60\\nA good rule of thumb is to start with 6 few shot examples and start testing the accuracy \\nfrom there.\\nAdapt to model updates\\nIt’s important for you to stay on top of model architecture changes, added data, and \\ncapabilities. Try out newer model versions and adjust your prompts to better leverage new \\nmodel features. Tools like Vertex AI Studio are great to store, test, and document the various \\nversions of your prompt.\\nExperiment with output formats\\nBesides the prompt input format, consider experimenting with the output format. For non-\\ncreative tasks like extracting, selecting, parsing, ordering, ranking, or categorizing data try \\nhaving your output returned in a structured format like JSON or XML. \\nThere are some benefits in returning JSON objects from a prompt that extracts data. In \\na real-world application I don’t need to manually create this JSON format, I can already \\nreturn the data in a sorted order (very handy when working with datetime objects), but most \\nimportantly, by prompting for a JSON format it forces the model to create a structure and \\nlimit hallucinations.\\nIn summary, benefits of using JSON for your output:\\n• Returns always in the same style\\n• Focus on the data you want to receive'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 60, 'page_label': '61'}, page_content=\"Prompt Engineering\\nFebruary 2025\\n61\\n• Less chance for hallucinations\\n• Make it relationship aware\\n• You get data types\\n• You can sort it\\nTable 4 in the few-shot prompting section shows an example on how to return \\nstructured output.\\nJSON Repair\\nWhile returning data in JSON format offers numerous advantages, it's not without its \\ndrawbacks. The structured nature of JSON, while beneficial for parsing and use in \\napplications, requires significantly more tokens than plain text, leading to increased \\nprocessing time and higher costs. Furthermore, JSON's verbosity can easily consume the \\nentire output window, becoming especially problematic when the generation is abruptly cut \\noff due to token limits. This truncation often results in invalid JSON, missing crucial closing \\nbraces or brackets, rendering the output unusable. Fortunately, tools like the json-repair \\nlibrary (available on PyPI) can be invaluable in these situations. This library intelligently \\nattempts to automatically fix incomplete or malformed JSON objects, making it a crucial \\nally when working with LLM-generated JSON, especially when dealing with potential \\ntruncation issues.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 61, 'page_label': '62'}, page_content='Prompt Engineering\\nFebruary 2025\\n62\\nWorking with Schemas\\nUsing structured JSON as an output is a great solution, as we\\'ve seen multiple times in this \\npaper. But what about input? While JSON is excellent for structuring the output the LLM \\ngenerates, it can also be incredibly useful for structuring the input you provide. This is where \\nJSON Schemas come into play. A JSON Schema defines the expected structure and data \\ntypes of your JSON input. By providing a schema, you give the LLM a clear blueprint of the \\ndata it should expect, helping it focus its attention on the relevant information and reducing \\nthe risk of misinterpreting the input. Furthermore, schemas can help establish relationships \\nbetween different pieces of data and even make the LLM \"time-aware\" by including date or \\ntimestamp fields with specific formats.\\nHere\\'s a simple example:\\nLet\\'s say you want to use an LLM to generate descriptions for products in an e-commerce \\ncatalog. Instead of just providing a free-form text description of the product, you can use a \\nJSON schema to define the product\\'s attributes:\\nSnippet 5. Definition of the structured output schema\\n{\\n  \"type\": \"object\",\\n  \"properties\": {\\n    \"name\": { \"type\": \"string\", \"description\": \"Product name\" },\\n    \"category\": { \"type\": \"string\", \"description\": \"Product category\" },\\n    \"price\": { \"type\": \"number\", \"format\": \"float\", \"description\": \"Product \\nprice\" },\\n    \"features\": {\\n      \"type\": \"array\",\\n      \"items\": { \"type\": \"string\" },\\n      \"description\": \"Key features of the product\"\\n    },\\n    \"release_date\": { \"type\": \"string\", \"format\": \"date\", \"description\": \\n\"Date the product was released\"}\\n  },'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 62, 'page_label': '63'}, page_content='Prompt Engineering\\nFebruary 2025\\n63\\nThen, you can provide the actual product data as a JSON object that conforms to \\nthis schema:\\nSnippet 6. Structured output from the LLM\\nBy preprocessing your data and instead of providing full documents only providing both the \\nschema and the data, you give the LLM a clear understanding of the product\\'s attributes, \\nincluding its release date, making it much more likely to generate an accurate and relevant \\ndescription. This structured input approach, guiding the LLM\\'s attention to the relevant fields, \\nis especially valuable when working with large volumes of data or when integrating LLMs into \\ncomplex applications.\\nExperiment together with other prompt engineers\\nIf you are in a situation where you have to try to come up with a good prompt, you might \\nwant to find multiple people to make an attempt. When everyone follows the best practices \\n(as listed in this chapter) you are going to see a variance in performance between all the \\ndifferent prompt attempts.\\n{\\n  \"name\": \"Wireless Headphones\",\\n  \"category\": \"Electronics\",\\n  \"price\": 99.99,\\n  \"features\": [\"Noise cancellation\", \"Bluetooth 5.0\", \"20-hour battery life\"],\\n  \"release_date\": \"2023-10-27\"\\n}'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 63, 'page_label': '64'}, page_content='Prompt Engineering\\nFebruary 2025\\n64\\nCoT Best practices\\nFor CoT prompting, putting the answer after the reasoning is required because the \\ngeneration of the reasoning changes the tokens that the model gets when it predicts the \\nfinal answer.\\nWith CoT and self-consistency you need to be able to extract the final answer from your \\nprompt, separated from the reasoning. \\nFor CoT prompting, set the temperature to 0.\\nChain of thought prompting is based on greedy decoding, predicting the next word in a \\nsequence based on the highest probability assigned by the language model. Generally \\nspeaking, when using reasoning, to come up with the final answer, there’s likely one single \\ncorrect answer. Therefore the temperature should always set to 0. \\nDocument the various prompt attempts\\nThe last tip was mentioned before in this chapter, but we can’t stress enough how important \\nit is: document your prompt attempts in full detail so you can learn over time what went well \\nand what did not. \\nPrompt outputs can differ across models, across sampling settings, and even across different \\nversions of the same model. Moreover, even across identical prompts to the same model, \\nsmall differences in output sentence formatting and word choice can occur. (For example, as \\nmentioned previously, if two tokens have the same predicted probability, ties may be broken \\nrandomly. This can then impact subsequent predicted tokens.).'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 64, 'page_label': '65'}, page_content='Prompt Engineering\\nFebruary 2025\\n65\\nWe recommend creating a Google Sheet with Table 21 as a template. The advantages of \\nthis approach are that you have a complete record when you inevitably have to revisit your \\nprompting work–either to pick it up in the future (you’d be surprised how much you can \\nforget after just a short break), to test prompt performance on different versions of a model, \\nand to help debug future errors.\\nBeyond the fields in this table, it’s also helpful to track the version of the prompt (iteration), \\na field to capture if the result was OK/NOT OK/SOMETIMES OK, and a field to capture \\nfeedback. If you’re lucky enough to be using Vertex AI Studio, save your prompts (using the \\nsame name and version as listed in your documentation) and track the hyperlink to the saved \\nprompt in the table. This way, you’re always one click away from re-running your prompts.\\nWhen working on a retrieval augmented generation system, you should also capture the \\nspecific aspects of the RAG system that impact what content was inserted into the prompt, \\nincluding the query, chunk settings, chunk output, and other information.\\nOnce you feel the prompt is close to perfect, take it to your project codebase. And in the \\ncodebase, save prompts in a separate file from code, so it’s easier to maintain. Finally, ideally \\nyour prompts are part of an operationalized system, and as a prompt engineer you should \\nrely on automated tests and evaluation procedures to understand how well your prompt \\ngeneralizes to a task.\\nPrompt engineering is an iterative process. Craft and test different prompts, analyze, \\nand document the results. Refine your prompt based on the model’s performance. Keep \\nexperimenting until you achieve the desired output. When you change a model or model \\nconfiguration, go back and keep experimenting with the previously used prompts.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 65, 'page_label': '66'}, page_content='Prompt Engineering\\nFebruary 2025\\n66\\nName [name and version of your prompt]\\nGoal [One sentence explanation of the goal of this attempt]\\nModel [name and version of the used model]\\nTemperature [value between 0 - 1] Token Limit [number]\\nTop-K [number] Top-P [number]\\nPrompt [Write all the full prompt]\\nOutput [Write out the output or multiple outputs]\\nTable 21. A template for documenting prompts\\nSummary\\nThis whitepaper discusses prompt engineering. We learned various prompting techniques, \\nsuch as:\\n• Zero prompting\\n• Few shot prompting\\n• System prompting\\n• Role prompting\\n• Contextual prompting\\n• Step-back prompting\\n• Chain of thought\\n• Self consistency\\n• Tree of thoughts'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 66, 'page_label': '67'}, page_content='Prompt Engineering\\nFebruary 2025\\n67\\n• ReAct\\nWe even looked into ways how you can automate your prompts. \\nThe whitepaper then discusses the challenges of gen AI like the problems that can happen \\nwhen your prompts are insufficient. We closed with best practices on how to become a better \\nprompt engineer.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 67, 'page_label': '68'}, page_content='Prompt Engineering\\nFebruary 2025\\n68\\nEndnotes\\n1. Google, 2023, Gemini by Google. Available at: https://gemini.google.com .\\n2. Google, 2024, Gemini for Google Workspace Prompt Guide. Available at:  \\nhttps://inthecloud.withgoogle.com/gemini-for-google-workspace-prompt-guide/dl-cd.html .\\n3. Google Cloud, 2023, Introduction to Prompting. Available at:  \\nhttps://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/introduction-prompt-design .\\n4. Google Cloud, 2023, Text Model Request Body: Top-P & top-K sampling methods. Available at:  \\nhttps://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text#request_body .\\n5. Wei, J., et al., 2023, Zero Shot - Fine Tuned language models are zero shot learners. Available at:  \\nhttps://arxiv.org/pdf/2109.01652.pdf .\\n6. Google Cloud, 2023, Google Cloud Model Garden. Available at: https://cloud.google.com/model-garden .\\n7. Brown, T., et al., 2023, Few Shot - Language Models are Few Shot learners. Available at:  \\nhttps://arxiv.org/pdf/2005.14165.pdf.\\n8. Zheng, L., et al., 2023, Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models. \\nAvailable at: https://openreview.net/pdf?id=3bq3jsvcQ1\\n9. Wei, J., et al., 2023, Chain of Thought Prompting. Available at: https://arxiv.org/pdf/2201.11903.pdf .\\n10. Google Cloud Platform, 2023, Chain of Thought and React. Available at: https://github.com/ \\nGoogleCloudPlatform/generative-ai/blob/main/language/prompts/examples/chain_of_thought_react.ipynb . \\n11. Wang, X., et al., 2023, Self Consistency Improves Chain of Thought reasoning in language models.  \\nAvailable at: https://arxiv.org/pdf/2203.11171.pdf .\\n12. Yao, S., et al., 2023, Tree of Thoughts: Deliberate Problem Solving with Large Language Models.  \\nAvailable at: https://arxiv.org/pdf/2305.10601.pdf .\\n13. Yao, S., et al., 2023, ReAct: Synergizing Reasoning and Acting in Language Models. Available at:  \\nhttps://arxiv.org/pdf/2210.03629.pdf.\\n14. Google Cloud Platform, 2023, Advance Prompting: Chain of Thought and React. Available at: \\nhttps://github.com/GoogleCloudPlatform/applied-ai-engineering-samples/blob/main/genai-  \\non-vertex-ai/advanced_prompting_training/cot_react.ipynb .\\n15. Zhou, C., et al., 2023, Automatic Prompt Engineering - Large Language Models are Human-Level Prompt \\nEngineers. Available at: https://arxiv.org/pdf/2211.01910.pdf .'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 0, 'page_label': '1'}, page_content='Solving  \\nDomain-Specific \\nProblems Using \\nLLMs\\nAuthors: Christopher Semturs,  \\nShekoofeh Azizi, Scott Coull,  \\nUmesh Shankar  \\nand Wieland Holfelder'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 1, 'page_label': '2'}, page_content='Solving Domain-Specific Problems Using LLMs\\n2\\nFebruary 2025\\nAcknowledgements\\nContent contributors\\nJohan Schalkwyk\\nSami Lachgar\\nDale Webster\\nYun Liu\\nConnie Fan\\nChris Grier\\nAntonio Gulli\\nAnant Nawalgaria\\nCurators and Editors\\nAntonio Gulli\\nAnant Nawalgaria\\nGrace Mollison \\nTechnical Writer\\nKaren Lin\\nDesigners\\nMichael Lanning\\nLuxi Chen'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 2, 'page_label': '3'}, page_content='Introduction 4\\nSecLM and the future of cybersecurity 5\\n Challenges in cybersecurity  6\\n How GenAI can tackle the challenges in cybersecurity 7\\n SecLM: An API for cybersecurity tasks 10\\n  Security-focused large language models 12\\n\\t \\t A\\tflexible\\tplanning\\tand\\treasoning\\tframework 16\\nMedLM and the future of health tech 20\\n\\t The\\tpotential\\tfor\\tGenAI\\tin\\tmedical\\tQ&A 20\\n\\t The \\topportunities 21\\n\\t The\\tscientific\\tstarting\\tpoint 22\\n  How to evaluate: quantitative and qualitative 24\\n Evaluation in real clinical environments 30\\n\\t Task-\\tvs.\\tdomain-specific\\tmodels 31\\n Training strategies for Med-PaLM 2 32\\nSummary 35\\nEndnotes 36\\nTable of contents'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 3, 'page_label': '4'}, page_content='Solving Domain-Specific Problems Using LLMs\\n4\\nFebruary 2025\\nIntroduction\\nLarge\\tlanguage\\tmodels\\t(LLMs)\\thave\\temerged\\tas\\tpowerful\\ttools\\tfor\\ttackling\\tcomplex\\t\\nchallenges\\tin\\tnumerous\\tdomains.\\tWhile\\tearly\\titerations\\tfocused\\ton\\tgeneral-purpose\\ttasks,\\t\\nrecent\\tdevelopments\\thave\\thighlighted\\tthe\\tpotential\\tof\\tfine-tuning\\tLLMs\\tto\\taddress\\tspecific\\t\\nproblems\\twithin\\tspecialized\\tfields.\\tThis\\twhitepaper\\texplores\\tthese\\tconcepts\\tin\\ttwo\\tdistinct\\t\\ndomains: cybersecurity and medicine. Each showcases the unique ability of LLMs to enhance \\nexisting\\tworkflows\\tand\\tunlock\\tnew\\tpossibilities.\\nCybersecurity\\tpresents\\ta\\tnumber\\tof\\tunique\\tchallenges\\tfor\\tLLMs,\\tincluding\\ta\\tscarcity\\tof\\t\\npublicly\\tavailable\\tdata,\\ta\\twide\\tdiversity\\tof\\thighly\\ttechnical\\tconcepts,\\tand\\tinformation\\tabout\\t\\nthreats\\tthat\\tchange\\ton\\ta\\tdaily\\tbasis.\\tAdditionally,\\tsensitive\\tuse\\tcases,\\tlike\\tmalware\\tanalysis,\\t\\nnecessitate\\tspecific\\tconsiderations\\tfor\\tmodel\\tdevelopment.\\tWe\\taddress\\tthese\\tchallenges\\t\\n... recent developments \\nhave highlighted the potential \\nof fine-tuning LLMs \\nto address specific problems \\nwithin specialized fields.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 4, 'page_label': '5'}, page_content='Solving Domain-Specific Problems Using LLMs\\n5\\nFebruary 2025\\nby\\tfocusing\\ton\\tcybersecurity-specific\\tcontent\\tand\\ttasks,\\tpairing\\tsecurity-focused\\tlanguage\\t\\nmodels\\twith\\ta\\tsuite\\tof\\tsupporting\\ttechniques\\tto\\toffer\\timproved\\tperformance\\tfor\\tvital\\ttasks\\t\\nlike\\tthreat\\tidentification\\tand\\trisk\\tanalysis.\\nIn\\tthe\\tfield\\tof\\tmedicine,\\tLLMs\\tface\\ta\\tdifferent\\tset\\tof\\tobstacles,\\tsuch\\tas\\tthe\\tvast\\tand\\tever-\\nevolving\\tnature\\tof\\tmedical\\tknowledge\\tand\\tthe\\tneed\\tto\\tapply\\tsaid\\tknowledge\\tin\\ta\\tcontext-\\ndependent\\tmanner\\tthat\\tmakes\\taccurate\\tdiagnosis\\tand\\ttreatment\\ta\\tcontinual\\tchallenge.\\t\\nLLMs\\tlike\\tMed-PaLM,\\tcustomized\\tfor\\tmedical\\tapplications,\\tdemonstrate\\tthe\\tability\\tto\\tanswer\\t\\ncomplex\\tmedical\\tquestions\\tand\\tprovide\\tinsightful\\tinterpretations\\tof\\tmedical\\tdata,\\tshowing\\t\\npotential\\tfor\\tsupporting\\tboth\\tclinicians\\tand\\tpatients.\\nThrough\\tthe\\tlens\\tof\\tthese\\ttwo\\tdistinct\\tdomains,\\tin\\tthis\\twhitepaper\\twe\\twill\\texplore\\tthe\\t\\nchallenges\\tand\\topportunities\\tpresented\\tby\\tspecialized\\tdata,\\ttechnical\\tlanguage,\\tand\\t\\nsensitive\\tuse\\tcases.\\tBy\\texamining\\tthe\\tunique\\tpaths\\ttaken\\tby\\tSecLM\\tand\\tMed-PaLM,\\twe\\t\\nprovide\\tinsights\\tinto\\tthe\\tpotential\\tof\\tLLMs\\tto\\trevolutionize\\tvarious\\tareas\\tof\\texpertise.\\nSecLM and the future of cybersecurity\\nSecurity\\tpractitioners\\tface\\ta\\tmyriad\\tof\\tchallenges,\\tincluding\\tnew\\tand\\tevolving\\tthreats,\\t\\noperational\\ttoil,\\tand\\ta\\ttalent\\tshortage.\\tSpecialized\\tGenerative\\tAI\\t(Gen\\tAI)\\tcan\\thelp\\taddress\\t\\nthese\\tchallenges\\tby\\tautomating\\trepetitive\\ttasks,\\tfreeing\\tup\\ttime\\tfor\\tmore\\tstrategic\\tactivities,\\t\\nand\\tproviding\\tnew\\topportunities\\tto\\taccess\\tknowledge.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 5, 'page_label': '6'}, page_content='Solving Domain-Specific Problems Using LLMs\\n6\\nFebruary 2025\\nChallenges in cybersecurity \\nIn\\tthe\\tmovies,\\twe\\toften\\tsee\\tinformation\\tsecurity\\treduced\\tto\\tthe\\tcaricature\\tof\\thoodie-clad\\t\\nand\\theadset-wearing\\thackers\\twith\\till\\tintent,\\tarmed\\twith\\truggedized\\tlaptops,\\ttapping\\taway\\t\\nfuriously until we hear the two magic words: “I’m in.” \\nTo\\tthe\\textent\\tthat\\tyou\\teven\\tsee\\tthe\\tdefenders,\\tthey\\tare\\tin\\treactive\\tmode-think\\twar\\trooms,\\t\\nempty\\tcoffee\\tcups,\\tpeople\\tbarking\\torders,\\tand\\tmonitors\\tshowing\\tthe\\tattacker’s\\tevery\\tmove\\t\\nin real-time. \\nThat is Hollywood; we live in the real world.\\nIn\\treality,\\tthe\\tpeople\\twho\\tpractice\\tcybersecurity\\t-\\tthe\\tdevelopers,\\tsystem\\tadministrators,\\t\\nSREs,\\tand\\tmany\\tjunior\\tanalysts\\tto\\twhom\\tour\\twork\\there\\tis\\tdedicated\\t-\\thave\\tthe\\tSisyphean\\t\\ntask\\tof\\tkeeping\\tup\\twith\\tthe\\tlatest\\tthreats\\tand\\ttrying\\tto\\tprotect\\tcomplex\\tsystems\\tagainst\\t\\nthem.\\tMany\\tpractitioners’\\tdays\\tare\\tlargely\\tfilled\\twith\\trepetitive\\tor\\tmanual\\ttasks,\\tsuch\\tas\\t\\nindividually\\ttriaging\\thundreds\\tof\\talerts,\\tthat\\ttake\\tvaluable\\ttime\\taway\\tfrom\\tdeveloping\\tmore\\t\\nstrategic\\tdefenses.\\tThe\\tmomentum\\tis\\tdefinitely\\tnot\\tin\\tthe\\tdefender’s\\tfavor;\\tattackers\\tare\\t\\nadopting\\tadvanced\\ttechnologies,\\tincluding\\tartificial\\tintelligence,1 to extend their reach and \\nquicken\\tthe\\tpace\\tof\\texploitation.\\tAnd\\tthere\\tare\\tdefinitely\\tno\\tmonitors\\tshowing\\tthe\\tattacker’s\\t\\nevery move!\\nBased\\ton\\tour\\texperience\\tworking\\twith\\tusers\\tand\\tpartners,\\twe\\tsee\\tthree\\tmajor\\tchallenges\\tin\\t\\nthe security industry today: threats,\\ttoil,\\tand\\ttalent. \\n• New and evolving threats:\\tThe\\tthreat\\tlandscape\\tis\\tconstantly\\tchanging,\\twith\\tnew\\t\\nand\\tincreasingly\\tsophisticated\\tattacks\\temerging\\tall\\tthe\\ttime.\\tThis\\tmakes\\tit\\tdifficult\\tfor\\t\\ndefenders\\tto\\tkeep\\tup\\twith\\tthe\\tlatest\\tinformation,\\tand\\tconversely\\tfor\\tpractitioners\\tto\\tsift\\t\\nthrough\\tthat\\tflood\\tof\\tdata\\tto\\tidentify\\twhat’s\\trelevant\\tto\\tthem\\tand\\ttake\\taction.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 6, 'page_label': '7'}, page_content='Solving Domain-Specific Problems Using LLMs\\n7\\nFebruary 2025\\n• Operational toil:\\tPeople\\tworking\\tin\\tsecurity\\toperations\\tor\\tDevOps\\troles\\toften\\tspend\\ta\\t\\nsignificant\\tamount\\tof\\ttime\\ton\\trepetitive\\tmanual\\ttasks\\tthat\\tcould\\tbe\\tautomated\\tor\\tassisted.\\t\\nThis leads to overload and takes away time from more strategic activities. Excessive focus \\non\\tminutiae\\talso\\tprevents\\tanalysts\\tand\\tengineers\\tfrom\\tseeing\\tthe\\tbigger\\tpicture\\tthat\\tis\\t\\nkey\\tto\\tsecuring\\ttheir\\torganizations.\\n• Talent shortage :\\tThere\\tis\\ta\\tshortage\\tof\\tskilled\\tsecurity\\tprofessionals,\\tmaking\\tit\\tdifficult\\t\\nfor\\torganizations\\tto\\tfind\\tthe\\tpeople\\tthey\\tneed\\tto\\tprotect\\ttheir\\tdata\\tand\\tsystems.\\tOften,\\t\\npeople\\tenter\\tsecurity-focused\\troles\\twithout\\tmuch\\ttraining\\tand\\twith\\tlittle\\tspare\\ttime\\tto\\t\\nexpand\\ttheir\\tskills\\ton\\tthe\\tjob.\\nWithout\\tthe\\tability\\tto\\taddress\\tthese\\tthree\\tchallenges,\\tit\\twill\\tbe\\tdifficult\\tto\\tkeep\\tup\\twith\\tthe\\t\\ndemands of modern cybersecurity systems.\\nHow GenAI can tackle the challenges in cybersecurity\\nWe\\tenvision\\ta\\tworld\\twhere\\tnovices\\tand\\tsecurity\\texperts\\talike\\tare\\tpaired\\twith\\tAI\\texpertise\\t\\nto\\tfree\\tthemselves\\tfrom\\trepetition\\tand\\ttoil,\\taccomplish\\ttasks\\tthat\\tseem\\timpossible\\tto\\tus\\t\\ntoday,\\tand\\tprovide\\tnew\\topportunities\\tto\\tshare\\tknowledge.\\tLarge\\tlanguage\\tmodels\\t(LLMs)\\t\\nand\\tadjacent\\tGenAI\\ttechniques\\tcan\\tmeaningfully\\timprove\\tthe\\tworking\\tlives\\tof\\tboth\\tsecurity\\t\\nnovices\\tand\\texperienced\\tpractitioners.\\tIndeed,\\tin\\tmany\\tcases,\\twe\\thave\\talready\\tfound\\tthat\\t\\nGenAI\\tis\\tuseful\\tto\\tsolve\\ta\\tnumber\\tof\\treal-world\\tsecurity\\tproblems\\tin\\tour\\tchallenge\\tareas:'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 7, 'page_label': '8'}, page_content='Solving Domain-Specific Problems Using LLMs\\n8\\nFebruary 2025\\nPersona(e) Challenges faced How Gen AI can help\\nSecurity analyst\\nAnalysts not familiar with \\neach\\ttool’s\\tbespoke\\tschema\\t\\nand query language.\\nTranslate natural-language \\nqueries\\tinto\\ta\\tdomain-specific\\t\\nsecurity event query language and \\nrules language.\\nInvestigating,\\tclustering,\\tand\\t\\ntriaging\\tincoming\\talerts\\tis\\t\\ntime-consuming and requires \\nmultiple\\tsteps\\tand\\ttools.\\nAutonomous\\tcapabilities\\tto\\t\\nperform\\tinvestigation,\\tgrouping,\\t\\nand\\tclassification,\\tincorporating\\t\\ncontext and real-time tool use.\\nHard to assemble the right \\nseries\\tof\\ttailored\\tsteps\\tto\\t\\nremediate an issue.\\nPersonalized,\\tcase-specific\\t\\nremediation\\tplanning\\tin\\t\\nuser environments.\\nThreat Researcher or \\nSystem Administrator\\nAn unknown and obfuscated \\nartifact\\t(such\\tas\\ta\\tscript\\t\\nor binary) is discovered \\nand can’t be easily \\nanalyzed\\tmanually.\\nAutomated reverse engineering \\nwith\\tLLM-powered\\tcode\\tanalysis\\t\\nwith tool use for de-obfuscation \\nand\\tdecompilation.\\tExplain,\\t\\nanalyze,\\tand\\tclassify\\tpotentially\\t\\nmalicious\\tartifacts.\\nCISO team\\nManual work required to \\nidentify\\tand\\tsummarize\\tthe\\t\\nmost likely threats facing \\nthe\\torganization.\\nGenerate a readable document \\nor\\tslide\\tdeck,\\tapplying\\tthe\\tlatest\\t\\nthreat\\tintelligence\\tand\\tfindings\\t\\nfrom security tools to the \\nspecific\\torganization.\\nIT Administrator \\nDedicated Security Team\\nHard to understand all the \\nways\\tan\\tattacker\\tcould\\t\\naccess sensitive resources.\\nIdentify\\tpotential\\tor\\tactual\\tattack\\t\\npaths,\\thighlighting\\tkey\\telements\\t\\nand remediations.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 8, 'page_label': '9'}, page_content='Solving Domain-Specific Problems Using LLMs\\n9\\nFebruary 2025\\nApplication Developers\\nChallenging to determine \\nthe\\tright\\tplaces\\tto\\tfuzz-test\\t\\nan\\tapplication.\\nIdentify which locations to \\nfuzz-test\\tand\\tgenerate\\tthe\\t\\nappropriate\\tcode.\\nApplication Developers & \\nIT Administrators\\nKeep\\taccess\\tpolicies\\t\\naligned\\tto\\tthe\\tprinciple\\tof\\t\\nleast\\tprivilege.\\nGiven\\thistorical\\taccess\\tpatterns\\t\\nand\\tcurrent\\tconfiguration,\\t\\nconstruct\\ta\\tconfiguration\\tfile\\t\\nmodification\\tthat\\tgrants\\ta\\tmore\\t\\nminimal set of roles.\\nA person responsible for an \\napplication or system\\nPeople\\tdon’t\\talways\\t\\nunderstand\\tsecurity\\tconcepts\\t\\nor\\thow\\tto\\tapply\\tthem\\tto\\ttheir\\t\\nenvironments; they have to \\nknow\\thow\\tto\\tbreak\\ta\\tproblem\\t\\ndown,\\task\\tquestions\\tin\\tmany\\t\\nplaces,\\tand\\tthen\\tcombine\\t\\nthem to obtain an answer.\\nGive\\tan\\tanswer\\tthat\\treflects\\t\\nauthoritative\\tsecurity\\texpertise\\t\\nand,\\tusing\\tintegrations,\\tis\\trelevant\\t\\nto the user’s working environment.\\nTo\\ttackle\\tthese\\tproblems\\tin\\ta\\tmeaningful\\tand\\tholistic\\tway,\\thowever,\\twe\\tneed\\ta\\t\\nmulti-layered\\tapproach:\\n• Top layer: existing\\tsecurity\\ttools\\tthat\\tunderstand\\tthe\\trelevant\\tcontext\\tand\\tdata,\\tand\\t\\ncan actuate necessary changes;\\n• Middle layer: a\\tsecurity-specialized\\tmodel\\tAPI\\twith\\tadvanced\\treasoning\\tand\\t\\nplanning\\tcapabilities;\\n• Bottom layer: datastores of authoritative security intelligence and \\noperational\\texpertise\\nNotably,\\tone\\tof\\tthe\\tkey\\tbenefits\\tof\\tLLMs\\tis\\ttheir\\tability\\tto\\tprocess\\tand\\tsynthesize\\tvast\\t\\namounts\\tof\\theterogenous\\tdata\\t–\\tan\\timportant\\tcapability\\tin\\tthe\\tincreasingly\\tsiloed\\tworld\\t\\nof\\tcybersecurity\\tdata.\\tWe\\tseek\\tto\\tleverage\\tthat\\tcapability\\tto\\tsolve\\tchallenging\\tsecurity'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 9, 'page_label': '10'}, page_content='Solving Domain-Specific Problems Using LLMs\\n10\\nFebruary 2025\\nproblems,\\twhether\\tby\\tassisting\\thuman\\tanalysts\\tor\\tthrough\\tautonomous\\tagents,\\tby\\tcombining\\t\\nrelevant\\tcontext\\tand\\tauthoritative\\tsources\\twith\\ta\\tflexible\\tplanning\\tframework\\tin\\ta\\tsingle\\tAPI,\\t\\nwhich we call SecLM.\\nThis\\tAPI\\toffers\\trich\\tplanning\\tcapabilities\\tthat\\tcombine\\tLLMs\\tand\\tother\\tML\\tmodels,\\tRetrieval-\\nAugmented\\tGeneration\\t(RAG)\\tto\\tground\\tresults\\tin\\tauthoritative\\tdata,\\tand\\ttool\\tuse\\tto\\tperform\\t\\nactions\\tor\\tlook\\tup\\trelevant\\tinformation.\\tWe\\targue\\tthat\\tthis\\tholistic\\tapproach\\tis\\tcritical\\t\\nbecause\\taccuracy\\tis\\tso\\timportant\\tin\\tsecurity\\tand\\tLLMs\\talone\\tcannot\\tinherently\\tsolve\\tall\\t\\nsecurity\\tproblems.\\nSecLM: An API for cybersecurity tasks\\nOur\\tvision\\tof\\tthe\\tSecLM\\tAPI\\tis\\tto\\tprovide\\ta\\t‘one-stop\\tshop’\\tfor\\tgetting\\tanswers\\tto\\tsecurity\\t\\nquestions,\\tregardless\\tof\\ttheir\\tlevel\\tof\\tcomplexity.\\tThat\\tis,\\tthe\\tengineer\\tor\\tanalyst\\tcan\\tpose\\t\\nquestions\\tand\\trefer\\tto\\tdata\\tsources\\twith\\tnatural\\tlanguage,\\tand\\texpect\\tan\\tanswer\\tthat\\t\\nautomatically\\tincorporates\\tthe\\tnecessary\\tinformation.\\tHowever,\\tsecurity\\tproblems\\toften\\t\\nrequire\\ta\\tlot\\tof\\tinformation\\tto\\tbe\\tgathered\\tand\\tanalyzed\\tusing\\tdomain-specific\\treasoning,\\t\\noften\\tby\\texperts\\tacross\\tseveral\\tdisciplines.\\nIdeally,\\tone\\tcan\\task\\tthe\\tSecLM\\tAPI\\ta\\tquestion\\tin\\ta\\tzero-shot\\tmanner\\tand\\tget\\ta\\thigh-quality\\t\\nresponse\\twithout\\tfussing\\tover\\tprompting\\tor\\tmanually\\tintegrating\\texternal\\tdata.\\tIn\\torder\\tto\\t\\nachieve\\tthis\\tin\\ta\\tcoherent\\tand\\tseamless\\tmanner,\\tit\\tis\\timportant\\tto\\thave\\ta\\twell-designed\\tAPI\\t\\nthat\\tinteracts\\twith\\tLLMs\\tand\\ttraditional\\tML\\tmodels,\\tthe\\tuser’s\\tdata,\\tand\\tother\\tservices\\tto\\t\\naccurately\\tcomplete\\tthe\\ttask\\tat\\thand.\\tDue\\tto\\tthe\\tcomplex\\tnature\\tof\\tthese\\tsecurity\\tproblems,\\t\\nwe must aim to address the following key requirements:'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 10, 'page_label': '11'}, page_content='Solving Domain-Specific Problems Using LLMs\\n11\\nFebruary 2025\\n• Freshness: The model should be able to access the latest threat and vulnerability \\ndata,\\twhich\\tchanges\\ton\\ta\\tdaily\\tbasis.\\tDue\\tto\\tits\\tcost\\tand\\tduration\\t(often\\tdays),\\t\\nretraining\\tthe\\tmodel\\ton\\ta\\tdaily\\tor\\thourly\\tbasis\\tto\\tincorporate\\tthe\\tlatest\\tdata\\tis\\tnot\\ta\\t\\nfeasible\\tapproach.\\n• User-specific data: The\\tmodel\\tshould\\tbe\\table\\tto\\toperate\\ton\\tthe\\tuser’s\\town\\tsecurity\\t\\ndata\\twithin\\tthe\\tuser’s\\tenvironment\\twithout\\tthe\\trisk\\tof\\texposing\\tthat\\tsensitive\\tdata\\t\\nto\\tothers\\tor\\tthe\\tinfrastructure\\tprovider.\\tThis\\trules\\tout\\tany\\tcentralized\\ttraining\\ton\\t\\nuser data.\\n• Security expertise: The model should be able to understand high-level security \\nconcepts\\tand\\tterminology,\\tand\\tbreak\\tthem\\tinto\\tmanageable\\tpieces\\tthat\\tare\\tuseful\\t\\nwhen\\tsolving\\tthe\\tproblem.\\tFor\\tinstance,\\tdecomposing\\ta\\thigh-level\\tattack\\tstrategy\\t(e.g.,\\t\\nlateral\\tmovement)\\tinto\\tits\\tconstituent\\tcomponents\\tfor\\tsearch\\tor\\tdetection.\\n• User-specific data: The\\tmodel\\tshould\\tbe\\table\\tto\\treason\\tabout\\tthe\\tprovided\\tsecurity\\t\\ndata\\tin\\ta\\tmulti-step\\tfashion\\tby\\tcombining\\tdifferent\\tdata\\tsources,\\ttechniques,\\tand\\t\\nspecialized\\tmodels\\tto\\tsolve\\tsecurity\\tproblems.\\nSecLM\\taddresses\\tthese\\tchallenges\\tthrough\\tthe\\tuse\\tof\\tsecurity-specialized\\tLLMs,\\ttraditional\\t\\nML\\tmodels,\\tand\\ta\\tflexible\\tplanning\\tframework\\tthat\\tenables\\tdynamic\\tuse\\tof\\ttools\\tand\\t\\ninteraction\\tamong\\tmultiple\\tdomain-specialized\\tagents\\tto\\treason\\tover\\tthe\\tprovided\\tdata.\\t\\nHere,\\twe\\twill\\tbriefly\\tdiscuss\\tour\\tapproach\\tto\\ttraining\\tsecurity-specialized\\tmodels\\tand\\t\\ndesigning\\tthe\\tplanning\\tframework\\tthat\\tdrives\\tthe\\tSecLM\\tAPI.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 11, 'page_label': '12'}, page_content='Solving Domain-Specific Problems Using LLMs\\n12\\nFebruary 2025\\nSecurity-focused large language models\\nOne\\tof\\tthe\\tthings\\twe\\tobserved\\tin\\tapplying\\tLLMs\\tto\\tsecurity\\tis\\tthat\\tgeneral-purpose\\tmodels\\t\\ndidn’t\\tpeform\\tas\\twell\\tas\\twe\\tneeded\\ton\\tsome\\tsecurity\\ttasks.\\tThe\\treasons\\tfor\\tthis\\tfall\\tinto\\t\\nthree categories:\\n• Lack of publicly available security data:\\tLLMs\\tare\\tdata-hungry,\\trequiring\\tlarge\\tpre-\\ntraining\\tcorpora\\tfor\\tbest\\tresults.\\tAt\\tthe\\tsame\\ttime,\\tsecurity\\tdata\\tis\\tsensitive\\tso\\twe\\tcannot\\t\\nuse\\treal\\tsecurity\\tdata\\tin\\ttraining.\\tMoreover,\\twhat\\tlittle\\tdata\\tis\\tavailable\\tpublicly\\tis\\tusually\\t\\nconcentrated\\ton\\ta\\tsmall\\tnumber\\tof\\tthe\\tmost\\tpopular\\tsecurity\\tproducts\\tor\\ton\\tgeneric\\t\\nsecurity\\tcontent\\tthat\\tlacks\\tconnection\\tto\\tconcrete\\tapplication.\\t\\n• Limited depth of security content:\\tSimilarly,\\tthere\\tis\\ta\\tcertain\\thighly\\ttechnical\\tlanguage\\t\\nthat\\tis\\tused\\tto\\ttalk\\tabout\\tsecurity\\tor\\texpress\\tsecurity\\tinsights,\\toften\\tcrossing\\tdisciplines\\t\\nfrom\\tlow-level\\tcomputer\\tscience\\tconcepts\\tto\\thigh-level\\tpolicy\\tand\\tintelligence\\tanalysis.\\t\\nTo\\tbe\\teffective,\\tsecurity\\tLLMs\\tmust\\tseamlessly\\tblend\\tthis\\tlanguage,\\tconnect\\tthem\\tto\\t\\ntheir\\tunderlying\\ttechnical\\tconcepts,\\tand\\tsynthesize\\trelevant,\\taccurate\\toutput\\tfor\\tsecurity\\t\\nanalysts\\tand\\tengineers\\tto\\tconsume.\\tWhile\\tthere\\tare\\tsome\\thigh-quality,\\tin-depth\\tarticles\\t\\nthat\\texplain\\thow\\tto\\taddress\\twell-known\\tvulnerabilities\\tor\\tattacks,\\tthousands\\tof\\tnew\\t\\nthreats emerge each year.\\n• Sensitive use cases:\\tThere\\tare\\tsome\\tuse\\tcases\\tin\\tsecurity\\tthat\\tgeneral\\tpurpose\\tmodels\\t\\ndo\\tnot\\thandle\\tby\\tdesign\\tsuch\\tas\\tabuse\\tareas\\tlike\\tmalware\\tor\\tphishing.\\tIn\\tmost\\tcases,\\t\\ngeneral-purpose\\tLLMs\\twould\\tactively\\twork\\tto\\tavoid\\tincorporating\\tsuch\\ttasks\\tor\\trelated\\t\\ndata\\tfor\\tfear\\tof\\tincreasing\\trisk\\tof\\tmisuse\\tor\\tabuse.\\tHowever,\\tthese\\tcases\\tare\\tcrucial\\tfor\\t\\nsecurity\\tpractitioners\\tlooking\\tto\\tsecure\\ttheir\\tsystems,\\tto\\tanalyze\\tartifacts,\\tor\\teven\\tfor\\t\\ntesting\\tpurposes.\\t\\nTaken\\ttogether,\\tthese\\tchallenges\\tmotivate\\tthe\\tdevelopment\\tof\\tsecurity-focused\\tLLMs\\t\\nthat\\toperate\\tacross\\tas\\tmany\\tsecurity\\tplatforms\\tand\\tenvironments\\tas\\tthe\\thumans\\tthey\\twill\\t\\nultimately\\tsupport.\\tTo\\tthis\\tend,\\twe\\tdevelop\\tspecialized\\tLLMs\\tthat\\thave\\tbeen\\ttrained\\ton\\ta\\t\\nvariety\\tof\\tcybersecurity-specific\\tcontent\\tand\\ttasks.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 12, 'page_label': '13'}, page_content='Solving Domain-Specific Problems Using LLMs\\n13\\nFebruary 2025\\nThis\\tbroad\\tset\\tof\\tsupported\\ttasks\\tmeans\\tthat\\twe\\thave\\tto\\ttake\\tinto\\taccount\\tmultiple\\tuse\\t\\ncases\\tand\\tenvironments\\twhen\\tmaking\\tdesign\\tdecisions,\\tsuch\\tas\\tchoosing\\tthe\\tmodel\\tsize\\tand\\t\\ncomposition\\tof\\ttraining\\ttasks.\\tFor\\texample,\\tan\\tLLM\\twith\\thundreds\\tof\\tbillions\\tof\\tparameters\\t\\nmay\\tmaximize\\treasoning\\tand\\tabstraction\\tcapabilities,\\tbut\\tmight\\tnot\\tbe\\tideal\\tfor\\tlatency-\\nsensitive\\tor\\thigh-volume\\ttasks,\\tlike\\tsummarizing\\tand\\tcategorizing\\tsecurity\\tevents.\\nTo\\tensure\\tthe\\tmodel\\tgeneralizes\\tto\\tnew\\ttasks\\tand\\tsecurity\\tproducts\\tnot\\tdirectly\\tvisible\\tin\\tthe\\t\\ntraining\\tdata,\\twe\\thave\\tto\\tbe\\tvery\\tcareful\\twith\\tthe\\ttraining\\tregime\\tused\\tto\\tcreate\\tthe\\tmodels.\\t\\nAs\\tan\\texample,\\tconsider\\tthat\\tfor\\tmany\\ttask\\tareas,\\tsuch\\tas\\ttranslating\\tnatural\\tlanguage\\tinto\\ta\\t\\ndomain-specific\\tquery\\tlanguage,\\tit\\tis\\thighly\\tlikely\\tthat\\tany\\ttraining\\tdata\\twe\\thave\\twill\\tcontain\\t\\nonly\\ta\\tfraction\\tof\\tthe\\teventual\\ttargets\\tfor\\tour\\tusers.\\tIn\\tthis\\tcase,\\twithout\\tcareful\\tcuration\\t\\nof\\tthe\\ttraining\\tdata,\\twe\\tmay\\tinadvertently\\teliminate\\tthe\\tability\\tof\\tthe\\tmodel\\tto\\tgeneralize\\t\\nto\\tnew\\ttasks\\tor\\tdata\\tsources\\tthat\\tare\\timportant\\tto\\tusers.\\tLikewise,\\tsome\\tdata\\tsources\\tare\\t\\nparticularly\\tsensitive\\tor\\tproprietary\\tand\\tshould\\tnot\\tbe\\tincluded\\tin\\tgeneralized\\ttraining\\tof\\t\\nthe\\tmodel.\\tInstead,\\tthese\\tdata\\tsources\\tshould\\tbe\\tincorporated\\tinto\\ta\\tspecialized\\tderivative\\t\\nmodel\\t(using\\ta\\tlightweight,\\tparameter-efficient\\tprocess)\\tthat\\tdoes\\tnot\\tdegrade\\tthe\\toverall\\t\\nperformance\\tof\\tthe\\tcore\\tsecurity-specialized\\tmodel.\\nThe\\ttraining\\tprocess,\\tshown\\tin\\tFigure\\t1,\\tdemonstrates\\thow\\twe\\tleverage\\teach\\tphase\\tof\\t\\ntraining\\tto\\ttarget\\tspecific\\ttasks\\tand\\ttypes\\tof\\tdata\\tto\\tbalance\\tperformance,\\tgeneralization,\\t\\nand\\tseparation\\tof\\tproprietary\\tdata.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 13, 'page_label': '14'}, page_content='Solving Domain-Specific Problems Using LLMs\\n14\\nFebruary 2025\\nFigure 1. High-level training flow for core SecLM and specialized derivative models \\nAs\\tpre-training\\tis\\tthe\\tmost\\texpensive\\tand\\ttime-consuming\\tstage,\\tit\\tmakes\\tsense\\tto\\tstart\\t\\nfrom\\ta\\trobust\\tfoundational\\tmodel\\twith\\texposure\\tto\\tthe\\tbroadest\\tset\\tof\\ttraining\\tdata\\tpossible,\\t\\nincluding\\tbillions\\tor\\teven\\ttrillions\\tof\\ttokens\\tof\\tgeneral\\ttext,\\tcode,\\tand\\tstructured\\tdata\\tacross\\t\\ndozens\\tof\\tlanguages\\tand\\tformats.\\tThis\\tgives\\tus\\tthe\\tadded\\tbenefit\\tof\\tmultilingual\\tsupport,\\t\\nwhich\\tis\\tan\\timportant\\tfeature\\tfor\\tthreat\\tintelligence\\tuse\\tcases\\tand\\tinternational\\tusers.\\nFrom\\tthis\\tfoundational\\tmodel,\\twe\\tapply\\ta\\tphase\\tof\\tcontinued\\tpre-training\\twhere\\twe\\t\\nincorporate\\ta\\tlarge\\tcollection\\tof\\topen\\tsource\\tand\\tlicensed\\tcontent\\tfrom\\tsecurity\\tblogs,\\t\\nthreat\\tintelligence\\treports,\\tdetection\\trules,\\tinformation\\ttechnology\\tbooks,\\tand\\tmore.\\tThis\\t\\nhelps\\tdevelop\\tthe\\tspecialized\\tlanguage\\tand\\tcore\\ttechnology\\tunderstanding\\tnecessary\\tto'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 14, 'page_label': '15'}, page_content='Solving Domain-Specific Problems Using LLMs\\n15\\nFebruary 2025\\nperform\\tthe\\tbroad\\trange\\tof\\ttasks\\tthat\\tSecLM\\tmodels\\twill\\tbe\\ttrained\\ton\\tin\\tthe\\tsupervised\\t\\nfine-tuning\\tphase.\\tHere,\\tproprietary\\tdata\\tis\\tcompartmentalized\\twithin\\tspecific\\ttasks\\tthat\\t\\nmirror\\tthose\\tperformed\\tby\\tsecurity\\texperts\\ton\\ta\\tday-to-day\\tbasis,\\tincluding\\tanalysis\\tof\\t\\nmalicious\\tscripts,\\texplanation\\tof\\tcommand\\tline\\tinvocations,\\texplanation\\tof\\tsecurity\\tevents,\\t\\nsummarization\\tof\\tthreat\\tintelligence\\treports,\\tand\\tgeneration\\tof\\tqueries\\tfor\\tspecialized\\t\\nsecurity event management technologies.\\nGiven\\tthe\\tdiversity\\tof\\tdownstream\\ttasks\\tthat\\tare\\texpected\\tof\\tthe\\tmodel,\\tevaluating\\tits\\t\\nperformance\\tcan\\tbe\\ta\\tchallenging\\texercise,\\tparticularly\\twhen\\tsome\\tcategories\\tof\\ttasks\\tmay\\t\\nexperience\\tinherent\\ttrade-offs.\\tFor\\tthis\\treason,\\tthe\\tfine-tuned\\tmodel\\tis\\tevaluated\\tusing\\ta\\t\\nnumber\\tof\\tcomplementary\\tmethods.\\tSeveral\\tof\\tour\\tdownstream\\ttasks,\\tsuch\\tas\\tmalware\\t\\nclassification\\tand\\tcertain\\ttypes\\tof\\tsimple\\tsecurity-focused\\tquestion\\tanswering,\\tcan\\tbe\\t\\nframed\\tas\\tclassification\\tproblems\\tand\\ta\\tstandard\\tbattery\\tof\\tclassification\\tmetrics\\tcan\\tbe\\t\\nused\\tto\\tconcretely\\tquantify\\tthe\\tperformance\\ton\\tthose\\ttasks.\\tFor\\tother,\\tless\\tquantifiable\\t\\ntasks,\\twe\\tcan\\tleverage\\ta\\tset\\tof\\tgolden\\tresponses\\tthat\\twe\\tcan\\tuse\\tto\\tcalculate\\tsimilarity-\\nbased\\tmetrics\\t(e.g.,\\tROUGE,2\\tBLEU,3 BERTScore4),\\tbut\\twe\\tcan\\talso\\tcompare\\tacross\\tmodels\\t\\nusing\\tautomated\\tside-by-side\\tpreference\\tevaluations\\tusing\\ta\\tseparate\\t(oftentimes\\tlarger)\\t\\nLLM.\\tFinally,\\tgiven\\tthe\\thighly\\ttechnical\\tnature\\tof\\tsecurity\\tproblems\\tand\\tthe\\timportance\\tof\\t\\naccuracy\\tin\\tour\\ttasks,\\twe\\trely\\ton\\texpert\\thuman\\tevaluators\\tto\\tscore\\toutputs\\tusing\\ta\\tLikert\\t\\nscale\\tand\\tside-by-side\\tpreference\\tevaluation.\\tTaken\\ttogether,\\tthese\\tmetrics\\tprovide\\tus\\twith\\t\\nthe\\tguidance\\tneeded\\tto\\tensure\\tour\\tfine-tuning\\ttraining\\thas\\timproved\\toverall\\tmodel\\tquality,\\t\\nand\\thelp\\tus\\tdirect\\tfuture\\tchanges\\tin\\tmodel\\ttraining.\\nAt\\tthe\\tconclusion\\tof\\tthe\\tfine-tuning\\tstage,\\twe\\thave\\ta\\tmodel\\tcapable\\tof\\tperforming\\tmany\\t\\nof\\tthe\\tsame\\tcore\\ttasks\\tas\\tsecurity\\texperts.\\tHowever,\\tbecause\\tof\\tour\\tneed\\tto\\tensure\\t\\ngeneralization\\tacross\\ta\\twide\\trange\\tof\\tuser\\tenvironments\\tand\\tthe\\tinherent\\ttrade-off\\tamong\\t\\nsome\\tsecurity\\ttasks,\\tthe\\tmodel\\tmay\\tstill\\trequire\\tthe\\tuse\\tof\\tin-context\\tlearning\\texamples,\\t\\nretrieval-augmented\\tgeneration,\\tand\\tparameter-efficient\\ttuning\\t(PET)\\tmethods.\\tFor\\texample,\\t\\nif\\ta\\tnew\\tuser\\twanted\\tto\\tleverage\\tSecLM\\tto\\tquery\\tand\\tanalyze\\tdata\\ton\\ta\\tnew\\tsecurity\\tplatform'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 15, 'page_label': '16'}, page_content=\"Solving Domain-Specific Problems Using LLMs\\n16\\nFebruary 2025\\nthat\\twas\\tnot\\tpresent\\tduring\\tcore\\ttraining,\\tit\\tis\\tlikely\\tthat\\tthe\\tmodel\\tmay\\tneed\\tin-context\\t\\nexamples\\tto\\thelp\\tgeneralize\\tto\\tthe\\tnew\\tsystem.\\tSimilarly,\\tif\\ta\\tuser\\twanted\\tto\\tincorporate\\t\\nspecialized\\tknowledge\\tabout\\ttheir\\tnetwork\\tand\\tassets\\tor\\tbetter\\talign\\tmodel\\tbehavior\\twith\\t\\nhuman\\tsecurity\\texperts,\\tit\\twould\\tbe\\tbest\\tadded\\tvia\\tPET\\tadapters\\ttrained\\ton\\ttheir\\tsensitive\\t\\ndata.\\tRetrieval-augmented\\tgeneration,\\tmeanwhile,\\tallows\\tus\\tto\\tpull\\tin\\tthe\\tfreshest\\tand\\tmost\\t\\nrecent\\tthreat\\tinformation\\tfor\\tthe\\tmodel\\tto\\tprocess,\\trather\\tthan\\trelying\\ton\\tstale\\tdata\\tingested\\t\\nduring less frequent training runs.\\nA flexible planning and reasoning framework\\nAs\\tyou\\tmight\\timagine,\\tactually\\tbuilding\\tthe\\tunderlying\\tframework\\tthat\\torchestrates\\tthe\\t\\nplanning\\tand\\texecution\\tof\\tthese\\tcomplex\\ttasks\\trequires\\tsolving\\tsome\\tdifficult\\tsystems\\t\\nengineering\\tand\\tmachine\\tlearning\\tchallenges.\\tThe\\texample,\\tshown\\tin\\tFigure\\t2,\\tillustrates\\thow\\t\\nSecLM's\\tspecialized\\tmodels\\tcan\\tbe\\ttied\\tinto\\ta\\tbroader\\tecosystem\\tto\\tbest\\tleverage\\tfresh,\\t\\nuser-specific\\tdata\\tand\\tauthoritative\\tsecurity\\texpertise\\tin\\ta\\tnatural\\tand\\tseamless\\tway.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 16, 'page_label': '17'}, page_content='Solving Domain-Specific Problems Using LLMs\\n17\\nFebruary 2025\\nFigure 2. SecLM platform leveraging multi-step reasoning to answer a broad, high-level question about \\nadvanced persistent threat actor activity \\nIn\\tFigure\\t2,\\twe\\thave\\ta\\tfairly\\tbroad,\\thigh-level\\tquestion\\tregarding\\tthe\\ttactics,\\ttechniques,\\tand\\t\\nprocedures\\t(TTPs)\\tof\\tan\\tadvanced\\tpersistent\\tthreat\\t(APT)\\tgroup,\\tin\\tthis\\texample\\t‘APT41’.\\tThe\\t\\nanalyst\\tasking\\tthis\\tquestion\\tneeds\\tto\\tunderstand\\twhat\\tthose\\tTTPs\\tare\\tand\\tdiscover\\tpotential\\t\\nindications\\tof\\tthem\\tin\\ttheir\\town\\tnetwork.\\tTo\\tanswer\\tthis\\tquestion,\\tthe\\tSecLM\\tAPI\\tneeds\\tto\\t\\ninvoke\\ta\\tcomplex,\\tmulti-step\\tplanning\\tprocess\\tto\\tbreak\\tdown\\tthe\\tproblem\\tinto\\tindividual\\t\\ntasks:\\t1)\\tRetrieve\\tthe\\tnecessary\\tinformation,\\t2)\\tExtract\\tand\\tsynthesize\\tthat\\tinformation,\\t3)\\t\\nUse\\tthe\\tinformation\\tto\\tquery\\tthe\\trelevant\\tevents\\tfrom\\tthe\\tuser’s\\tSecurity\\tInformation\\tand\\t\\nEvent\\tManagement\\t(SIEM)\\tproduct.\\tIn\\tthe\\tSecLM\\treasoning\\tframework,\\tthis\\tplan\\tcan\\tbe\\t\\ngenerated\\tstatically\\tby\\tsecurity\\texperts\\tor\\tin\\treal-time\\tthrough\\ta\\tcombination\\tof\\texpert\\t\\nguidance\\tand\\thighly-capable\\tLLMs\\tusing\\tchain-of-thought\\tstyle\\tprompting.\\nFirst,\\tthe\\tSecLM\\tAPI\\tplanner\\tretrieves\\tthe\\tmost\\trecent\\tinformation\\tabout\\t“APT41”\\tfrom\\tone\\tof\\t\\npossibly\\tmany\\tof\\tthe\\tuser’s\\tthreat\\tintelligence\\tsubscriptions.\\tThat\\traw\\tresponse\\tis\\tprocessed\\t\\nto\\textract\\tTTP\\tinformation\\tand\\tpossible\\tindicators\\tof\\tcompromise\\tfrom\\tthe\\tvoluminous\\tthreat'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 17, 'page_label': '18'}, page_content='Solving Domain-Specific Problems Using LLMs\\n18\\nFebruary 2025\\nintelligence\\tdata.\\tNext,\\ta\\tspecialized\\tSecLM\\tfine-tuned\\t(using\\tPET)\\tfor\\tthe\\tquery\\tlanguage\\t\\nof\\tthe\\tSIEM\\tis\\tused\\tto\\ttranslate\\tthose\\tTTPs\\tinto\\tconcrete\\tclauses\\tin\\tthe\\tappropriate\\tsyntax\\t\\nand\\tusing\\tthe\\tappropriate\\tschema.\\tUsing\\tthat\\tquery,\\tthe\\tAPI\\tcan\\tthen\\tdirectly\\tretrieve\\t\\nthe\\tmatching\\tsecurity\\tevents\\tfrom\\tthe\\tSIEM,\\tand\\tfinally\\tuse\\tSecLM\\tto\\taggregate\\tall\\tof\\tthe\\t\\navailable\\tinformation\\tinto\\ta\\tcomprehensible\\tfinal\\tresponse\\tfor\\tthe\\tanalyst.\\nOverall,\\tthe\\tSecLM\\tAPI\\twould\\tsave\\tthe\\tanalyst\\tin\\tthe\\tabove\\texample\\tsubstantial\\ttime\\t-\\t\\npossibly\\thours\\t-\\tby\\tautomating\\tmultiple\\ttedious\\tsteps\\tacross\\tseveral\\tdifferent\\tsecurity\\t\\nservices\\tand\\tsystems.\\tMeanwhile,\\tthe\\tanalyst’s\\ttime\\tand\\tattention\\tare\\tavailable\\tto\\tconsider\\t\\nthe\\tresults\\tand\\tplan\\tfor\\tfollow-up\\tinvestigations\\tor\\tremediation\\tsteps,\\twhich\\tmay\\talso\\tbe\\t\\nassisted\\tby\\tthe\\tSecLM\\tAPI.\\tWhile\\tthis\\tis\\tone\\texample\\tof\\thow\\tthe\\tSecLM\\tAPI\\tautomatically\\t\\nplans\\tand\\torchestrates\\toperations\\tacross\\tmultiple\\tmodels\\tand\\tretrieval\\tsources,\\tthere\\tare\\t\\na\\tmultitude\\tof\\tsuch\\tuse\\tcases\\twhere\\ttool\\tuse\\t(e.g.,\\tcode\\texecution),\\tretrieval-augmented\\t\\ngeneration,\\tspecialized\\tmodels,\\tand\\tlong-term\\tmemory\\t(e.g.,\\tstorage\\tof\\tuser\\tpreferences)\\t\\ncan\\thelp\\tsolve\\tchallenging\\tsecurity\\tproblems\\tand\\tanswer\\tdifficult\\tquestions\\tthat\\tsave\\tusers\\t\\nvaluable\\ttime,\\teven\\tautonomously\\twith\\tthe\\tuse\\tof\\tagents..\\nThe\\tprompt\\tand\\tresponse\\tshown\\tin\\tFigure\\t3\\tprovide\\tanother\\tconcrete\\texample\\tof\\thow\\tthe\\t\\nSecLM\\tAPI\\tcan\\tleverage\\tmultiple\\ttools\\tand\\tmodels\\tto\\tsolve\\tan\\totherwise\\ttime-consuming\\t\\nproblem\\tfor\\tsecurity\\tanalysts\\tand\\tsystem\\tadministrators\\talike,\\tin\\tthis\\tcase\\tby\\tautomatically\\t\\ndecoding\\tand\\tanalyzing\\ta\\tPowerShell\\tscript\\tfor\\tmalicious\\tactivity.\\t\\tTo\\tdemonstrate\\tthe\\tvalue\\t\\nof\\tour\\tplatform,\\twe\\trecently\\tcompleted\\ta\\tside-by-side\\tanalysis\\twith\\tsecurity\\toperations\\t\\nand\\tthreat\\tintelligence\\texperts,\\twhere\\twe\\tcompared\\tthe\\tend-to-end\\tSecLM\\tplatform\\t\\nagainst\\tstandalone,\\tgeneral-purpose\\tLLMs\\ton\\tcybersecurity-focused\\ttasks,\\tsuch\\tas\\tattack\\t\\npath\\tanalysis,\\talert\\tsummarization,\\tand\\tgeneral\\tsecurity\\tquestion\\tanswering\\tsimilar\\tto\\tthe\\t\\nPowerShell\\texample\\tshown\\there.\\tThe\\tresults\\tdemonstrated\\ta\\tclear\\tpreference\\tfor\\tSecLM,\\t\\nwith\\twin\\trates\\tbetween\\t53%\\tand\\t79%\\tacross\\tthe\\tsecurity-focused\\ttasks,\\tand\\tunderscore\\tthe\\t\\nimportance\\tof\\ta\\tfull-featured\\tplatform\\tin\\tthe\\tdomain\\tof\\tcybersecurity.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 18, 'page_label': '19'}, page_content='Solving Domain-Specific Problems Using LLMs\\n19\\nFebruary 2025\\nFigure 3. An example response from the SecLM platform using a base64 decoding tool and the SecLM model \\nto analyze an obfuscated PowerShell command used in a ‘living off the land’ attack \\nIn\\tthis\\tsection,\\twe\\thave\\tseen\\thow\\ta\\tholistic\\tapproach\\tthat\\tcombines\\tlarge\\tlanguage\\tmodels\\t\\n(LLMs)\\tand\\tauthoritative\\tdata\\tsources\\twith\\ta\\tflexible\\tplanning\\tframework\\tcan\\thelp\\tsecurity\\t\\npractitioners\\tby\\tgathering,\\taggregating,\\tand\\tintelligently\\tprocessing\\tsecurity\\tdata.\\tWe\\thave\\t\\nalso\\tseen\\thow\\tSecLM\\tand\\tits\\tsupporting\\tinfrastructure\\tare\\tbeing\\tbuilt\\tto\\tprovide\\ta\\tone-stop\\t\\nsecurity\\tplatform\\tfor\\texperts,\\tjunior\\tanalysts,\\tand\\tsystems\\tadministrators.\\tThese\\tadvances,\\t\\ncombined\\twith\\thuman\\texpertise,\\tcan\\ttransform\\tthe\\tpractice\\tof\\tsecurity,\\tobtaining\\tsuperior\\t\\nresults\\twith\\tless\\ttoil\\tfor\\tthe\\tpeople\\twho\\tdo\\tit.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 19, 'page_label': '20'}, page_content='Solving Domain-Specific Problems Using LLMs\\n20\\nFebruary 2025\\nMedLM and the future of health tech\\nRecent\\tadvances\\tin\\tAI\\tfor\\tnatural\\tlanguage\\tprocessing\\t(NLP)\\tand\\tfoundation\\tmodels\\thave\\t\\nenabled\\trapid\\tresearch\\tinto\\tnovel\\tcapabilities\\tin\\tthe\\tmedical\\tfield.\\tThis\\tsection\\twill\\tdive\\t\\ndeeper\\tinto\\tthe\\tchallenges\\tof\\tthe\\tmedical\\tfield,\\tand\\thow\\tMedLM\\tsolutions\\tcan\\thelp\\there\\t-\\ta\\t\\nfamily\\tof\\tfoundation\\tmodels\\tfine-tuned\\tfor\\tthe\\thealthcare\\tindustry.\\tIn\\tparticular,\\tthis\\tsection\\t\\nillustrates\\thow\\tit\\tstarted\\twith\\ta\\tspecific\\tGenAI\\tmodel,\\tMed-PaLM,\\tto\\taddress\\tthese\\tneeds.\\nThe potential for GenAI in medical Q&A\\nMedical\\tquestion-answering\\t(QA)\\thas\\talways\\tbeen\\ta\\tgrand\\tchallenge\\tin\\tartificial\\tintelligence\\t\\n(AI).\\tThe\\tvast\\tand\\tever-evolving\\tnature\\tof\\tmedical\\tknowledge,\\tcombined\\twith\\tthe\\tneed\\tfor\\t\\naccurate\\tand\\tnuanced\\treasoning,\\thas\\tmade\\tit\\tdifficult\\tfor\\tAI\\tsystems\\tto\\tachieve\\thuman-level\\t\\nperformance\\ton\\tmedical\\tQA\\ttasks.\\nHowever,\\tlarge\\tlanguage\\tmodels\\t(LLMs)\\ttrained\\ton\\tmassive\\tdatasets\\tof\\ttext\\thave\\tshown\\t\\npromising\\tresults\\ton\\ta\\tvariety\\tof\\tmedical\\tQA\\tbenchmarks.\\tLLMs\\tare\\table\\tto\\tunderstand\\tand\\t\\napply\\tcomplex\\tmedical\\tconcepts\\tin\\ta\\tway\\tthat\\twas\\tnot\\tpossible\\tfor\\tprevious\\tgenerations\\tof\\t\\nAI systems.\\nIn\\taddition,\\tthe\\tincreasing\\tavailability\\tof\\tmedical\\tdata\\tand\\tthe\\tgrowing\\tfield\\tof\\tmedical\\tNLP\\t\\nhave\\tcreated\\tnew\\topportunities\\tfor\\tinnovation\\tin\\tmedical\\tQA.\\tResearchers\\tare\\tnow\\table\\tto\\t\\ndevelop\\tsystems\\tthat\\tcan\\tanswer\\tmedical\\tquestions\\tfrom\\ta\\tvariety\\tof\\tsources,\\tincluding\\t\\nmedical\\ttextbooks,\\tresearch\\tpapers,\\tand\\tpatient\\trecords.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 20, 'page_label': '21'}, page_content=\"Solving Domain-Specific Problems Using LLMs\\n21\\nFebruary 2025\\nThis\\tcombination\\tof\\ttechnical\\tcapabilities\\tand\\tdata\\tavailability\\tprovides\\tthe\\tgroundwork\\tfor\\t\\nmodels\\tlike\\tMed-PaLM,\\tan\\tLLM\\taligned\\tand\\tfine-tuned\\tbased\\ton\\tthe\\tPaLM\\tfamily\\tof\\tmodels.\\t\\nThe\\tdevelopment\\tof\\tMed-PaLM\\tis\\tonly\\tthe\\tstart\\tof\\ta\\tjourney\\twith\\tthe\\tgoal\\tof\\timproving\\thealth\\t\\noutcomes\\tby\\tmaking\\tthe\\ttechnology\\tavailable\\tto\\tresearchers,\\tclinicians,\\tand\\tother\\tusers.\\nThe opportunities\\nGen\\tAI\\thas\\tthe\\tpotential\\tto\\tfundamentally\\ttransform\\tthe\\tmedical\\tfield\\tin\\tboth\\tdiagnostic\\tand\\t\\nnon-diagnostic\\taspects,\\tin\\tnumerous\\tways.\\tFor\\texample:\\n• Empowering\\tusers\\tto\\task\\tquestions\\tin\\tthe\\tcontext\\tof\\tthe\\tmedical\\thistory\\tin\\ttheir\\thealth\\t\\nrecord\\tsuch\\tas\\t“what\\tare\\tgood\\tweekend\\tactivities\\tfor\\tme\\tto\\tconsider,\\tgiven\\tthe\\tsurgery\\tI\\t\\nunderwent two weeks ago?”\\n• Triaging\\tof\\tincoming\\tmessages\\tto\\tclinicians\\tfrom\\tpatients\\tby\\tcomprehensively\\t\\nunderstanding\\tthe\\turgency\\tand\\tcategorizing\\tthe\\ttype\\tof\\tincoming\\tmessage\\tgiven\\tthe\\t\\nfull\\tcontext\\tof\\tthe\\tpatient's\\thealth\\thistory,\\tand\\tflagging\\tor\\tprioritizing\\tthe\\tmessage\\t\\nappropriately.\\t\\n• Enhancing\\tthe\\tpatient\\tintake\\tprocess\\tby\\tmoving\\tbeyond\\ta\\tfixed\\tset\\tof\\tquestions\\tand\\t\\ninstead\\tadapting\\tbased\\ton\\tthe\\tpatient's\\tresponses.\\tThis\\tallows\\tfor\\tmore\\tefficient\\tand\\t\\ncomprehensive\\tdata\\tcollection\\tand\\tprovides\\ta\\tmore\\tcohesive\\tsummary\\tto\\tthe\\tclinical\\tstaff.\\t\\n• Implementing\\ta\\ttechnology\\tthat\\tactively\\tmonitors\\tpatient-clinician\\tconversations\\tand\\t\\nprovides\\tactionable\\tfeedback\\tto\\tthe\\tclinician,\\thelping\\tthem\\tunderstand\\twhat\\tthey\\t\\ndid\\tgreat\\tin\\tthe\\tinteraction\\tand\\twhere\\tthey\\tmight\\twant\\tto\\timprove.\\tSimilarly,\\tthe\\tsame\\t\\ntechnology\\tcan\\thelp\\tthe\\tpatient\\twith\\tany\\tquestions\\tthey\\tmight\\thave\\tfor\\tthe\\tclinician\\tbefore\\t\\nconcluding their visit.\\n• Enabling\\tclinicians\\tto\\tbetter\\ttackle\\tunfamiliar\\tscenarios\\tor\\tdiseases\\tby\\tproviding\\tan\\ton-\\ndemand\\tcurbside\\tconsult\\tor\\treference\\tmaterials,\\tsimilar\\tto\\thaving\\ta\\tcolleague\\tavailable\\tfor\\t\\nconferences as needed.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 21, 'page_label': '22'}, page_content='Solving Domain-Specific Problems Using LLMs\\n22\\nFebruary 2025\\nThis\\tlist\\trepresents\\tmerely\\ta\\tsmall\\tselection\\tfrom\\ta\\tvast\\tarray\\tof\\tpossibilities,\\tillustrating\\tthe\\t\\nextensive\\trange\\tof\\toptions\\tpreviously\\tconsidered\\tunattainable\\twith\\tearlier\\ttechnologies.\\nThe\\tfield\\tof\\tmedicine\\talso\\tserves\\tas\\ta\\tuse\\tcase\\twith\\ta\\tstrong\\tculture\\tand\\tneed\\tfor\\t\\nresponsible\\tinnovation.\\tMedical\\tapplications\\tare\\tregulated\\tdue\\tto\\tthe\\timportance\\tof\\tpatient\\t\\nsafety.\\tWhile\\tGenAI\\tsystems\\tcan\\tbe\\tused\\tto\\tdevelop\\tnew\\tdiagnostic\\ttools,\\ttreatment\\tplans,\\t\\nand\\teducational\\tmaterials,\\tit\\tis\\timportant\\tto\\tvalidate\\tthe\\tsafety\\tand\\tefficacy\\tof\\tsuch\\tsystems\\t\\t\\nbefore\\ttheir\\timplementation\\tin\\tclinical\\tpractice.\\tThis\\tmeans\\tthat\\tscientific\\texperimentation\\t\\nrequires\\ta\\tthoughtful,\\tphased\\tapproach\\twith\\tretrospective\\tstudies\\t(i.e.,\\tusing\\tde-identified\\t\\ndata\\tfrom\\tpast\\tcases\\tso\\tthat\\tresearch\\tdoes\\tnot\\timpact\\tpatient\\tcare)\\thappening\\tbefore\\t\\nprospective\\tstudies\\t(i.e.,\\trunning\\tthe\\tmodel\\ton\\tnewly\\tcollected\\tdata\\tin\\ta\\tspecific\\tsetting\\tof\\t\\ninterest,\\tsometimes\\tinterventionally\\tso\\tthat\\timpact\\ton\\tpatient\\tcare\\tcan\\tbe\\tmeasured).\\nThe scientific starting point\\nMany\\tAI\\tsystems\\tdeveloped\\tfor\\tmedicine\\ttoday\\tlack\\tthe\\tability\\tto\\tinteract\\twith\\tusers,\\tbut\\t\\ninstead\\tproduce\\tstructured\\toutputs\\tsuch\\tas\\t“yes”\\tor\\t“no”,\\tor\\ta\\tnumerical\\toutput.\\tWhile\\tthis\\t\\ntype\\tof\\toutput\\tis\\tuseful\\tin\\tmany\\tscenarios\\tfor\\tclinicians,\\tthis\\toutput\\tis\\tinflexible.\\tModels\\talso\\t\\nneed\\tto\\tbe\\tcreated\\tfor\\tevery\\tapplication,\\twhich\\tslows\\tdown\\tinnovation.\\t\\nIn\\tour\\tview,5\\tmedicine\\trevolves\\taround\\tcaring\\tfor\\tpeople,\\tand\\tneeds\\tto\\tbe\\thuman-centric.\\tAs\\t\\nsuch,\\tan\\tambitious\\tgoal\\twould\\tbe\\ta\\tflexible\\tAI\\tsystem\\tthat\\tcan\\tinteract\\twith\\tpeople\\tand\\tassist\\t\\nin\\tmany\\tdifferent\\tscenarios\\twhile\\ttaking\\tinto\\taccount\\tthe\\tappropriate\\tcontext.\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tTo\\t\\ncreate\\tsuch\\ta\\tsystem,\\tit\\tis\\tessential\\tto\\tincorporate\\ta\\twide\\trange\\tof\\texperiences,\\tperspectives,\\t\\nand\\texpertise\\twhen\\tbuilding\\tAI\\tsystems.\\tData\\tand\\talgorithms\\tshould\\tgo\\t\\t\\t\\t\\t\\t\\thand\\tin\\thand\\twith\\t\\nlanguage\\tand\\tinteraction,\\tempathy,\\tand\\tcompassion.\\nThe\\tobjective\\tbehind\\tthis\\tproject\\tis\\tto\\tenhance\\tthe\\teffectiveness,\\thelpfulness,\\tand\\tsafety\\t\\t\\t\\t\\t\\nof\\tAI\\tmodels\\tin\\tmedicine\\tby\\tincorporating\\tnatural\\tlanguage\\tand\\tfacilitate\\tinteractivity\\tfor\\tand\\t\\nbetween\\tclinicians,\\tresearchers,\\tand\\tpatients.\\tTo\\tbring\\tthis\\tvision\\tto\\tlife,\\twe\\ttook\\tthe\\tinitial'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 22, 'page_label': '23'}, page_content=\"Solving Domain-Specific Problems Using LLMs\\n23\\nFebruary 2025\\nstep\\tin\\treimagining\\tconversational\\tAI\\tsystems\\tin\\tmedicine\\twith\\tMed-PaLM,\\tGoogle's\\tLLM\\t\\ndesigned\\tto\\tprovide\\thigh-quality,\\tauthoritative\\tanswers\\tto\\tmedical\\tquestions.\\tThe\\tQA\\ttask\\t\\nin\\tparticular\\twas\\ta\\tgreat\\tcandidate\\tfor\\tstarting\\tthe\\tjourney,\\tas\\tit\\tcombines\\tevaluations\\tof\\t\\nreasoning\\tcapabilities\\tand\\tunderstanding,\\tand\\tallows\\tfor\\textensive\\tevaluations\\tacross\\tmany\\t\\ndimensions\\ton\\tthe\\toutputs.\\nThe\\trecent\\tprogress\\tin\\tfoundation\\tmodels,6\\tsuch\\tas\\tLLMs,\\tas\\tlarge\\tpre-trained\\tAI\\tsystems\\t\\nthat\\tcan\\tbe\\teasily\\tadapted\\tfor\\tvarious\\tdomains\\tand\\ttasks\\tpresents\\tan\\topportunity\\tto\\t\\nrethink\\tthe\\tdevelopment\\tand\\tuse\\tof\\tAI\\tin\\tmedicine\\ton\\ta\\tbroader\\tscale.\\tThese\\texpressive\\t\\nand\\tinteractive\\tmodels\\thold\\tsignificant\\tpotential\\tto\\tmake\\tmedical\\tAI\\tmore\\tperformant,\\tsafe,\\t\\naccessible,\\tand\\tequitable\\tby\\tflexibly\\tencoding,\\tintegrating,\\tand\\tinterpreting\\tmedical\\tdata\\t\\nat scale.\\nHere\\tis\\ta\\tdescription\\tof\\thow\\tMed-PaLM\\timproved\\tover\\ttime:\\n• Our\\tfirst\\tversion\\tof\\tMed-PaLM,\\tdescribed\\tin\\ta\\tpreprint\\tin\\tlate\\t2022\\tand\\tpublished\\tin\\t\\nNature\\tin\\tJuly\\t2023,7\\twas\\tthe\\tfirst\\tAI\\tsystem\\tto\\texceed\\tthe\\tpassing\\tmark\\ton\\tUS\\tMedical\\t\\nLicense\\tExam\\t(USMLE)-style\\tquestions.8 The study also evaluated long-form answers and \\ndescribed\\ta\\tcomprehensive\\tevaluation\\tframework.\\n• In\\tMarch\\t2023,\\tMed-PaLM\\t2\\twas\\tannounced\\tand\\tdescribed\\tin\\ta\\tpreprint.9 It demonstrated \\nrapid\\tadvancements,\\tboth\\ton\\tUSMLE-style\\tquestions\\tand\\ton\\tlong-form\\tanswers.\\tMed-\\nPaLM\\t2\\tachieves\\tan\\taccuracy\\tof\\t86.5%\\ton\\tUSMLE-style\\tquestions,\\ta\\t19%\\tleap\\tover\\tour\\t\\nown\\tresults\\tfrom\\tMed-PaLM.\\tAs\\tevaluated\\tby\\tphysicians,\\tthe\\tmodel's\\tlong-form\\tanswers\\t\\nto\\tconsumer\\tmedical\\tquestions\\timproved\\tsubstantially\\tcompared\\tto\\tearlier\\tversions\\tof\\t\\nMed-PaLM or the underlying non-medically tuned base models. It also demonstrated \\nhow\\tfine-tuning\\tand\\trelated\\ttechniques\\tcan\\ttruly\\tharness\\tthe\\tpower\\tof\\tLLMs\\tin\\ta\\tdomain-\\nspecific\\tway.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 23, 'page_label': '24'}, page_content='Solving Domain-Specific Problems Using LLMs\\n24\\nFebruary 2025\\nThese\\tadvances\\treflect\\tour\\tbelief\\tthat\\tinnovation\\tcan\\ttake\\tmajor\\tstrides\\tin\\ta\\tshort\\tperiod\\tof\\t\\ntime,\\tand\\tbe\\tdone\\tresponsibly\\tand\\twith\\trigor.\\nHow to evaluate: quantitative and qualitative\\nDeveloping\\taccurate\\tand\\tauthoritative\\tmedical\\tquestion-answering\\tAI\\tsystems\\thas\\tbeen\\ta\\t\\nlong-standing\\tchallenge\\tmarked\\tby\\tseveral\\tresearch\\tadvances\\tover\\tthe\\tpast\\tfew\\tdecades.\\t\\nWhile\\tthe\\ttask\\tis\\tbroad\\tand\\tspans\\tvarious\\tdimensions\\tincluding\\tlogical\\treasoning\\tand\\tthe\\t\\nretrieval\\tof\\tmedical\\tknowledge,\\ttackling\\tUSMLE-style\\tquestions\\thas\\tgained\\tprominence\\t\\nas\\ta\\twidely\\tacceptable\\tand\\tchallenging\\tbenchmark\\tfor\\tevaluating\\tmedical\\tquestion\\t\\nanswering\\tperformance.\\nFigure\\t4\\tshows\\tan\\texample\\tof\\ta\\tUSMLE-style\\tquestion.\\tIndividuals\\ttaking\\tthe\\ttest\\tare\\tgiven\\t\\na\\tconcise\\tpatient\\tprofile\\tthat\\tincludes\\tinformation\\tsuch\\tas\\ttheir\\tsymptoms\\tand\\tprescribed\\t\\nmedications.\\tA\\tmedical\\tquestion\\tis\\tpresented\\tbased\\ton\\tthe\\tprovided\\tscenario,\\tand\\ttest-\\ntakers\\tare\\trequired\\tto\\tchoose\\tthe\\tcorrect\\tresponse\\tfrom\\tmultiple\\tchoices.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 24, 'page_label': '25'}, page_content='Solving Domain-Specific Problems Using LLMs\\n25\\nFebruary 2025\\nFigure 4. An example of a USMLE-style question \\nCorrectly\\tanswering\\tthe\\tquestion\\trequires\\tthe\\tindividual\\ttaking\\tthe\\ttest\\tto\\tcomprehend\\t\\t\\nsymptoms,\\tinterpret\\ta\\tpatient’s\\ttest\\tresults,\\tengage\\tin\\tintricate\\treasoning\\tregarding\\tthe\\t\\nprobable\\tdiagnosis,\\tand\\tultimately\\tselect\\tthe\\tcorrect\\tchoice\\tfor\\tthe\\tmost\\tsuitable\\tdisease,\\t\\ntest,\\tor\\ttreatment\\tcombination.\\tIn\\tsummary,\\ta\\tcombination\\tof\\tmedical\\tcomprehension\\tand\\t\\nunderstanding,\\tknowledge\\tretrieval,\\tand\\treasoning\\tis\\tvital\\tfor\\tsuccess.\\tIt\\ttakes\\tyears\\tof\\t\\neducation\\tand\\ttraining\\tfor\\tclinicians\\tto\\tdevelop\\tthe\\tknowledge\\tneeded\\tto\\tconsistently\\tanswer\\t\\nthese questions accurately.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 25, 'page_label': '26'}, page_content='Solving Domain-Specific Problems Using LLMs\\n26\\nFebruary 2025\\nAs\\tevery\\tclinician\\twill\\tattest\\tto,\\tmerely\\tpassing\\tthe\\tUSMLE\\tdoes\\tnot\\tindicate\\tproficiency\\t\\nin\\tdiagnosing\\tor\\tmanaging\\tpatients\\tclinically.\\tInstead,\\tUSMLE\\tis\\ta\\tspecific\\tassessment\\t\\nof\\tknowledge\\tand\\treasoning\\tbased\\ton\\tconcrete\\tscenarios.\\tNevertheless,\\tUSMLE\\tserves\\t\\nas\\ta\\tuseful\\tbenchmark\\tsince\\tthe\\tanswer\\tis\\ttypically\\tdocumented\\tand\\tevaluation\\tcan\\tbe\\t\\nconducted\\tprogrammatically\\tat\\tscale.\\tThis\\tcontributed\\tto\\tits\\thistorical\\tpopularity\\tas\\ta\\t\\nbenchmark\\tin\\tscientific\\tresearch\\tas\\ta\\tgrand\\tchallenge\\tin\\tthe\\tpast,\\twhich\\tmakes\\tit\\tso\\tpowerful\\t\\nto\\tdemonstrate\\thow\\ttechnology\\tfacilitates\\tsignificant\\tadvancements.\\nFigure 5. Med-PaLM 2 reached expert-level performance on the MedQA medical exam benchmark \\nMed-PaLM\\twas\\tthe\\tfirst\\tAI\\tmodel\\tto\\texceed\\tthe\\tpassing\\tmark,\\treaching\\tthe\\tperformance\\tof\\t\\n67%,\\tand\\tMed-PaLM\\t2\\twas\\tthe\\tfirst\\tAI\\tmodel\\tto\\treach\\t86.5%,\\twhich\\tindicates\\texpert-level\\t\\nperformance\\t(Figure\\t5).\\nCrucially,\\tto\\testablish\\ta\\tmore\\tmeaningful\\tconnection\\tto\\tpotential\\tfuture\\tdevelopments\\tand\\t\\nenable\\tthe\\tdetailed\\tanalysis\\trequired\\tfor\\treal-world\\tclinical\\tapplications,\\tthe\\tscope\\tof\\tthe\\t\\nevaluation\\tmethods\\tproposed\\tby\\tMed-PaLM\\tframework\\textends\\tbeyond\\tmere\\taccuracy\\tin'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 26, 'page_label': '27'}, page_content='Solving Domain-Specific Problems Using LLMs\\n27\\nFebruary 2025\\nmultiple-choice\\tquestions.\\tThe\\tevaluation\\textends\\tto\\tqualitative\\tassessment\\tof\\tfactuality,\\t\\nuse\\tof\\texpert\\tknowledge\\tin\\treasoning,\\thelpfulness,\\thealth\\tequity,\\tand\\tpotential\\tharm\\twhen\\t\\nproviding\\tlong-form\\tanswers\\tto\\topen-ended\\tquestions.\\nThe\\trubric\\tfor\\tevaluation\\tby\\texpert\\tclinicians\\tincludes:\\n• How\\tdoes\\tthe\\tanswer\\trelate\\tto\\tthe\\tconsensus\\tin\\tthe\\tscientific\\tand\\tclinical\\tcommunity?\\n• What\\tis\\tthe\\textent\\tof\\tpossible\\tharm?\\n• What\\tis\\tthe\\tlikelihood\\tof\\tpossible\\tharm?\\n• Does\\tthe\\tanswer\\tcontain\\tany\\tevidence\\tof\\tcorrect\\treading\\tcomprehension?\\n• Does the answer contain any evidence of correct recall of knowledge?\\n• Does\\tthe\\tanswer\\tcontain\\tany\\tevidence\\tof\\tcorrect\\treasoning\\tsteps?\\n• Does\\tthe\\tanswer\\tcontain\\tany\\tevidence\\tof\\tincorrect\\treading\\tcomprehension?\\n• Does the answer contain any evidence of incorrect recall of knowledge?\\n• Does\\tthe\\tanswer\\tcontain\\tany\\tevidence\\tof\\tincorrect\\treasoning\\tsteps?\\n• Does the answer contain any content it shouldn’t?\\n• Does the answer omit any content it shouldn’t?\\n• Does\\tthe\\tanswer\\tcontain\\tinfo\\tthat\\tis\\tinapplicable\\tor\\tinaccurate\\tfor\\tany\\tparticular\\t\\nmedical\\tdemographic?\\n• How well does the answer address the intent of the question?\\n• How\\thelpful\\tis\\tthis\\tanswer\\tto\\tthe\\tuser?\\tDoes\\tit\\tenable\\tthem\\tto\\tdraw\\ta\\tconclusion\\tor\\thelp\\t\\nclarify\\tnext\\tsteps?'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 27, 'page_label': '28'}, page_content='Solving Domain-Specific Problems Using LLMs\\n28\\nFebruary 2025\\nFigure\\t6\\tshows\\tthe\\tevaluation\\trubric\\tapplied\\tto\\tan\\texample\\toutput\\tby\\tMed-PaLM\\t2.\\nFigure 6. Example of clinician review of Med-PaLM 2'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 28, 'page_label': '29'}, page_content='Solving Domain-Specific Problems Using LLMs\\n29\\nFebruary 2025\\nThe\\thuman\\tevaluation\\tfor\\tMed-PaLM\\tfollows\\tthis\\tprocedure:\\n• Each\\tquestion\\tis\\tpresented\\tto\\tboth\\tMed-PaLM\\tand\\ta\\tboard-certified\\tphysician.\\n• Both\\tMed-PaLM\\tand\\tthe\\tphysician\\tindependently\\tprovide\\ttheir\\tanswers.\\n• Those\\tanswers\\tare\\tthen\\tpresented\\tin\\ta\\tblinded\\tway\\t(i.e.,\\twho\\tprovided\\teach\\tanswer\\tis\\tnot\\t\\nindicated)\\tto\\tseparate\\traters.\\n• Additionally,\\tdirect\\tside-by-side\\tcomparisons\\twere\\tconducted,\\tsuch\\tas\\tdetermining\\twhich\\t\\nanswer\\tis\\tbetter\\tbetween\\tA\\tand\\tB\\t(where\\tA\\tand\\tB\\tare\\tblinded\\tand\\tcould\\trefer\\tto\\tphysician-\\nprovided\\tor\\toutputs\\tfrom\\tdifferent\\tAI\\tmodels).\\nIt\\tis\\timportant\\tto\\temphasize\\tthat\\tthe\\tevaluation\\tprimarily\\tfocuses\\ton\\tthe\\tsubstance\\tover\\tthe\\t\\nstyle\\t/\\tdelivery.\\tIn\\tcertain\\tinstances,\\ta\\tclinician’s\\tresponse\\tmay\\tbe\\tconcise\\tyet\\teffectively\\t\\nmeets\\tthe\\tevaluation\\tcriteria,\\twhile\\tin\\tother\\tscenarios,\\ta\\tmore\\tdetailed\\tbut\\tverbose\\tanswer\\t\\nmay\\tbe\\tmore\\tappropriate.\\nOur\\thuman\\tevaluation\\tresults\\tas\\tof\\tMay\\t2023\\tindicate\\tthat\\tthe\\tanswers\\tprovided\\tby\\t\\nour\\tmodels\\tcompare\\twell\\tto\\tthose\\tfrom\\tphysicians\\tacross\\tseveral\\tcritical\\tclinically\\t\\nimportant\\taxes.\\nSince\\tconducting\\tevaluations\\twith\\tscientific\\trigor\\trequires\\tthe\\tinvolvement\\tof\\texpert\\tlaborers,\\t\\nsuch\\tas\\tboard-certified\\tphysicians,\\tthe\\tprocess\\tis\\tnotably\\tcostlier\\tthan\\tevaluating\\tmultiple-\\nchoice\\tquestions.\\tIt\\tis\\tpromising\\tto\\tsee\\tthat\\tother\\tstudies10\\thave\\tadopted\\tand\\texpanded\\tupon\\t\\nthe\\tsuggested\\tframework\\tfor\\tthe\\tpurpose\\tof\\tbeing\\tcomparative\\tand\\taligned\\twith\\tAI\\tsafety.\\t\\nThe\\texpert\\tevaluation\\tplays\\ta\\tvital\\trole\\tcritical\\tin\\tdiscerning\\tstyle\\t(i.e.,\\tdelivery)\\tand\\tcontent\\t\\nas well as correctness.\\nWe\\talso\\tlearned\\tthat\\tmore\\twork\\tremains,\\tincluding\\timprovements\\talong\\tspecific\\tevaluation\\t\\naxes\\twhere\\tphysicians’\\tperformance\\tremained\\tsuperior.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 29, 'page_label': '30'}, page_content='Solving Domain-Specific Problems Using LLMs\\n30\\nFebruary 2025\\nThe detailed results are the cornerstone of understanding and identifying areas in need of  \\nfuture\\tscientific\\tmodeling\\tand\\tevaluation,\\tas\\twell\\tas\\tdetermining\\tthe\\tfeasibility\\tof\\tthe\\tnext\\t\\nstep\\tin\\tour\\tjourney.\\nAlthough\\tquantitative\\tand\\tqualitative\\timprovements\\tcan\\tbe\\tmade\\tin\\torder\\tto\\tachieve\\t\\nperfect\\tperformance\\ton\\tbenchmarks,\\tthe\\ttechnology\\tcan\\tstill\\tprovide\\tpractical\\tvalue\\tin\\t\\nreal-world\\tsettings.\\nEvaluation in real clinical environments\\nThe\\tintegration\\tof\\ttechnology\\tinto\\tthe\\tclinical\\tenvironment\\tis\\ta\\twell-established\\tarea,\\tand\\t\\nGoogle\\thas\\tgained\\tits\\town\\texpertise5\\tin\\tthe\\tfield\\tthrough\\tscreening\\tfor\\tdiabetic\\tretinopathy.\\t\\nOne\\tof\\tthe\\tmain\\tinsights\\tlearned\\tis\\tthat\\tachieving\\thigh\\tperformance\\ton\\tretrospective\\t\\ndatasets\\tdoes\\tnot\\tautomatically\\ttranslate\\tinto\\tclinical\\tperformance.\\tIt\\tis\\timperative\\tto\\t\\ncarefully validate AI solutions in real-world environments in a meticulous manner to ensure \\ntheir robustness and reliability.\\nEach\\ttechnology\\tintegrated\\tinto\\ta\\tpatient’s\\tjourney,\\twhether\\tit\\tfalls\\tunder\\tregulatory\\t\\noversight\\tor\\tnot,\\tis\\tencouraged\\tto\\tadhere\\tto\\tthese\\tscientific\\tsteps:\\n• Retrospective evaluation: Evaluate the technology against real-world data collected \\nfrom\\tpast\\tcases.\\n• Prospective observational (non-interventional): Evaluate on newly collected real-world \\ndata,\\tbut\\tensure\\tthat\\tthe\\toutputs\\tof\\tthe\\ttechnology\\tdo\\tnot\\timpact\\tpatient\\tcare\\tor\\tsafety.\\t\\nAn\\texample\\tis\\tfeeding\\tlive\\tdata\\tinto\\tthe\\ttechnology\\tand\\tthen\\thaving\\tthe\\tappropriate\\t\\nexperts\\tevaluate\\tthe\\ttechnology’s\\toutput.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 30, 'page_label': '31'}, page_content=\"Solving Domain-Specific Problems Using LLMs\\n31\\nFebruary 2025\\n• Prospective interventional :\\tDeploy\\tthe\\ttechnology\\twithin\\ta\\tlive\\tclinical\\tenvironment\\t\\nwith\\tconsented\\tpatients\\tand\\tinfluence\\tpatient\\tcare\\tand\\tpotentially\\thealth\\toutcomes.\\t\\nThis\\tstep\\trequires\\ta\\tdetailed\\tand\\tIRB-approved\\tstudy\\tprotocol\\tand\\tcare\\ttaken\\tto\\tensure\\t\\npatient\\tsafety.\\nThese\\tsteps\\tare\\tcrucial\\tnot\\tjust\\tfor\\tassessing\\tthe\\tmodel's\\tperformance\\ton\\tnew\\tunseen\\tdata\\t\\nbut\\talso,\\tmore\\tsignificantly,\\tfor\\tevaluating\\tthe\\teffectiveness\\tof\\tthe\\tend-to-end\\tsystem\\twhen\\t\\nintegrated\\tinto\\treal\\tworkflows.\\tOccasionally,\\tthe\\toptimal\\tway\\tto\\tuse\\tGenAI\\tmodels\\tlike\\tMed-\\nPaLM\\tmay\\tdiverge\\tfrom\\tinitial\\tassumptions,\\tand\\tintroducing\\ta\\tnew\\ttool\\tinto\\ta\\tclinical\\tworkflow\\t\\nmight\\trequire\\tunexpected\\tadjustments\\tto\\tthe\\toverall\\tprocess.11,12 End-to-end assessment is \\nessential\\tfor\\tunderstanding\\tthe\\trole\\tand\\tbenefit\\tof\\tthe\\ttechnology\\tand\\ttailoring\\tAI\\tsolutions\\t\\nto\\tmeet\\tthe\\tneeds\\teffectively.\\nTask- vs. domain-specific models\\nMed-PaLM7\\thighlighted\\tthe\\tsignificance\\tand\\tvalue\\tof\\ta\\tspecialized\\tmodel\\tfor\\tthe\\tmedical\\t\\ndomain.\\tMed-PaLM\\t2,\\tan\\taligned\\tand\\tfine-tuned\\titeration\\tof\\tPaLM\\t2\\ttailored\\tto\\tmedical\\t\\nknowledge,\\tachieves\\ta\\tninefold\\tenhancement\\tin\\tprecise\\treasoning\\tcompared\\tto\\tthe\\t\\nbaseline.13\\tHowever,\\tit's\\tcrucial\\tto\\trecognize\\tthat\\texcelling\\tin\\tone\\tmedical\\tdomain\\ttask\\tdoesn't\\t\\nnecessarily\\tguarantee\\tand\\timply\\tsuccess\\tin\\ta\\tdifferent\\tmedical\\tdomain\\ttask.\\tFor\\tinstance,\\t\\ndoes\\ta\\tgreat\\tgeneral\\tmedical\\tQA\\tsystem\\talso\\tperform\\twell\\ton\\ta\\tmental\\thealth\\tassessment\\t\\ntask? While it's reasonable to assume that a demonstrated understanding of clinical \\nknowledge\\tcan\\tgeneralize\\teffectively\\tto\\ttasks\\theavily\\trelying\\ton\\tthis\\tknowledge,\\teach\\tspecific\\t\\ntask\\trequires\\tvalidation\\tand\\tpossible\\tadaptation,\\tsuch\\tas\\tthe\\tmeasurement\\tof\\tpsychiatric\\t\\nfunctioning,14\\tbefore\\tproceeding\\tfurther.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 31, 'page_label': '32'}, page_content=\"Solving Domain-Specific Problems Using LLMs\\n32\\nFebruary 2025\\nThe\\tmedical\\tdomain\\talso\\textends\\twell\\tbeyond\\ttextual\\tinformation.\\tThe\\tpractice\\tof\\tmedicine\\tis\\t\\ninherently\\tmulti-modal\\tand\\tincorporates\\tinformation\\tfrom\\timages,\\telectronic\\thealth\\trecords,\\t\\nsensors,\\twearables,\\tgenomics,\\tand\\tmore.\\tMultimodal\\tversions15 of MedLM and related \\napproaches16,17,18\\tare\\tin\\tearly\\tstages\\tof\\tresearch,\\tand\\tfollow\\tthe\\tsame\\tvalidation\\tprinciples\\tand\\t\\nworkflow\\tintegration\\tapproach.\\tWe\\twill\\tbe\\tobserving\\tthe\\tmultimodal-enabled\\tset\\tof\\tuse-\\ncases\\tevaluated\\tand\\tdeployed\\tin\\tthe\\tfield.\\nLastly,\\ta\\tmedically\\tspecialized\\tmodel\\tcan\\tbe\\tapplied\\tnot\\tonly\\tto\\tclinical\\tuse\\tcases\\tthat\\trelate\\t\\ndirectly\\tto\\tpatient\\tcare,\\tbut\\talso\\tto\\tuse\\tcases\\tthat\\tbenefit\\tfrom\\tleveraging\\tmedical\\tknowledge\\t\\nin\\ta\\tflexible\\tway.\\tAn\\texample\\tis\\tin\\tscientific\\tdiscovery,\\twhere\\tMed-PaLM\\tcan\\tbe\\tused\\tto\\t\\naccurately identify genes associated with biomedical traits.19\\tWe'll\\tbe\\texploring\\ta\\t\\tbreadth\\t\\nof\\tpossibilities\\twith\\tvertical-specific\\tmodels,\\tand\\twe\\texpect\\tnew\\tapplications\\tand\\tideas\\tto\\t\\nemerge\\tin\\tthe\\tfield\\tover\\tthe\\tnext\\tfew\\tyears.\\tWe’re\\talso\\texploring\\tsafe\\tand\\tresponsible\\tways\\t\\nto\\tbring\\tthese\\tmodels\\tto\\tthe\\thealthcare\\tindustry.\\tWith\\tMedLM,\\ta\\tsuite\\tof\\tmodels\\tfine-tuned\\t\\nfor\\thealthcare\\tuse\\tcases,\\tbuilt\\ton\\tMed-PaLM\\t2,\\twe’re\\tmaking\\tsolutions\\tcommercially\\tavailable\\t\\nso\\thealthcare\\torganizations\\tcan\\tbuild\\tGenAI\\tuse\\tcases\\tsuitable\\tfor\\ttheir\\tworkflows.\\nTraining strategies for Med-PaLM 2\\nMed-PaLM\\t2\\tis\\tan\\tadvancement\\tof\\tthe\\tbase\\tLLM\\tmodel\\tPaLM\\t2,\\tGoogle's\\tenhanced\\tLLM\\twith\\t\\nsubstantial\\tperformance\\timprovements\\ton\\tmultiple\\tLLM\\tbenchmark\\ttasks.\\tTo\\ttailor\\tMed-\\nPaLM\\t2\\tfor\\tmedical\\tapplications,\\tinstruction\\tfine-tuning7\\twas\\tperformed\\tusing\\tMultiMedQA,7 \\nincluding\\tMedQA,\\tMedMCQA,\\tHealthSearchQA,\\tLiveQA,\\tand\\tMedicationQA\\tdatasets.\\tDataset\\t\\nmixture\\tratios\\twere\\tempirically\\tdetermined.\\t\\nTo\\tenhance\\tthe\\tspecialized\\tvariant\\tof\\tMed-PaLM\\t2\\tfocusing\\ton\\tmultiple-choice\\tquestions,\\t\\na\\trange\\tof\\tprompting\\tstrategies\\tincluding\\tfew-shot\\tprompting,\\tchain-of-thought\\t(CoT)\\t\\nprompting,\\tand\\tself-consistency\\twere\\temployed.\\tCoT\\tinvolves\\taugmenting\\teach\\tfew-shot\\t\\nexample\\tin\\ta\\tprompt\\twith\\ta\\tstep-by-step\\texplanation\\ttowards\\tthe\\tfinal\\tanswer,\\tallowing\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 32, 'page_label': '33'}, page_content=\"Solving Domain-Specific Problems Using LLMs\\n33\\nFebruary 2025\\nthe\\tlanguage\\tmodel\\tto\\tcondition\\ton\\tits\\town\\tintermediate\\toutputs\\tfor\\tmulti-step\\tproblem-\\nsolving.\\tSelf-consistency\\tplays\\ta\\trole\\tin\\tenhancing\\tthe\\tmodel's\\tperformance\\ton\\tmultiple-\\nchoice\\tquestions\\tby\\tsampling\\tmultiple\\texplanations\\tand\\tanswers\\tfrom\\tthe\\tmodel,\\twith\\tthe\\t\\nfinal\\tanswer\\tdetermined\\tby\\ta\\tmajority\\tvote\\tamong\\tthe\\tgenerated\\toptions.\\tThese\\tstrategies\\t\\ncollectively\\timprove\\tthe\\tmodel's\\tability\\tto\\treason\\tand\\tprovide\\tmore\\taccurate\\tresponses\\tto\\t\\ncomplex\\tand\\tmulti-faceted\\tqueries.\\nAnother\\tnoteworthy\\tmethodological\\timprovement\\tis\\tthe\\tintroduction\\tof\\tensemble\\trefinement\\t\\n(ER),\\twhich\\tbuilds\\ton\\tother\\ttechniques\\tthat\\tinvolve\\tconditioning\\tan\\tLLM\\ton\\tits\\town\\t\\ngenerations\\tbefore\\tproducing\\ta\\tfinal\\tanswer.\\tIn\\tthe\\tfirst\\tstage,\\tmultiple\\tpossible\\texplanations\\t\\nand\\tanswers\\tare\\tstochastically\\tgenerated\\tvia\\ttemperature\\tsampling.\\tIn\\tthe\\tsecond\\tstage,\\t\\nthe\\tmodel\\tis\\tconditioned\\ton\\tthe\\toriginal\\tprompt,\\tquestion,\\tand\\tgenerated\\tcontents\\tfrom\\t\\nthe\\tfirst\\tstage,\\tresulting\\tin\\tthe\\tproduction\\tof\\ta\\trefined\\texplanation\\tand\\tanswer.\\tThis\\tprocess\\t\\nfacilitated\\tthe\\teffective\\taggregation\\tof\\tanswers,\\textending\\tits\\tutility\\tbeyond\\tquestions\\twith\\t\\na\\tlimited\\tset\\tof\\tpotential\\tanswers,\\tthereby\\tenhancing\\tthe\\toverall\\tperformance\\tof\\tthe\\tmodel.\\t\\nThe\\toverall\\tmechanism\\tof\\tensemble\\trefinement\\tis\\tdepicted\\tin\\tFigure\\t7.\\nFigure 7. Ensemble refinement (ER) in Med-PaLM 2.  This approach involves conditioning an LLM on multiple \\npotential reasoning pathways it generates, facilitating the answer refinement and improvement\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 33, 'page_label': '34'}, page_content=\"Solving Domain-Specific Problems Using LLMs\\n34\\nFebruary 2025\\nThe\\tgoal\\tbehind\\tthe\\tinception\\tof\\tthe\\tMed-PaLM\\tresearch\\teffort\\twas\\tto\\timprove\\thealth\\t\\noutcomes\\tvia\\tusing\\tand\\tadvancing\\temerging\\tAI\\ttechnologies.\\tAchieving\\texpert-level\\t\\nperformance\\tin\\tmedical\\tQA\\ttasks\\twas\\tthe\\tfirst\\tstep,\\twith\\tmany\\tmore\\tto\\tfollow\\tin\\tclose\\t\\ncollaboration\\twith\\tthe\\tclinical\\tcommunity\\tas\\twe\\tprogress\\ton\\tthis\\tjourney.\\t\\nOur\\thealth\\tresearch\\texperience\\tat\\tGoogle\\tdemonstrated\\trepeatedly\\tthat\\ttechnology\\tis\\toften\\t\\nnot\\tthe\\tsole\\tchallenge\\tin\\tapplying\\tAI\\tproductively\\tto\\thealthcare.\\tInstead,\\tmany\\tother\\tfactors,\\t\\nincluding\\tthoughtful\\tevaluation\\tstrategies\\tand\\tworking\\ton\\tclinically\\tmeaningful\\tapplications\\t\\nin\\tpartnership\\twith\\tclinicians\\tand\\ta\\tbroad\\tcross-functional\\tteam,\\tare\\tpivotal\\tto\\tsuccess.5 This \\nvaluable\\tinsight\\tis\\tlikely\\tapplicable\\tto\\tother\\tvertical\\tdomains\\tas\\twell.\\nAs\\tAI\\ttechnology\\tmatures\\tand\\tmoves\\tcloser\\tto\\tpractical\\tuse\\tcases\\tand\\treal-world\\tscenarios,\\t\\ncareful\\tmulti-step\\tevaluations,\\tincluding\\tboth\\tretrospective\\tand\\tprospective\\tassessments,\\t\\nare\\tbeneficial\\tto\\tbetter\\tunderstand\\tthe\\treal\\trole\\tand\\tbenefits\\tof\\tthe\\ttechnology\\tin\\tthe\\twhole\\t\\nworkflow.\\tGuidance\\tby\\ta\\tclinical\\tpartner\\timproves\\tthe\\tchances\\tof\\tbuilding\\tthe\\tright\\tsolution\\t\\nfor\\tbetter\\thealth\\toutcomes.\\tMany\\tpromising\\tapplications\\tlie\\tin\\tthe\\tcollaboration\\tof\\thealthcare\\t\\nworkers\\tand\\ttechnology,\\tcombining\\tthe\\tstrengths\\tof\\tboth.\\tIt\\tis\\talso\\timportant\\tto\\tuse\\tGenAI\\t\\nsystems\\tin\\ta\\tway\\tthat\\tis\\trespectful\\tof\\tpatients'\\tautonomy\\tand\\tprivacy.\\nFor\\tthe\\tforeseeable\\tfuture,\\tit\\tis\\treasonable\\tto\\tassume\\tthat\\tmodels\\tcustomized\\tfor\\tspecific\\t\\napplications\\tor\\tdomains\\twill\\tyield\\tbetter\\tresults,\\tand\\twe\\tare\\ttracking\\ttrends\\tand\\tany\\t\\nconvergence\\tin\\tperformance\\tbetween\\tgeneral\\tand\\tspecific\\tmodels\\tin\\tthe\\tyears\\tahead.\\tFor\\t\\nMed-PaLM\\tspecifically,\\tour\\tresearch\\tprogress\\twill\\tbe\\ttracked\\tat\\tthe\\tMed-PaLM\\tresearch\\t\\nwebpage.20\\tWe\\taim\\tto\\tmake\\tprogress\\tmore\\tbroadly\\tin\\tthe\\tfield\\tof\\tusing\\tAI\\tand\\tGenAI\\tfor\\tthe\\t\\nbetterment\\tof\\tpatients,\\tclinicians,\\tand\\tresearchers.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 34, 'page_label': '35'}, page_content='Solving Domain-Specific Problems Using LLMs\\n35\\nFebruary 2025\\nSummary\\nThis\\twhitepaper\\texplores\\tthe\\tpotential\\tof\\tLLMs\\tin\\ttackling\\tcomplex\\tchallenges\\twithin\\tspecific\\t\\ndomains,\\twith\\ta\\tparticular\\tfocus\\ton\\thealthcare\\tand\\tcybersecurity.\\n• Cybersecurity:\\tThe\\tever-evolving\\tlandscape\\tof\\tcyber\\tthreats\\tdemands\\tinnovative\\t\\nsolutions.\\tSecLM,\\tan\\tLLM\\tdesigned\\tfor\\tcybersecurity,\\tacts\\tas\\ta\\tforce\\tmultiplier\\tfor\\t\\nsecurity\\tprofessionals\\tby\\tintelligently\\tprocessing\\tvast\\tamounts\\tof\\tdata.\\tThis\\tempowers\\t\\nthem\\tto\\tanalyze\\tand\\trespond\\tto\\tthreats\\tmore\\teffectively.\\tThe\\tvision\\tfor\\tSecLM\\tis\\tto\\tcreate\\t\\na\\tcomprehensive\\tplatform\\tthat\\tcaters\\tto\\tthe\\tdiverse\\tneeds\\tof\\tsecurity\\tpractitioners,\\t\\nregardless\\tof\\ttheir\\texpertise.\\tThe\\tcombination\\tof\\tLLMs\\tand\\thuman\\texpertise\\thas\\tthe\\t\\npotential\\tto\\trevolutionize\\tthe\\tfield\\tof\\tcybersecurity,\\tachieving\\tsuperior\\tresults\\twith\\t\\nless\\teffort.\\n• Healthcare:\\tHealthcare\\tdata\\tis\\tincreasing\\tin\\tquantity\\tand\\tcomplexity,\\tleading\\tto\\ta\\t\\nneed\\tfor\\tinnovative\\tsolutions\\tto\\trender\\tmedical\\tinformation\\tmore\\thelpful,\\tuseful,\\tand\\t\\naccessible.\\tMedLM,\\ta\\tfamily\\tof\\tmodels\\tfine-tuned\\tfor\\tthe\\thealthcare\\tindustry,\\tcan\\thelp\\t\\nunlock\\tknowledge\\tand\\tmake\\tmedicine\\tmore\\teffective.\\tMedLM\\tis\\tbuilt\\ton\\tMed-PaLM,\\t\\nan\\tLLM\\tdeveloped\\tfor\\tmedical\\tapplications.\\tMed-PaLM\\thas\\tdemonstrated\\texpert-level\\t\\nperformance\\tin\\tmedical\\tquestion-and-answering\\ttasks.\\tThis\\tachievement\\tis\\tjust\\tthe\\tfirst\\t\\nstep\\tin\\ta\\tjourney\\ttowards\\timproving\\thealth\\toutcomes\\tthrough\\tthe\\tutilization\\tof\\tGenAI.\\tThe\\t\\nkey takeaway from this research is that technology alone is not enough. Collaboration \\nwith\\tthe\\tclinical\\tcommunity\\tand\\tcareful\\tmulti-step\\tevaluations\\tare\\tcrucial\\tfor\\tsuccessful\\t\\napplication\\tof\\tLLMs\\tin\\thealthcare.\\tGoing\\tforward,\\tvertical-specific\\tmodels\\tlike\\tthe\\tMedLM\\t\\nfoundation\\tmodels\\tare\\texpected\\tto\\tyield\\teven\\tbetter\\tresults\\tfor\\tspecific\\tapplications\\tof\\t\\ninterest,\\tfurthering\\tthe\\tpotential\\tof\\tAI\\tin\\thealthcare.\\nThis\\twhitepaper\\tshowcases\\tthe\\tpossibilities\\tof\\tLLMs\\tin\\tsolving\\tdomain-specific\\tproblems.\\tBy\\t\\nleveraging\\tthe\\tpower\\tof\\tthese\\tadvanced\\tmodels,\\tcombined\\twith\\thuman\\texpertise\\tand\\tcareful\\t\\nimplementation,\\twe\\tcan\\ttackle\\tcomplex\\tchallenges\\tand\\tachieve\\tbreakthrough\\tadvancements\\t\\nin\\tvarious\\tfields,\\tfor\\tthe\\tbenefit\\tof\\tpeoples’\\tlives.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 35, 'page_label': '36'}, page_content='Solving Domain-Specific Problems Using LLMs\\n36\\nFebruary 2025\\nEndnotes\\n1.\\t Cantos,\\tJ.,\\tet\\tal.,\\t2023.\\tThreat\\tActors\\tare\\tInterested\\tin\\tGenerative\\tAI,\\tbut\\tUse\\tRemains\\tLimited.\\t[online]\\t\\nAvailable at: https://cloud.google.com/blog/topics/threat-intelligence/threat-actors-generative-ai-limited/ .\\n2.\\t Lin,\\tC.Y.,\\tet\\tal.,\\t2003.\\tAutomatic\\tEvaluation\\tof\\tSummaries\\tUsing\\tn-gram\\tCo-occurrence\\tStatistics.\\t[online]\\t\\nAvailable at: https://aclanthology.org/N03-1020.pdf .\\n3.\\t Papineni,\\tK.,\\tet\\tal.,\\t2002.\\tBLEU:\\tA\\tMethod\\tfor\\tAutomatic\\tEvaluation\\tof\\tMachine\\tTranslation.\\t[online]\\tAvailable\\t\\nat: https://aclanthology.org/P02-1040.pdf.\\n4.\\t Zhang,\\tT.,\\tet\\tal.,\\t2019.\\tBERTScore:\\tEvaluating\\tText\\tGeneration\\twith\\tBERT.\\t[online]\\tAvailable\\tat:\\thttps://\\nopenreview.net/attachment?id=SkeHuCVFDr&name=original_pdf .\\n5.\\t Google,\\t2023.\\t5\\tmyths\\tabout\\tmedical\\tAI,\\tdebunked.\\t[online]\\tAvailable\\tat:\\thttps://blog.google/technology/\\nhealth/5-myths-about-medical-ai-debunked/ .\\n6.\\t Bommasani,\\tR.,\\tet\\tal.,\\t2021.\\tOn\\tthe\\topportunities\\tand\\trisks\\tof\\tfoundation\\tmodels.\\tarXiv\\tpreprint\\t\\narXiv:2108.07258.\\t[online]\\tAvailable\\tat:\\thttps://arxiv.org/pdf/2108.07258.\\n7.\\t Singhal,\\tK.,\\tet\\tal.,\\t2023.\\tLarge\\tlanguage\\tmodels\\tencode\\tclinical\\tknowledge.\\tNature,\\t620(7972),\\tpp.172-180.\\t\\n[online]\\tAvailable\\tat:\\thttps://www.nature.com/articles/s41586-023-06291-2.\\n8.\\t Jin,\\tD.,\\tet\\tal.,\\t2021.\\tWhat\\tdisease\\tdoes\\tthis\\tpatient\\thave?\\ta\\tlarge-scale\\topen\\tdomain\\tquestion\\tanswering\\t\\ndataset\\tfrom\\tmedical\\texams.\\tApplied\\tSciences,\\t11(14),\\tp.6421.\\n9.\\t Singhal,\\tK.,\\tet\\tal.,\\t2023.\\tTowards\\texpert-level\\tmedical\\tquestion\\tanswering\\twith\\tlarge\\tlanguage\\tmodels.\\tarXiv\\t\\npreprint\\tarXiv:2305.09617.\\t[online]\\tAvailable\\tat:\\thttps://arxiv.org/abs/2305.09617.\\n10.\\tBernstein,\\tI.A.,\\tet\\tal.,\\t2023.\\tComparison\\tof\\tophthalmologist\\tand\\tlarge\\tlanguage\\tmodel\\tchatbot\\tresponses\\t\\nto\\tonline\\tpatient\\teye\\tcare\\tquestions.\\tJAMA\\tNetwork\\tOpen,\\t6(8),\\tpp.e2330320-e2330320.\\t[online]\\tAvailable\\tat:\\t\\nhttps://jamanetwork.com/journals/jamanetworkopen/fullarticle/2808557 .\\n11.\\t Beede,\\tE.,\\tet\\tal.,\\t2019.\\tA\\tHuman-Centered\\tEvaluation\\tof\\ta\\tDeep\\tLearning\\tSystem\\tDeployed\\tin\\tClinics\\tfor\\tthe\\t\\nDetection\\tof\\tDiabetic\\tRetinopathy.\\t[online]\\tAvailable\\tat:\\thttps://dl.acm.org/doi/abs/10.1145/3313831.3376718.\\n12.\\t Pedersen,\\tS.,\\tet\\tal.,\\t2021.\\tRedesigning\\tClinical\\tPathways\\tfor\\tImmediate\\tDiabetic\\tRetinopathy\\tScreening\\t\\nResults.\\tNEJM\\tCatalyst,\\tJuly.\\t[online]\\tAvailable\\tat:\\thttps://catalyst.nejm.org/doi/pdf/10.1056/CAT.21.0096.\\n13.\\tGoogle,\\t2023.\\tGoogle\\tI/O\\tKeynote\\t2023.\\t[online]\\tAvailable\\tat:\\t\\thttps://www.youtube.com/live/\\ncNfINi5CNbY?si=jQFi-Y3mG0rGD3Xd&t=810 .'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 36, 'page_label': '37'}, page_content='Solving Domain-Specific Problems Using LLMs\\n37\\nFebruary 2025\\n14.\\tGalatzer-Levy,\\tI.R.,\\tet\\tal.,\\t2023.\\tThe\\tCapability\\tof\\tLarge\\tLanguage\\tModels\\tto\\tMeasure\\tPsychiatric\\t\\nFunctioning.\\tarXiv\\tpreprint\\tarXiv:2308.01834.\\t[online]\\tAvailable\\tat:\\thttps://arxiv.org/abs/2308.01834.\\n15.\\tTu,\\tT.,\\tet\\tal.,\\t2023.\\tTowards\\tgeneralist\\tbiomedical\\tAI.\\tarXiv\\tpreprint\\tarXiv:2307.14334.\\t[online]\\tAvailable\\tat:\\t\\nhttps://arxiv.org/abs/2307.14334.\\n16.\\tLiu,\\tX.,\\tet\\tal.,\\t2023.\\tLarge\\tLanguage\\tModels\\tare\\tFew-Shot\\tHealth\\tLearners.\\tarXiv:2305.15525.\\t[online]\\t\\nAvailable at: https://arxiv.org/abs/2305.15525 .\\n17.\\t Belyaeva,\\tA.,\\tet\\tal.,\\t2023.\\tMultimodal\\tLLMs\\tfor\\thealth\\tgrounded\\tin\\tindividual-specific\\tdata.\\tarXiv:2307.09018.\\t\\n[online]\\tAvailable\\tat:\\thttps://arxiv.org/abs/2307.09018.\\n18.\\tShawn,\\tX.,\\tet\\tal.,\\t2022.\\tELIXR:\\tTowards\\ta\\tgeneral\\tpurpose\\tX-ray\\tartificial\\tintelligence\\tsystem\\tthrough\\t\\nalignment\\tof\\tlarge\\tlanguage\\tmodels\\tand\\tradiology\\tvision\\tencoders.\\tarXiv:2308.01317.\\t[online]\\tAvailable\\tat:\\t\\nhttps://arxiv.org/abs/2308.01317 .\\n19.\\t Tu,\\tT.,\\tet\\tal.,\\t2023.\\tGenetic\\tDiscovery\\tEnabled\\tby\\ta\\tLarge\\tLanguage\\tModel.\\t[online]\\tAvailable\\tat:\\thttps://www.\\nbiorxiv.org/content/10.1101/2023.11.09.566468v1.full.pdf .\\n20.\\tMed-PaLM,\\t[n.d.].\\tHomepage.\\t[online]\\tAvailable\\tat:\\thttps://g.co/research/medpalm.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 0, 'page_label': '1'}, page_content='Embeddings  \\n& Vector Stores\\nAuthors: Anant Nawalgaria, \\nXiaoqi Ren, and Charles Sugnet'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 1, 'page_label': '2'}, page_content='Embeddings & Vector Stores\\n2\\nFebrurary 2025\\nContent contributors\\nAntonio Gulli\\nGrace Mollison\\nRuiqi Guo\\nIftekhar Naim\\nJinhyuk Lee\\nAlan Li\\nPatricia Florissi\\nAndrew Brook\\nOmid Fatemieh\\nZhuyun Dai\\nLee Boonstra\\nPer Jacobsson\\nSiddhartha Reddy Jonnalagadda\\nXi Cheng\\nRaphael Hoffmann\\nCurators and Editors\\nAntonio Gulli\\nAnant Nawalgaria\\nGrace Mollison \\nTechnical Writer\\nJoey Haymaker\\nDesigner\\nMichael Lanning \\nAcknowledgements'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 2, 'page_label': '3'}, page_content='Introduction 5\\nWhy embeddings are important 6\\nEvaluating Embedding Quality 9\\nSearch Example 11\\nTypes of embeddings 16\\nText embeddings 16\\nWord embeddings 19\\nDocument embeddings 23\\nShallow BoW models 24\\nDeeper pretrained large language models 26\\nImage & multimodal embeddings 30\\nStructured data embeddings 32\\nGeneral structured data 32\\nUser/item structured data 33\\nTable of contents'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 3, 'page_label': '4'}, page_content='Graph embeddings 33\\nTraining Embeddings 34\\nVector search 36\\nImportant vector search algorithms 37\\nLocality sensitive hashing & trees 38\\nHierarchical navigable small worlds  41\\nScaNN 44\\nVector databases  47\\nOperational considerations 49\\nApplications 51\\nQ & A with sources (retrieval augmented generation) 52\\nSummary 57\\nEndnotes 59'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 4, 'page_label': '5'}, page_content=\"Embeddings & Vector Stores\\n5\\nFebrurary 2025\\nIntroduction\\nModern machine learning thrives on diverse data—images, text, audio, and more. This \\nwhitepaper explores the power of embeddings, which transform this heterogeneous data into \\na unified vector representation for seamless use in various applications.\\nWe'll guide you through:\\n• Understanding Embeddings: Why they are essential for handling multimodal data and \\ntheir diverse applications.\\n• Embedding Techniques: Methods for mapping different data types into a common \\nvector space.\\nThese low-dimensional numerical \\nrepresentations of real-world data \\nsignificantly helps efficient large-\\nscale data processing and storage \\nby acting as means of lossy \\ncompression of the original data.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 5, 'page_label': '6'}, page_content='Embeddings & Vector Stores\\n6\\nFebrurary 2025\\n• Efficient Management: Techniques for storing, retrieving, and searching vast collections \\nof embeddings.\\n• Vector Databases: Specialized systems for managing and querying embeddings, \\nincluding practical considerations for production deployment.\\n• Real-World Applications: Concrete examples of how embeddings and vector databases \\nare combined with large language models (LLMs) to solve real-world problems.\\nThroughout the whitepaper, code snippets provide hands-on illustrations of key concepts.\\nWhy embeddings are important\\nIn essence, embeddings are numerical representations of real-world data such as text, \\nspeech, image, or videos. The name embeddings refers to a similar concept in mathematics \\nwhere one space can be mapped, or embedded, into another space. For example, the \\noriginal BERT Model [ref] embeds text into a vector of 768 numbers, thus mapping from the \\nvery high dimensional space of all sentences to a much smaller 768 dimensions. Embeddings \\nare expressed as low-dimensional vectors where the geometric distance between two \\nvectors in the vector space is a projection of the relationship and semantic similarity between \\nthe two real-world objects that the vectors represent. In other words, they help you with \\nproviding compact representations of data of different types, while simultaneously also \\nallowing you to compare two different data objects and tell how similar or different they \\nare on a numerical scale. For example: the word ‘computer’ has a similar meaning to the \\npicture of a computer, as well as to the word ’laptop’ but not to the word ‘car’. These low-\\ndimensional numerical representations of real-world data significantly help efficient large-\\nscale data processing and storage by acting as means of lossy compression of the original \\ndata while retaining its important semantic properties.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 6, 'page_label': '7'}, page_content='Embeddings & Vector Stores\\n7\\nFebrurary 2025\\nFor some intuition about embeddings consider the familiar latitude and longitude which \\nare used to map locations on earth to a pair of numbers, or a vector of length two. Latitude \\nand longitude can be thought of as an embedding of a particular location. While seemingly \\nobvious now, this simple mapping of a location to a pair of numbers transformed human \\nnavigation and is still critical to this day. Given the latitude and longitude of two addresses \\nit is relatively easy to see how distant they are from each other, or look up other nearby \\nlocations. As with latitutde and longitude if two text embeddings are close to each other \\nin the embeddings space they will be semantically similar in their text meaning. Also, it \\nis possible to find new semantically similar text phrases by looking nearby in that vector \\nspace. This ability to find similar items in very large data sets with very low latency using \\nvector databases is critical for many production use cases today including search, \\nrecommendations, advertising, fraud detection and many more. Note that while the latitude \\nand longitude embedding model was designed based on the spherical shape of the earth, \\nthe embedding space for text is learned by the neural network model. Importantly, the \\nembeddings learned by different models will not be comparable to each other and it is \\ncritical to make sure in practice that compatible and consistent versions of embeddings are \\nbeing used.\\nKey applications for embeddings are retrieval and recommendations, where the results are \\nusually selected from a massive search space. For example, Google Search is a retrieval task \\nover the search space of the entire internet. Today’s retrieval and recommendation systems’ \\nsuccess depends on the following steps:\\n1. Precomputing the embeddings for billions of items in the search space.\\n2. Mapping query embeddings into the same embedding space.\\n3. Efficient computing and retrieving of the items whose embeddings are the nearest \\nneighbors of the query embeddings in the search space.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 7, 'page_label': '8'}, page_content='Embeddings & Vector Stores\\n8\\nFebrurary 2025\\nEmbeddings also shine in the world of multimodality. Many applications work with large \\namounts of data of various modalities: text, speech, image, and videos to name a few. \\nJoint embeddings are when multiple types of objects are being mapped into the same \\nembeddings space, for example retrieving videos based on text queries. These embedding \\nrepresentations are designed to capture as much of the original object’s characteristics \\nas possible.\\nFigure 1. Projecting objects/content into a joint vector space with semantic meaning\\nEmbeddings are designed so they place objects with similar semantic properties closer in \\nthe embedding space (a low-dimensional vector space where items can be projected). The \\nembeddings can then be used as a condensed, meaningful input in downstream applications. \\nFor example, you can use them as features for ML models, recommender systems, search'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 8, 'page_label': '9'}, page_content='Embeddings & Vector Stores\\n9\\nFebrurary 2025\\nengines, and many more. So your data not only gets a compact numerical representation, \\nbut this representation also preserves the semantic meanings for a specific task or across \\na variety of tasks. The fact that these representations are task-specific means you can \\ngenerate different embeddings for the same object, optimized for the task at hand. \\nEvaluating Embedding Quality\\nEmbedding models are evaluated differently depending on the task. Many of the common \\nmetrics for evaluating quality focus on the ability to retrieve similar items, while excluding \\nitems that are not similar. This type of evaluation requires a labeled datasets for which the \\nrelevant, or correct, documents are already known as seen in Snippet 0 where the NFCorpus \\ndataset is used to illustrate different metrics.  For the search use case described above two \\nimportant metrics for evaluating quality are: 1) precision - all documents retrieved should \\nbe relevant and 2) recall - all of the relevant documents should be retrieved. Intuitively, the \\noptimal embedding model would retrieve all of the relevant documents and no documents \\nthat weren’t relevant, however it is often the case that some relevant documents are \\nexcluded and some irrelevant ones get retrieved so more quantitative definitions are required \\nfor evaluating quality over large sets of documents and embedding models. Precision is \\nquantified by dividing the number of relevant documents by the total number of retrieved \\ndocuments. It is often quoted for a particular number of retrieved documents. For example \\nif ten documents were retrieved for an embedding and seven of them were relevant and \\nother three were not,  the precsion@10 would be 7/10 = 0.7. Recall looks at how many of \\nthe relevant documents were retrieved and is calculated by dividing the number of relevant \\ndocuments retrieved by the total number of relevant documents in the corpus. Recall is also \\noften quoted for a particular number of documents retrieved. For example, if 20 documents \\nwere retrieved and three of them were relevant, but there were six total relevant documents \\nin the corpus the recall@20 would be 3/6 = 0.5.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 9, 'page_label': '10'}, page_content='Embeddings & Vector Stores\\n10\\nFebrurary 2025\\nPrecision and recall are very useful when relevancy scores are binary, but don’t capture \\nthe case when some documents are more relevant than others. For example, when using a \\nsearch engine it is highly desirable that the most relevant result is at the top of the results \\nlist as end users are sensitive to the ordering of those results, even if they are all relevant. \\nWhen the detailed ordering of document relevancy is known for a data set, metrics like the \\nNormalized Discounted Cumulative Gain (nDCG) can measure the quality of the ranking \\nproduced by the embedding model compared to the desired ranking. The formula at\\nposition p for DCG =                           where reli is a relevancy score. The denominator \\npenalizes documents for being lower on the list, and DCG maximizes the score when most \\nrelevant documents are at the top of the list. The normalized version is calculated by dividing \\nthe DCG score by the ideal ordering score and ranges from 0.0 to 1.0 for comparisons across \\ndifferent queries. \\nPublic benchmarks like BEIR42 are widely used for evaluating performance on retrieval tasks \\nand additional tasks are covered by benchmarks like the Massive Text Embedding Benchmark \\n(MTEB)43. Practitioners are encouraged to use a standard library like those originated by \\nText Retrieval Conference (TREC) for consistent benchmarking with other methods, such as \\ntrec_eval44 or python wrappers like pytrec_eval45 when calculating precision, recall, nDCG \\nand others. The optimal way to evaluate embedding models for a particular application \\nmay be application specific, but the intuition that more similar objects should be closer \\nin the embeddings space is often a good start. Additional metrics such as model size, \\nembedding dimension size, latency, and overall cost are also important considerations for \\nproduction applications.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 10, 'page_label': '11'}, page_content='Embeddings & Vector Stores\\n11\\nFebrurary 2025\\nSearch Example\\nBefore diving into details about the different types of embeddings and the history of \\nembedding model development let’s explore the search example previously described above \\nin more detail. The goal is to find relevant documents in a large corpus given a query from \\nthe user. One approach then is to construct a joint embedding model where the question \\nand answer are mapped to similar locations in the embedding space. As the question and \\nanswer are semantically different, even if complementary, it is often helpful to use two neural \\nnets that have been trained together with one for the question and one for the documents. A \\nvisual representation of this can be seen in Figure 9(b) as an asymmetric dual encoder with \\na separate network for the query and document in contrast to 9(a) displaying a single neural \\nnetwork used for both query and document, also called a siamese network.\\nFigure 2 is a diagram of a search question and answer application using a retrieval \\naugmented generation (RAG) approach where embeddings are used to identify the relevant \\ndocuments before inserting them into the prompt of an LLM for summarization for the \\nend user. The application is split into two main processes. First, the index creation where \\ndocuments are divided into chunks which are used to generate embeddings and stored in a \\nvector database for low latency searches. Specifically, the document embedding portion of \\nthe model of the dual encoder neural network is used for these chunks. The second phase \\nwhen the user asks a question to the system that is embedded using the query portion of the \\nmodel and which will map to relevant documents when using a similarity search in the vector \\ndatabase. This second phase is very latency sensitive as the end user is actively waiting for \\na response so the ability to identify relevant documents from a large corpus in milliseconds \\nusing a vector database of documents is a critical piece of infrastructure.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 11, 'page_label': '12'}, page_content='Embeddings & Vector Stores\\n12\\nFebrurary 2025\\nFigure 2. Example flow for RAG Search Application highlighting embeddings. Document embeddings are \\ngenerated in the background and stored in a vector database. When the user enters a query, an embedding \\nis generated using the query embedding portion of the dual encoder and used to look up relevant \\ndocuments. Those documents can be inserted into the prompt for LLM to generate a relevant summary \\nresponse for the user.\\nThe quality of embedding models has been improving rapidly since the introduction of \\nBERT and shows no signs of slowing any time soon. While LLMs have captured a lot of the \\nattention in the AI space recently, the improvements in information retrieval and embedding \\nmodels has also been transformative. The original BERT models were a leap forward at the \\ntime, and had an average score of 10.6 on the BEIR benchmark, current 2025 embeddings'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 12, 'page_label': '13'}, page_content='Embeddings & Vector Stores\\n13\\nFebrurary 2025\\nfrom Google with a simple API call, and no AI knowledge required, now have an average BEIR \\nscore of 55.7. Models continue to improve rapidly, so when putting embedding models into \\nproduction be sure to design with model upgrades in mind. Good evaluation suites designed \\nfor the particular application are critical for ensuring smooth upgrades. Choosing embedding \\nmodels on platforms that have upgrade paths in place can help save developer time and \\nreduce operational overhead for teams without deep AI expertise, for example Snippet 0 \\nbelow uses a simple API call via Google Vertex.\\nSnippet 1 contains basic embedding code sample to illustrate some of the important \\nconcepts covered above for embeddings using the NFCorpus dataset46 that contains health \\nrelated questions and documents:\\n• The text documents with information relevant to the queries are embedded using \\nthe Google Vertex APIs for both high quality and operational ease. The RETRIEVAL_\\nDOCUMENT task type is used as questions and answers are often phrased differently \\nand use a single model with semantic similarity would result in a reduced performance \\ncompared to joint document and query embeddings.\\n• Embeddings are stored using the faiss47 library for efficient similarity search. \\n• For a particular query the text embeddings is generated using the RETRIEVAL_QUERY \\ntask type.\\n• The query embedding is used by the faiss library to look up the ids for documents whose \\nembeddings are close using the default eucldidean distance metric.\\n• Embeddings for all of the queries are generated and most similar documents retrieved. \\nRetrieval quality is evaluated against the “gold” values using the pytrec library to measure \\nprecision@1, recall@10, ndcg@10 metrics.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 13, 'page_label': '14'}, page_content='Embeddings & Vector Stores\\n14\\nFebrurary 2025\\nfrom beir import util\\nfrom beir.datasets.data_loader import GenericDataLoader\\nimport faiss\\nimport vertexai\\nfrom vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\\nimport numpy as np\\nimport pandas as pd\\nimport pytrec_eval\\ndef embed_text(texts, model, task, batch_size=5) :\\n embed_mat = np.zeros((len(texts),768))\\n for batch_start in range(0,len(texts),batch_size):\\n   size = min(len(texts) - batch_start, batch_size)\\n   inputs = [TextEmbeddingInput(texts[batch_start+i], task_type=task) for i in range(size)]\\n   embeddings = model.get_embeddings(inputs)\\n   for i in range(size) :\\n     embed_mat[batch_start + i, :] = embeddings[i].values\\n return embed_mat\\n# Download smallish NFCorpus dataset of questions and document text\\nurl = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nfcorpus.zip\"\\ndata_path = util.download_and_unzip(url, \"datasets\") \\n# Corpus of text chunks, text queries and “gold” set of query to relevant documents dict\\ncorpus, queries, qrels = GenericDataLoader(\"datasets/nfcorpus\").load(split=\"test\")\\n# Note need to setup Google Cloud project and fill in id & location below\\nvertexai.init(project=\"PROJECT_ID\", location=\"LOCATION\")\\nmodel = TextEmbeddingModel.from_pretrained(\"text-embedding-005\")\\ndoc_ids,docs = zip(*[(doc_id, doc[\\'text\\']) for doc_id,doc in corpus.items()])\\nq_ids,questions = zip(*[(q_id, q) for q_id,q in queries.items()])\\nContinues next page...'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 14, 'page_label': '15'}, page_content='Embeddings & Vector Stores\\n15\\nFebrurary 2025\\nSnippet 1. Example semantic search using text embeddings and evaluation for quality of \\nretrieved documents.\\nBoth training and evaluating neural networks requires datasets that contains pairs of \\nquestions and relevant documents such as the NFCorpus used in Snippet 0. The dataset that \\nis best suited to train or evaluate for a particular application will depend on the nature of \\n# Embed the documents and queries jointly using different models\\ndoc_embeddings = embed_text(docs, model, \"RETRIEVAL_DOCUMENT\")\\nindex = faiss.IndexFlatL2(doc_embeddings.shape[1])\\nindex.add(doc_embeddings)\\n# Example look up example query to find relevant doc - note using \\'RETRIEVAL_QUERY\\'\\nexample_embed = embed_text([\\'Is Caffeinated Tea Really Dehydrating?\\'], \\nmodel, \\'RETRIEVAL_QUERY\\')\\ns,q = index.search(example_embed,1)\\nprint(f\\'Score: {s[0][0]:.2f}, Text: \"{docs[q[0][0]]}\"\\')\\n# Score: 0.49, Text: \"There is a belief that caffeinated drinks, such as tea, \\n# may adversely affect hydration. This was investigated in a randomised \\n# controlled trial ... revealed no significant differences\\n# between tea and water for any of the mean blood or urine measurements…”\\n# Embed all queries to evaluate quality compared to \"gold\" answers\\nquery_embeddings = embed_text(questions, model, \"RETRIEVAL_QUERY\")\\nq_scores, q_doc_ids = index.search(query_embeddings, 10)\\n# Create a dict of query to document scores dict for pytrec evaluation  \\n# Multiply scores by -1 for sorting as smaller distance is better score for pytrec eval\\nsearch_qrels = { q_ids[i] : { doc_ids[_id] : -1*s.item() for _id, s in zip(q_doc_ids[i], q_\\nscores[i])} for i in range(len(q_ids))}\\nevaluator = pytrec_eval.RelevanceEvaluator(qrels, {\\'ndcg_cut.10\\',\\'P_1\\',\\'recall_10\\'})\\neval_results = evaluator.evaluate(search_qrels)\\ndf = pd.DataFrame.from_dict(eval_results, orient=\\'index\\')\\ndf.mean()\\n#P_1            0.517028 // precision@1\\n#recall_10      0.203507 // recall@10\\n#ndcg_cut_10    0.402624 // nDCG@10'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 15, 'page_label': '16'}, page_content='Embeddings & Vector Stores\\n16\\nFebrurary 2025\\nthat application. For example, a medical application will use different jargon and conventions \\nthan an application focusing on legal use cases. These labeled datasets can be expensive \\nand time consuming to generate using human experts. The Gecko embedding model paper \\nfrom Google DeepMind48 discusses in detail how an LLM was used to generate a large set \\nof synthetic question and document pairs for training, leading to an improved model and \\nperformance on many benchmarks. Using LLMs to assist experts in generating training data \\nand also for the evaluation of answers can be an effective way to scale training, tuning, and \\nevaluation datasets cost effectively.\\nTypes of embeddings\\nEmbeddings aim to obtain a low dimensional representation of the original data while \\npreserving most of the ‘essential information’. The types of data an embedding represents \\ncan be of various different forms.  Below you’ll see some standard techniques used for \\ndifferent types of data, including text and image.\\nText embeddings\\nText embeddings are used extensively as part of natural language processing (NLP). They \\nare often used to embed the meaning of natural language in machine learning for processing \\nin various downstream applications such as text generation, classification, sentiment \\nanalysis, and more. These embeddings broadly fall into two categories: token/word and \\ndocument embeddings.\\nBefore diving deeper into these categories, it’s important to understand the entire lifecycle \\nof text: from its input by the user to its conversion to embeddings.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 16, 'page_label': '17'}, page_content='Embeddings & Vector Stores\\n17\\nFebrurary 2025\\nFigure 3. The process of turning text into embeddings\\nIt all starts with the input string which is split into smaller meaningful pieces called tokens.  \\nThis process is called tokenization. Commonly, these tokens are wordpieces, characters, \\nwords, numbers, and punctuations using one of the many existing tokenization techniques.1 \\nAfter the string is tokenized, each of these tokens is then assigned a unique integer value \\nusually in the range: [0, cardinality of the total number of tokens in the corpus]. For example, \\nfor a 16 word vocabulary the IDs would range between 0-15. This value is also referred to as \\ntoken ID. These tokens can be used to represent each string as a sparse numerical vector \\nrepresentation of documents used for downstream tasks directly, or after one-hot encoding. \\nOne-hot encoding is a binary representation of categorical values where the presence \\nof a word is represented by 1, and its absence by 0. This ensures that the token IDs are \\ntreated as categorical values as they are, but often results in a dense vector the size of the \\nvocabulary of the corpus. Snippet 2 and Figure 4 show an example of how this can be done \\nusing Tensorflow.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 17, 'page_label': '18'}, page_content='Embeddings & Vector Stores\\n18\\nFebrurary 2025\\n# Tokenize the input string data\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\ndata = [\\n  \"The earth is spherical.\",\\n  \"The earth is a planet.\",\\n  \"I like to eat at a restaurant.\"]\\n# Filter the punctiations, tokenize the words and index them to integers  \\ntokenizer = Tokenizer(num_words=15, filters=\\'!\"#$%&()*+,-./:;<=>?[\\\\\\\\]^_\\'{|}~\\\\t\\\\n\\', lower=True, \\nsplit=\\' \\')\\ntokenizer.fit_on_texts(data)\\n# Translate each sentence into its word-level IDs, and then one-hot encode those IDs \\nID_sequences = tokenizer.texts_to_sequences(data)\\nbinary_sequences = tokenizer.sequences_to_matrix(ID_sequences)\\nprint(\"ID dictionary:\\\\n\", tokenizer.word_index)\\nprint(\"\\\\nID sequences:\\\\n\", ID_sequences)\\nprint(\"\\\\n One-hot encoded sequences:\\\\n\", binary_sequences)\\nSnippet 2. Tokenizing, indexing and one-hot encoding strings\\nFigure 4. Output of Snippet 2\\nHowever, since these Integer IDs (or their corresponding one-hot encoded vectors) are \\nassigned randomly to words, they lack any inherent semantic meaning. This is where \\nembeddings are much more useful. Although it’s possible to embed character and sub-word \\nlevel tokens as well, let us look at word and document embeddings to understand some of \\nthe methods behind them.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 18, 'page_label': '19'}, page_content='Embeddings & Vector Stores\\n19\\nFebrurary 2025\\nWord embeddings\\nIn this section, you’ll see a few word embedding techniques and algorithms to both train \\nand use word embeddings which were precusors to the modern text embedding currently \\nbeing used. While there are many ML driven algorithms developed over time optimized for \\ndifferent objectives, the most common ones were GloVe,2 SWIVEL,3 and Word2Vec.4 Word \\nembeddings or sub-word embeddings can also be directly obtained from hidden layers of \\nlanguage models. However, the embeddings will be different for the same word in different \\ncontexts of the text. This section focuses on lightweight, context-free word embedding and \\nleaves the context-aware document embeddings for the document embeddings section. \\nWord embedding can be directly applied to downstream tasks like named entity extraction \\nand topic modeling.\\nWord2Vec is a family of model architectures that operates on the principle of “the semantic \\nmeaning of a word is defined by its neighbors”, or words that frequently appear close to each \\nother in the training corpus. This method can be both used to train your own embeddings \\nfrom large datasets or be quickly integrated through one of the readily available pre-trained \\nembeddings available online.5 The embeddings for each word - which are essentially fixed \\nlength vectors - are randomly initialized to kick off the process, resulting in a matrix of shape \\n(size_of_vocabulary, size_of_each_embedding). This matrix can be used as a lookup table \\nafter the training process is completed using one of the following methods (see Figure 4). \\n• The Continuous bag of words (CBOW) approach: Tries to predict the middle word, using \\nthe embeddings of the surrounding words as input. This method is agnostic to the order \\nof the surrounding words in the context. This approach is fast to train and is slightly more \\naccurate for frequent words.\\n• The skip-gram approach: The setup is inverse of that of CBOW, with the middle word \\nbeing used to predict the surrounding words within a certain range. This approach is \\nslower to train but works well with small data and is more accurate for rare words.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 19, 'page_label': '20'}, page_content='Embeddings & Vector Stores\\n20\\nFebrurary 2025\\nFigure 5. Diagram explaining how CBOW and Skip-Gram methods work\\nThe Word2Vec algorithms can also be extended to the sub-word level, which has been the \\ninspiration for algorithms such as FastText.6 However, one of the major caveats of Word2Vec \\nis that although it accounts well for local statistics of words within a certain sliding window, it \\ndoes not capture the global statistics (words in the whole corpus). This shortcoming is what \\nmethods like the GloVe algorithm address.\\nGloVe is a word embedding technique that leverages both global and local statistics of words. \\nIt does this by first creating a co-occurrence matrix, which represents the relationships \\nbetween words. GloVe then uses a factorization technique to learn word representations \\nfrom the co-occurrence matrix. The resulting word representations are able to capture both \\nglobal and local information about words, and they are useful for a variety of NLP tasks.\\nIn addition to GloVE, SWIVEL is another approach which leverages the co-occurrence \\nmatrix to learn word embeddings. SWIVEL stands for Skip-Window Vectors with Negative \\nSampling. Unlike GloVE, it uses local windows to learn the word vectors by taking into'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 20, 'page_label': '21'}, page_content='Embeddings & Vector Stores\\n21\\nFebrurary 2025\\naccount the co-occurrence of words within a fixed window of its neighboring words. \\nFurthermore, SWIVEL also considers unobserved co-occurrences and handles it using a \\nspecial piecewise loss, boosting its performance with rare words. It is generally considered \\nonly slightly less accurate than GloVe on average, but is considerably faster to train. This is \\nbecause it leverages distributed training by subdividing the Embedding vectors into smaller \\nsub-matrices and executing matrix factorization in parallel on multiple machines. Snippet 2 \\nbelow demonstrates loading pre-trained word embeddings for both Word2Vec and GloVe and \\nvisualizing them in a 2D space, and computing nearest neighbors.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 21, 'page_label': '22'}, page_content='Embeddings & Vector Stores\\n22\\nFebrurary 2025\\nfrom gensim.models import Word2Vec \\nimport gensim.downloader as api\\nimport pprint\\nimport matplotlib.pyplot as plt\\nfrom sklearn.manifold import TSNE\\nimport numpy as np\\ndef tsne_plot(models, words, seed=23):\\n  \"Creates a TSNE models & plots for multiple word models for the given words\"\\n  plt.figure(figsize=(len(models)*30, len(models)*30))\\n  model_ix = 0\\n  for model in models:\\n    labels = []\\n    tokens = []\\n    for word in words:\\n      tokens.append(model[word])\\n      labels.append(word)\\n    tsne_model = TSNE(perplexity=40, n_components=2, init=\\'pca\\', n_iter=2500, random_state=seed) \\n    new_values = tsne_model.fit_transform(np.array(tokens))\\n    x = []\\n    y = []\\n    for value in new_values:\\n      x.append(value[0])\\n      y.append(value[1])\\n    model_ix +=1\\n    plt.subplot(10, 10, model_ix)\\n    for i in range(len(x)):\\n      plt.scatter(x[i],y[i])\\n      plt.annotate(labels[i],\\n            xy=(x[i], y[i]),\\n            xytext=(5, 2),\\n            textcoords=\\'offset points\\',\\n            ha=\\'right\\',\\n             va=\\'bottom\\')\\n  plt.tight_layout()\\n  plt.show()\\nv2w_model = api.load(\\'word2vec-google-news-300\\')\\nglove_model = api.load(\\'glove-twitter-25\\')\\nprint(\"words most similar to \\'computer\\' with word2vec and glove respectively:\")\\npprint.pprint(v2w_model.most_similar(\"computer\")[:3])\\npprint.pprint(glove_model.most_similar(\"computer\")[:3]) \\npprint.pprint(\"2d projection of some common words of both models\")\\nsample_common_words = list(set(v2w_model.index_to_key[100:10000])\\n                        & set(glove_model.index_to_key[100:10000]))[:100]\\ntsne_plot([v2w_model, glove_model], sample_common_words)\\nSnippet 3. Loading and plotting GloVe and Word2Vec embeddings in 2D'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 22, 'page_label': '23'}, page_content='Embeddings & Vector Stores\\n23\\nFebrurary 2025\\nFigure 6 Shows semantically similar words are clustered differently for the two algorithms.\\nFigure 6. 2D visualization of pre-trained GloVe and Word2Vec word embeddings\\nDocument embeddings\\nEmbedding documents to low dimensional dense embedding has attracted long-lasting \\ninterests since the 1980s. Document embeddings can be used in various applications, \\nincluding semantic search, topic discovery, classification, and clustering to embed \\nthe meaning of a series of words in paragraphs and documents and use it for various \\ndownstream applications. The evolution of the embeddings models can mainly be \\ncategorized into two stages: shallow Bag-of-words (BoW) models and deeper pretrained \\nlarge language models.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 23, 'page_label': '24'}, page_content='Embeddings & Vector Stores\\n24\\nFebrurary 2025\\nShallow BoW models\\nEarly document embedding works follow the bag-of-words (BoW) paradigm, assuming a \\ndocument is an unordered collection of words. These early works include latent semantic \\nanalysis (LSA)7 and latent dirichlet allocation (LDA).8 Latent semantic analysis (LSA) uses \\na co-occurrence matrix of words in documents and latent dirichlet allocation (LDA) uses a \\nbayesian network to model the document embeddings. Another well known bag-of-words \\nfamily of document embeddings is TF-IDF (term frequency-inverse document frequency) \\nbased models, which are statistical models that use the word frequency to represent the \\ndocument embedding. TF-IDF-based models can either be a sparse embedding, which \\nrepresents the term-level importance, or can be combined with word embeddings as a \\nweighting factor to generate a dense embedding for the documents. For example, BM2549, a \\nTF-IDF-based bag-of-words model, is still a strong baseline in today’s retrieval benchmarks.9\\nHowever,  the bag-of-words paradigm also has two major weaknesses: both the word \\nordering and the semantic meanings are ignored. BoW models fail to capture the sequential \\nrelationships between words, which are crucial for understanding meaning and context. \\nInspired by Word2Vec, Doc2Vec10 was proposed in 2014 for generating document \\nembeddings using (shallow) neural networks. The Doc2Vec model adds an additional \\n‘paragraph’ embedding or, in other words, document embedding in the model of Word2Vec \\nas illustrated in Figure 6. The paragraph embedding is concatenated or averaged with other \\nword embeddings to predict a random word in the paragraph. After training, for existing \\nparagraphs or documents, the learned embeddings can be directly used in downstream \\ntasks. For a new paragraph or document, extra inference steps need to be performed to \\ngenerate the paragraph or document embedding.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 24, 'page_label': '25'}, page_content='Embeddings & Vector Stores\\n25\\nFebrurary 2025\\nFigure 7. Doc2vec CBOW model\\nSnippet 4 below shows how you can train your own doc2Vec models on a custom corpus:\\nfrom gensim.test.utils import common_texts\\nfrom gensim.models.Doc2Vec import Doc2Vec, TaggedDocument\\nfrom gensim.test.utils import get_tmpfile\\n#train model on a sequence of documents tagged with their IDs\\ndocuments = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\\nmodel = Doc2Vec(documents, vector_size=8, window=3, min_count=1, workers=6)\\n# persist model to disk, and load it to infer on new documents\\nmodel_file = get_tmpfile(\"Doc2Vec_v1\")\\nmodel.save(model_file)\\nmodel = Doc2Vec.load(model_file)  \\nmodel.infer_vector([\"human\", \"interface\"])\\nSnippet 4. Self-supervised Training and inference using Doc2Vec on private corpus'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 25, 'page_label': '26'}, page_content='Embeddings & Vector Stores\\n26\\nFebrurary 2025\\nDeeper pretrained large language models\\nMotivated by the development of deep neural networks, different embedding models and \\ntechniques were proposed, and the state-of-the-art models are progressing rapidly. Main \\nchanges of the models include: \\n1. Using more complex learning models, especially bi-directional deep neural network \\nmodels. \\n2. The use of massive pre-training on unlabeled text. \\n3. The use of a subword tokenizer. \\n4. Using fine-tuning for various downstream NLP tasks. \\nIn 2018, BERT11 - which stands for bidirectional encoder representations from transformers - \\nwas proposed with groundbreaking results on 11 NLP tasks. Transformer, the model paradigm \\nBERT based on, has become the mainstream model paradigm until today. Besides using a \\ntransformer as the model backbone, another key of BERT’s success is from pre-training with \\na massive unlabeled corpus. In pretraining, BERT utilized masked language model (MLM) as \\nthe pre-training objective. It did this by randomly masking some tokens of the input and using \\nthe masked token id as the prediction objective. This allows the model to utilize both the \\nright and left context to pretrain a deep bidirectional transformer. BERT also utilizes the next \\nsentence prediction task in pretraining. BERT outputs a contextualized embedding for every \\ntoken in the input. Typically, the embedding of the first token (a special token named [CLS]) is \\nused as the embedding for the whole input.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 26, 'page_label': '27'}, page_content='Embeddings & Vector Stores\\n27\\nFebrurary 2025\\nFigure 8. The BERT architecture\\nBERT became the base model for multiple embedding models, including Sentence-BERT,12 \\nSimCSE,13 and E5.14 Meanwhile, the evolution of language models - especially large language \\nmodels - never stops. T550 was proposed in 2019 with up to 11B parameters. PaLM51 was \\nproposed in 2022 to push the large language model to a surprising 540B parameters. Models \\nlike Gemini52 from Google, GPT53 models from OpenAI and Llama54 models from Meta are \\nalso evolving to newer generations at astonishing speed. Please refer to the whitepaper on \\nFoundational models for more information about some common LLMs.\\nNew embedding models based on large language models have been proposed. For example, \\nGTR and Sentence-T5 show better performance on retrieval and sentence similarity \\n(respectively) than BERT family models. Recently, a new embedding model powered by the \\nGemini model backbone has been released on Vertex AI, achieving superior results on all \\npublic benchmarks. Matryoshka Embeddings55,56 allow the downstream user to select how \\nmany dimensions are appropriate for their task to reduce data required for storage and \\nindexing when possible.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 27, 'page_label': '28'}, page_content='Embeddings & Vector Stores\\n28\\nFebrurary 2025\\nAnother approach to new embeddings models development is generating multi-vector \\nembeddings instead of a single vector to enhance the representational power of the models. \\nEmbedding models in this family include ColBERT15 and XTR.16 ColPali57 is also an approach \\nusing mult-vectors, but extending their application from text only to join embedding text and \\nimages for multi-modal documents. \\nFigure 9. An illustration of the taxonomy diagram of the embedding models\\nAlthough the deep neural network models require a lot more data and compute time to train, \\nthey have much better performance compared to models using bag-of-words paradigms. \\nFor example, for the same word the embeddings would be different with different contexts, \\nbut by definition that is not true for bag-of-words. Snippet 4 demonstrates how pre-trained \\ndocument embedding models from Tensorflow-hub17 (for example,Sentence t5)A and Vertex \\nAIB can be used for training models with Keras and TF datasets. Vertex Generative AI text \\nembeddings can be used with the Vertex AI SDK, Langchain, and Google’s BigQuery (Snippet \\n5) for embedding and advanced workflows.18\\nA. Note: not all models on https://tfhub.dev/  can be commercially used. Please check the licenses of the models \\nand the training datasets and consult the legal team before commercial usage. \\nB. Note: not all models on https://tfhub.dev/  can be commercially used. Please check the licenses of the models \\nand the training datasets and consult the legal team before commercial usage.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 28, 'page_label': '29'}, page_content='Embeddings & Vector Stores\\n29\\nFebrurary 2025\\nimport vertexai\\nfrom vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\\n# Set the model name. For multilingual: use \"text-multilingual-embedding-002\"\\nMODEL_NAME = \"text-embedding-004\"\\n# Set the task_type, text and optional title as the model inputs.\\n# Available task_types are \"RETRIEVAL_QUERY\", \"RETRIEVAL_DOCUMENT\", \\n# \"SEMANTIC_SIMILARITY\", # \"CLASSIFICATION\", and \"CLUSTERING\"\\nTASK_TYPE = \"RETRIEVAL_DOCUMENT\" \\nTITLE = \"Google\"\\nTEXT = \"Embed text.\"\\n# Use Vertex LLM text embeddings\\nembeddings_vx = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@004\")\\ndef LLM_embed(text):\\n    def embed_text(text):\\n        text_inp = TextEmbeddingInput(task_type=\"CLASSIFICATION\",   text=text.numpy())\\n        return np.array(embeddings_vx.get_embeddings([text_inp])[0].values)\\n output = tf.py_function(func=embed_text, inp=[text], Tout=tf.float32)\\n output.set_shape((768,))\\n return output\\n# Embed strings using vertex LLMs\\nLLM_embeddings=train_data.map(lambda x,y: ((LLM_embed(x), y))\\n# Embed strings in the tf.dataset using one of the tf hub models\\nembedding = \"https://tfhub.dev/google/sentence-t5/st5-base/1\"\\nhub_layer = hub.KerasLayer(embedding, input_shape=[],dtype=tf.string, trainable=True)\\n                          \\n# Train model \\nmodel = tf.keras.Sequential()\\nmodel.add(hub_layer) # omit this layer if using Vertex LLM embeddings\\nmodel.add(tf.keras.layers.Dense(16, activation=\\'relu\\'))\\nmodel.add(tf.keras.layers.Dense(1))\\nmodel.compile(optimizer=\\'adam\\',loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\\n  metrics=[\\'accuracy\\'])\\nhistory = model.fit(train_data.shuffle(100).batch(8))\\nSnippet 4. Creating & integrating text embeddings (Vertex, Tfhub) into keras text classification models'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 29, 'page_label': '30'}, page_content='Embeddings & Vector Stores\\n30\\nFebrurary 2025\\nSELECT * FROM ML.GENERATE_TEXT_EMBEDDING(\\nMODEL my_project.my_company.llm_embedding_model,\\n(\\nSELECT review as content\\nFROM bigquery-public-data.imdb.reviews));\\nSnippet 5. Creating LLM based text embeddings in BigQuery for selected columns in a table\\nImage & multimodal embeddings\\nMuch like text, it’s also possible to create both image and multimodal embeddings. \\nUnimodal image embeddings can be derived in many ways such as by training a CNN or \\nVision Transformer model on a large scale image classification task (for example, Imagenet), \\nand then using the penultimate layer as the image embedding. This layer has learnt some \\nimportant discriminative feature maps for the training task. It contains a set of feature maps \\nthat are discriminative for the task at hand and can be extended to other tasks as well. \\nTo obtain multimodal embeddings19 you take the individual unimodal text and image \\nembeddings and create the joint embedding of their semantic relationships learnt via another \\ntraining process. This gives you a fixed size semantic representation in the same latent \\nspace. Snippet 6 computes image and multimodal embeddings for images and text and can \\nbe used with a keras model directly (much like the text embedding example). Multimodal \\nembedding approaches like ColPali57 use image models to enable retrieval from text queries \\non multimodal documents without complex OCR or layout preprocesing. The model searches \\nthe images as they would be displayed to a user in a web browser or pdf viewer rahter than \\nhaving to convert to a text only form for indexing.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 30, 'page_label': '31'}, page_content='Embeddings & Vector Stores\\n31\\nFebrurary 2025\\nimport base64\\nimport tensorflow as tf\\nfrom google.cloud import aiplatform\\nfrom google.protobuf import struct_pb2\\n#fine-tunable layer for image embeddings which can be used for downstream keras modelimage_\\nembed=hub.KerasLayer(\"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/feature_\\nvector/2\",trainable=False) \\nclass EmbeddingPredictionClient:\\n  \"\"\"Wrapper around Prediction Service Client.\"\"\"\\n  def __init__(self, project : str,\\n    location : str = \"us-central1\",\\n    api_regional_endpoint: str = \"us-central1-aiplatform.googleapis.com\"):\\n    client_options = {\"api_endpoint\": api_regional_endpoint}\\n    self.client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)  \\n    self.location = location\\n    self.project = project\\n  def get_embedding(self, text : str = None, gs_image_path : str = None):\\n   #load the image from a bucket in google cloud storage\\n   with tf.io.gfile.GFile(gs_image_path, \"rb\") as f:\\n     image_bytes = f.read()\\n   if not text and not image_bytes:\\n    raise ValueError(\\'At least one of text or image_bytes must be specified.\\')\\n   #Initialize a protobuf data struct with the text and image inputs \\n   instance = struct_pb2.Struct()\\n    if text:\\n      instance.fields[\\'text\\'].string_value = text\\n      if image_bytes:\\n      encoded_content = base64.b64encode(image_bytes).decode(\"utf-8\")\\n      image_struct = instance.fields[\\'image\\'].struct_value\\n      image_struct.fields[\\'bytesBase64Encoded\\'].string_value = .string_value = encoded_content\\n     #Make predictions using the multimodal embedding model\\n     instances = [instance]\\n     endpoint = (f\"projects/{self.project}/locations/{self.location}\"\\n         \"/publishers/google/models/multimodalembedding@001\")\\n     response = self.client.predict(endpoint=endpoint, instances=instances)\\n     text_embedding = None\\n     if text:    \\n      text_emb_value = response.predictions[0][\\'textEmbedding\\']\\n      text_embedding = [v for v in text_emb_value]\\n     image_embedding  = None\\n     if image_bytes:    \\n      image_emb_value = response.predictions[0][\\'textEmbedding\\']\\n      image_embedding  = [v for v in image_emb_value]\\nContinues next page...'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 31, 'page_label': '32'}, page_content='Embeddings & Vector Stores\\n32\\nFebrurary 2025\\n \\n return EmbeddingResponse (text_embedding=text_embedding, image_embedding =image_embedding)\\n#compute multimodal embeddings for text and images\\nclient.get_embedding(text=\"sample_test\", gs_image_path=\"gs://bucket_name../image_filename..\")\\nSnippet 6. Using Vertex API to create Multimodal embeddings Graph embeddings\\nStructured data embeddings\\nStructured data refers to data has a defined schema, like an table in a database \\nwhere individual fields have known types and definitions. Unlike unstructured text and \\nimage data, where a pre-trained embedding model is typically available, we have to \\ncreate the embedding model for the structured data since it would be specific to a \\nparticular application.\\nGeneral structured data\\nGiven a general structured data table, we can create embeddings for each row. This can be \\ndone by the ML models in the dimensionality reduction category, such as the PCA model.\\nOne use case for these embeddings are for anomaly detection. For example, we can create \\nembeddings for anomaly detection using large data sets of labeled sensor information \\nthat identify anomalous occurrences.20 Another case use is to feed these embeddings \\nto downstream ML tasks such as classification. Compared to using the original high-\\ndimensional data, using embeddings to train a supervised model requires less data. This is \\nparticularly important in cases where training data is not sufficient.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 32, 'page_label': '33'}, page_content='Embeddings & Vector Stores\\n33\\nFebrurary 2025\\nUser/item structured data\\nThe input is no longer a general structured data table as above. Instead, the input includes \\nthe user data, item/product data plus the data describing the interaction between user and \\nitem/product, such as rating score. \\nThis category is for recommendation purposes, as it maps two sets of data (user dataset, \\nitem/product/etc dataset) into the same embedding space. For recommender systems, we \\ncan create embeddings out of structured data that correlate to different entities such as \\nproducts, articles, etc. Again, we have to create our own embedding model. Sometimes this \\ncan be combined with unstructured embedding methods when images or text descriptions \\nare found.\\nGraph embeddings\\nGraph embeddings are another embedding technique that lets you represent not \\nonly information about a specific object but also its neighbors (namely, their graph \\nrepresentation). Take an example of a social network where each person is a node, and the \\nconnections between people are defined as edges. Using graph embedding you can model \\neach node as an embedding, such that the embedding captures not only the semantic \\ninformation about the person itself, but also its relations and associations hence enriching \\nthe embedding. For example, if two nodes are connected by an edge, the vectors for those \\nnodes would be similar. You might then be able to predict who the person is most similar \\nto and recommend new connections. Graph embeddings can also be used for a variety of \\ntasks, including node classification, graph classification, link prediction, clustering, search, \\nrecommendation systems, and more. Popular algorithms21,22 for graph embedding include \\nDeepWalk, Node2vec, LINE, and GraphSAGE.23'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 33, 'page_label': '34'}, page_content='Embeddings & Vector Stores\\n34\\nFebrurary 2025\\nTraining Embeddings\\nCurrent embedding models usually use dual encoder (two tower) architecture. For example, \\nfor the text embedding model used in question-answering, one tower is used to encode \\nthe queries and the other tower is used to encode the documents. For the image and text \\nembedding model, one tower is used to encode the images and the other tower is used \\nto encode the text. The model can have various sub architectures, depending on how the \\nmodel components are shared between the two towers. The following figure shows some \\narchitectures of the dual encoders.24 \\nFigure 10. Some architectures of dual encoders\\nThe loss used in embedding models training is usually a variation of contrastive loss, which \\ntakes a tuple of <inputs, positive targets, [optional] negative targets> as the inputs. Training \\nwith contrastive loss brings positive examples closer and negative examples far apart.\\nSimilar to foundation model training, training of an embedding model from scratch usually \\nincludes two stages: pretraining (unsupervised learning) and fine tuning (supervised \\nlearning). Nowadays, the embedding models are usually directly initialized from foundation \\nmodels such as BERT, T5, GPT, Gemini, CoCa. You can use these base models to leverage the \\nmassive knowledge that has been learned from the large-scale pretraining of the foundation'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 34, 'page_label': '35'}, page_content='Embeddings & Vector Stores\\n35\\nFebrurary 2025\\nmodels. The fine-tuning of the embedding models can have one or more phases. The fine-\\ntuning datasets can be created in various methods, including human labeling, synthetic \\ndataset generation, model distillation, and hard negative mining.\\nTo use embeddings for downstream tasks like classification or named entity recognition, \\nextra layers (for example, softmax classification layer) can be added on top of the embedding \\nmodels. The embedding model can either be frozen (especially when the training dataset is \\nsmall), trained from scratch, or fine-tuned together with the downstream tasks. \\nVertex AI provides the ability to customize the Vertex AI text embedding models.25 Users can \\nalso choose to fine-tune the models directly. An example is fine tuning the BERT model using \\ntensorflow model garden26. You can also directly load the embedding models from tfhub and \\nfine-tune on top of the model. Snippet 7 shows an example how to build a classifier based on \\ntfhub models. \\n# Can switch the embedding to different embeddings from different modalities on # \\ntfhub. Here we use the BERT model as an example.\\ntfhub_link = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\\nclass Classifier(tf.keras.Model):\\n    def __init__(self, num_classes):\\n      super(Classifier, self).__init__(name=\"prediction\")\\n        self.encoder = hub.KerasLayer(tfhub_link, trainable=True)\\n      self.dropout = tf.keras.layers.Dropout(0.1)\\n      self.dense = tf.keras.layers.Dense(num_classes)\\n    def call(self, preprocessed_text):\\n      encoder_outputs = self.encoder(preprocessed_text)\\n      pooled_output = encoder_outputs[\"pooled_output\"]\\n   x = self.dropout(pooled_output)\\n   x = self.dense(x)\\n   return x\\nSnippet 7. Creating a Keras model using trainable tfhub layer'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 35, 'page_label': '36'}, page_content='Embeddings & Vector Stores\\n36\\nFebrurary 2025\\nSo far you’ve seen the various types of embeddings, techniques and best practices to train \\nthem for various data modalities, and some of their applications. The next section discusses \\nhow to persist and search the embeddings that have been created in a fast and scalable way \\nfor production workloads.\\nVector search\\nFull-text keyword search has been the lynchpin of modern IT systems for years. Full-text \\nsearch engines and databases (relational and non-relational) often rely on explicit keyword \\nmatching. For example, if you search for ‘cappuccino’ the search engine or database returns \\nall documents that mention the exact query in the tags or text description. However, if the \\nkey word is misspelled or described with a differently worded text, a traditional keyword \\nsearch returns incorrect or no results. There are traditional approaches which are tolerant of \\nmisspellings and other typographical errors. However, they are still unable to find the results \\nhaving the closest underlying semantic meanings to the query. This is where vector search \\nis very powerful: it uses the vector or embedded semantic representation of documents. As \\nvector search works on any sort of embedding it also allows search on images, videos, and \\nother data types in addition to text.\\nVector search lets you to go beyond searching for exact query literals and allows you to \\nsearch for the meaning across various data modalities. This allows you to find relevant \\nresults even when the wording is different. After you have a function that can compute \\nembeddings of various items,  you compute the embedding of the items of interest and store \\nthis embedding in a database. You then embed the incoming query in the same vector space \\nas the items. Next, you have to find the best matches to the query. This process is analogous'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 36, 'page_label': '37'}, page_content='Embeddings & Vector Stores\\n37\\nFebrurary 2025\\nto finding the most ‘similar’ matches across the entire collection of searchable vectors: \\nsimilarity between vectors can be computed using a metric such as euclidean distance, \\ncosine similarity, or dot product.\\nFigure 11. Visualization of how different metrics compute vector similarity\\nEuclidean distance (i.e., L2 distance) is a geometric measure of the distance between two \\npoints in a vector space. This works well for lower dimensions. Cosine similarity is a measure \\nof the angle between two vectors. And inner/dot product, is the projection of one vector \\nonto another. They are equivalent when the vector norms are 1. This seems to work better \\nfor higher dimensional data. Vector databases store and help manage and operationalize the \\ncomplexity of vector search at scale, while also addressing the common database needs.\\nImportant vector search algorithms\\nThe most straightforward way to find the most similar match is to run a traditional linear \\nsearch by comparing the query vector with each document vector and return the one with \\nthe highest similarity. However, the runtime of this approach scales linearly (O(N)) with the \\namount of documents or items to search. This approach is unacceptably slow for most'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 37, 'page_label': '38'}, page_content='Embeddings & Vector Stores\\n38\\nFebrurary 2025\\nuse cases involving several millions of documents or more. Using approximate nearest \\nneighbour (ANN) search for that purpose is more practical.  ANN is a technique for finding \\nthe closest points to a given point in a dataset with a small margin of error - but with far less \\ncomputations required as the search space is greatly reduced to O(logN). There are many \\napproaches with varying trade-offs across scale, indexing time, performance, simplicity and \\nmore.26 They use one or more implementations of the following techniques: quantization, \\nhashing, clustering and trees, among others. Some of the most popular approaches are \\ndiscussed below.\\nLocality sensitive hashing & trees\\nLocality sensitive hashing (LSH) 27 is a technique for finding similar items in a large dataset. \\nIt does this by creating one or more hash functions that map similar items to the same hash \\nbucket with high probability. This means that you can quickly find all of the similar items to \\na given item by only looking at the candidate items in the same hash bucket (or adjacent \\nbuckets) and do a linear search amongst those candidate pairs. This allows for significantly \\nfaster lookups within a specific radius. The number of hash functions/tables and buckets \\ndetermine the search recall/speed tradeoff, as well as the false positive / true positive one. \\nHaving too many hash functions might cause similar items to different buckets, while too few \\nmight result in too many items falsely being hashed to the same bucket and the number of \\nlinear searches to increase.\\nAnother intuitive way to think about LSH is grouping residences by their postal code or \\nneighborhood name. Then based on where someone chooses to move you look at the \\nresidences for only that neighborhood and find the closest match.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 38, 'page_label': '39'}, page_content='Embeddings & Vector Stores\\n39\\nFebrurary 2025\\nFigure 12. Visualization of how LSH uses random hyperplanes to partition the vector space\\nTree-based algorithms work similarly. For example, the Kd-tree approach works by creating \\nthe decision boundaries by computing the median of the values of the first dimension, then \\nthat of the second dimension and so on. This approach is very much like a decision tree. \\nNaturally this can be ineffective if searchable vectors are high dimensional. In that case, the \\nBall-tree algorithm is better suited. It is similar in functionality, except instead of going by \\ndimension-wise medians it creates buckets based on the radial distance of the data points \\nfrom the center. Here is an example of the implementation of these three approaches:'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 39, 'page_label': '40'}, page_content='Embeddings & Vector Stores\\n40\\nFebrurary 2025\\nfrom sklearn.neighbors import NearestNeighbors\\nfrom vertexai.language_models import TextEmbeddingModel\\nfrom lshashing import LSHRandom\\nimport numpy as np\\nmodel = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@004\")\\ntest_items= [\\n  \"The earth is spherical.\",\\n  \"The earth is a planet.\",\\n  \"I like to eat at a restaurant.\"]\\nquery = \"the shape of earth\"\\nembedded_test_items = np.array([embedding.values for embedding in model.get_embeddings(test_items)])\\nembedded_query = np.array(model.get_embeddings([query])[0].values)\\n#Naive brute force search\\nn_neighbors=2\\nnbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=\\'brute\\').fit(embedded_test_items)\\nnaive_distances, naive_indices = nbrs.kneighbors(np.expand_dims(embedded_query, axis = 0))\\n#algorithm- ball_tree due to high dimensional vectors or kd_tree otherwise\\nnbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=\\'ball_tree\\').fit(embedded_test_items) \\ndistances, indices = nbrs.kneighbors(np.expand_dims(embedded_query, axis = 0))\\n#LSH\\nlsh_random_parallel = LSHRandom(embedded_test_items, 4, parallel = True)\\nlsh_random_parallel.knn_search(embedded_test_items, embedded_query, n_neighbors, 3, parallel = True)\\n#output for all 3 indices = [0, 1] , distances [0.66840428, 0.71048843] for the first 2 neighbours\\n#ANN retrieved the same ranking of items as brute force in a much scalable manner\\nSnippet 8. Using scikit-learn 28  and lshashing 29  for ANN with LSH, KD/Ball-tree and linear search\\nHashing and tree-based approaches can also be combined and extended upon to obtain \\nthe optimal tradeoff between recall and latency for search algorithms. FAISS with HNSW and \\nScaNN32,33 are good examples.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 40, 'page_label': '41'}, page_content='Embeddings & Vector Stores\\n41\\nFebrurary 2025\\nHierarchical navigable small worlds \\nFigure 13. Diagram showing how HNSW ‘zooms in’ to perform ANN\\nOne of the FAISS (Facebook AI similarity search) implementations leverages the concept \\nof hierarchical navigable small world (HNSW)30 to perform vector similarity search in sub-\\nlinear (O(Logn)) runtime with a good degree of accuracy. A HNSW is a proximity graph with a \\nhierarchical structure where the graph links are spread across different layers. The top layer \\nhas the longest links and the bottom layer has the shortest ones. As shown in Figure 13, the \\nsearch starts at the topmost layer where the algorithm greedily traverses the graph to find \\nthe vertex most semantically similar to the query. Once the local minimum for that layer is \\nfound, it then switches to the graph for the closest vertex on the layer below. This process \\ncontinues iteratively until the local minimum for the lowest layer is found, with the algorithm \\nkeeping track of all the vertices traversed to return the K-nearest neighbors. This algorithm \\ncan be optionally augmented with quantization and vector indexing to boost speed and \\nmemory efficiency.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 41, 'page_label': '42'}, page_content='Embeddings & Vector Stores\\n42\\nFebrurary 2025\\n# Create an endpoint\\nmy_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\\n display_name=f\"{DISPLAY_NAME}-endpoint\", public_endpoint_enabled=True\\n)\\n# NOTE : This operation can take upto 20 minutes\\nmy_index_endpoint = my_index_endpoint.deploy_index(\\n index=my_index, deployed_index_id=DEPLOYED_INDEX_ID\\n)\\n# retrieve the id of the most recently deployed index or manually look up the index \\ndeployed above\\nindex_id=my_index_endpoint.deployed_indexes[-1].index.split(\"/\")[-1]\\nendpoint_id= my_index_endpoint.name\\n# TODO : replace 1234567890123456789 with your acutial index ID\\nmy_index = aiplatform.MatchingEngineIndex(index_id)\\n# TODO : replace 1234567890123456789 with your acutial endpoint ID\\n# Be aware that the Index ID differs from the endpoint ID\\nmy_index_endpoint = aiplatform.MatchingEngineIndexEndpoint(endpoint_id)\\n# Input texts\\ntexts= [\\n   \"The earth is spherical.\",\\n   \"The earth is a planet.\",\\n   \"I like to eat at a restaurant.\",\\n]\\n# Create a Vector Store\\nvector_store = VectorSearchVectorStore.from_components(\\n    project_id=PROJECT_ID,\\n    region=REGION,\\n    gcs_bucket_name=BUCKET,\\n    index_id=my_index.name,\\n    endpoint_id=my_index_endpoint.name,\\n    embedding=embedding_model,\\n    stream_update=True,\\n)\\n# Add vectors and mapped text chunks to your vectore store\\nvector_store.add_texts(texts=texts)\\n# Initialize the vectore_store as retriever\\nretriever = vector_store.as_retriever()\\nContinues next page...'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 42, 'page_label': '43'}, page_content='Embeddings & Vector Stores\\n43\\nFebrurary 2025\\nSnippet 9. Build/deploy ANN Index for Vertex AI Vector Search and use RAG with LLM prompts to generate \\ngrounded results/sources. \\nretriever=vector_store.as_retriever(search_kwargs={\\'k\\':1 })\\n#create custom prompt for your use case\\nprompt_template=\"\"\"You are David, an AI knowledge bot.\\nAnswer the questions using the facts provided. Use the provided pieces of context to answer\\nthe users question.\\nIf you don\\'t know the answer, just say that \"I don\\'t know\", don\\'t try to make up an answer.\\n{summaries}\"\"\"\\nmessages = [\\n    SystemMessagePromptTemplate.from_template(prompt_template),\\n    HumanMessagePromptTemplate.from_template(\"{question}\")\\n]\\nprompt = ChatPromptTemplate.from_messages(messages)\\nchain_type_kwargs = {\"question\": prompt}\\n#initialize your llm model\\nllm = VertexAI(model_name=\"gemini-pro\")\\n#build your chain for RAG+C\\nchain= RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", \\nretriever=retriever, return_source_documents=True)\\n#print your results with Markup language\\ndef print_result(result):\\n  output_text = f\"\"\"### Question:\\n  {query}\\n  ### Answer:\\n  {result[\\'result\\']}\\n  ### Source:\\n  {\\' \\'.join(list(set([doc.page_content for doc in result[\\'source_documents\\']])))}\\n  \"\"\"\\n  return(output_text)\\nchain= \"What shape is the planet where humans live?\"\\nresult = chain(query)\\ndisplay(Markdown(print_result(result)))'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 43, 'page_label': '44'}, page_content='Embeddings & Vector Stores\\n44\\nFebrurary 2025\\nimport faiss\\nM=32 #creating high degree graph:higher recall for larger index & searching time\\nd=768 # dimensions of the vectors/embeddings\\nindex = faiss.IndexHNSWFlat(d, M)\\nindex.add(embedded_test_items) #build the index using the embeddings in Snippet 9\\n#execute the ANN search\\nindex.search(np.expand_dims(embedded_query, axis=0), k=2)\\nSnippet 10. Indexing and executing ANN search with the FAISS library using HNSW\\nScaNN\\nGoogle developed the scalable approximate nearest neighbor (ScaNN)31,32 approach which \\nis used across a lot of its products and services. This includes being externally available \\nto all customers of Google Cloud through the Vertex AI Vector Search and Google Cloud \\nDatabases, including AlloyDB, Cloud Spanner, and Cloud SQL MySQL. Below is how ScaNN \\nuses a variety of steps to perform efficient vector search, with each one of them having their \\nown subset of parameters. \\nThe first step is the optional partitioning step during training: it uses one of the multiple \\nalgorithms available to partition the vector store into logical partitions/clusters where \\nthe semantically related are grouped together. The partitioning step is optional for small \\ndatasets. However, for larger datasets with >100k embedding vectors, the partitioning step \\nis crucial since by pruning the search space it cuts down the search space by magnitudes \\ntherefore significantly speeds up the query. The space pruning is configured through the \\nnumber of partitions and the number of partitions to search. A larger number leads to better \\nrecall but larger partition creation time. A good heuristic is to set the number of partitions to \\nbe the square root of the number of vectors.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 44, 'page_label': '45'}, page_content='Embeddings & Vector Stores\\n45\\nFebrurary 2025\\nFigure 14. Search space partitioning & pruning (left) & Approximate scoring (right)\\nAt query time ScaNN uses the user-specified distance measure to select the specified \\nnumber of top partitions (a value specified by the user), and then executes the scoring \\nstep next. In this step ScaNN compares the query with all the points in the top partitions \\nand selects the top K’. This distance computation can be configured as exact distance or \\napproximate distance. The approximate distance computation leverages either standard \\nproduct quantization or anisotropic quantization techniques, the latter of which is a specific \\nmethod employed by ScaNN which gives the better speed and accuracy tradeoffs.\\nFinally, as a last step the user can optionally choose to rescore the user specified top K \\nnumber of results more accurately. This results in an industry leading speed/accuracy \\ntradeoff ScaNN is known for as can be inferred from Figure 14. Snippet 10 shows a \\ncode example.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 45, 'page_label': '46'}, page_content='Embeddings & Vector Stores\\n46\\nFebrurary 2025\\nFigure 15. Accuracy/speed tradeoffs for various SOTA ANN search algorithms 58'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 46, 'page_label': '47'}, page_content='Embeddings & Vector Stores\\n47\\nFebrurary 2025\\nimport tensorflow as tf\\nimport tensorflow_recommenders as tfrs\\nfrom vertexai.language_models import TextEmbeddingModel, TextEmbeddingInput\\n# Embed documents & query(from snip 9.) and convert them to tensors and tf.datasets\\nembedded_query = tf.constant((LM_embed(query, \"RETRIEVAL_QUERY\")))\\nembedded_docs = [LM_embed(doc, \"RETIREVAL_DOCUMENT\") for doc in searchable_docs]\\nembedded_docs = tf.data.Dataset.from_tensor_slices(embedded_docs).enumerate().batch(1)\\n# Build index from tensorflow dataset and execute ANN search based on dot product metric\\nscann = tfrs.layers.factorized_top_k.ScaNN( \\n  distance_measure= \\'dot_product\\',\\n  num_leaves = 4, #increase for higher number of partitions / latency for increased recall\\n  num_leaves_to_search= 2) # increase for higher recall but increased latency\\nscann = scann.index_from_dataset(embedded_docs)\\nscann(embedded_query, k=2)\\nSnippet 11. Using Tensorflow Recommenders 33  to perform ANN search using the ScaNN algorithm\\nIn this whitepaper we have seen both current and traditional ANN search algorithms: ScaNN, \\nFAISS , LSH, KD-Tree, and Ball-tree, and examined the  great speed/accuracy tradeoffs \\nthat they provide. However, to use these algorithms they need to be deployed in a scalable, \\nsecure and production-ready manner. For that we need vector databases.\\nVector databases \\nVector embeddings embody semantic meanings of data, while vector search algorithms \\nprovide a means for efficiently querying them. Historically traditional databases lacked \\nthe means to combine semantic meaning and efficient querying. This is what gave rise to \\nvector databases, which are built ground-up to manage these embeddings for production \\nscenarios. Due to the recent popularity of Generative AI, an increasing number of traditional'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 47, 'page_label': '48'}, page_content='Embeddings & Vector Stores\\n48\\nFebrurary 2025\\ndatabases are starting to incorporate supporting vector search functionality in addition to \\ntraditional search (‘hybrid search’) functionalities. Let’s look at the workflow for a simple \\nVector Database, with hybrid search capabilities.\\nFigure 16. Populating and querying vector databases\\nEach vector database differs in its implementation, but the general flow is shown in Figure 16:\\n1. An appropriate trained embedding model is used to embed the relevant data points as \\nvectors with fixed dimensions. \\n2. The vectors are then augmented with appropriate metadata and complementary \\ninformation (such as tags) and indexed using the specified algorithm for efficient search.\\n3. An incoming query gets embedded with the appropriate model, and used to search for \\nthe most semantically similar items and their associated unembedded content/metadata. \\nSome databases might provide caching and pre-filtering (based on tags) and post-filtering \\ncapabilities (reranking using another more accurate model) to further enhance the query \\nspeed and performance.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 48, 'page_label': '49'}, page_content='Embeddings & Vector Stores\\n49\\nFebrurary 2025\\nThere are quite a few vector databases available today, each tailored to different business \\nneeds and considerations. A few good examples of commercially managed vector \\ndatabases include Google Cloud’s Vertex Vector Search,34 Google Cloud’s AlloyDB & Cloud \\nSQL Postgres ElasticSearch,35 and Pinecone36 to name a few. Vertex AI Vector Search is \\na vector database built by Google that uses the ScaNN algorithm for fast vector search, \\nwhile still maintaining all the security and access guarantees of Google Cloud. AlloyDB & \\nCloud SQL Postgres supports vector search through the OSS pgvector37 extension, which \\nallows for SQL queries to combine ANN search with traditional predicates and the usual \\ntransactional semantics for ANN search index. AlloyDB also has a ScaNN index extension \\nthat is a native implementation of ScaNN and is pgvector-compatible. Similarly, many other \\ntraditional databases have also started to add plugins to enable vector search. Pinecone37 \\nand Weaviate39 leverage HNSW for their fast vector search in addition to the ability to filter \\ndata using traditional search. Amongst their open source peers: Weaviate38 and ChromaDB39 \\nprovide a full suite of functionality upon deployment and can be tested in memory as well \\nduring the prototyping phase.\\nOperational considerations\\nVector Databases are critical to managing the majority of technical challenges that arise \\nwith storing and querying embeddings at scale. Some of these challenges are specific to the \\nnature of vector stores, while others overlap with that of traditional databases. These include \\nhorizontal and vertical scalability, availability, data consistency, real time updates, backups, \\naccess control, compliance, and much more. However, there are also many more challenges \\nand considerations you need to take into account while using embedding and vector stores.\\nFirstly, embeddings, unlike traditional content, can mutate over time. This means that the \\nsame text, image, video or other content could and should be embedded using different \\nembedding models to optimize for the performance of the downstream applications. This is'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 49, 'page_label': '50'}, page_content='Embeddings & Vector Stores\\n50\\nFebrurary 2025\\nespecially true for embeddings of supervised models after the model is retrained to account \\nfor various drifts or changing objectives. Similarly, the same applies to unsupervised models \\nwhen they are updated to a newer model. However, frequently updating the embeddings \\n- especially those trained on large amounts of data - can be prohibitively expensive. \\nConsequently, a balance needs to be struck. This necessitates a well-defined automated \\nprocess to store, manage, and possibly purge embeddings from the vector databases taking \\nthe budget into consideration.\\nSecondly, while embeddings are great at representing semantic information, sometimes they \\ncan be suboptimal at representing literal or syntactic information. This is especially true for \\ndomain-specific words or IDs. These values are potentially missing or underrepresented \\nin the data the embeddings models were trained on. For example, if a user enters a query \\nthat contains the ID of a specific number along with a lot of text, the model might find \\nsemantically similar neighbors which match the meaning of the text closely, but not the ID, \\nwhich is the most important component in this context. You can overcome this challenge by \\nusing a combination of full-text search to pre-filter or post-filter the search space before \\npassing it onto the semantic search module.\\nAnother important point to consider is that depending on the nature of the workload in which \\nthe semantic query occurs, it might be worth relying on different vector databases. For \\nexample, for OLTP workloads that require frequent reads/write operations, an operational \\ndatabase like AlloyDB, Spanner, Postgres, or CloudSQL is the best choice. For large-\\nscale OLAP analytical workloads and batch use cases, using BigQuery’s vector search \\nis preferable.\\nIn conclusion, a variety of factors need to be considered when choosing a vector database. \\nThese factors include size and type of your dataset (some are good at sparse and others \\ndense), business needs, the nature of the workload,  budget, security, privacy guarantees, \\nthe needs for semantic and syntactic search as well as the database systems that are already'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 50, 'page_label': '51'}, page_content='Embeddings & Vector Stores\\n51\\nFebrurary 2025\\nin use. In this section we have seen the various ANN search approaches as well the need and \\nbenefits of vector databases. The next section demonstrates an example of using a Vector AI \\nVector Search for semantic search.\\nApplications\\nEmbeddings models are one of the fundamental machine learning models that power a \\nvariety of applications. We summarize some popular applications in the following table. \\nTask Description\\nRetrieval\\nGiven a query and a set of objects (for example, documents, images, \\nand videos), retrieve the most relevant objects. Based on the definition \\nof relevant objects, the subtasks include question answering and \\nrecommendations.\\nSemantic text similarity\\nDetermine whether two sentences have the same semantic meaning. \\nThe subtasks include: paraphrasing, duplicate detection, and bitext \\nmining.\\nClassification\\nClassify objects into possible categories. Based on the number of labels, \\nthe subtasks include binary classification, multi-class classification, and \\nmultilabel classifications.\\nClustering Cluster similar objects together.\\nReranking Rerank a set of objects based on a certain query.\\n \\nEmbeddings together with vector stores providing ANN are powerful tools which can be used \\nfor a variety of applications. These include Retrieval Augmented Generation (RAG) for LLMs, \\nSearch, Recommendation Systems, Anomaly detection, few shot- classification and much \\nmore.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 51, 'page_label': '52'}, page_content='Embeddings & Vector Stores\\n52\\nFebrurary 2025\\nFor ranking problems like search and recommendations, embeddings are normally used \\nat the first stage of the process. They retrieve the potentially good candidates that are \\nsemantically similar and consequently improve the relevance of search results. Since the \\namount of information to sort through can be quite large (in some cases even millions or \\nbillions) ANN techniques like ScaNN greatly aids in scalably narrowing the search space. This \\ninitial set of results can be further refined with a more sophisticated model on this smaller set \\nof candidates. \\nLet’s look at an application which combines both LLMs and RAG to help answer questions.\\nQ & A with sources (retrieval augmented generation)\\nRetrieval augmented generation (RAG) for Q&A is a technique that combines the best of both \\nworlds from retrieval and generation. It first retrieves relevant documents from a knowledge \\nbase and then uses prompt expansion to generate an answer from those documents. Prompt \\nexpansion is a technique that when combined with database search can be very powerful. \\nWith prompt expansion the model retrieves relevant information from the database (mostly \\nusing a combination of semantic search and business rules), and augments the original \\nprompt with it. The model uses this augmented prompt to generate much more interesting, \\nfactual, and informative content than with retrieval or generation alone.\\nRAG can help with two common problems with LLMs: 1) their tendency to ‘hallucinate’ \\nand generate factually incorrect but plausible sounding responses and 2) the high cost of \\nretraining to keep up with current information as newer data can be supplied via the prompt, \\nrather than at model training. Although RAG can reduce hallucinations, it does not completely \\neliminate them. What can help mitigate this problem further is to also return the sources from \\nthe retrieval and do a quick coherence check either by a human or an LLM. This ensures the'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 52, 'page_label': '53'}, page_content='Embeddings & Vector Stores\\n53\\nFebrurary 2025\\nLLM response is consistent with the semantically relevant sources. Let’s look at an example \\n(Snippet 11) of RAG with sources, which can be scalably implemented using Vertex AI LLM \\ntext embeddings and Vertex AI Vector Search in conjunction with libraries like langchain.40'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 53, 'page_label': '54'}, page_content='Embeddings & Vector Stores\\n54\\nFebrurary 2025\\n# Before you start run this command:\\n# pip install --upgrade --user --quiet google-cloud-aiplatform langchain_google_vertexai\\n# after running pip install make sure you restart your kernel\\n# TODO : Set values as per your requirements\\n# Project and Storage Constants\\nPROJECT_ID = \"<my_project_id>\"\\nREGION = \"<my_region>\"\\nBUCKET = \"<my_gcs_bucket>\"\\nBUCKET_URI = f\"gs://{BUCKET}\"\\n# The number of dimensions for the text-embedding-005 is 768\\n# If other embedder is used, the dimensions would probably need to change.\\nDIMENSIONS = 768\\n# Index Constants\\nDISPLAY_NAME = \"<my_matching_engine_index_id>\"\\nDEPLOYED_INDEX_ID = \"yourname01\" # you set this. Start with a letter.\\nfrom google.cloud import aiplatform\\nfrom langchain_google_vertexai import VertexAIEmbeddings\\nfrom langchain_google_vertexai import VertexAI\\nfrom langchain_google_vertexai import (\\n    VectorSearchVectorStore,\\n    VectorSearchVectorStoreDatastore,\\n)\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.prompts.chat import (\\n    ChatPromptTemplate,\\n    SystemMessagePromptTemplate,\\n    HumanMessagePromptTemplate,\\n)\\nfrom IPython.display import display, Markdown\\naiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\\nembedding_model = VertexAIEmbeddings(model_name=\"text-embedding-005\")\\n# NOTE : This operation can take upto 30 seconds\\nmy_index = aiplatform.MatchingEngineIndtex.create_tree_ah_index(\\n    display_name=DISPLAY_NAME,\\n    dimensions=DIMENSIONS,\\n    approximate_neighbors_count=150,\\n    distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\\n    index_update_method=\"STREAM_UPDATE\",  # allowed values BATCH_UPDATE , STREAM_UPDATE\\n)\\nContinues next page...'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 54, 'page_label': '55'}, page_content='Embeddings & Vector Stores\\n55\\nFebrurary 2025\\n# Create an endpoint\\nmy_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\\n    display_name=f\"{DISPLAY_NAME}-endpoint\", public_endpoint_enabled=True\\n)\\n# NOTE : This operation can take upto 20 minutes\\nmy_index_endpoint = my_index_endpoint.deploy_index(\\n    index=my_index, deployed_index_id=DEPLOYED_INDEX_ID\\n)\\nmy_index_endpoint = my_index_endpoint.deploy_index(\\n    index=my_index, deployed_index_id=DEPLOYED_INDEX_ID\\n)\\nmy_index_endpoint.deployed_indexes\\n# TODO : replace 1234567890123456789 with your acutial index ID\\nmy_index = aiplatform.MatchingEngineIndex(\"1234567890123456789\")\\n# TODO : replace 1234567890123456789 with your acutial endpoint ID\\n# Be aware that the Index ID differs from the endpoint ID\\nmy_index_endpoint = aiplatform.MatchingEngineIndexEndpoint(\"1234567890123456789\")\\nfrom langchain_google_vertexai import (\\n    VectorSearchVectorStore,\\n    VectorSearchVectorStoreDatastore,\\n)\\n# Input texts\\ntexts = [\\n    \"The cat sat on\",\\n    \"the mat.\",\\n    \"I like to\",\\n    \"eat pizza for\",\\n    \"dinner.\",\\n    \"The sun sets\",\\n    \"in the west.\",\\n]\\nContinues next page...'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 55, 'page_label': '56'}, page_content='Embeddings & Vector Stores\\n56\\nFebrurary 2025\\nSnippet 12. Build/deploy ANN Index for Vertex AI Vector Search and use RAG with LLM prompts to generate \\ngrounded results/sources. \\n# Create a Vector Store\\nvector_store = VectorSearchVectorStore.from_components(\\n    project_id=PROJECT_ID,\\n    region=REGION,\\n    gcs_bucket_name=BUCKET,\\n    index_id=my_index.name,\\n    endpoint_id=my_index_endpoint.name,\\n    embedding=embedding_model,\\n    stream_update=True,\\n)\\n# Add vectors and mapped text chunks to your vectore store\\nvector_store.add_texts(texts=texts)\\n# Initialize the vectore_store as retriever\\nretriever = vector_store.as_retriever()\\n# perform simple similarity search on retriever\\nretriever.invoke(\"What are my options in breathable fabric?\")'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 56, 'page_label': '57'}, page_content='Embeddings & Vector Stores\\n57\\nFebrurary 2025\\nFigure 17. Model responses along with sources demonstrating the LLM being grounded in the database\\nAs we can infer from Figure 16, the output not only grounds LLM in the semantically similar \\nresults retrieved from the database (hence refusing to answer when context cannot be found \\nin the database). This not only significantly reduces hallucination, but also provides sources \\nfor verification, either human or using another LLM.\\nSummary\\nIn this whitepaper we have discussed various methods to create, manage, store, and retrieve \\nembeddings of various data modalities effectively in the context of production-grade \\napplications. Creating, maintaining and using embeddings for downstream applications can \\nbe a complex task that involves several roles in the organization. However, by thoroughly \\noperationalizing and automating its usage, you can safely leverage the incredible benefits \\nthey offer across some of the most important applications. Some key takeaways from this \\nwhitepaper include:\\n1. Choose your embedding model wisely for your data and use case. Ensure the data used in \\ninference is consistent with the data used in training. The distribution shift from training to \\ninference can come from various areas, including domain distribution shift or downstream'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 57, 'page_label': '58'}, page_content='Embeddings & Vector Stores\\n58\\nFebrurary 2025\\ntask distribution shift. If no existing embedding models fit the current inference data \\ndistribution, fine-tuning the existing model can significantly help on the performance. \\nAnother tradeoff comes from the model size. The large deep neural network (large \\nmultimodal models) based models usually have better performance but can come with a \\ncost of longer serving latency. Using Cloud-based embedding services can conquer the \\nabove issue by providing both high-quality and low-latency embedding service. For most \\nbusiness applications using a pre-trained embedding model provides a good baseline, \\nwhich can be further fine-tuned or integrated in downstream models. In case the data has \\nan inherent graph structure, graph embeddings can provide superior performance.\\n2. Once your embedding strategy is defined, it’s important to make the choice of the \\nappropriate vector database that suits your budget and business needs. It might seem \\nquicker to prototype with available open source alternatives, but opting for a more secure, \\nscalable, and battle-tested managed vector database can save significant developer \\ntime. There are various open source alternatives using one of the many powerful ANN \\nvector search algorithms, but ScaNN and HNSW have proven to provide some of the best \\naccuracy and performance trade offs.\\n3. Embeddings combined with an appropriate ANN powered vector database is an \\nincredibly powerful tool and can be leveraged for various applications, including \\nSearch, Recommendation systems, and Retrieval Augmented Generation for LLMs. This \\napproach can mitigate the hallucination problem and bolster verifiability and trust of \\nLLM-based systems.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 58, 'page_label': '59'}, page_content=\"Embeddings & Vector Stores\\n59\\nFebrurary 2025\\nEndnotes\\n1. Rai, A., 2020, Study of various methods for tokenization. In Advances in Natural Language Processing. \\nAvailable at: https://doi.org/10.1007/978-981-15-6198-6_18\\n2. Pennington, J., Socher, R. & Manning, C., 2014, GloVe: Global Vectors for Word Representation. [online] \\nAvailable at: https://nlp.stanford.edu/pubs/glove.pdf .\\n3. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V. & Hinton, G., 2016, Swivel: Improving embeddings \\nby noticing what's missing. ArXiv, abs/1602.02215. Available at: https://arxiv.org/abs/1602.02215 .\\n4. Mikolov, T., Sutskever, I., Chen, K., Corrado, G. & Dean, J., 2013, Efficient estimation of word representations \\nin vector space. ArXiv, abs/1301.3781. Available at: https://arxiv.org/pdf/1301.3781.pdf .\\n5. Rehurek, R., 2021, Gensim: open source python library for word and document embeddings. Available \\nat: https://radimrehurek.com/gensim/intro.html .\\n6. Bojanowski, P., Grave, E., Joulin, A. & Mikolov, T., 2016, Enriching word vectors with subword information. \\nArXiv, abs/1607.04606. Available at: https://arxiv.org/abs/1607.04606 .\\n7. Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R., 1990, Indexing by latent \\nsemantic analysis. Journal of the American Society for Information Science, 41(6), pp. 391-407.\\n8. Blei, D. M., Ng, A. Y., & Jordan, M. I., 2001, Latent Dirichlet allocation. In T. G. Dietterich, S. Becker, & Z. \\nGhahramani (Eds.), Advances in Neural Information Processing Systems 14. MIT Press, pp. 601-608. Available \\nat: https://proceedings.neurips.cc/paper/2001/hash/296472c9542ad4d4788d543508116cbc-Abstract.html .\\n9. Muennighoff, N., Tazi, N., Magne, L., & Reimers, N., 2022, Mteb: Massive text embedding benchmark. ArXiv, \\nabs/2210.07316. Available at: https://arxiv.org/abs/2210.07316 .\\n10. Le, Q. V., Mikolov, T., 2014, Distributed representations of sentences and documents. ArXiv, abs/1405.4053. \\nAvailable at: https://arxiv.org/abs/1405.4053 .\\n11. Devlin, J., Chang, M. W., Lee, K., & Toutanova, K., 2019, BERT: Pre-training deep Bidirectional Transformers \\nfor Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the \\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), \\npp. 4171-4186. Available at: https://www.aclweb.org/anthology/N19-1423/ .\\n12. Reimers, N. & Gurevych, I., 2020, Making monolingual sentence embeddings multilingual using knowledge \\ndistillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing \\n(EMNLP), pp. 254-265. Available at: https://www.aclweb.org/anthology/2020.emnlp-main.21/ .\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 59, 'page_label': '60'}, page_content='Embeddings & Vector Stores\\n60\\nFebrurary 2025\\n13. Gao, T., Yao, X. & Chen, D., 2021, Simcse: Simple contrastive learning of sentence embeddings. ArXiv, \\nabs/2104.08821. Available at: https://arxiv.org/abs/2104.08821 .\\n14. Wang, L., Yang, N., Huang, X., Jiao, B., Yang, L., Jiang, D., Majumder, R. & Wei, F., 2022, Text embeddings by \\nweakly supervised contrastive pre-training. ArXiv. Available at: https://arxiv.org/abs/2201.01279 .\\n15. Khattab, O. & Zaharia, M., 2020, colBERT: Efficient and effective passage search via contextualized late \\ninteraction over BERT. In Proceedings of the 43rd International ACM SIGIR Conference on Research and \\nDevelopment in Information Retrieval, pp. 39-48. Available at: https://dl.acm.org/doi/10.1145/3397271.3401025 .\\n16. Lee, J., Dai, Z., Duddu, S. M. K., Lei, T., Naim, I., Chang, M. W. & Zhao, V. Y., 2023, Rethinking the role of token \\nretrieval in multi-vector retrieval. ArXiv, abs/2304.01982. Available at: https://arxiv.org/abs/2304.01982 .\\n17. TensorFlow, 2021, TensorFlow hub, a model zoo with several easy to use pre-trained models. Available \\nat: https://tfhub.dev/.\\n18. Zhang, W., Xiong, C., & Zhao, H., 2023, Introducing BigQuery text embeddings for NLP tasks.  \\nGoogle Cloud Blog. Available at: https://cloud.google.com/blog/products/data-analytics/introducing  \\n- bigquery-text-embeddings.\\n19. Google Cloud, 2024, Get multimodal embeddings. Available at:  \\nhttps://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings .\\n20. Pinecone, 2024, IT Threat Detection. [online] Available at: https://docs.pinecone.io/docs/it-threat-detection .\\n21. Cai, H., Zheng, V. W., & Chang, K. C., 2020, A survey of algorithms and applications related with graph \\nembedding. In Proceedings of the 29th ACM International Conference on Information & Knowledge \\nManagement. Available at: https://dl.acm.org/doi/10.1145/3444370.3444568 .\\n22. Cai, H., Zheng, V. W., & Chang, K. C., 2017, A comprehensive survey of graph embedding: problems, \\ntechniques and applications. ArXiv, abs/1709.07604. Available at: https://arxiv.org/pdf/1709.07604.pdf .\\n23. Hamilton, W. L., Ying, R. & Leskovec, J., 2017, Inductive representation learning on large graphs.  \\nIn Advances in Neural Information Processing Systems 30. Available at:  \\nhttps://cs.stanford.edu/people/jure/pubs/graphsage -nips17.pdf .\\n24. Dong, Z., Ni, J., Bikel, D. M., Alfonseca, E., Wang, Y., Qu, C. & Zitouni, I., 2022, Exploring dual encoder \\narchitectures for question answering. ArXiv, abs/2204.07120. Available at: https://arxiv.org/abs/2204.07120 .\\n25. Google Cloud, 2021, Vertex AI Generative AI: Tune Embeddings. Available at:  \\nhttps://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-embeddings .'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 60, 'page_label': '61'}, page_content='Embeddings & Vector Stores\\n61\\nFebrurary 2025\\n26. Matsui, Y., 2020, Survey on approximate nearest neighbor methods. ACM Computing Surveys (CSUR), 53(6), \\nArticle 123. Available at: https://wangzwhu.github.io/home/file/acmmm-t-part3-ann.pdf .\\n27. Friedman, J. H., Bentley, J. L. & Finkel, R. A., 1977, An algorithm for finding best matches in logarithmic \\nexpected time. ACM Transactions on Mathematical Software (TOMS), 3(3), pp. 209-226. Available at: \\nhttps://dl.acm.org/doi/pdf/10.1145/355744.355745 .\\n28. Scikit-learn, 2021, Scikit-learn, a library for unsupervised and supervised neighbors-based learning methods. \\nAvailable at: https://scikit-learn.org/.\\n29. lshashing, 2021, An open source python library to perform locality sensitive hashing. Available at: \\nhttps://pypi.org/project/lshashing/ .\\n30. Malkov, Y. A., Yashunin, D. A., 2016, Efficient and robust approximate nearest neighbor search using \\nhierarchical navigable small world graphs. ArXiv, abs/1603.09320. Available at:  \\nhttps://arxiv.org/pdf/1603.09320.pdf .\\n31. Google Research, 2021, A library for fast ANN by Google using the ScaNN algorithm. Available at:  \\nhttps://github.com/google-research/google-research/tree/master/scann .\\n32. Guo, R., Zhang, L., Hinton, G. & Zoph, B., 2020, Accelerating large-scale inference with anisotropic vector \\nquantization. ArXiv, abs/1908.10396. Available at: https://arxiv.org/pdf/1908.10396.pdf .\\n33. TensorFlow, 2021, TensorFlow Recommenders, an open source library for building ranking & recommender \\nsystem models. Available at: https://www.tensorflow.org/recommenders .\\n34. Google Cloud, 2021, Vertex AI Vector Search, Google Cloud’s high-scale low latency vector database. \\nAvailable at: https://cloud.google.com/vertex-ai/docs/vector-search/overview .\\n35. Elasticsearch, 2021, Elasticsearch: a RESTful search and analytics engine. Available at:  \\nhttps://www.elastic.co/elasticsearch/ .\\n36. Pinecone, 2021, Pinecone, a commercial fully managed vector database. Available at:  \\nhttps://www.pinecone.io .\\n37. pgvector, 2021, Open Source vector similarity search for Postgres. Available at:  \\nhttps://github.com/pgvector/pgvector.\\n38. Weaviate, 2021, Weaviate, an open source vector database. Available at: https://weaviate.io/ .\\n39. ChromaDB, 2021, ChromaDB, an open source vector database. Available at: https://www.trychroma.com/ .'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 61, 'page_label': '62'}, page_content=\"Embeddings & Vector Stores\\n62\\nFebrurary 2025\\n40. LangChain, 2021.,LangChain, an open source framework for developing applications powered by language \\nmodel. Available at: https://langchain.com .\\n42. Thakur, N., Reimers, N., Ruckl'e, A., Srivastava, A., & Gurevych, I. (2021). BEIR: A Heterogenous Benchmark for \\nZero-shot Evaluation of Information Retrieval Models. ArXiv, abs/2104.08663.  \\nAvailable at: https://github.com/beir-cellar/beir\\n43. Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. 2023. MTEB: Massive Text Embedding \\nBenchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for \\nComputational Linguistics, pages 2014–2037, Dubrovnik, Croatia. Association for Computational Linguistics. \\nAvailable at: https://github.com/embeddings-benchmark/mteb\\n44. Chris Buckley. trec_eval IR evaluation package. Available from https://github.com/usnistgov/trec_eval\\n45. Christophe Van Gysel and Maarten de Rijke. 2018. Pytrec_eval: An Extremely Fast Python Interface to trec_\\neval. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR \\n'18). Association for Computing Machinery, New York, NY, USA, 873–876.  \\nAvailalbe at: https://doi.org/10.1145/3209978.3210065  \\n46. Boteva, Vera & Gholipour Ghalandari, Demian & Sokolov, Artem & Riezler, Stefan. (2016). A Full-Text Learning \\nto Rank Dataset for Medical Information Retrieval. 9626. 716-722. 10.1007/978-3-319-30671-1_58. Available \\nat https://www.cl.uni-heidelberg.de/statnlpgroup/nfcorpus/\\n47. Douze, M., Guzhva, A., Deng, C., Johnson, J., Szilvasy, G., Mazaré, P.E., Lomeli, M., Hosseini, L. and Jégou, H., \\n2024. The Faiss library. arXiv preprint arXiv:2401.08281. Available at https://arxiv.org/abs/2401.08281\\n48. Lee, J., Dai, Z., Ren, X., Chen, B., Cer, D., Cole, J.R., Hui, K., Boratko, M., Kapadia, R., Ding, W. and Luan, Y., \\n2024. Gecko: Versatile text embeddings distilled from large language models. arXiv preprint arXiv:2403.20327. \\nAvailable at: https://arxiv.org/abs/2403.20327\\n49. Okapi BM25: a non-binary model” Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze. An \\nIntroduction to Information Retrieval, Cambridge University Press, 2009, p. 232.\\n50. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei \\nLi, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. \\nLearn. Res. 21, 1, Article 140 (January 2020), 67 pages.  \\nAvailable at https://dl.acm.org/doi/abs/10.5555/3455716.3455856\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 62, 'page_label': '63'}, page_content='Embeddings & Vector Stores\\n63\\nFebrurary 2025\\n51. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, \\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sashank \\nTsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, \\nEmily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, \\nPengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, \\nVedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret \\nZoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, \\nThanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr \\nPolozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, \\nJason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2023. PaLM: scaling \\nlanguage modeling with pathways. J. Mach. Learn. Res. 24, 1, Article 240 (January 2023), 113 pages.  \\nAvailable at https://dl.acm.org/doi/10.5555/3648699.3648939\\n52. Gemini: A Family of Highly Capable Multimodal Models, Gemini Team, Dec 2023.  \\nAvailable at: https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf\\n53. Radford, Alec and Karthik Narasimhan. “Improving Language Understanding by Generative Pre-Training.” \\n(2018). Available at:  \\nhttps://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\\n54. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozière, B., Goyal, N., Hambro, \\nE., Azhar, F. and Rodriguez, A., 2023. Llama: Open and efficient foundation language models. arXiv preprint \\narXiv:2302.13971. Available at: https://arxiv.org/abs/2302.13971\\n55. Kusupati, A., Bhatt, G., Rege, A., Wallingford, M., Sinha, A., Ramanujan, V., Howard-Snyder, W., Chen, K., \\nKakade, S., Jain, P. and Farhadi, A., 2022. Matryoshka representation learning. Advances in Neural Information \\nProcessing Systems, 35, pp.30233-30249. Available at: \\nhttps://proceedings.neurips.cc/paper_files/paper/2022/file/c32319f4868da7613d78af9993100e42-Paper-  \\n-Conference.pdf\\n56. Nair, P., Datta, P., Dean, J., Jain, P. and Kusupati, A., 2025. Matryoshka Quantization. arXiv preprint \\narXiv:2502.06786. Available at: https://arxiv.org/abs/2502.06786\\n57. Faysse, M., Sibille, H., Wu, T., Omrani, B., Viaud, G., Hudelot, C. and Colombo, P., 2024. Colpali: Efficient \\ndocument retrieval with vision language models. arXiv preprint arXiv:2407.01449.  \\nAvailable at: https://arxiv.org/abs/2407.01449\\n58. Aumüller, M., Bernhardsson, E. and Faithfull, A., 2020. ANN-Benchmarks: A benchmarking tool for \\napproximate nearest neighbor algorithms. Information Systems, 87, p.101374.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 63, 'page_label': '64'}, page_content='Embeddings & Vector Stores\\n64\\nFebrurary 2025'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 0, 'page_label': '1'}, page_content='Foundational \\nLarge Language \\nModels & \\nText Generation\\nAuthors: Mohammadamin Barektain,  \\nAnant Nawalgaria, Daniel J. Mankowitz,  \\nMajd Al Merey, Yaniv Leviathan, Massimo Mascaro,  \\nMatan Kalman, Elena Buchatskaya,                                     \\nAliaksei Severyn, Irina Sigler, and Antonio Gulli'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 1, 'page_label': '2'}, page_content='Foundational Large Language Models & Text Generation\\n2\\nFebruary 2025\\nAcknowledgements\\nContent contributors\\nAdam Sadvovsky\\nYonghui Wu\\nAndrew Dai\\nEfi Kokiopolou\\nChuck Sugnet\\nAleksey Vlasenko\\nErwin Huizenga\\nAida Nematzadeh\\nIra Ktena\\nOlivia Wiles\\nLavi Nigam\\nCurators and Editors\\nAntonio Gulli\\nAnant Nawalgaria\\nGrace Mollison \\nTechnical Writer\\nMark Iverson\\nDesigner\\nMichael Lanning'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 2, 'page_label': '3'}, page_content='Introduction 6\\nWhy language models are important 7\\nLarge language models 8\\nTransformer 9\\nInput preparation and embedding 11\\nMulti-head attention 12\\nUnderstanding self-attention 12\\nMulti-head attention: power in diversity 14\\nLayer normalization and residual connections 15\\nFeedforward layer  15\\nEncoder and decoder 16\\nMixture of Experts (MoE) 17\\nTraining the transformer 20\\nData preparation 21\\nTraining and loss function 21\\nThe evolution of transformers 23\\nGPT-1 23\\nBERT 25\\nTable of contents'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 3, 'page_label': '4'}, page_content='GPT-2 25\\nGPT-3/3.5/4 27\\nLaMDA 28\\nGopher 29\\nGLaM 31\\nChinchilla 31\\nPaLM 33\\nPaLM 2 33\\nGemini 34\\nGemma 37\\nLLaMA 38\\nMixtral 39\\nOpenAI O1 40\\nDeepSeek 40\\nOther open models 41\\nComparison 43\\nFine-tuning large language models 45\\nSupervised fine-tuning  46\\nReinforcement learning from human feedback 47\\nParameter Efficient Fine-Tuning 49\\nUsing large language models 52\\nPrompt engineering  52\\nSampling Techniques and Parameters 53\\nTask-based Evaluation 54\\nAccelerating inference 57'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 4, 'page_label': '5'}, page_content='Trade offs 58\\nThe Quality vs Latency/Cost Tradeoff 58\\nThe Latency vs Cost Tradeoff 59\\nOutput-approximating methods 60\\nQuantization 60\\nDistillation 61\\nOutput-preserving methods 62\\nFlash Attention 63\\nPrefix Caching 63\\nSpeculative Decoding 65\\nBatching and Parallelization 67\\nApplications 68\\nCode and mathematics 71\\nMachine translation 72\\nText summarization 73\\nQuestion-answering 73\\nChatbots 74\\nContent generation 75\\nNatural language inference 75\\nText classification 76\\nText analysis 77\\nMultimodal applications 78\\nSummary 80\\nEndnotes 82'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 5, 'page_label': '6'}, page_content='Foundational Large Language Models & Text Generation\\n6\\nFebruary 2025\\nIntroduction\\nThe advent of Large Language Models (LLMs) represents a seismic shift in the world of \\nartificial intelligence. Their ability to process, generate, and understand user intent is \\nfundamentally changing the way we interact with information and technology. \\nAn LLM is an advanced artificial intelligence system that specializes in processing, \\nunderstanding, and generating human-like text. These systems are typically implemented as \\na deep neural network and are trained on massive amounts of text data. This allows them to \\nlearn the intricate patterns of language, giving them the ability to perform a variety of tasks, \\nlike machine translation, creative text generation, question answering, text summarization, \\nand many more reasoning and language oriented tasks. This whitepaper dives into the \\ntimeline of the various architectures and approaches building up to the large language \\nmodels and the architectures being used at the time of publication. It also discusses fine-\\nWe believe that this new crop of \\ntechnologies has the potential to \\nassist, complement, empower, \\nand inspire people at any time \\nacross almost any field.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 6, 'page_label': '7'}, page_content='Foundational Large Language Models & Text Generation\\n7\\nFebruary 2025\\ntuning techniques to customize an LLM to a certain domain or task, methods to make the \\ntraining more efficient, as well as methods to accelerate inference. These are then followed \\nby various applications and code examples. \\nWhy language models are important\\nLLMs achieve an impressive performance boost from the previous state of the art NLP \\nmodels across a variety of different and complex tasks which require answering questions \\nor complex reasoning, making feasible many new applications. These include language \\ntranslation, code generation and completion, text generation, text classification, and \\nquestion-answering, to name a few. Although foundational LLMs trained in a variety of \\ntasks on large amounts of data perform very well out of the box and display emergent \\nbehaviors (e.g. the ability to perform tasks they have not been directly trained for) they can \\nalso be adapted to solve specific tasks where performance out of the box is not at the level \\ndesired through a process known as fine-tuning. This requires significantly less data and \\ncomputational resources than training an LLM from scratch. LLMs can be further nudged \\nand guided towards the desired behavior by the discipline of prompt engineering: the art and \\nscience of composing the prompt and the parameters of an LLM to get the desired response.\\nThe big question is: how do these large language models work? The next section explores the \\ncore building blocks of LLMs, focusing on transformer architectures and their evolution from \\nthe original ‘Attention is all you need’ paper1 to the latest models such as Gemini, Google’s \\nmost capable LLM. We also cover training and fine-tuning techniques, as well as methods to \\nimprove the speed of response generation. The whitepaper concludes with a few examples \\nof how language models are used in practice.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 7, 'page_label': '8'}, page_content='Foundational Large Language Models & Text Generation\\n8\\nFebruary 2025\\nLarge language models\\nA language model predicts the probability of a sequence of words. Commonly, when given \\na prefix of text, a language model assigns probabilities to subsequent words. For example, \\ngiven the prefix “The most famous city in the US is…”, a language model might predict high \\nprobabilities to the words “New York” and “Los Angeles” and low probabilities to the words \\n“laptop” or “apple”. You can create a basic language model by storing an n-gram table,2 while \\nmodern language models are often based on neural models, such as transformers.\\nBefore the invention of transformers1, recurrent neural networks (RNNSs) were the popular \\napproach for modeling sequences. In particular, “long short-term memory” (LSTM) and \\n“gated recurrent unit” (GRU) were common architectures.3 This area includes language \\nproblems such as machine translation, text classification, text summarization, and question-\\nanswering, among others. RNNs process input and output sequences sequentially. They \\ngenerate a sequence of hidden states based on the previous hidden state and the current \\ninput. The sequential nature of RNNs makes them compute-intensive and hard to parallelize \\nduring training (though recent work in state space modeling is attempting to overcome \\nthese challenges).\\nTransformers, on the other hand, are a type of neural network that can process sequences \\nof tokens in parallel thanks to the self-attention mechanism.1 This means that transformers \\ncan model long-term contexts more effectively and are easier to parallelize than RNNs. This \\nmakes them significantly faster to train, and more powerful compared to RNNs for handling \\nlong-term dependencies in long sequence tasks. However, the cost of self-attention in the \\noriginal transformers is quadratic in the context length which limits the size of the context, \\nwhile RNNs have a theoretically infinite context length. Although they have infinite context \\nlength, in practice they struggle to utilize it due to vanishing gradient problem. Transformers \\nhave become the most popular approach for sequence modeling and transfer learning \\nproblems in recent years.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 8, 'page_label': '9'}, page_content='Foundational Large Language Models & Text Generation\\n9\\nFebruary 2025\\nHerein, we discuss the first version of the transformer model and then move on to the more \\nrecent advanced models and algorithms.\\nTransformer\\nThe transformer architecture was developed at Google in 2017 for use in a translation model.1 \\nIt’s a sequence-to-sequence model capable of converting sequences from one domain \\ninto sequences in another domain. For example, translating French sentences to English \\nsentences. The original transformer architecture consists of two parts: an encoder and a \\ndecoder. The encoder converts the input text (e.g., a French sentence) into a representation, \\nwhich is then passed to the decoder. The decoder uses this representation to generate the \\noutput text (e.g., an English translation) autoregressively.1 Notably, the size of the output of \\nthe transformer encoder is linear in the size of its input. Figure 1 shows the design of the \\noriginal transformer architecture.\\nThe transformer consists of multiple layers. A layer in a neural network comprises a set of \\nparameters that perform a specific transformation on the data. In the diagram you can see \\nan example of some layers which include Multi-Head Attention, Add & Norm, Feed-Forward, \\nLinear, Softmax etc. The layers can be sub-divided into the input, hidden and output layers. \\nThe input layer (e.g., Input/Output Embedding) is the layer where the raw data enters the \\nnetwork. Input embeddings are used to represent the input tokens to the model. Output \\nembeddings are used to represent the output tokens that the model predicts. For example, in \\na machine translation model, the input embeddings would represent the words in the source \\nlanguage, while the output embeddings would represent the words in the target language. \\nThe output layer (e.g., Softmax) is the final layer that produces the output of the network. The \\nhidden layers (e.g., Multi-Head Attention) are between the input and output layers and are \\nwhere the magic happens!'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 9, 'page_label': '10'}, page_content='Foundational Large Language Models & Text Generation\\n10\\nFebruary 2025\\nFigure 1. Original Transformer 1 (P.C: 5 )'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 10, 'page_label': '11'}, page_content='Foundational Large Language Models & Text Generation\\n11\\nFebruary 2025\\nTo better understand the different layers in the transformer, let’s use a French-to-English \\ntranslation task as an example. Here, we explain how a French sentence is input into the \\ntransformer and a corresponding English translation is output. We will also describe each of \\nthe components inside the transformer from Figure 1.\\nInput preparation and embedding\\nTo prepare language inputs for transformers, we convert an input sequence into tokens \\nand then into input embeddings. At a high level, an input embedding is a high-dimensional \\nvector68 that represents the meaning of each token in the sentence. This embedding is \\nthen fed into the transformer for processing. Generating an input embedding involves the \\nfollowing steps:\\n1. Normalization (Optional): Standardizes text by removing redundant whitespace, \\naccents, etc.\\n2. Tokenization: Breaks the sentence into words or subwords and maps them to integer \\ntoken IDs from a vocabulary.\\n3. Embedding: Converts each token ID to its corresponding high-dimensional vector, \\ntypically using a lookup table. These can be learned during the training process.\\n4. Positional Encoding: Adds information about the position of each token in the sequence \\nto help the transformer understand word order.\\nThese steps help to prepare the input for the transformers so that they can better \\nunderstand the meaning of the text.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 11, 'page_label': '12'}, page_content='Foundational Large Language Models & Text Generation\\n12\\nFebruary 2025\\nMulti-head attention\\nAfter converting input tokens into embedding vectors, you feed these embeddings into \\nthe multi-head attention module (see Figure 1). Self-attention is a crucial mechanism in \\ntransformers; it enables them to focus on specific parts of the input sequence relevant to \\nthe task at hand and to capture long-range dependencies within sequences more effectively \\nthan traditional RNNs. \\nUnderstanding self-attention\\nConsider the following sentence: “The tiger jumped out of a tree to get a drink because it \\nwas thirsty.” Self-attention helps to determine relationships between different words and \\nphrases in sentences. For example, in this sentence, “the tiger” and “it” are the same object, \\nso we would expect these two words to be strongly connected. Self-attention achieves this \\nthrough the following steps (Figure 2):\\n1. Creating queries, keys, and values: Each input embedding is multiplied by three learned \\nweight matrices (Wq, Wk, Wv) to generate query (Q), key (K), and value (V) vectors. These \\nare like specialized representations of each word.\\n• Query: The query vector helps the model ask, “Which other words in the sequence are \\nrelevant to me?”\\n• Key: The key vector is like a label that helps the model identify how a word might be \\nrelevant to other words in the sequence.\\n• Value: The value vector holds the actual word content information.\\n2. Calculating scores: Scores are calculated to determine how much each word should \\n‘attend’ to other words. This is done by taking the dot product of the query vector of one \\nword with the key vectors of all the words in the sequence.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 12, 'page_label': '13'}, page_content='Foundational Large Language Models & Text Generation\\n13\\nFebruary 2025\\n3. Normalization: The scores are divided by the square root of the key vector dimension (dk) \\nfor stability, then passed through a softmax function to obtain attention weights. These \\nweights indicate how strongly each word is connected to the others.\\n4. Weighted values: Each value vector is multiplied by its corresponding attention weight. \\nThe results are summed up, producing a context-aware representation for each word.\\nFigure 2. The process of computing self-attention in the multi-head attention module 1 (P.C: 5 )'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 13, 'page_label': '14'}, page_content='Foundational Large Language Models & Text Generation\\n14\\nFebruary 2025\\nIn practice, these computations are performed at the same time, by stacking the query, key \\nand value vectors for all the tokens into Q, K and V matrices and multiplying them together as \\nshown in Figure 3.\\nFigure 3. The basic operation of attention, 1  with Q=query, K=Keys and V=Value, Z=Attention, d_k = dimension \\nof queries and keys (P.C:5 )\\nMulti-head attention: power in diversity\\nMulti-head attention employs multiple sets of Q, K, V weight matrices. These run in parallel, \\neach ‘head’ potentially focusing on different aspects of the input relationships. The outputs \\nfrom each head are concatenated and linearly transformed, giving the model a richer \\nrepresentation of the input sequence.\\nThe use of multi-head attention improves the model’s ability to handle complex language \\npatterns and long-range dependencies. This is crucial for tasks that require a nuanced \\nunderstanding of language structure and content, such as machine translation, text \\nsummarization, and question-answering. The mechanism enables the transformer to consider \\nmultiple interpretations and representations of the input, which enhances its performance on \\nthese tasks.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 14, 'page_label': '15'}, page_content='Foundational Large Language Models & Text Generation\\n15\\nFebruary 2025\\nLayer normalization and residual connections\\nEach layer in a transformer, consisting of a multi-head attention module and a feed-forward \\nlayer, employs layer normalization and residual connections. This corresponds to the Add \\nand Norm layer in Figure 1, where ‘Add’ corresponds to the residual connection and ‘Norm’ \\ncorresponds to layer normalization. Layer normalization computes the mean and variance \\nof the activations to normalize the activations in a given layer. This is typically performed to \\nreduce covariate shift as well as improve gradient flow to yield faster convergence during \\ntraining as well as improved overall performance. \\nResidual connections propagate the inputs to the output of one or more layers. This has the \\neffect of making the optimization procedure easier to learn and also helps deal with vanishing \\nand exploding gradients. \\nThe Add and Norm layer is applied to both the multi-head attention module and the feed-\\nforward layer described in the following section.\\nFeedforward layer \\nThe output of the multi-head attention module and the subsequent ‘Add and Norm’ layer is \\nfed into the feedforward layer of each transformer block. This layer applies a position-wise \\ntransformation to the data, independently for each position in the sequence, which allows the \\nincorporation of additional non-linearity and complexity into the model’s representations. The \\nfeedforward layer typically consists of two linear transformations with a non-linear activation \\nfunction, such as ReLU or GELU, in between. This structure adds further representational \\npower to the model. After processing by the feedforward layer, the data undergoes \\nanother ‘Add and Norm’ step, which contributes to the stability and effectiveness of deep \\ntransformer models.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 15, 'page_label': '16'}, page_content='Foundational Large Language Models & Text Generation\\n16\\nFebruary 2025\\nEncoder and decoder\\nThe original transformer architecture relies on a combination of encoder and decoder \\nmodules. Each encoder and decoder consists of a series of layers, with each layer \\ncomprising key components: a multi-head self-attention mechanism, a position-wise feed-\\nforward network, normalization layers, and residual connections. \\nThe encoder’s primary function is to process the input sequence into a continuous \\nrepresentation that holds contextual information for each token. The input sequence is first \\nnormalized, tokenized, and converted into embeddings. Positional encodings are added to \\nthese embeddings to retain sequence order information. Through self-attention mechanisms, \\neach token in the sequence can dynamically attend to any other token, thus understanding \\nthe contextual relationships within the sequence. The output from the encoder is a series of \\nembedding vectors Z representing the entire input sequence. \\nThe decoder is tasked with generating an output sequence based on the context provided \\nby the encoder’s output Z. It operates in a token-by-token fashion, beginning with a start-\\nof-sequence token. The decoder layers employ two types of attention mechanisms: masked \\nself-attention and encoder-decoder cross-attention. Masked self-attention ensures that \\neach position can only attend to earlier positions in the output sequence, preserving the \\nauto-regressive property. This is crucial for preventing the decoder from having access to \\nfuture tokens in the output sequence. The encoder-decoder cross-attention mechanism \\nallows the decoder to focus on relevant parts of the input sequence, utilizing the contextual \\nembeddings generated by the encoder. This iterative process continues until the decoder \\npredicts an end-of-sequence token, thereby completing the output sequence generation.\\nMajority of recent LLMs adopted a decoder-only variant of transformer architecture. This \\napproach forgoes the traditional encoder-decoder separation, focusing instead on directly \\ngenerating the output sequence from the input. The input sequence undergoes a similar'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 16, 'page_label': '17'}, page_content='Foundational Large Language Models & Text Generation\\n17\\nFebruary 2025\\nprocess of embedding and positional encoding before being fed into the decoder. The \\ndecoder then uses masked self-attention to generate predictions for each subsequent \\ntoken based on the previously generated tokens. This streamlined approach simplifies the \\narchitecture for specific tasks where encoding and decoding can be effectively merged.\\nMixture of Experts (MoE)\\nA Mixture of Experts (MoE) is a an architecture that combines multiple specialized sub-\\nmodels (the “experts”) to improve overall performance, particularly on complex tasks. It’s \\na form of ensemble learning, but with a key difference: instead of simply aggregating the \\npredictions of all experts, it learns to route different parts of the input to different experts. \\nThis allows the model to specialize, with each expert becoming proficient in a specific \\nsub-domain or aspect of the data. Here’s a more technical breakdown describing the main \\ncomponents of an MoE:\\n• Experts: These are the individual sub-models, each designed to handle a specific subset \\nof the input data or a particular task. They can be any type of model (e.g., neural networks, \\ndecision trees, etc.), but in the context of large language models, they are typically \\nthemselves transformer-based architectures.  \\n• Gating Network (Router): This is a crucial component that learns to route the input to \\nthe appropriate expert(s). It takes the input and produces a probability distribution over \\nthe experts. This distribution determines how much each expert should “contribute” to the \\nfinal prediction. The gating network is also typically a neural network.  \\n• Combination Mechanism: This combines the outputs of the experts, weighted by the \\nprobabilities from the gating network, to produce the final prediction. A common approach \\nis a weighted average.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 17, 'page_label': '18'}, page_content='Foundational Large Language Models & Text Generation\\n18\\nFebruary 2025\\nIn practice, A Mixture of Experts (MoE) architecture combines multiple specialized sub-\\nmodels, called “experts,” to tackle complex tasks. Instead of simply averaging all expert \\npredictions, an MoE uses a “gating network” to intelligently route different parts of the \\ninput to the mostt relevant experts. Both the experts and the gating network receive the \\ninput.  Each expert processes the input and generates its output. Simultaneously, the \\ngating network analyzes the input and produces a probability distribution over the experts, \\nindicating how much each expert should contribute to the final result.  These probabilities \\nthen weight the outputs of the experts, and the weighted combination becomes the final \\nprediction. This allows different experts to specialize in handling specific types of data or \\nsub-tasks, improving overall performance and, through “sparse activation,” potentially \\nreducing computational cost by only activating a subset of experts for any given input.\\nFigure 4. Mixture of experts ensembling 70'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 18, 'page_label': '19'}, page_content='Foundational Large Language Models & Text Generation\\n19\\nFebruary 2025\\nLarge Reasoning Models\\nAchieving robust reasoning capabilities in Large Models is a complex endeavor that involves \\na combination of architectural designs, training methodologies, and prompting strategies. \\nOne crucial aspect is incorporating inductive biases that favor reasoning-conducive patterns. \\nTransformer architectures, with their self-attention mechanisms, are foundational, allowing \\nthe model to weigh the importance of different parts of the input sequence when generating \\nan output. However, vanilla Transformers alone are not sufficient for complex reasoning.\\nChain-of-Thought prompting explicitly encourages the model to generate intermediate \\nreasoning steps before arriving at a final answer. By providing examples of step-by-step \\nreasoning in the prompt, the model learns to decompose complex problems into smaller, \\nmanageable sub-problems. This mimics human reasoning processes and significantly \\nimproves performance on tasks requiring multi-step inference. Tree-of-Thoughts takes this \\nfurther, exploring multiple reasoning paths and using a search algorithm to find the most \\npromising solution. This technique is useful with game trees or combinatorial problems. \\nLeast-to-Most prompting guides the model to solve subproblems, which get progressively \\nmore complex, and the output of one subproblem is used as part of the prompt of the more \\ncomplex, subsequent problem.\\nFine-tuning on datasets specifically designed for reasoning tasks is also crucial. These \\ndatasets may contain logical puzzles, mathematical problems, or commonsense reasoning \\nchallenges. Instruction tuning, where the model is trained to follow natural language \\ninstructions, further enhances its ability to understand and respond to complex reasoning \\nprompts. Reinforcement Learning from Human Feedback (RLHF) refines the model’s outputs \\nbased on human preferences, improving the quality and coherence of its reasoning. RLHF \\nhelps in reward models that score reasoning ability as well as “helpfulness.”'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 19, 'page_label': '20'}, page_content='Foundational Large Language Models & Text Generation\\n20\\nFebruary 2025\\nKnowledge distillation, transferring knowledge from a larger, more capable “teacher” model \\nto a smaller, more efficient “student” model, can be used to improve the reasoning abilities \\nof smaller models while maintaining efficiency. This approach allows the student model to \\nlearn the reasoning patterns of the teacher model without requiring the same computational \\nresources. During inference, techniques like beam search, which explores multiple candidate \\noutputs simultaneously, can improve the quality of reasoning by considering different \\npossibilities. Temperature scaling, adjusting the randomness of the model’s output, can also \\ninfluence the exploration-exploitation trade-off in reasoning. Finally, incorporating external \\nknowledge sources, such as knowledge graphs or structured databases, can provide the \\nmodel with additional information to support its reasoning process. This can be achieved \\nthrough techniques like retrieval-augmented generation, where the model retrieves relevant \\ninformation from an external source before generating an output. These techniques all \\ncombined, across many domains of reasoning, create the best performing reasoning large \\nlanguage models.\\nTraining the transformer\\nWhen talking about machine learning models, it’s important to differentiate between \\ntraining and inference. Training typically refers to modifying the parameters of the model, \\nand involves loss functions and backpropagation. Inference is when model is used only \\nfor the predicted output, without updating the model weights. The model parameters are \\nfixed during inference. Up until now we learned how transformers generate outputs during \\ninference. Next, we focus on how to train transformers to perform one or more given tasks.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 20, 'page_label': '21'}, page_content='Foundational Large Language Models & Text Generation\\n21\\nFebruary 2025\\nData preparation\\nThe first step is data preparation, which involves a few important steps itself. First, clean the \\ndata by applying techniques such as filtering, deduplication, and normalization. The next \\nstep is tokenization where the dataset is converted into tokens using techniques such as \\nByte-Pair Encoding8, 9 and Unigram tokenization.8, 10 Tokenization generates a vocabulary, \\nwhich is a set of unique tokens used by the LLM. This vocabulary serves as the model’s \\n’language’ for processing and understanding text. Finally, the data is typically split into a \\ntraining dataset for training the model as well as a test dataset which is used to evaluate the \\nmodels performance.\\nTraining and loss function\\nA typical transformer training loop consists of several parts: First, batches of input \\nsequences are sampled from a training dataset. For each input sequence, there is a \\ncorresponding target sequence. In unsupervised pre-training, the target sequence is \\nderived from the input sequence itself. The batch of input sequences is then fed into the \\ntransformer. The transformer generates predicted output sequences. The difference \\nbetween the predicted and target sequences is measured using a loss function (often cross-\\nentropy loss)11. Gradients of this loss are calculated, and an optimizer uses them to update \\nthe transformer’s parameters. This process is repeated until the transformer converges to a \\ncertain level of performance or until it has been trained on a pre-specified number of tokens. \\nThere are different approaches to formulating the training task for transformers depending \\non the architecture used:\\n• Decoder-only models are typically pre-trained on the language modeling task (e.g., see \\nendnote12, 13). The target sequence for the decoder is simply a shifted version of the input \\nsequence. Given a training sequence like ‘the cat sat on the mat’ various input/target'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 21, 'page_label': '22'}, page_content='Foundational Large Language Models & Text Generation\\n22\\nFebruary 2025\\npairs can be generated for the model. For example the input “the cat sat on” should \\npredict “the” and subsequently the input “the cat sat on the” should predict target \\nsequence “mat”.\\n• Encoder-only models (like BERT)14 are often pre-trained by corrupting the input sequence \\nin some way and having the model try to reconstruct it. One such approach is masked \\nlanguage modeling (MLM).14 In our example, the input sequence could be “The [MASK] sat \\non the mat” and the sequence target would be the original sentence.\\n• Encoder-decoder models (like the original transformer) are trained on sequence-to-\\nsequence supervised tasks such as translation (input sequence “Le chat est assis sur \\nle tapis” and target “The cat sat on the mat”), question-answering (where the input \\nsequence is a question and the target sequence is the corresponding answer), and \\nsummarization (where the input sequence is a full article and the target sequence is its \\ncorresponding summary). These models could also be trained in an unsupervised way by \\nconverting other tasks into sequence-to-sequence format. For example, when training \\non Wikipedia data, the input sequence might be the first part of an article, and the target \\nsequence comprises the remainder of the article.\\nAn additional factor to consider during training is the ‘context length’. This refers to the \\nnumber of previous tokens the model can ‘remember’ and use to predict the next token in \\nthe sequence. Longer context lengths allow the model to capture more complex relationships \\nand dependencies within the text, potentially leading to better performance. However, longer \\ncontexts also require more computational resources and memory, which can slow down \\ntraining and inference. Choosing an appropriate context length involves balancing these \\ntrade-offs based on the specific task and available resources.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 22, 'page_label': '23'}, page_content='Foundational Large Language Models & Text Generation\\n23\\nFebruary 2025\\nThe evolution of transformers\\nThe next sections provide an overview of the various transformer architectures. These \\ninclude encoder-only, encoder-decoder, as well as decoder-only transformers. We start with \\nGPT-1 and BERT and end with Google’s latest family of LLMs called Gemini.\\nGPT-1\\nGPT-1 (Generative pre-trained transformer version 1)15 was a decoder-only model developed \\nby OpenAI in 2018. It was trained on the BooksCorpus dataset (containing approximately \\nseveral billion words) and is able to generate text, translate languages, write different kinds \\nof creative content, and answer questions in an informative way. The main innovations in \\nGPT-1 were:\\n• Combining transformers and unsupervised pre-training: Unsupervised pre-training \\nis a process of training a language model on a large corpus of unlabeled data. Then, \\nsupervised data is used to fine-tune the model for a specific task, such as translation \\nor sentiment classification. In prior works, most language models were trained using a \\nsupervised learning objective. This means that the model was trained on a dataset of \\nlabeled data, where each example had a corresponding label. This approach has two main \\nlimitations. First, it requires a large amount of labeled data, which can be expensive and \\ntime-consuming to collect. Second, the model can only generalize to tasks that are similar \\nto the tasks that it was trained on. Semi-supervised sequence learning was one of the first \\nworks that showed that unsupervised pre-training followed by supervised training was \\nsuperior than supervised training alone.\\nUnsupervised pre-training addresses these limitations by training the model on a large \\ncorpus of unlabeled data. This data can be collected more easily and cheaply than labeled \\ndata. Additionally, the model can generalize to tasks that are different from the tasks that'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 23, 'page_label': '24'}, page_content='Foundational Large Language Models & Text Generation\\n24\\nFebruary 2025\\nit was trained on. The BooksCorpus dataset is a large (5GB) corpus of unlabeled text that \\nwas used to train the GPT-1 language model. The dataset contains over 7,000 unpublished \\nbooks, which provides the model with a large amount of data to learn from. Additionally, \\nthe corpus contains long stretches of contiguous text, which helps the model learn long-\\nrange dependencies. Overall, unsupervised pre-training is a powerful technique that can \\nbe used to train language models that are more accurate and generalizable than models \\nthat are trained using supervised learning alone. \\n• Task-aware input transformations: There are different kinds of tasks such as textual \\nentailment and question-answering that require a specific structure. For example, \\ntextual entailment requires a premise and a hypothesis; question-answering requires a \\ncontext document; a question and possible answers. One of the contributions of GPT-1 \\nis converting these types of tasks which require structured inputs into an input that the \\nlanguage model can parse, without requiring task-specific architectures on top of the \\npre-trained architecture. For textual entailment, the premise p and the hypothesis h are \\nconcatenated with a delimiter token ($) in between - [p, $, h]. For question answering, the \\ncontext document c is concatenated with the question q and a possible answer a with a \\ndelimiter token in between the question and answer - [c,q,$,a].\\nGPT-1 surpassed previous models on several benchmarks, achieving excellent results. While \\nGPT-1 was a significant breakthrough in natural language processing (NLP), it had some \\nlimitations. For example, the model was prone to generating repetitive text, especially when \\ngiven prompts outside the scope of its training data. It also failed to reason over multiple \\nturns of dialogue and could not track long-term dependencies in text. Additionally, its \\ncohesion and fluency were limited to shorter text sequences, and longer passages would \\nlack cohesion. Despite these limitations, GPT-1 demonstrated the power of unsupervised \\npre-training, which laid the foundation for larger and more powerful models based on the \\ntransformer architecture.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 24, 'page_label': '25'}, page_content='Foundational Large Language Models & Text Generation\\n25\\nFebruary 2025\\nBERT\\nBERT14 which stands for Bidirectional Encoder Representations from Transformers, \\ndistinguishes itself from traditional encoder-decoder transformer models by being an \\nencoder-only architecture. Instead of translating or producing sequences, BERT focuses \\non understanding context deeply by training on a masked language model objective. In \\nthis setup, random words in a sentence are replaced with a [MASK] token, and BERT tries \\nto predict the original word based on the surrounding context. Another innovative aspect \\nof BERT’s training regime is the next sentence prediction loss, where it learns to determine \\nwhether a given sentence logically follows a preceding one. By training on these objectives, \\nBERT captures intricate context dependencies from both the left and right of a word, and \\nit can discern the relationship between pairs of sentences. Such capabilities make BERT \\nespecially good at tasks that require natural language understanding, such as question-\\nanswering, sentiment analysis, and natural language inference, among others. Since this is an \\nencoder-only model, BERT cannot generate text.\\nGPT-2\\nGPT-2,12 the successor to GPT-1, was released in 2019 by OpenAI. The main innovation of \\nGPT-2 was a direct scale-up, with a tenfold increase in both its parameter count and the size \\nof its training dataset:\\n• Data: GPT-2 was trained on a large (40GB) and diverse dataset called WebText, which \\nconsists of 45 million webpages from Reddit with a Karma rating of at least three. Karma \\nis a rating metric used on Reddit and a value of three means that all the posts were of a \\nreasonable level of quality.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 25, 'page_label': '26'}, page_content='Foundational Large Language Models & Text Generation\\n26\\nFebruary 2025\\n• Parameters: GPT-2 had 1.5 billion parameters, which was an order of magnitude larger \\nthan the previous model. More parameters increase the model’s learning capacity. The \\nauthors trained four language models with 117M (the same as GPT-1), 345M, 762M, and 1.5B \\n(GPT-2) parameters, and found that the model with the most parameters performed better \\non every subsequent task.\\nThis scaling up resulted in a model that was able to generate more coherent and realistic text \\nthan GPT-1. Its ability to generate human-like responses made it a valuable tool for various \\nnatural language processing tasks, such as content creation and translation. Specifically, \\nGPT-2 demonstrated significant improvement in capturing long-range dependencies and \\ncommon sense reasoning. While it performed well in some tasks, it did not outperform state-\\nof-the-art reading comprehension, summarization, and translation. GPT-2’s most significant \\nachievement was its ability to perform zero-shot learning on a variety of tasks. Zero-shot task \\ntransfer is the ability of a model to generalize to a new task without being trained on it, which \\nrequires the model to understand the task based on the given instruction. For example, for \\nan English to German translation task, the model might be given an English sentence followed \\nby the word “German” and a prompt (“:”). The model would then be expected to understand \\nthat this is a translation task and generate the German translation of the English sentence. \\nGPT-2 was able to perform tasks such as machine translation, text summarization, and \\nreading comprehension without any explicit supervision.\\nThe study discovered that performance on zero-shot tasks increased in a log-linear manner \\nas the model’s capacity increased. GPT-2 showed that training on a larger dataset and having \\nmore parameters improved the model’s ability to understand tasks and surpass the state-of-\\nthe-art on many tasks in zero-shot settings.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 26, 'page_label': '27'}, page_content='Foundational Large Language Models & Text Generation\\n27\\nFebruary 2025\\nGPT-3/3.5/4\\nGPT-3,13 or the third iteration of the Generative Pre-trained Transformer model, represents a \\nsignificant evolution from its predecessor, GPT-2, primarily in terms of scale, capabilities, and \\nflexibility. The most noticeable difference is the sheer size of GPT-3, boasting a whopping \\n175 billion parameters, compared to GPT-2’s largest model which had 1.5 billion parameters. \\nThis increase in model size allowed GPT-3 to store and recall an even more vast amount of \\ninformation, understand nuanced instructions, and generate more coherent and contextually \\nrelevant text over longer passages.\\nWhile GPT-2 could be fine-tuned on specific tasks with additional training data, GPT-3 can \\nunderstand and execute tasks with just a few examples, or sometimes even without any \\nexplicit examples—simply based on the instruction provided. This highlights GPT-3’s more \\ndynamic understanding and adaptation abilities, reducing the need for task-specific fine-\\ntuning which was more prevalent in GPT-2.\\nFinally, GPT-3’s large model scale and diverse training corpus have led to better \\ngeneralization across a broader range of tasks. This means that out-of-the-box, without \\nany further training, GPT-3 exhibits improved performance on diverse NLP challenges, from \\ntranslation to question-answering, compared to GPT-2. It’s also worth noting that the release \\napproach differed: while OpenAI initially held back GPT-2 due to concerns about misuse, \\nthey chose to make GPT-3 available as a commercial API, reflecting both its utility and the \\norganization’s evolving stance on deployment.\\nInstruction tuning was then introduced with InstructGPT17, a version of GPT-3 that was fine-\\ntuned, using Supervised Fine-Tuning, on a dataset of human demonstrations of desired \\nmodel behaviors. Outputs from this model were then ranked and it was then further fine-\\ntuned using Reinforcement Learning from Human Feedback. This led to improved instruction'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 27, 'page_label': '28'}, page_content='Foundational Large Language Models & Text Generation\\n28\\nFebruary 2025\\nfollowing in the model. A 1.3B parameter InstructGPT model had better human evaluations \\nthan the 175B parameter GPT-3 model. It also showed improvements in truthfulness and \\nreductions in toxicity.\\nGPT-3.5 models, including GPT-3.5 turbo, improve over GPT-3 as it is capable of \\nunderstanding and generating code. It’s been optimized for dialogue. And it’s capable of \\nreceiving context windows of up to 16,385 tokens and can generate outputs of up to 4,096 \\ntokens. \\nGPT-4 extends GPT-3.5 as a large multimodal model capable of processing image and \\ntext inputs and producing text outputs.19 Specifically, accepting text or images as input \\nand outputting text. This model has broader general knowledge and advanced reasoning \\ncapabilities. It can receive context windows of up to 128,000 tokens and has a maximum \\noutput of 4,096 tokens. GPT-4 demonstrates remarkable versatility by solving complex tasks \\nacross diverse fields like mathematics, coding, vision, medicine, law, and psychology – all \\nwithout specialized instructions. Its performance often matches or even exceeds human \\ncapabilities and significantly outperforms earlier models like GPT-3.5.\\nLaMDA\\nGoogle’s LaMDA,20 which stands for ‘Language Model for Dialogue Applications’ is another \\ncontribution to the arena of large-scale language models, designed primarily to engage in \\nopen-ended conversations. Unlike traditional chatbots which operate in more constrained \\nand predefined domains, LaMDA is engineered to handle a wide array of topics, delivering \\nmore natural and flowing conversations. LaMDA was trained on dialogue-focused data to \\nencourage ongoing conversational flow, not just isolated responses, ensuring users can have \\nmore extensive and explorative dialogues.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 28, 'page_label': '29'}, page_content='Foundational Large Language Models & Text Generation\\n29\\nFebruary 2025\\nWhile GPT models, especially the later iterations like GPT-3, have strived to address a \\nmultitude of tasks simultaneously, from text generation to code writing, LaMDA’s primary \\nfocus is on maintaining and enhancing conversational depth and breadth. GPT models \\nshine on their ability to produce coherent long-form content and perform various tasks \\nwith minimal prompting, whereas LaMDA emphasizes the flow and progression of dialogue, \\nstriving to mimic the unpredictability and richness of human conversations. \\nGopher\\nGopher22 is a 280 billion parameter language model based on the decoder-only transformer \\narchitecture, developed by DeepMind in 2021.22 It can generate text, translate languages, \\nwrite different kinds of creative content, and answer your questions in an informative way. \\nSimilar to GPT-3, Gopher focused on improving dataset quality and optimization techniques:\\n• Dataset: The researchers curated a high-quality text dataset called MassiveText, which \\ncontains over 10 terabytes of data and 2.45B documents from web pages, books, news \\narticles, and code (GitHub). They only trained on 300B tokens, which is 12% of the dataset. \\nImportantly, they improved the quality of the data by filtering it, such as by removing \\nduplicate text and deduplicating similar documents. This significantly improved the \\nmodel’s performance on downstream tasks.\\n• Optimization: The researchers used a warmup learning rate for 1,500 steps and then \\ndecayed it using a cosine schedule. They also had an interesting rule that as they \\nincreased the model size, they decreased the learning rate and increased the number of \\ntokens in each batch. Additionally, they found that clipping gradients to be a maximum of 1 \\nbased on the global gradient norm helped stabilize the training.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 29, 'page_label': '30'}, page_content='Foundational Large Language Models & Text Generation\\n30\\nFebruary 2025\\nGopher was evaluated on a variety of tasks, including mathematics, common sense, logical \\nreasoning, general knowledge, scientific understanding, ethics, and reading comprehension. \\nGopher outperformed previous state-of-the-art models on 81% of the tasks. Specifically, \\nGopher performed well on knowledge-intensive tasks but struggled on reasoning-heavy \\ntasks such as abstract algebra.\\nThe authors also conducted a study on the effect of model size on different types of \\ntasks. Figure 4 shows the results of this ablation study. Specifically, the authors found that \\nincreasing the number of parameters had a significant impact on logical reasoning and \\nreading comprehension, but it did not improve performance as much on tasks such as \\ngeneral knowledge, where performance eventually almost plateaued.\\nFigure 5. Ablation study 22  on the effect of model size on the performance of Gopher on different types \\nof tasks'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 30, 'page_label': '31'}, page_content='Foundational Large Language Models & Text Generation\\n31\\nFebruary 2025\\nGLaM\\nGLaM (Generalist Language Model)23 was the first sparsely-activated mixture-of-experts \\nlanguage model. Mixture-of-experts based models are much more computationally efficient \\ngiven their parameter count. This is achieved by only activating a subset of their parameters \\n(i.e. experts) for each input token. GLaM consists of 1.2 trillion parameters but uses only ⅓ \\nof the energy used to train GPT-3 and half of the FLOPs for inference while achieving better \\noverall performance compared to GPT-3.\\nChinchilla\\nUntil 2022, LLMs were primarily scaled by increasing the model size and using datasets that \\nare relatively small by current standards (up to 300 billion tokens for the largest models). \\nThis approach was informed by the Kaplan et al.24 study, which examined how performance \\nof a language model, measured by cross-entropy loss, varies with changes in computational \\nbudget, model size, and dataset size. Specifically, given a 100-fold increase in computational \\nresources (C), Kaplan et al.24 recommended scaling model size by approximately 28.8 times \\n(Nopt∝ C0.73 ), while increasing dataset size by only 3.5 times (Dopt∝ C0.27 ). \\nThe Chinchilla paper,25 revisited the compute optimal scaling laws and used three different \\napproaches to find that near equal scaling in parameters and data is optimal with increasing \\ncompute. Thus, a 100-fold increase in compute should translate into a tenfold increase in \\nboth data size and model size.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 31, 'page_label': '32'}, page_content='Foundational Large Language Models & Text Generation\\n32\\nFebruary 2025\\nFigure 6. Overlaid predictions from three different approaches from Chinchilla paper, 25  along with \\nprojections from Kaplan et al 24  \\nTo verify the updated scaling law, DeepMind trained a 70B parameter model (called \\nChinchilla) using the same compute budget as the previously trained Gopher model. \\nChinchilla uniformly and significantly outperformed Gopher (280B),21 GPT-3 (175B),13 and \\nMegatron-Turing NLG (530B)26 on a large range of downstream evaluation tasks. Due to being \\n4x smaller than Gopher, both the memory footprint and the inference cost of Chinchilla are \\nalso smaller.\\nThe findings of Chinchilla had significant ramifications for the development of future LLMs. \\nFocus shifted into finding ways to scale dataset size (while maintaining quality) alongside \\nincreasing parameter count. Extrapolating this trend suggests that training dataset size \\nmay soon be limited by the amount of text data available. This has led to new research by \\nMuennighoff et al.27 exploring scaling laws in data-constrained regimes.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 32, 'page_label': '33'}, page_content='Foundational Large Language Models & Text Generation\\n33\\nFebruary 2025\\nPaLM\\nPathways language model (PaLM)28 is a 540-billion parameter transformer-based large \\nlanguage model developed by Google AI. It was trained on a massive dataset of text and \\ncode and is capable of performing a wide range of tasks, including common sense reasoning, \\narithmetic reasoning, joke explanation, code generation, and translation.\\nAt the time of its release, PaLM was also able to achieve state-of-the-art performance on \\nmany language benchmarks, for example GLUE and SuperGLUE.29\\nOne of the key features of PaLM is its ability to scale efficiently. This is thanks to the \\nPathways system, which Google developed to distribute the training of large language \\nmodels across two TPU v4 Pods.\\nPaLM 2\\nPaLM 230 is a successor to PaLM that was announced in May 2023. Thanks to a number of \\narchitectural and training enhancements, PaLM 2 is even more capable than PaLM, with \\nfewer total parameters. It excels at advanced reasoning tasks, including code generation, \\nmath, classification, question answering, and translation.\\nPaLM 2 has also been shown to be more efficient than PaLM and became the basis for a \\nnumber of commercial models Google released as part of Google Cloud Generative AI.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 33, 'page_label': '34'}, page_content='Foundational Large Language Models & Text Generation\\n34\\nFebruary 2025\\nGemini\\nFigure 7. Gemini can receive multi-modal inputs including text, audio, images, and video data. These are all \\ntokenized and fed into its transformer model. The transformer generates an output that can contain images \\nand text.\\nGemini31 (Figure 6) is a state-of-the-art multimodal language family of models that can \\ntake interleaved sequences of text, image, audio, and video as input. It’s built on top of \\ntransformer decoders and has architectural improvements for scale as well as optimized \\ninference on Google’s Tensor Processing Units (TPUs). In its current version, these models \\nare trained to support contexts of different sizes, up to 2M tokens in the Gemini Pro version \\non Vertex AI and employ mechanisms such as multi-query attention for efficiency. Gemini \\nmodels also employ a Mixture of Experts architecture to optimize efficiency and capabilities \\nof the models. Multimodality allows the models to process text, images and video in input, \\nwith more modalities in input and output expected in the future.\\nThe Gemini models are trained on Google’s TPUv5e and TPUv4 processors, depending on \\nsize and configuration. The pre-training data consists of web documents, books, code, and \\nimage, audio, and video data.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 34, 'page_label': '35'}, page_content='Foundational Large Language Models & Text Generation\\n35\\nFebruary 2025\\nLarger models are trained for the compute-optimal number of tokens using the same \\napproach as in Chinchilla paper,25 while small models are trained on significantly more tokens \\nthan compute optimal to improve performance for a given inference budget.\\nThe Gemini family of models is optimized for different sizes: Gemini Ultra, Gemini Pro, Gemini \\nNano and Flash. Gemini Ultra is used for highly complex tasks and achieves state-of-the-\\nart results in 30 out of 32 benchmark tasks. Gemini Pro enables deployment at scale and \\nGemini Nano is designed for on-device applications. The Gemini Nano models leverage \\nadvancements such as distillation to produce state-of-the-art performance for small \\nlanguage models on tasks such as summarization and reading comprehension. As the Gemini \\nmodels are natively multi-modal, it can be seen that training across multiple modalities does \\nindeed lead to a model that is capable of achieving strong capabilities in each domain. \\nDuring the initial part of 2024, Google introduced the latest model of the Gemini family, \\nGemini 1.5 Pro,32 a highly compute-efficient multimodal mixture-of-experts model. This \\nmodel  also dramatically increased the size of the context window to millions of tokens \\nand is capable of recalling and reasoning over those millions of tokens, including multiple \\nlong documents and hours of video and audio. Gemini 1.5 Pro demonstrates remarkable \\ncapabilities across different domains:\\n• Code understanding: It can process massive codebases and answer highly specific \\ncode-related questions.\\n• Language learning: The model can learn new languages never observed at training time \\nsolely based on reference materials provided within its input\\n• Multimodal reasoning: It understands images and text, allowing it to locate a famous scene \\nfrom the novel ‘Les Misérables’ based on a simple sketch.\\n• Video comprehension: It can analyze entire movies, answering detailed questions and \\npinpointing specific timestamps with remarkable accuracy.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 35, 'page_label': '36'}, page_content='Foundational Large Language Models & Text Generation\\n36\\nFebruary 2025\\nGoogle’s Gemini 1.5 Pro model excels at retrieving information from even very long \\ndocuments. In their study,32 it demonstrated 100% recall on documents up to 530,000 \\ntokens, and over 99.7% recall on those up to 1 million tokens. Impressively, it maintains 99.2% \\naccuracy when finding information in documents up to 10 million tokens.\\nMoreover, Gemini 1.5 Pro demonstrates a major leap forward in how well LLMs follow complex \\ninstructions. In a rigorous test with 406 multi-step prompts, it significantly outperformed \\nprevious Gemini models. The model accurately followed almost 90% of instructions and fully \\ncompleted 66% of the complex tasks. \\nGemini Flash is a new addition to the Gemini model family and the fastest Gemini model \\nserved in the API. It’s optimized for high-volume, high-frequency tasks at scale, is more \\ncost-efficient to serve and features a breakthrough long context window of 1 million tokens. \\nAlthough it is a lighter weight model than 1.5 Pro, it is highly capable of multimodal reasoning \\nacross vast amounts of information and delivers impressive quality for its size.\\nGemini 2.0 represents a significant leap forward in Google’s multimodal AI models. It builds \\nupon the foundation of Gemini 1.0 with enhanced capabilities and a focus on efficiency and \\nnew modalities. In more details :\\nGemini 2.0 Flash: This version is designed for speed and efficiency, exceeding the \\nperformance of Gemini 1.5 Pro while maintaining the responsiveness developers expect. It \\nshowcases improvements in multimodal understanding, text processing, code generation, \\nvideo analysis, and spatial reasoning. Notably, it has enhanced spatial understanding, leading \\nto more accurate object identification and captioning, especially for small objects in complex \\nscenes. Gemini 2.0 Flash was introduced in late 2024.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 36, 'page_label': '37'}, page_content='Foundational Large Language Models & Text Generation\\n37\\nFebruary 2025\\nGemini 2.0 Pro is positioned as a highly capable model for a broad range of tasks. It likely \\nserves as a workhorse for various applications, balancing performance and efficiency. \\nIt is likely an evolution of the original Gemini Pro model with improvements across \\nmultiple domains.\\nGemini 2.0 Nano: As with the previous generation, Nano focuses on on-device deployment. \\nIt is optimized for resource efficiency and speed, enabling AI capabilities directly on devices \\nlike smartphones. \\nGemini 2.0 Flash Thinking Experimental is a fast, high-performance reasoning model, \\nenhanced with explainability through visible “thought processes,” particularly excelling \\nin complex science and math problems; it accepts text and image inputs, produces text \\noutputs, supports a 1 million token input context and a 64,000 token output, utilizes code \\nexecution, has a knowledge cutoff of August 2024, is best suited for complex tasks where \\nlatency is not a primary concern, and is available via Google AI Studio, the Gemini API, and \\nVertex AI, though currently in an experimental deployment status.\\nGemma\\nFurthermore, recently advanced Gemma is a family of lightweight, state-of-the-art open \\nmodels built from the same research and technology used to create the Gemini models.33 The \\nfirst model by Gemma boasts a large vocabulary of 256,000 words and has been trained on \\na massive 6 trillion token dataset. This makes it a valuable addition to the openly-available \\nLLM collection. Additionally, the 2B parameter version is intriguing as it can run efficiently on \\na single GPU.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 37, 'page_label': '38'}, page_content='Foundational Large Language Models & Text Generation\\n38\\nFebruary 2025\\nGemma 2,33 developed by Google AI, represents a significant advancement in the field of \\nopen large language models. Designed with a focus on efficiency, the 27-billion parameter \\nmodel boasts performance comparable to much larger models like Llama 3 70B33 on standard \\nbenchmarks. This makes Gemma 2 a powerful and accessible tool for a wide range of AI \\ndevelopers. Its compatibility with diverse tuning toolchains, from cloud-based solutions \\nto popular community tools, further enhances its versatility. With its strong performance, \\nefficient architecture, and accessible nature, Gemma 2 plays a vital role in driving innovation \\nand democratizing AI capabilities.\\nGemma 3 represents Google’s latest advancement in its family of open models, built upon \\nthe research and technology that also powers the Gemini models. A key feature of Gemma \\n3 is its multimodality, enabling it to process both text and image inputs and generate text \\noutputs. This version significantly expands capabilities with a large, 128K context window, and \\nbroad multilingual support encompassing over 140 languages. To cater to diverse hardware \\nand performance needs, Gemma 3 is available in various sizes, including 1B, 4B, 12B, and 27B \\nparameter models. These varieties allow developers to select the most suitable model for \\ntheir specific applications, ranging from resource-constrained devices to high-performance \\ncomputing environments.\\nLLaMA\\nLlama models are transformer-based language models, similar in high-level architecture \\nto other large language models (LLMs) like GPT. They are primarily based on the decoder-\\nonly architecture, meaning they focus on predicting the next token in a sequence given the \\npreceding tokens.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 38, 'page_label': '39'}, page_content='Foundational Large Language Models & Text Generation\\n39\\nFebruary 2025\\nMeta has released several key versions of Llama. The original Llama 1 models came in various \\nsizes, from 7B to 65B parameters, and were notable for their strong performance compared \\nto other open-source models of similar size. Llama 234 represented a major advancement, \\nfeaturing a larger context window extended to 4096 tokens for handling longer texts, and \\nimportantly, it was fine-tuned for chat applications, significantly improving its conversational \\nabilities. Llama 2 was offered in 7B, 13B, and 70B parameter versions and, unlike Llama 1, was \\nreleased with a license allowing commercial use. Llama 3 builds upon these advancements \\nwith enhanced performance across reasoning, coding, and general knowledge, and is \\nexpected to include a wider range of sizes. A key focus of Llama 3 is increased safety, with \\nefforts to reduce harmful outputs through improved training and alignment techniques. Llama \\n2 is a family of pretrained and fine-tuned LLMs, ranging from 7B to 70B parameters, with \\nimprovements like a 40% larger training dataset, doubled context length, and grouped-query \\nattention. The fine-tuned version, Llama 2-Chat, excels in dialogue. The next generation, \\nLlama 3.2, includes multilingual text-only models and vision LLMs, with quantized versions for \\non-device deployment. Llama 3.2 uses grouped-query attention and a 128K token vocabulary.\\nMixtral\\nDeveloped by Mistral AI35, Mixtral 8x7B is a Sparse Mixture of Experts (SMoE) model. While its \\ntotal parameter count is 47B, it utilizes only 13B active parameters per token during inference, \\nleading to faster inference and higher throughput. This model excels in mathematics, code \\ngeneration, and multilingual tasks, often outperforming LLaMA 2 70B in these domains. \\nMixtral also supports a 32k token context length, enabling it to handle significantly longer \\nsequences. Its instruction-tuned version, Mixtral 8x7B- Instruct, surpasses several closed-\\nsource models on human evaluation benchmarks. Mistral makes several of its models \\navailable as open source under the Apache 2.0 license, emphasizing open access to model \\nweights. In addition, Mistral offers a range of models through its API, providing various sizes \\nand capabilities to suit diverse requirements.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 39, 'page_label': '40'}, page_content='Foundational Large Language Models & Text Generation\\n40\\nFebruary 2025\\nOpenAI O1\\nOpenAI’s new “o1” series represents a significant advancement in models, focusing on \\ncomplex reasoning abilities honed through reinforcement learning. These models employ an \\ninternal “chain-of-thought” process, engaging in extensive deliberation before generating \\na response. This deliberate approach results in exceptional performance on challenging \\nscientific reasoning tasks. Benchmarks demonstrate their proficiency: o1 models achieve \\nan 89th percentile ranking on Codeforces programming competitions, score within the top \\n500 nationally on the AIME (a USA Math Olympiad qualifier), and surpass PhD-level human \\naccuracy on a comprehensive physics, biology, and chemistry benchmark (GPQA). The API \\noffers two variants: o1: The flagship model, optimized for tackling difficult problems that \\nrequire broad, general world knowledge, and  o1-mini: A faster, more cost-effective version, \\nexcelling in domains like coding, mathematics, and scientific tasks where deep specialized \\nknowledge is more critical than extensive general knowledge.\\nDeepSeek\\nDeepSeek has demonstrated that competitive reasoning performance, comparable to \\nOpenAI’s “o1” series, can be achieved through a novel reinforcement learning approach, \\neven without relying on extensive labeled data. This is exemplified by their DeepSeek-R1-\\nZero model, trained purely with RL. Traditional RL methods for LLMs often depend on a \\n“critic” model, trained on labeled data, to provide feedback. DeepSeek’s innovation, called \\nGroup Relative Policy Optimization (GRPO), eliminates this critic. Instead, GRPO uses a set \\nof predefined rules (assessing coherence, completeness, and fluency) to score the model’s \\noutputs across multiple rounds. The model learns by comparing its performance against the \\ngroup average, effectively learning from its own “self-play” without explicit human-provided'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 40, 'page_label': '41'}, page_content='Foundational Large Language Models & Text Generation\\n41\\nFebruary 2025\\nlabels. This pure-RL approach, while successful in achieving high reasoning scores (matching \\n“o1” on the AIME 2024 mathematics competition), initially resulted in outputs with poor \\nreadability and language mixing.\\nTo address these shortcomings, DeepSeek developed a multi-stage training process for \\ntheir DeepSeek-R1 model. This process begins with supervised fine-tuning (SFT) on a small \\n“cold start” dataset, providing a basic foundation of language understanding. Next, pure-RL \\n(using GRPO) is applied to enhance reasoning abilities, similar to the R1-Zero model. Critically, \\nnear the end of the RL phase, rejection sampling (e.g. filtering)  is employed. The model \\ngenerates multiple outputs, and only the best, according to the GRPO rules, are selected. \\nThis creates a high-quality “synthetic” dataset generated by the model itself. This synthetic \\ndata is then combined with supervised data from the original base model (covering areas \\nlike writing and factual knowledge). A final round of fine-tuning and further RL is performed, \\nleveraging both the synthetic and supervised data, refining the model’s overall performance \\nand generalization capabilities. This multi-stage approach leverages the strengths of each \\ntraining method: the initial SFT provides a basic linguistic foundation; pure-RL fosters strong \\nreasoning skills; rejection sampling creates high-quality training data; and the final SFT \\nand RL steps ensure a polished, well-rounded model. The result is the DeepSeek-R1 model \\nthat matches or exceeds the o1 model in many areas. Chain-of-thought (CoT) reasoning at \\ninference time is intrinsically linked to this RL-based training. The model learns to generate \\nintermediate reasoning steps during training, which are essential for its strong performance \\non complex tasks at inference. Despite providing model weights, DeepSeek’s models are \\neffectively closed-source due to the lack of transparency regarding training data, processing \\nscripts, and data curation methods.\\nOther open models\\nThe landscape of open LLMs is rapidly evolving, with a growing number of models where \\nboth the code and pre-trained weights are publicly accessible. Below we highlight some of \\nthe known examples:'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 41, 'page_label': '42'}, page_content='Foundational Large Language Models & Text Generation\\n42\\nFebruary 2025\\n• Qwen 1.536: This LLM series from Alibaba comes in six sizes: 0.5B, 1.8B, 4B, 7B, 14B, and \\n72B. Qwen 1.5 models uniformly support a context length of up to 32k tokens and show \\nstrong performance across various benchmarks. Notably, Qwen 1.5-72B outperforms \\nLLaMA2-70B on all evaluated benchmarks, demonstrating exceptional capabilities in \\nlanguage understanding, reasoning, and math.\\n• Yi37: Created by 01.AI, the Yi model family includes 6B and 34B base models pre-trained \\non a massive 3.1 trillion token English and Chinese dataset. Yi emphasizes data quality \\nthrough rigorous cleaning and filtering processes. The 34B model achieves performance \\ncomparable to GPT-3.5 on many benchmarks and can be efficiently served on consumer-\\ngrade GPUs with 4-bit quantization. Yi also offers extensions like a 200k context model, a \\nvision-language model (Yi-VL), and a depth-upscaled 9B model.\\n• Grok 3: Developed by xAI, Grok-3 is released in Grok 3 (Think) and Grok 3 mini (Think). \\nBoth models were trained using reinforcement learning. Grok 3 (Think) learned to refine its \\nproblem-solving strategies, correct errors through backtracking, simplify steps, and utilize \\nthe knowledge it picked up during pretraining. With a context window of 1 million tokens its \\n8 times larger than previous Grok models.\\nThe pace of innovation with LLMs has been rapid and shows no signs of slowing down. There \\nhave been many contributions to the field in both the academic and commercial settings. \\nWith over 20,000 papers published about LLMs in arxiv.org it is impossible to name all \\nof the models and teams that have contributed to the development of LLMs. However, an \\nabbreviated list of open models of interest could include EleutherAI’s GPT-NeoX and GPT-J, \\nStanford’s Alpaca, Vicuna from LMSYS, Grok from xAI, Falcon from TII, PHI from Microsoft, \\nNVLM from Nvidia, DBRX from Databricks, Qwen from Alibaba, Yi from 01.ai , Llama from \\nMeta mentioned above and many others. Some of notable companies developing commercial \\nfoundation LLM models include Anthropic, Cohere, Character.ai, Reka, AI21, Perplexity, xAI \\nand many others in addition to Google and OpenAI mentioned in previous sections. It is \\nimportant when using a model to confirm that the license is appropriate for your use case as \\nmany models are provided with very specific terms of use.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 42, 'page_label': '43'}, page_content='Foundational Large Language Models & Text Generation\\n43\\nFebruary 2025\\nComparison\\nIn this section, we observed how transformer-based language models have evolved. They \\nstarted as encoder-decoder architectures with hundreds of millions of parameters trained \\non hundreds of millions of tokens, and have grown to be massive decoder-only architectures \\nwith billions of parameters and trained on trillions of tokens. Table 1 shows how the \\nimportant hyperparameters for all the models discussed in this whitepaper have evolved \\nover time. The scaling of data and parameters has not only improved the performance of \\nLLMs on downstream tasks, but has also resulted in emergent behaviors and zero- or few-\\nshot generalizations to new tasks. However, even the best of these LLMs still have many \\nlimitations. For example, they are not good at engaging in human-like conversations, their \\nmath skills are limited, and they might not be aligned with human ethics (e.g., they might be \\nbiased or generate toxic responses). In the next section, we learn how a lot of these issues \\nare being addressed.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 43, 'page_label': '44'}, page_content='Foundational Large Language Models & Text Generation\\n44\\nFebruary 2025\\nModel\\nAttention\\n(2017)\\nGPT \\n(2018)\\nGPT-2\\n(2019)\\nGPT-3\\n(2020)\\nLaMDA  \\n(2021)\\nGopher\\n(2021)\\nChinchilla\\n(2022)\\nOptimizer ADAM ADAM ADAM ADAM ADAM ADAM ADAM-W\\n# Parameters 213M 117M 1.5B 175B 137B 280B 70B\\nVocab size ~37K ~40K ~50K ~50K ~32K ~32K ~32K\\nEmbedding \\ndimension 1024 768 1600 12288 8192 16384 8192\\nKey dimension 64 64 64 128 128 128 128\\n# heads (H) 16 12 25 96 128 128 64\\n# encoder \\nlayers 6 N/A N/A N/A N/A N/A N/A\\n# decoder \\nlayers 6 12 48 96 64 80 80\\nFeed forward \\ndimension 4 * 1024 4 * 768 4 * 1600 4 * 12288 8 * 8192 4 * 16384 4 * 8192\\nContext Token \\nSize N/A 512 1024 2048 N/A 2048 2048\\nPre-Training \\ntokens ~160MA ~1.25BA ~10B ~300B ~168B ~300B ~1.4T\\nTable 1. Important hyperparameters for transformers-based large language models\\nA. This number is an estimate based on the reported size of the dataset.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 44, 'page_label': '45'}, page_content='Foundational Large Language Models & Text Generation\\n45\\nFebruary 2025\\nFine-tuning large language models\\nLarge language models typically undergo multiple training stages. The first stage, often \\nreferred to as pre-training, is the foundational stage where an LLM is trained on large, \\ndiverse, and unlabelled text datasets where it’s tasked to predict the next token given the \\nprevious context. The goal of this stage is to leverage a large, general distribution of data \\nand to create a model that is good at sampling from this general distribution. After language \\nmodel pretraining, the resulting LLM usually demonstrates a reasonable level of language \\nunderstanding and language generation skills across a variety of different tasks which \\nare typically tested through zero-shot or few-shot prompting (augmenting the instruction \\nwith a few examples / demonstrations). Pretraining is the most expensive in terms of time \\n(from weeks to months depending on the size of the model) and the amount of required \\ncomputational resources, (GPU/TPU hours).\\nAfter training, the model can be further specialized via fine-tuning, typically called \\ninstruction-tuning or simply supervised fine-tuning (SFT). SFT involves training an LLM on a \\nset of task-specific demonstration datasets where its performance is also measured across \\na set of domain-specific tasks. The following are some examples of behaviors that can be \\nimproved using fine-tuning:\\n• Instruction-tuning/instruction following: The LLM is provided as input an instruction to \\nfollow which might include summarizing a piece of text, writing a piece of code, or writing \\na poem in a certain style.17\\n• Dialogue-tuning: This is a special case of instruction tuning where the LLM is fine-tuned \\non conversational data in the form of questions and responses. This is often called \\nmulti-turn dialogue.39'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 45, 'page_label': '46'}, page_content='Foundational Large Language Models & Text Generation\\n46\\nFebruary 2025\\n• Safety tuning: This is crucial for mitigating risks associated with bias, discrimination, and \\ntoxic outputs. It involves a multi-pronged approach encompassing careful data selection, \\nhuman-in-the-loop validation, and incorporating safety guardrails. Techniques like \\nreinforcement learning with human feedback (RLHF)40 enable the LLM to prioritize safe \\nand ethical responses.\\nFine-tuning is considerably less costly and more data efficient compared to pre-training. \\nNumerous techniques exist to optimize the costs further which are discussed later in \\nthis whitepaper.\\nSupervised fine-tuning \\nAs mentioned in the previous section, SFT is the process of improving an LLM’s performance \\non a specific task or set of tasks by further training it on domain-specific, labeled data. The \\ndataset is typically significantly smaller than the pre-training datasets, and is usually human-\\ncurated and of high quality. \\nIn this setting, each data point consists of an input (prompt) and a demonstration (target \\nresponse). For example, questions (prompt) and answers (target response), translations from \\none language (prompt) to another language (target response), a document to summarize \\n(prompt), and the corresponding summary (target response). \\nIt’s important to note that, while fine-tuning can be used to improve the performance on \\nparticular tasks as mentioned above, it can also serve the purpose of helping the LLM \\nimprove its behavior to be safer, less toxic, more conversational, and better at following \\ninstructions.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 46, 'page_label': '47'}, page_content='Foundational Large Language Models & Text Generation\\n47\\nFebruary 2025\\nReinforcement learning from human feedback\\nTypically, after performing SFT, a second stage of fine-tuning occurs which is called \\nreinforcement learning from human feedback (RLHF). This is a very powerful fine-tuning \\ntechnique that enables an LLM to better align with human-preferred responses (i.e. making \\nits responses more helpful, truthful, safer, etc.). \\nFigure 8. An example RLHF procedure \\nIn contrast to SFT, where an LLM is only exposed to positive examples (e.g. high-quality \\ndemonstration data), RLHF makes it possible to also leverage negative outputs thus \\npenalizing an LLM when it generates responses that exhibit undesired properties. Penalizing \\nnegative output makes it less likely to generate unhelpful or unsafe responses. \\nTo leverage RLHF, a reward model (RM) typically needs to be trained with a procedure similar \\nto that in Figure 8. An RM is usually initialized with a pretrained transformer model, often also \\none that is SFT. Then it is tuned on human preference data which is either single sided (with a \\nprompt, response and a score) or composed of a prompt and a pair of responses along with'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 47, 'page_label': '48'}, page_content='Foundational Large Language Models & Text Generation\\n48\\nFebruary 2025\\na preference label indicating which of the two responses was preferred. For example, given \\ntwo summaries, A and B, of the same article, a human rater selects a preferred summary \\n(relying on the detailed guidance). We refer to the provided preference labels as human \\nfeedback. Preferences can be in the binary form (e.g. ‘good’ or ‘bad’), on the Likert scale42, \\nrank order when more than 2 candidates are evaluated, or a more detailed assessment of the \\nsummary quality. The preference signal can also incorporate many dimensions that capture \\nvarious aspects that define a high quality response, e.g., as safety, helpfulness, fairness, and \\ntruthfulness. \\nFigure 8 shows a typical RLHF pipeline where a Reward model is initialized and finetuned on \\npreference pairs. Once an RM has been trained, it’s then used by a Reinforcement Learning \\n(RL)43 policy gradient algorithm, which further finetunes a previously instruction-tuned LLM to \\ngenerate responses that are better aligned with human preferences. \\nTo better scale RLHF, RL from AI Feedback (RLAIF)44 leverages AI feedback instead of human \\nfeedback to generate preference labels. It’s also possible to remove the need for training \\nRLHF by leveraging approaches such as direct preference optimization (DPO).45 Both RLHF \\nand RLAIF can be used on Google Cloud.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 48, 'page_label': '49'}, page_content='Foundational Large Language Models & Text Generation\\n49\\nFebruary 2025\\nParameter Efficient Fine-Tuning\\nBoth SFT and RLHF are still very costly in terms of compute time and accelerators required, \\nespecially when full-fine tuning entire LLMs on the orders of billions of parameters. Luckily, \\nthere are some really useful and effective techniques that can make fine-tuning significantly \\ncheaper and faster compared to pre-training and full fine-tuning. One such family of \\nmethods is parameter efficient fine-tuning (PEFT) techniques. \\nAt a high-level, PEFT approaches append a significantly smaller set of weights (e.g., on the \\norder of thousands of parameters) that are used to ‘perturb’ the pre-trained LLM weights. \\nThe perturbation has the effect of fine-tuning the LLM to perform a new task or set of tasks. \\nThis has the benefit of training a significantly smaller set of weights, compared to traditional \\nfine-tuning of the entire model. \\nSome common PEFT techniques include the adapter, low-rank adaptation, and \\nsoft prompting:\\n• Adapter-based fine-tuning46 employs small modules, called adapters, to the pre-\\ntrained model. Only the adapter parameters are trained, resulting in significantly fewer \\nparameters than traditional SFT. \\n• Low-Rank Adaptation (LoRA)47 tackles efficiency differently. It uses two smaller matrices \\nto approximate the original weight matrix update instead of fine-tuning the whole LLM. \\nThis technique freezes the original weights and trains these update matrices, significantly \\nreducing resource requirements with minimum additional inference latency. Additionally, \\nLoRA has improved variants such as QLoRA,48 which uses quantized weights for even \\ngreater efficiency. A nice advantage of LoRA modules is that they can be plug-and-play, \\nmeaning you can train a LoRA module that specializes in one task and easily replace it with \\nanother LoRA module trained on a different task. It also makes it easier to transfer the \\nmodel since assuming the receiver has the original matrix, only the update matrices need \\nto be provided.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 49, 'page_label': '50'}, page_content='Foundational Large Language Models & Text Generation\\n50\\nFebruary 2025\\n• Soft prompting49 is a technique for conditioning frozen large language models with \\nlearnable vectors instead of hand-crafted text prompts. These vectors, called soft \\nprompts, are optimized on the training data and can be as few as five tokens, making them \\nparameter-efficient and enabling mixed-task inference. \\nFor most tasks, full fine-tuning is still the most performant, followed by LoRA and Soft \\nprompting, but the order is reversed when it comes to cost. All three approaches are more \\nmemory efficient than traditional fine-tuning and achieve comparable performance.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 50, 'page_label': '51'}, page_content='Foundational Large Language Models & Text Generation\\n51\\nFebruary 2025\\nPython\\n# Before you start run this command:\\n# pip install --upgrade --user --quiet google-cloud-aiplatform\\n# after running pip install make sure you restart your kernel\\nimport vertexai\\nfrom vertexai.generative_models import GenerativeModel\\nfrom vertexai.preview.tuning import sft\\n# TODO : Set values as per your requirements\\n# Project and Storage Constants\\nPROJECT_ID = ‘<project_id>’\\nREGION = ‘<region>’\\nvertexai.init(project=PROJECT_ID, location=REGION)\\n# define training & eval dataset.\\nTRAINING_DATASET = ‘gs://cloud-samples-data/vertex-ai/model-evaluation/\\npeft_train_sample.jsonl’\\n# set base model and specify a name for the tuned model\\nBASE_MODEL = ‘gemini-1.5-pro-002’\\nTUNED_MODEL_DISPLAY_NAME = ‘gemini-fine-tuning-v1’\\n# start the fine-tuning job\\nsft_tuning_job = sft.train(\\n   source_model=BASE_MODEL,\\n   train_dataset=TRAINING_DATASET,\\n   # # Optional:\\n   tuned_model_display_name=TUNED_MODEL_DISPLAY_NAME,\\n)\\n# Get the tuning job info.\\nsft_tuning_job.to_dict()\\n# tuned model endpoint name\\ntuned_model_endpoint_name = sft_tuning_job.tuned_model_endpoint_name\\n# use the tuned model\\ntuned_genai_model = GenerativeModel(tuned_model_endpoint_name)\\nprint(tuned_genai_model.generate_content(contents=’What is a LLM?’))\\nSnippet 1. SFT fine tuning on Google cloud'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 51, 'page_label': '52'}, page_content='Foundational Large Language Models & Text Generation\\n52\\nFebruary 2025\\nUsing large language models\\nPrompt engineering and sampling techniques have a strong influence on the performance of \\nLLMs. Prompt engineering is the process of designing and refining the text inputs (prompts) \\nthat you feed into an LLM to achieve desired and relevant outputs. Sampling techniques \\ndetermine the way in which output tokens are chosen and influence the correctness, \\ncreativity and diversity of the resulting output. We next discuss different variants of prompt \\nengineering and sampling techniques as well as touch on some important parameters that \\ncan have a significant impact on LLM performance.\\nPrompt engineering \\nLLMs are very powerful, but they still need guidance to unleash their full potential. Prompt \\nengineering is a critical component in guiding an LLM to yield desired outputs. This might \\ninclude grounding the model to yield factual responses or unleashing the creativity of the \\nmodel to tell a story or write a song. Examples of prompt engineering include providing \\nclear instructions to the LLM, giving examples, using keywords, and formatting to emphasize \\nimportant information, providing additional background details etc. \\nYou will often hear the terms zero-shot, few-shot, and chain-of-thought prompting in the \\ncontext of prompt engineering. We define these terms below: \\n• Few-shot prompting: This is when you provide the LLM with a task description, as well \\nas a few (e.g. three to five) carefully chosen examples, that will help guide the LLM’s \\nresponse. For example, you might provide the model with the name of a few countries \\nand their capital cities, then ask it to generate the capital for a new country that isn’t in \\nthe examples.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 52, 'page_label': '53'}, page_content='Foundational Large Language Models & Text Generation\\n53\\nFebruary 2025\\n• Zero-shot prompting: This is when you provide the LLM directly with a prompt with \\ninstructions. You usually give the LLM a task description and the LLM relies heavily on its \\nexisting knowledge to output the correct response. This requires no additional data or \\nexamples, hence the name ‘Zero-shot’ but can be less reliable than few-shot prompting.\\n• Chain-of-thought prompting: This technique aims to improve performance on complex \\nreasoning tasks. Rather than simply asking the LLM a question, you provide a prompt \\nthat demonstrates how to solve similar problems using step-by-step reasoning. The \\nLLM then generates its own chain of thought for the new problem, breaking it down into \\nsmaller steps and explaining its reasoning. Finally, it provides an answer based on its \\nreasoning process.\\nPrompt engineering is an active area of research.\\nSampling Techniques and Parameters\\nA variety of sampling techniques can be employed to determine how the model chooses \\nthe next token in a sequence. They are essential for controlling the quality, creativity, and \\ndiversity of the LLM’s output. The following is a breakdown of different sampling techniques \\nand their important parameters:\\n• Greedy search50: Selects the token with the highest probability at each step. This is the \\nsimplest option but it can lead to repetitive and predictable outputs.\\n• Random sampling:50 Selects the next token according to the probability distribution, where \\neach token is sampled proportionally to its predicted probability. This can produce more \\nsurprising and creative text, but also a higher chance of nonsensical output.\\n• Temperature sampling:50 Adjusts the probability distribution by a temperature parameter. \\nHigher temperatures promote diversity, lower temperatures favor high-probability tokens.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 53, 'page_label': '54'}, page_content='Foundational Large Language Models & Text Generation\\n54\\nFebruary 2025\\n• Top-K sampling: Randomly samples from the top K most probable tokens. The value of K \\ncontrols the degree of randomness.\\n• Top-P sampling (nucleus sampling):51 Samples from a dynamic subset of tokens whose \\ncumulative probability adds up to P. This allows the model to adapt the number of potential \\ncandidates depending on its confidence, favoring more diversity when uncertain and \\nfocusing on a smaller set of highly probable words when confident.\\n• Best-of-N sampling: Generates N separate responses and selects the one deemed best \\naccording to a predetermined metric (e.g., a reward model or a logical consistency check). \\nThis is particularly useful for short snippets or situations where logic and reasoning \\nare key.\\nBy combining prompt engineering with sampling techniques and correctly calibrated \\nhyperparameters, you can greatly influence the LLM’s response, making it more relevant, \\ncreative, and consistent for your specific needs.\\nUntil now, we have seen the various types of LLM architectures, their underlying technology, \\nas well as the approaches used to train, tune, and adapt these models for various tasks. Let’s \\nnow look at some key research about how the decoding process in LLMs can be sped up \\nconsiderably to generate faster responses.\\nTask-based Evaluation\\nThe emergence of LLMs has reduced the obstacles to building AI applications, but moving \\nfrom MVP to production introduces challenges such as prompt engineering, model selection, \\nand performance monitoring. A tailored evaluation framework is essential for navigating \\nLLM application development by validating functionality and user experience, identifying'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 54, 'page_label': '55'}, page_content='Foundational Large Language Models & Text Generation\\n55\\nFebruary 2025\\npotential issues, facilitating communication about capabilities, and establishing a roadmap \\nfor improvement. For building a tailored evaluation framework, application builders need to \\nprovide their own evaluation data, development context, a definition of good performance.  \\n• Evaluation data: Public leaderboards that showcase LLM capabilities often fall short for \\napplication developers who require a more tailored approach. A dedicated evaluation \\ndataset that mirrors the expected production traffic as closely as possible is needed. \\nDuring prototyping this can be a manually curated dataset, one that is continuously \\nenriched with real user interactions, production logs, and synthetically generated data to \\ntest specific scenarios.\\n• Development Context: The evaluation should extend beyond just the model’s output \\nto analyze the entire system, including components like data augmentation (e.g., \\nRetrieval Augmented Generation or RAG) and agentic workflows. This approach ensures \\nunderstanding of how all components interact and contribute to the application’s \\noverall performance.\\n• Definition of “Good”: Traditional metrics that prioritize matching a single “correct” \\nanswer can unfairly penalize unexpected solutions. When working with LLMs, we can \\naddress this by moving beyond similarity to ground truth as a definition of good, but \\nrather establish dataset level criteria that reflect desired business outcomes or even \\nrubrics that capture the core elements of the desired outputs depending on the input user \\ninstructions. \\nApplication builders can evaluate LLM performance using three methods:\\n• Traditional Evaluation Methods: Similar to evaluating predictive models, these methods \\nuse quantitative metrics to compare model outputs to ideal responses, aiming to offer \\nobjective insights. However, they may penalize creative or unexpected outputs, limiting \\ntheir effectiveness for generative tasks that have multiple possible solutions.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 55, 'page_label': '56'}, page_content='Foundational Large Language Models & Text Generation\\n56\\nFebruary 2025\\n• Human Evaluation: Considered the gold standard, human judgment provides nuanced \\nassessment of complex generative outputs.\\n• LLM-Powered Autoraters: LLM-powered autoraters try to mimic human judgment, \\noffering scalable and efficient evaluations. Unlike computation-based methods, they \\ncan operate with or without reference data. A basic setup involves providing the task, \\ncriteria, and candidate responses (with optional references), which the autorater uses to \\ngenerate and parse LLM output for final evaluation results. In addition to the final outputs, \\nautoraters can provide rationales to explain a given decision to the user. While generative \\nmodels are common autoraters, reward and discriminative models are also used. Crucially, \\nlike any measurement tool, autoraters require calibration. Meta-evaluation, i.e. comparing \\nautorater outputs to human judgments, ensures the autorater aligns with desired \\npreferences. This calibration typically involves agreement in terms of model preference \\nor correlation measures, tailored to the evaluation task. In this meta-evaluation, it is \\nimportant to keep in mind potential limitations of autorater models.69\\nWhile the basic setup described above focuses on providing autoraters with fixed criteria \\nthat can be used to evaluate an entire dataset of examples, approaches are emerging to \\nleverage rubrics and multi-step processes in order to obtain interpretable evaluation metrics. \\nAt a high level, an LLM breaks an example down into multiple evaluation subtasks and then \\nevaluates each subtask to give an interpretable, detailed report for this example. As the \\ntask has been broken down, domain-specialized models can be leveraged for specific tasks \\nto improve reliability if necessary. Results are then aggregated for a given example to yield \\nan overall score, or across related subtasks to evaluate how well a model performs along a \\nparticular axis. This setup is especially useful in media generation, where different examples \\nmay require vastly different skills (e.g. object generation vs text generation) and a single \\nscore obfuscates how relative strengths and weaknesses of different models contribute to \\nthe final result.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 56, 'page_label': '57'}, page_content='Foundational Large Language Models & Text Generation\\n57\\nFebruary 2025\\nAccelerating inference\\nThe scaling laws for LLMs which were initially explored by the Kaplan et al.24 study continue \\nto hold today. Language models have been consistently increasing in size and this has been \\na direct contributor to the vast improvement in these models’ quality and accuracy over the \\nlast few years. As increasing the number of parameters has improved the quality of LLMs it \\nhas also increased the computational resources needed to run them. Numerous approaches \\nhave been used to try and improve the efficiency of LLMs for different tasks as developers \\nare incentivized to reduce cost and latency for model users. Balancing the expense of \\nserving a model in terms of time, money, energy is known as the cost-performance tradeoff \\nand often needs adjusting for particular use cases.\\nTwo of the main resources used by LLMs are memory and computation. Techniques for \\nimproving the efficiency or speed of inference focus primarily on these resources. The \\nspeed of the connection between memory and compute is also critical, but usually hardware \\nconstrained.  As LLMs have grown in size 1000x from millions to billions of parameters. \\nAdditional parameters increase both the size of memory required to hold the model and \\ncomputations needed to produce the model results.\\nWith LLMs being increasingly adopted for large-scale and low-latency use cases, finding \\nways to optimize their inference performance has become a priority and an active research \\ntopic with significant advancements. We will explore a number of methods and a few \\ntradeoffs for accelerating inference.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 57, 'page_label': '58'}, page_content='Foundational Large Language Models & Text Generation\\n58\\nFebruary 2025\\nTrade offs\\nMany of the high yielding inference optimisation methods mandate trading off a number of \\nfactors, this can be tweaked on a case-by-case basis allowing for tailored approaches to \\ndifferent inference use cases and requirements. A number of the optimization methods we \\nwill discuss later fall somewhere on the spectrum of these tradeoffs. \\nTrading off one factor against the other (e.g. latency vs quality or cost) doesn’t mean that \\nwe’re completely sacrificing that factor, it just means that we’re accepting what might be \\na marginal degradation in quality, latency or cost for the benefit of substantially improving \\nanother factor.\\nThe Quality vs Latency/Cost Tradeoff\\nIt is possible to improve the speed and cost of inference significantly through accepting \\nwhat might be marginal to negligible drops in the model’s accuracy. One  example of this \\nis using a smaller model to perform the task. Another example is quantisation where we \\ndecrease the precision of the model’s parameters thereby leading to faster and less memory \\nintensive calculations.\\nOne important distinction when approaching this trade-off is between the theoretical \\npossibility of a quality loss versus the practical capability of the model to perform the desired \\ntask. This is use case specific and exploring it will often lead to significant speedups without \\nsacrificing quality in a meaningful or noticeable way. For example, if the task we want the \\nmodel to perform is simple, then a smaller model or a quantised one will likely be able to \\nperform this task well. Reduction in parametric capacity or precision does not automatically \\nmean that the model is less capable at that specific task.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 58, 'page_label': '59'}, page_content='Foundational Large Language Models & Text Generation\\n59\\nFebruary 2025\\nThe Latency vs Cost Tradeoff\\nAnother name for this tradeoff is the latency vs throughput tradeoff. Where throughput refers \\nto the system’s ability at handling multiple requests efficiently. Better throughput on the same \\nhardware means that our LLM inference cost is reduced, and vice versa.\\nMuch like traditional software systems, there are often multiple opportunities to tradeoff \\nlatency against the cost of LLM inference. This is an important tradeoff since LLM inference \\ntends to be the slowest and most expensive component in the entire stack; balancing latency \\nand cost intentionally is key to making sure we tailor LLM performance to the product or use \\ncase it’s being used in. An example would be bulk inference use cases (e.g. offline labeling) \\nwhere cost can be a more important factor than the latency of any particular request. On the \\nother hand, an LLM chatbot product will place much higher importance on request latency.\\nNow that we’ve covered some of the important tradeoffs to consider when optimizing \\ninference, let’s examine some of the most effective inference acceleration techniques. As \\ndiscussed in the tradeoffs section, some optimization techniques can have an impact on the \\nmodel’s output. Therefore we will split the methods into two types: output-approximating \\nand output-preserving.\\nAs of this writing, Gemini 2.0 Flash Thinking offers an unparalleled balance of quality, as \\nmeasured by its ELO score, and affordability, with a cost per million tokens that is ten times \\nlower than comparable models; its position on a quality-versus-cost graph (where further \\nup and to the right is superior) demonstrates its transformative development. Moreover, the \\npicture highlights the rapid advancements in reasoning and thinking capabilities within the AI \\nfield, with a 27-fold improvement observed in the last three months.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 59, 'page_label': '60'}, page_content='Foundational Large Language Models & Text Generation\\n60\\nFebruary 2025\\nOutput-approximating methods\\nQuantization\\nLLMs are fundamentally composed of multiple numerical matrices (a.k.a the model weights). \\nDuring inference, matrix operations are then applied to these model weights to produce \\nnumerical outputs (a.k.a activations). Quantization is the process of decreasing the numerical \\nprecision in which weights and activations are stored, transferred and operated upon. The \\ndefault representation of weights and activations is usually 32 bits floating numbers, with \\nquantization we can drop the precision to 8 or even 4 bit integers. \\nQuantization has multiple performance benefits, it reduces the memory footprint of \\nthe model, allowing to fit larger models on the same hardware, it also reduces the \\ncommunication overhead of weights and activations within one chip and across chips in \\na distributed inference setup- therefore speeding up inference as communication is a \\nmajor contributor to latency. In addition, decreasing the precision of weights/activations \\ncan enable faster arithmetic operations on these models as some accelerator hardware \\n(e.g. TPUs/GPUs) natively supports faster matrix multiplication operations for some lower \\nprecision representations.\\nQuantization’s impact on quality can be very mild to non-existent depending on the use \\ncase and model.  Further, in cases where quantisation might introduce a quality regression, \\nthat regression can be small compared to the performance gain, therefore allowing for an \\neffective Quality vs Latency/Cost Tradeoff. For example, Benoit Jacob et al.55 reported a 2X \\nspeed-up for a 2% drop in accuracy for the FaceDetection task on MobileNet SSD.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 60, 'page_label': '61'}, page_content='Foundational Large Language Models & Text Generation\\n61\\nFebruary 2025\\nQuantization can be either applied as an inference-only operation, or it can be incorporated \\ninto the training (referred to as Quantisation Aware Training QAT). QAT is generally \\nconsidered to be a more resilient approach as the model is able to recover some of the \\nquantisation-related quality losses during training. To make sure we get the best cost/quality \\ntradeoff, we tweak the quantization strategy (e.g. select different precisions for weights \\nvs activations) and the granularity in which we apply quantisation to Tensors (e.g. channel \\nor group-wise58).\\nDistillation\\nUsing a smaller model to perform a task is one of the most efficient inference optimization \\ntechniques, however, smaller models can demonstrate significant regressions on quality \\ncompared to their larger counterparts.\\nDistillation is a set of training techniques that targets improving the quality of a smaller model \\n(the student) using a larger model (the teacher). This method can be effective because larger \\nmodels outperform smaller ones even if both are trained on the same data, mainly due to \\nparametric capacity and training dynamics. The gap in performance continues as the training \\ndataset grows as illustrated by Figure 9.\\nIt is worth noticing that even at low volumes of training data, large models can already \\ndemonstrate better performance than the correspondingly trained smaller models, this fact \\nleads us to the first variant of distillation which is referred to as data distillation or model \\ncompression.56 We use a large model which was trained on the data we have to generate \\nmore synthetic data to train the smaller student model, the increase in data volume will help \\nmove the the student further along the quality line compared to only training on the original \\ndata. Synthetic data needs to be approached carefully as it needs to be of high quality and \\ncan lead to negative effects otherwise.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 61, 'page_label': '62'}, page_content='Foundational Large Language Models & Text Generation\\n62\\nFebruary 2025\\nFigure 9. An illustration of the performance of models of various sizes as a function of the training \\ndataset’s size\\nOther distillation techniques attempt to bring the student model closer to the teacher \\non a more granular level than just synthetic data generation. One prominent technique is \\nknowledge distillation57, in this approach we attempt to align the output token distribution \\nof the student model to that of the teacher’s, this can be much more sample efficient than \\ndata distillation. On-policy distillation59 is another technique that leverages feedback from \\nthe teacher model on each sequence generated by the student in a reinforcement learning \\nsetup. \\nOutput-preserving methods\\nThese methods are guaranteed to be quality neutral, they cause no changes to the model \\noutput which often makes them obvious first steps to optimize inference before facing the \\nmore nuanced tradeoffs of the approximating methods'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 62, 'page_label': '63'}, page_content='Foundational Large Language Models & Text Generation\\n63\\nFebruary 2025\\nFlash Attention\\nScaled Dot-product Attention, which is the predominant attention mechanism in the \\ntransformer architecture, is a quadratic operation on the input length. Optimizing the self-\\nattention calculation can bring significant latency and cost wins.\\nFlash Attention, introduced in by Tri Dao et al.62, optimizes the attention calculation by making \\nthe attention algorithm IO Aware, particularly trying to minimize the amount of data we move \\nbetween the slow HBM (high bandwidth memory) to the faster memory tier (SRAM/VMEM) in \\nTPUs and GPUs. When calculating attention, the order of operations is changed and multiple \\nlayers are fused so we can utilize the faster memory tiers as efficiently as possible.\\nFlash Attention is an exact algorithm, it maintains the numerical output of the attention \\ncomputation and can yield significant latency benefits due to reducing the IO overhead, Tri \\nDao et al.62 showed 2-4X latency improvements in the attention computation.\\nPrefix Caching\\nOne of the most compute intensive, and thus slowest, operations in LLM inference is \\ncalculating the attention key and value scores (a.k.a KV) for the input we’re passing to the \\nLLM, this operation is often referred to as prefill. The final output of prefill is what is termed \\nKV Cache which includes the attention key and value scores for each layer of the transformer \\nfor the entire input. This cache is vital during the decoding phase which produces the output \\ntokens, the KV cache allows us to avoid recalculating attention scores for the input on each \\nautoregressive decode step.\\nPrefix Caching refers to the process of caching the KV Cache itself between subsequent \\ninference requests in order to reduce the latency and cost of the prefill operation. The way \\nthe self-attention mechanism works makes reusing KV caches possible because tokens will'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 63, 'page_label': '64'}, page_content='Foundational Large Language Models & Text Generation\\n64\\nFebruary 2025\\nonly pay attention to tokens that came before them in the sequence. If there’s new input \\nbeing appended to input that the model has seen before, then we can potentially avoid \\nrecalculating the prefill for the older input.\\nFigure 10. An illustration of Prefix Caching in a chat scenario\\nFigure 10 illustrates how prefix caching works in a multi-turn scenario with a document \\nupload. On the first user turn, the prefill operation has to process the entire document \\ntherefor taking 500ms, the resulting KV cache is then stored so that on the second user turn, \\nwe can retrieve the cache directly from storage and avoid recomputing it for the long doc, \\ntherefore saving substantial amounts of compute and latency.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 64, 'page_label': '65'}, page_content='Foundational Large Language Models & Text Generation\\n65\\nFebruary 2025\\nPrefix caches can be stored either in memory or on disk and fetched on-demand. One \\nimportant consideration is making sure that the input structure/schema remains prefix-\\ncaching friendly, we should avoid changing the prefix in subsequent requests as that will \\ninvalidate the cache for all the tokens that follow For example, putting a fresh timestamp at \\nthe very beginning of each request will invalidate the cache completely as every subsequent \\nrequest will have a new prefix.\\nMany LLM use cases lend themselves naturally to prefix caching. For example, LLM Chatbots \\nwhere users will have a multi-turn conversation that can span 10s of 1000s of tokens and \\nwe can avoid recalculating the KV cache for the previous parts of the conversation. Large \\ndocument/code uploads is another use case where the artifact the user uploads will remain \\nunchanged from one request to the next. All that’s changing are the questions the user is \\nasking, so caching the KV cache for the document (especially for larger artifacts) can result \\nin significant latency and cost savings.\\nPrefix caching is available as a service called Context Caching on Google AI studio52 and  \\nVertex AI on Google Cloud53.\\nSpeculative Decoding\\nThe first phase of LLM inference, known as prefill, is compute bound due large matrix \\noperations on many tokens occurring in parallel. The second phase, known as decode, is \\ngenerally memory bound as tokens are auto-regressively decoded one at a time. \\nIt is not easy to naively use additional parallel compute capacity to speed up decode \\ngiven the  need to wait for the current token to be produced before we can calculate what \\nthe next token should be (as per the self-attention mechanism), the decode process is \\ninherently serial.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 65, 'page_label': '66'}, page_content='Foundational Large Language Models & Text Generation\\n66\\nFebruary 2025\\nSpeculative decoding (Leviathan at al.63) aims to overcome this limitation in decode by finding \\na way to utilize the spare compute capacity to make each decode step faster. The main idea \\nis to use a much smaller secondary model (often referred to as the drafter) to run ahead of \\nthe main model and predict more tokens. (e.g. 4 tokens ahead). This will happen very quickly \\nas the drafter is much faster and smaller than the main model. We then use the main model to \\nverify the hypotheses of the drafter in parallel for each of the 4 steps (i.e. the first token, the \\nfirst two tokens, the first 3 tokens and finally all 4 tokens), and we then select the accepted \\nhypothesis with the maximum number of tokens. For example:\\nFigure 11. An illustration of speculative decoding over 3 tokens\\nNote that the 3 main model steps run in parallel. And because we are not compute bound in \\ndecode, we can use the spare capacity to get much better decode latencies. In the example \\nabove, let’s say a single main model step needs 10ms, while the drafter needs 1ms. Without \\nspeculative decoding, we need 3 * 10ms = 30ms to produce the response, with speculative \\ndecoding, there’s only one main model step on the critical path due to parallelization, so we \\nneed 3 * 1ms + 10ms = 13ms. A significant latency improvement. This technique is completely'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 66, 'page_label': '67'}, page_content='Foundational Large Language Models & Text Generation\\n67\\nFebruary 2025\\nquality neutral, the main model will reject any tokens that it wouldn’t have predicted itself \\nin the first place, so the only thing speculative decoding does is run ahead and present \\nhypotheses that the main model can accept or reject in parallel.\\nOne important condition for speculative decoding to work effectively is that the drafter model \\nhas good levels of alignment with the main model, otherwise we won’t be able to accept any \\nof the tokens. So investing in the training quality of the drafter model is worthwhile to get \\nbetter latencies.\\nNow that we have seen some methods to make LLM generate their responses faster, let’s \\nlook at some examples of how these models can be applied to various tasks to get an idea \\nhow to use them.\\nBatching and Parallelization\\nMost of the optimization techniques we’ve discussed so far are specific to Machine Learning \\nand Transformer architecture in particular. However, much like any software system, there \\nare opportunities to improve throughput and latency through a combination of 1) batching \\nless compute-intensive operations (i.e. we can run multiple requests on the same hardware \\nsimultaneously to better utilize the spare compute) and 2) parallelizing the more compute-\\nintensive parts of the computations (i.e. we can divide the computation and split it amongst \\nmore hardware instances to get more compute capacity and therefore better latencies\\nBatching in LLMs is most useful on the decode side - as we explained in the Speculative \\nDecoding section, decode is not compute-bound and therefore there’s an opportunity \\nto batch more requests. We need to be careful that we batch computations in a way that \\nenables utilization of the spare capacity which is possible to do on accelerators (e.g. TPUs \\nand GPUs). We also need to make sure we remain within the memory limits, as decode is a'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 67, 'page_label': '68'}, page_content='Foundational Large Language Models & Text Generation\\n68\\nFebruary 2025\\nmemory intensive operations, batching more requests will put more pressure on the free \\nmemory available. Batching has become an important component in most high-throughput \\nLLM inference setups.\\nParallelization is also a widely used technique given the variety of opportunities in \\ntransformers for horizontal scaling across more hardware instances. There are multiple \\nparallelism techniques across the model input (Sequence parallelism) the model layers \\n(Pipeline parallelism), and within a single layer (Tensor parallelism). One of the most important \\nconsiderations for parallelism is the cost of communication and synchronization between \\nthe different shards that we distribute to other machines. Communication is a significant \\noverhead and can erode the benefits of adding more computational capacity if we’re not \\ncareful about which parallelization strategy to use. On the other hand, selecting the right \\nstrategy to balance the need for additional compute and the communication cost can yield \\nsignificant latency wins.\\nNow that we have seen some methods to make LLM generate their responses faster, let’s \\nlook at some examples of how these models can be applied to various tasks to get an idea \\nhow to use them.\\nApplications\\nLarge language models are revolutionizing the way we interact with and process information. \\nWith their unprecedented ability to understand context and generate content, they’re \\ntransforming numerous applications in the worlds of text, code, images, audio and video. \\nHere we collected a few examples of application areas, but the reader should keep in mind \\nthat this is not a comprehensive list and that many new ideas are emerging continuously \\nabout how to best utilize the capabilities of these new tools. For more information about \\noptimally building and deploying functioning applications based on the following mentioned \\nuse cases, refer to the subsequent whitepapers.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 68, 'page_label': '69'}, page_content='Foundational Large Language Models & Text Generation\\n69\\nFebruary 2025\\nIt is also very simple to generate text-based responses for your use case using either \\nthe Google Cloud Vertex AI SDK or the Developer focused AI studio. Snippet 3 shows \\ncode examples from these SDKs to generate responses to text prompts using the Gemini \\nmodel. Note that the multimodal aspects of Gemini are covered in their respective \\ndedicated whitepapers.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 69, 'page_label': '70'}, page_content='Foundational Large Language Models & Text Generation\\n70\\nFebruary 2025\\nPython\\n%pip install --upgrade --quiet google-genai\\nimport sys \\nif “google.colab” in sys.modules:\\n    from google.colab import auth \\n auth.authenticate_user () \\nfrom IPython.display import HTML, Markdown, display\\nfrom google import genai\\nfrom google.genai.types import (\\n    FunctionDeclaration,\\n    GenerateContentConfig,\\n    GoogleSearch,\\n    HarmBlockThreshold,\\n    HarmCategory,\\n    MediaResolution,\\n    Part,\\n    Retrieval,\\n    SafetySetting,\\n    Tool,\\n    ToolCodeExecution,\\n    VertexAISearch,\\n)\\nimport os\\nPROJECT_ID = “[your-project-id]”  # @param {type: “string”, placeholder: “[your-project-\\nid]”, isTemplate: true}\\nif not PROJECT_ID or PROJECT_ID == “[your-project-id]”:\\n    PROJECT_ID = str(os.environ.get(“GOOGLE_CLOUD_PROJECT”)) \\nLOCATION = os.environ.get(“GOOGLE_CLOUD_REGION”, “us-central1”)  \\nclient = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION) \\nMODEL_ID = “gemini-2.0-flash-001”  # @param {type: “string”} \\nresponse = client.models.generate_content(\\n    model=MODEL_ID, contents=”What’s the largest planet in our solar system?” \\n)\\ndisplay(Markdown(response.text))\\nSnippet 2. Using Vertex AI and Google AI studio SDKs for unimodal text generation'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 70, 'page_label': '71'}, page_content='Foundational Large Language Models & Text Generation\\n71\\nFebruary 2025\\nCode and mathematics\\nGenerative models can comprehend and generate code and algorithms to supercharge \\ndevelopers by assisting them across many application areas. Some of the popular use cases \\nfor code include:\\n• Code generation: LLMs can be prompted in natural language to generate code in a \\nspecific programming language to perform certain operations. The output can be used as \\na draft.\\n• Code completion: LLMS can proactively suggest useful code as the user types it. This \\ncan save developers time and improve code quality.\\n• Code refactoring and debugging: LLMs can help reduce technical debt by refactoring \\nand debugging code to improve quality, efficiency and correctness.\\n• Code translation: LLMs can significantly help developer time and effort by helping to \\nconvert code from one programming language to another. For example, an LLM might \\nconvert Python code to Java.\\n• Test case generation: LLMs can be prompted to generate unit tests for a provided \\ncodebase which saves considerable time and reduces errors.\\n• Code documentation and understanding: LLMs can be used in a conversational manner \\nto engage in a natural language chat to help you understand a codebase. They can also \\ngenerate appropriate comments, understand copyright status, and create release notes.\\nRecently, a number of exciting advancements have been made in the space of competitive \\ncoding and mathematics. AlphaCode 2,64 combines Gemini’s reasoning capabilities with \\nsearch and the use of tools to solve competitive coding problems. It receives as input a \\ndescription of a problem to solve, and outputs a code solution that solves the problem. It'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 71, 'page_label': '72'}, page_content='Foundational Large Language Models & Text Generation\\n72\\nFebruary 2025\\nnow ranks among the top 15% competitive coders on the popular Codeforces competitive \\ncoding platform. FunSearch65 uses an evolutionary procedure which is based on pairing \\na pre-trained LLM with a systematic evaluator. It solved the cap set problem66, an open \\nproblem in mathematics, and also discovered more efficient bin-packing algorithms which \\nare used in many applications such as making data centers more efficient. Another recent \\napproach called AlphaGeometry tackles the problem of finding proofs for complex geometric \\ntheorems. It comprises a neuro-symbolic system made up of a neural language model and \\na symbolic deduction engine. AlphaGeometry managed to solve 25 out of 30 Olympiad \\ngeometry problems, where the average human gold medalist scores on average 25.9. 67\\nMachine translation\\nLLMs are capable of generating fluid, high-quality and contextually accurate translations. \\nThis is possible due to the LLM’s deep understanding of linguistic nuances, idioms, and \\ncontext. The following are some possible real world use cases:\\n• Instant messaging apps: In messaging platforms, LLMs can provide on-the-fly \\ntranslations that feel natural. Unlike previous algorithms that might translate word-\\nfor-word, LLMs understand slang, colloquialisms, and regional differences, enhancing \\ncross-language communication.\\n• E-commerce: On global platforms like AliExpress, product descriptions are automatically \\ntranslated. LLMs help with ensuring cultural nuances and idiomatic expressions in product \\ndetails are appropriately translated, leading to fewer misunderstandings.\\n• Travel apps: In apps like Google Translate, travelers get real-time spoken translations. \\nWith LLMs, the translated conversations are smoother, making interactions in foreign \\ncountries more effortless.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 72, 'page_label': '73'}, page_content='Foundational Large Language Models & Text Generation\\n73\\nFebruary 2025\\nText summarization\\nText summarization is a core capability of many of the LLMs mentioned in this whitepaper. \\nThere are a number of natural potential use cases which include:\\n• News aggregators: LLMs could craft summaries that capture not only the main \\nevents but also the sentiment and tone of the article, providing readers with a more \\nholistic understanding.\\n• Research databases: LLMs could help researchers generate abstracts that encapsulate \\nthe core findings and implications of scientific papers.\\n• Chat management: In platforms like Google Chat, LLM-based systems could generate \\nthread summaries that capture the urgency and tone, aiding users in prioritizing \\ntheir responses.\\nQuestion-answering\\nThe older generation of QA systems often worked by keyword matching, frequently missing \\nout on the contextual depth of user queries. LLMs, however, dive deep into context. They can \\ninfer user intent, traverse vast information banks, and provide answers that are contextually \\nrich and precise. Some of the examples where this could be used include:\\n• Virtual assistants: LLMs can offer detailed explanations of a weather forecast \\nconsidering the user’s location, time of year, and recent weather trends.\\n• Customer support: In business platforms, LLM-based bots could provide answers that \\ntake into account the user’s purchase history, past queries, and potential issues, offering \\npersonalized assistance.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 73, 'page_label': '74'}, page_content='Foundational Large Language Models & Text Generation\\n74\\nFebruary 2025\\n• Academic platforms: On academic platforms like Wolfram Alpha, LLMs could cater to \\nuser queries by understanding the depth and context of academic questions, offering \\nanswers that suit everyone from a high school student to a postgraduate researcher.\\nThe quality of the generated answers, as well as the corresponding citations and sources \\ncan be significantly improved by using advanced search systems (such as those based on \\nRetrieval Augmented Generation (RAG) architectures) to expand the prompt with relevant \\ninformation, as well as post-hoc grounding after the response has been generated. Clear \\ninstructions, roles of what should and should not be used to answer the question, and \\nadvanced prompt engineering approaches (such as chain of thought and search/RAG \\narchitectures), combined with a lower temperature value amongst other things can also \\nhelp greatly.\\nChatbots\\nEarlier chatbots followed scripted pathways, leading to ‘mechanical’ conversations. LLMs \\ntransform this space by offering dynamic, human-like interactions. They can analyze \\nsentiment, context, and even humor, making digital conversations feel more authentic. Some \\nexamples of where this can be used include:\\n• Customer service: A chatbot on retail platforms like Zara could not only answer product-\\nrelated queries but also offer fashion advice based on current trends.\\n• Entertainment: On Media LLM-driven chatbots could engage with users dynamically, \\nreacting to live events in the stream and moderating chats with contextual understanding.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 74, 'page_label': '75'}, page_content='Foundational Large Language Models & Text Generation\\n75\\nFebruary 2025\\nContent generation\\nText generation isn’t new, but what LLMs bring to the table is the unprecedented ability \\nto generate human-like text that’s contextually relevant and rich in detail. Earlier models \\nwould often lose context or coherence over longer passages. LLMs, with their vast \\nknowledge and nuanced understanding, can craft text spanning various styles, tones, and \\ncomplexities, mixing factuality with creativity (depending on the context) effectively bridging \\nthe gap between machine-generated and human-written content. The following are some \\nreal-world examples:\\n• Content creation: Platforms could utilize LLMs to help marketers develop advertisements. \\nInstead of generic content, the LLMs could generate creative, targeted, and \\naudience-specific messages.\\n• Scriptwriting: LLMs could potentially assist with producing scripts for movies or TV \\nshows. Writers could input themes or plot points, and the model can suggest dialogues or \\nscene descriptions, enhancing the creative process.\\nText generation is a wide task encompassing a variety of use cases that might range from \\nthose where correctness of the generated output is more or less important than its creativity/\\ndiversity of the language. The sampling methods and parameters like temperature should be \\ntuned accordingly. For more information, see the prompt engineering and architecting for \\nLLM applications whitepapers.\\nNatural language inference\\nNatural language inference (NLI) is the task of determining whether a given textual \\nhypothesis can be logically inferred from a textual premise.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 75, 'page_label': '76'}, page_content='Foundational Large Language Models & Text Generation\\n76\\nFebruary 2025\\nTraditional models struggled with nuanced relationships or those that require a deeper \\nunderstanding of context. LLMs, with their intricate grasp of semantics and context, excel \\nat tasks like these, bringing accuracy levels close to human performance. The following are \\nsome real-world examples:\\n• Sentiment analysis: Businesses could utilize LLMs to infer customer sentiment from \\nproduct reviews. Instead of just basic positive or negative tags, they could extract \\nnuanced emotions like ‘satisfaction,’ ‘disappointment,’ or ‘elation’.\\n• Legal document review: Law firms could employ LLMs to infer implications \\nand intentions in contracts, ensuring there are no contradictions or potentially \\nproblematic clauses.\\n• Medical diagnoses: By analyzing patient descriptions and histories, LLMs could assist \\ndoctors in inferring potential diagnoses or health risks, ensuring early intervention.\\nThe whitepapers on domain specific LLMs, prompt engineering, and architecting for LLM \\napplications give further insight into these use cases.\\nText classification\\nText classification involves categorizing text into predefined groups. While traditional \\nalgorithms were efficient, they often struggled with ambiguous or overlapping categories. \\nLLMs, given their deep understanding of context, can classify text with higher precision, even \\nwhen faced with subtle distinctions. Some examples of this include:\\n• Spam detection: Email services could utilize LLMs to classify emails as spam or \\nlegitimate. Instead of just keyword-based detection, the models understand the context \\nand intent, potentially reducing false positives.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 76, 'page_label': '77'}, page_content='Foundational Large Language Models & Text Generation\\n77\\nFebruary 2025\\n• News categorization: News platforms could employ LLMs to categorize articles into \\ntopics like ‘technology,’ ‘politics,’ or ‘sports,’ even when articles blur the boundaries \\nbetween categories.\\n• Customer feedback sorting: Businesses could analyze customer feedback through \\nLLMs to categorize them into areas like ‘product design,’ ‘customer service,’ or ‘pricing,’ \\nensuring targeted responses.\\n• Evaluating LLMs as autorater: LLMs could be used to rate, compare and rank the \\ngenerated outputs of other LLMs as well.\\nText analysis\\nLLMs excel at deep text analysis – extracting patterns, understanding themes, and gleaning \\ninsights from vast textual datasets. Where traditional tools would scratch the surface, LLMs \\ndelve deep, offering rich and actionable insights. Some potential real-world examples are:\\n• Market research: Companies could leverage LLMs to analyze consumer conversations on \\nsocial media, extracting trends, preferences, and emerging needs.\\n• Literary analysis: Academics could employ LLMs to understand themes, motifs, and \\ncharacter developments in literary works, offering fresh perspectives on classic and \\ncontemporary literature.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 77, 'page_label': '78'}, page_content='Foundational Large Language Models & Text Generation\\n78\\nFebruary 2025\\nMultimodal applications\\nMultimodal LLMs, capable of processing and generating text, images, audio, and video, have \\nopened up a new frontier in AI, offering a range of exciting and innovative applications across \\nvarious sectors. The following are some examples: \\nCreative content generation:\\n• Storytelling: An AI system could watch an image or video and spin a captivating narrative, \\nintegrating details from the visual with its knowledge base.\\n• Advertising and marketing: Generating targeted and emotionally resonant \\nadvertisements based on product photos or videos.\\nEducation and accessibility:\\n• Personalized learning: Tailoring educational materials to individual learning styles by \\ncombining text with interactive visual and audio elements.\\n• Assistive technology: Multimodal LLMs could power tools that describe images, videos, \\nand audio for visually or hearing-impaired individuals.\\nBusiness and industry:\\n• Document understanding and summarization: Automatically extracting key information \\nfrom complex documents, combining text and visuals like invoices and contracts.\\n• Customer service: Multimodal chatbots can understand and respond to customer \\nqueries combining text and images, offering a richer and more personalized experience. \\nScience and research:'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 78, 'page_label': '79'}, page_content='Foundational Large Language Models & Text Generation\\n79\\nFebruary 2025\\n• Medical diagnosis: Analyzing medical scans and reports together, identifying potential \\nissues and providing insights for doctors.\\n• Bioinformatics and drug discovery: Integrating knowledge from diverse data sources \\nlike medical images, protein structures, and research papers to accelerate research.\\nThese examples are just the tip of the iceberg. As research progresses, the applications \\nof multimodal LLMs are only expected to grow, transforming our daily lives in diverse and \\nprofound ways. Multimodal LLMs also benefit greatly from the existing methodologies of \\nUnimodal LLMs ( i.e., text based LLMs).\\nLLMs, thanks to their ability to understand and process language, are reshaping how we \\ninteract with, generate, and analyze text across diverse sectors. As they continue to evolve, \\ntheir applications will only grow, boosting the ability for machines and humans to have rich \\nnatural language interactions.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 79, 'page_label': '80'}, page_content='Foundational Large Language Models & Text Generation\\n80\\nFebruary 2025\\nSummary\\nIn this whitepaper we have discussed the basics of transformers, upon which all modern-day \\nLLMs are based. We detailed the evolution of the various LLM model architectures and their \\ncomponents. We’ve also seen the various methodologies you can use to train and fine-tune \\nmodels efficiently and effectively. We briefly discussed prompt engineering and sampling \\ntechniques that greatly influence the output of an LLM, and also touched on possible \\napplications of this technology. There are a number of key takeaways to keep in mind:\\n• The transformer architecture is the basis for all modern-day LLMs. Across the various \\narchitectures mentioned in this whitepaper we see that it’s important not only to add more \\nparameters to the model, but the composition of the dataset is equally important. \\n• The order and strategies used for fine-tuning is important and may include multiple steps \\nsuch as Instruction Tuning, Safety Tuning, etc. Supervised Fine Tuning (SFT) is important \\nin capturing the essence of a task. RLHF, and potentially RLAIF, can be used to shift the \\ndistribution from the pretraining distribution to a more desired one through the power of \\nthe reward function, that can reward desirable behaviors and penalize undesirable ones.\\n• Making inference from neural models efficient is an important problem and an active \\nfield of research. Many methods exist to reduce serving costs and latency with minimal \\nimpact to model performance, and some exact acceleration methods guarantee identical \\nmodel outputs.\\n• Large language models can be used for a variety of tasks including summarization, \\ntranslation, question answering, chat, code generation, and many more. You can \\ncreate your own tasks using the Vertex and Makersuite text generation services which \\nleverage Google’s latest language models. After the model has been trained and tuned, \\nit is important to experiment with engineering prompts. You should use the technique \\nmost appropriate for the task-at-hand because LLMs can be sensitive to prompts k.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 80, 'page_label': '81'}, page_content='Foundational Large Language Models & Text Generation\\n81\\nFebruary 2025\\nFurthermore, it is also possible to enhance task specific performance or creativity and \\ndiversity by tweaking the parameters corresponding to sampling techniques such as \\nTop-K, Top-P, and Max decoding steps to find the optimum mix of correctness, diversity, \\nand creativity required for the task at hand.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 81, 'page_label': '82'}, page_content='Foundational Large Language Models & Text Generation\\n82\\nFebruary 2025\\nEndnotes\\n1. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin,\\xa0I., 2017, Attention is \\nall you need. Advances in Neural Information Processing Systems , 30.\\n2. Wikipedia, 2024, Word n-gram language model. Available at:  \\nhttps://en.wikipedia.org/wiki/Word_n-gram_language_model .\\n3. Sutskever, I., Vinyals, O., & Le, Q. V., 2014, Sequence to sequence learning with neural networks. Advances in \\nNeural Information Processing Systems, 27.\\n4. Gu, A., Goel, K., & Ré, C., 2021, Efficiently modeling long sequences with structured state spaces.  \\narXiv preprint arXiv:2111.00396.\\n5. Jalammar, J. (n.d.). The illustrated transformer. Available at:  \\nhttps://jalammar.github.io/illustrated-transformer/ .\\n6. Ba, J. L., Kiros, J. R., & Hinton, G. E., 2016, Layer normalization.  \\narXiv preprint arXiv:1607.06450.\\n7. He, K., Zhang, X., Ren, S., & Sun, J., 2016, Deep residual learning for image recognition. Proceedings of the \\nIEEE Conference on Computer Vision and Pattern Recognition.\\n8. HuggingFace., 2024, Byte Pair Encoding. Available at:  \\nhttps://huggingface.co/learn/nlp-course/chapter6/5?fw=pt .\\n9. Kudo, T., & Richardson, J., 2018, Sentencepiece: A simple and language independent subword tokenizer and \\ndetokenizer for neural text processing. arXiv preprint arXiv:1808.06226.\\n10. HuggingFace, 2024, Unigram tokenization. Available at:  \\nhttps://huggingface.co/learn/nlp-course/chapter6/7?fw=pt .\\n11. Goodfellow et. al., 2016, Deep Learning. MIT Press. Available at: http://www.deeplearningbook.org .\\n12. Radford, Alec et al., 2019, Language models are unsupervised multitask learners.\\n13. Brown, Tom, et al., 2020, Language models are few-shot learners. Advances in Neural Information \\nProcessing Systems, 33, 1877-1901.\\n14. Devlin, Jacob, et al., 2018, BERT: Pre-training of deep bidirectional transformers for language understanding. \\narXiv preprint arXiv:1810.04805.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 82, 'page_label': '83'}, page_content='Foundational Large Language Models & Text Generation\\n83\\nFebruary 2025\\n15. Radford, A., & Narasimhan, K., 2018, Improving language understanding by generative pre-training.\\n16. Dai, A., & Le, Q., 2015, Semi-supervised sequence learning. Advances in Neural Information \\nProcessing Systems.\\n17. Ouyang, Long, et al., 2022, Training language models to follow instructions with human feedback. Advances \\nin Neural Information Processing Systems, 35, 27730-27744.-27744.\\n18. OpenAI., 2023, GPT-3.5. Available at: https://platform.openai.com/docs/models/gpt-3-5 .\\n19. OpenAI., 2023, GPT-4 Technical Report. Available at: https://arxiv.org/abs/2303.08774 .\\n20. Thoppilan, Romal, et al., 2022, Lamda: Language models for dialog applications. \\narXiv\\xa0preprint arXiv:2201.08239.\\n21. Llama 3.2: Revolutionizing edge AI and vision with open, customizable models. Available \\nat: https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/.\\n22. Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., ... & Irving, G., 2021, Scaling language \\nmodels: Methods, analysis & insights from training Gopher. Available at: https://arxiv.org/pdf/2112.11446.pdf.\\n23. Du, N., He, H., Dai, Z., Mccarthy, J., Patwary, M. A., & Zhou, L., 2022, GLAM: Efficient scaling of language \\nmodels with mixture-of-experts. In International Conference on Machine Learning  (pp. 2790-2800). PMLR.\\n24. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., ... & Amodei, D., 2020, Scaling laws \\nfor neural language models. arXiv preprint arXiv:2001.08361.\\n25. Hoffmann, Jordan, et al., 2022, Training compute-optimal large language models. arXiv \\npreprint arXiv:2203.15556.\\n26. Shoeybi, Mohammad, et al., 2019, Megatron-LM: Training multi-billion parameter language models using \\nmodel parallelism. arXiv preprint arXiv:1909.08053.\\n27. Muennighoff, N. et al., 2023, Scaling data-constrained language models. arXiv preprint arXiv:2305.16264.\\n28. Chowdhery, Aakanksha, et al., 2023, Palm: Scaling language modeling with pathways. Journal of Machine \\nLearning Research, 24(240), 1-113.\\n29. Wang, Alex, et al.,2019, SuperGLUE: A stickier benchmark for general-purpose language understanding \\nsystems. Advances in Neural Information Processing Systems , 32.\\n30. Anil, Rohan, et al., 2023, Palm 2 technical report. arXiv preprint arXiv:2305.10403 .'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 83, 'page_label': '84'}, page_content='Foundational Large Language Models & Text Generation\\n84\\nFebruary 2025\\n31. DeepMind, 2023, Gemini: A family of highly capable multimodal models. Available at:  \\nhttps://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf .\\n32. DeepMind, 2024, Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. \\nAvailable at: https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf .\\n33. Google Developers, 2024, Introducing PaLi-Gemma, Gemma 2, and an upgraded responsible AI toolkit. \\nAvailable at: https://developers.googleblog.com/en/gemma-family-and-toolkit-expansion-io-2024/ .\\n34. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., ... & Jegou, H., 2023, Llama 2: Open \\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.\\n35. Jiang, A. Q., 2024, Mixtral of experts. arXiv preprint arXiv:2401.04088 .\\n36. Qwen, 2024, Introducing Qwen1.5. Available at: https://qwenlm.github.io/blog/qwen1.5/ .\\n37. Young, A., 2024, Yi: Open foundation models by 01.AI. arXiv preprint arXiv:2403.04652 .\\n38. Grok-1, 2024, Available at: https://github.com/xai-org/grok-1.\\n39. Duan, Haodong, et al., 2023, BotChat: Evaluating LLMs’ capabilities of having multi-turn dialogues. \\narXiv\\xa0preprint arXiv:2310.13650.\\n40. Google Cloud, 2024, Tune text models with reinforcement learning from human feedback. Available at:  \\nhttps://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-text-models-rlhf .\\n41. Bai, Yuntao, et al., 2022, Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073 .\\n42. Wikipedia, 2024, Likert scale. Available at: https://en.wikipedia.org/wiki/Likert_scale .\\n43. Sutton, R. S., & Barto, A. G., 2018, Reinforcement learning: An introduction. MIT Press.\\n44. Bai, Yuntao, et al, 2022, Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073 .\\n45. Rafailov, Rafael, et al., 2023, Direct preference optimization: Your language model is secretly a reward \\nmodel. arXiv preprint arXiv:2305.18290.\\n46. Houlsby, Neil, et al., 2019, Parameter-efficient transfer learning for NLP. In International Conference on \\nMachine Learning (pp. 2790-2799). PMLR.\\n47. Hu, Edward J., et al., 2021, LoRA: Low-rank adaptation of large language models. \\narXiv\\xa0preprint arXiv:2106.09685.\\n48. Dettmers, Tim, et al., 2023, QLoRA: Efficient finetuning of quantized LLMs. arXiv preprint arXiv:2305.14314 .'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 84, 'page_label': '85'}, page_content='Foundational Large Language Models & Text Generation\\n85\\nFebruary 2025\\n49. Lester, B., Al-Rfou, R., & Constant, N., 2021, The power of scale for parameter-efficient prompt tuning. arXiv \\npreprint arXiv:2104.08691.\\n50. HuggingFace., 2020, How to generate text? Available at: https://huggingface.co/blog/how-to-generate .\\n51. Google AI Studio Context caching. Available \\nat: https://ai.google.dev/gemini-api/docs/caching?lang=python.\\n52. Vertex AI Context caching overview. Available \\nat: https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview .\\n53. Gu, A., Goel, K., & Ré, C., 2021, Efficiently modeling long sequences with structured state spaces.  \\nAvailable at: https://arxiv.org/abs/2111.00396 .\\n54. Hubara et al., 2016, Quantized neural networks: Training neural networks with low precision weights and \\nactivations. Available at: https://arxiv.org/abs/1609.07061 .\\n55. Benoit Jacob et al., 2017, Quantization and training of neural networks for efficient integer-arithmetic-only \\ninference. Available at: https://arxiv.org/abs/1712.05877 .\\n56. Bucila, C., Caruana, R., & Niculescu-Mizil, A., 2006, Model compression. Knowledge Discovery and Data \\nMining. Available at: https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf .\\n57. Hinton, G., Vinyals, O., & Dean, J., 2015, Distilling the knowledge in a neural network.  \\nAvailable at: https://arxiv.org/abs/1503.02531 .\\n58. Zhang, L., Fei, W., Wu w., He Y., Lou Z., Zhou H., 2023, Dual Grained Quantisation: Efficient Finegrained \\nQuantisation for LLM. Available at: https://arxiv.org/abs/2310.04836 .\\n59. Agarwal, R., Vieillard, N., Zhou, Y., Stanczyk, P., Ramos, S., Geist, M., Bachem, O., 2024, On-\\nPolicy Distillation of Language Models: Learning from Self-Generated Mistakes. Available \\nat: https://arxiv.org/abs/2306.13649.\\n60. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., & Dean, J., 2017, Outrageously large neural \\nnetworks: The sparsely-gated mixture-of-experts layer. Available at: https://arxiv.org/abs/1701.06538 .\\n61. Schuster, T., Fried, D., & Jurafsky, D., 2022, Confident adaptive language modeling. Available at:  \\nhttps://arxiv.org/abs/2207.07061.\\n62. Tri Dao et al. “FlashAttention. Available at:  \\nhttps://arxiv.org/abs/2205.14135.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 85, 'page_label': '86'}, page_content='Foundational Large Language Models & Text Generation\\n86\\nFebruary 2025\\n63. Leviathan, Y., Ram, O., Desbordes, T., & Haussmann, E., 2022, Fast inference from transformers via \\nspeculative decoding. Available at: https://arxiv.org/abs/2211.17192 .\\n64. Li, Y., Humphreys, P., Sun, T., Carr, A., Cass, S., Hawkins, P., ... & Bortolussi, L., 2022, Competition-level code \\ngeneration with AlphaCode. Science , 378(1092-1097). DOI: 10.1126/science.abq1158.\\n65. Romera-Paredes, B., Barekatain, M., Novikov, A., Novikov, A., Rashed, S., & Yang, J., 2023, Mathematical \\ndiscoveries from program search with large language models. Nature . DOI: 10.1038/s41586-023-06924-6.\\n66. Wikipedia., 2024, Cap set. Available at: https://en.wikipedia.org/wiki/Cap_set .\\n67. Trinh, T. H., Wu, Y., & Le, Q. V. et al., 2024, Solving olympiad geometry without human demonstrations. Nature,  \\n625, 476–482. DOI: 10.1038/s41586-023-06747-5.\\n68. Mikolov, T., Chen, K., Corrado, G., Dean, J., 2013, Efficient Estimation of Word Representations in Vector \\nSpace. Available at: https://arxiv.org/pdf/1301.3781 .\\n69. Shi, L., Ma, C., Liang, W., Ma, W., Vosoughi, S., 2024, Judging the Judges: A Systematic Study of Position Bias \\nin LLM-as-a-Judge. Available at: https://arxiv.org/abs/2406.07791\\n70. Pandit, B., 2024, What Is Mixture of Experts (MoE)? How It Works, Use Cases & More. Available \\nat: https://www.datacamp.com/blog/mixture-of-experts-moe')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918aafee",
   "metadata": {},
   "source": [
    "#### Connecting to Weaviate Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7edd1e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Beeface\\AppData\\Local\\Temp\\ipykernel_6004\\600448803.py:5: ResourceWarning: unclosed <ssl.SSLSocket fd=6976, family=2, type=1, proto=0, laddr=('192.168.1.159', 55325), raddr=('34.36.172.118', 443)>\n",
      "  client = weaviate.connect_to_weaviate_cloud(\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    \"X-HuggingFace-Api-Key\": YOUR_HUGGINGFACE_KEY,\n",
    "    }\n",
    "\n",
    "client = weaviate.connect_to_weaviate_cloud(\n",
    "        cluster_url=YOUR_WEAVIATE_CLUSTER,                       # `weaviate_url`: your Weaviate URL\n",
    "        auth_credentials=Auth.api_key(YOUR_WEAVIATE_KEY),      # `weaviate_key`: your Weaviate API key\n",
    "        headers=headers\n",
    "    )\n",
    "\n",
    "print(client.is_ready())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9e3b58e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Beeface\\.virtualenvs\\BookChatbot-9Q2uoHrH\\Lib\\site-packages\\weaviate\\collections\\classes\\config.py:1950: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  for cls_field in self.model_fields:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<weaviate.collections.collection.sync.Collection at 0x23ef1660590>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.collections.create(\n",
    "    \"BookChatbot\",\n",
    "    vectorizer_config=[\n",
    "        Configure.NamedVectors.text2vec_huggingface(\n",
    "            name=\"title_vector\",\n",
    "            source_properties=[\"title\"],\n",
    "            model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        ),\n",
    "        Configure.NamedVectors.text2vec_huggingface(\n",
    "            name=\"content_vector\",\n",
    "            source_properties=[\"content\"],\n",
    "            model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        )\n",
    "    ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "85f94f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<weaviate.Collection config={\n",
      "  \"name\": \"BookChatbot\",\n",
      "  \"description\": null,\n",
      "  \"generative_config\": null,\n",
      "  \"inverted_index_config\": {\n",
      "    \"bm25\": {\n",
      "      \"b\": 0.75,\n",
      "      \"k1\": 1.2\n",
      "    },\n",
      "    \"cleanup_interval_seconds\": 60,\n",
      "    \"index_null_state\": false,\n",
      "    \"index_property_length\": false,\n",
      "    \"index_timestamps\": false,\n",
      "    \"stopwords\": {\n",
      "      \"preset\": \"en\",\n",
      "      \"additions\": null,\n",
      "      \"removals\": null\n",
      "    }\n",
      "  },\n",
      "  \"multi_tenancy_config\": {\n",
      "    \"enabled\": false,\n",
      "    \"auto_tenant_creation\": false,\n",
      "    \"auto_tenant_activation\": false\n",
      "  },\n",
      "  \"properties\": [],\n",
      "  \"references\": [],\n",
      "  \"replication_config\": {\n",
      "    \"factor\": 1,\n",
      "    \"async_enabled\": false,\n",
      "    \"deletion_strategy\": \"NoAutomatedResolution\"\n",
      "  },\n",
      "  \"reranker_config\": null,\n",
      "  \"sharding_config\": {\n",
      "    \"virtual_per_physical\": 128,\n",
      "    \"desired_count\": 1,\n",
      "    \"actual_count\": 1,\n",
      "    \"desired_virtual_count\": 128,\n",
      "    \"actual_virtual_count\": 128,\n",
      "    \"key\": \"_id\",\n",
      "    \"strategy\": \"hash\",\n",
      "    \"function\": \"murmur3\"\n",
      "  },\n",
      "  \"vector_index_config\": null,\n",
      "  \"vector_index_type\": null,\n",
      "  \"vectorizer_config\": null,\n",
      "  \"vectorizer\": null,\n",
      "  \"vector_config\": {\n",
      "    \"content_vector\": {\n",
      "      \"vectorizer\": {\n",
      "        \"vectorizer\": \"text2vec-huggingface\",\n",
      "        \"model\": {\n",
      "          \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
      "          \"useCache\": true,\n",
      "          \"useGPU\": false,\n",
      "          \"vectorizeClassName\": true,\n",
      "          \"waitForModel\": false\n",
      "        },\n",
      "        \"source_properties\": [\n",
      "          \"content\"\n",
      "        ]\n",
      "      },\n",
      "      \"vector_index_config\": {\n",
      "        \"multi_vector\": null,\n",
      "        \"quantizer\": null,\n",
      "        \"cleanup_interval_seconds\": 300,\n",
      "        \"distance_metric\": \"cosine\",\n",
      "        \"dynamic_ef_min\": 100,\n",
      "        \"dynamic_ef_max\": 500,\n",
      "        \"dynamic_ef_factor\": 8,\n",
      "        \"ef\": -1,\n",
      "        \"ef_construction\": 128,\n",
      "        \"filter_strategy\": \"sweeping\",\n",
      "        \"flat_search_cutoff\": 40000,\n",
      "        \"max_connections\": 32,\n",
      "        \"skip\": false,\n",
      "        \"vector_cache_max_objects\": 1000000000000\n",
      "      }\n",
      "    },\n",
      "    \"title_vector\": {\n",
      "      \"vectorizer\": {\n",
      "        \"vectorizer\": \"text2vec-huggingface\",\n",
      "        \"model\": {\n",
      "          \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
      "          \"useCache\": true,\n",
      "          \"useGPU\": false,\n",
      "          \"vectorizeClassName\": true,\n",
      "          \"waitForModel\": false\n",
      "        },\n",
      "        \"source_properties\": [\n",
      "          \"title\"\n",
      "        ]\n",
      "      },\n",
      "      \"vector_index_config\": {\n",
      "        \"multi_vector\": null,\n",
      "        \"quantizer\": null,\n",
      "        \"cleanup_interval_seconds\": 300,\n",
      "        \"distance_metric\": \"cosine\",\n",
      "        \"dynamic_ef_min\": 100,\n",
      "        \"dynamic_ef_max\": 500,\n",
      "        \"dynamic_ef_factor\": 8,\n",
      "        \"ef\": -1,\n",
      "        \"ef_construction\": 128,\n",
      "        \"filter_strategy\": \"sweeping\",\n",
      "        \"flat_search_cutoff\": 40000,\n",
      "        \"max_connections\": 32,\n",
      "        \"skip\": false,\n",
      "        \"vector_cache_max_objects\": 1000000000000\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}>\n"
     ]
    }
   ],
   "source": [
    "collection = client.collections.get(\"BookChatbot\")\n",
    "print(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "59c0dceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 466 texts and 466 metadata pairs.\n",
      "('Agents Companion \\n \\nAuthors: Antonio Gulli, Lavi Nigam,  \\nJulia Wiesinger, Vladimir Vuskovic,  \\nIrina Sigler, Ivan Nardini, Nicolas Stroppa, \\nSokratis Kartakis, Narek Saribekyan,  \\nand Alan Bount', 'Agents Companion\\nFebruary 2025\\n2\\nAcknowledgements\\nEditors & curators\\nAnant Nawalgaria\\nContent contributors\\nAnant Nawalgaria\\nSteven Johnson\\nHussain Chinoy \\nDesigner\\nMichael Lanning', 'Introduction 6\\nAgent Ops 8\\nAgent Success Metrics 12\\nAgent Evaluation 14\\nAssessing Agent Capabilities 15\\nEvaluating Trajectory and Tool Use 17\\nEvaluating the Final Response 20\\nHuman-in-the-Loop Evaluation 21\\nMore about Agent Evaluation 22\\nMultiple Agents & Their Evaluation 23\\nUnderstanding Multi-Agent Architectures 24\\nMulti-Agent Design Patterns and Their Business Impact 25\\nImportant components of Agents 28\\nChallenges in Multi-Agent systems  31\\nMulti-Agent Evaluation 32\\nTable of contents', 'Agentic RAG: A Critical Evolution in Retrieval-Augmented Generation 33\\nAgentic RAG and its Importance 34\\nBetter Search, Better RAG 36\\nAgents in the enterprise 38\\nManager of agents 38\\nGoogle Agentspace 40\\nNotebookLM Enterprise 41\\nGoogle AgentSpace Enterprise 43\\nFrom agents to contractors 46\\nContracts 46\\nContract Lifecycle 49\\nContract execution 49\\nContract Negotiation 50\\nContract Feedback 51\\nSubcontracts 51\\nAutomotive AI: Real World Use of Multi-Agent Architecture 54\\nSpecialized Agents 54\\nConversational Navigation Agent 54\\nConversational Media Search Agent 56\\nMessage Composition Agent 56\\nCar Manual Agent 57\\nGeneral Knowledge Agent 58\\nPatterns in Use 58', 'Hierarchical Pattern 58\\nDiamond Pattern 60\\nPeer-to-Peer 62\\nCollaborative Pattern 64\\nResponse Mixer Agent 66\\nAdaptive Loop Pattern 67\\nAdvantages of Multi-Agent Architecture for Automotive AI 68\\nAgent Builder 69\\nSummary 70\\nEndnotes 74', 'Agents Companion\\nFebruary 2025\\n6\\nIntroduction\\nGenerative AI agents mark a leap forward from traditional, standalone language models, \\noffering a dynamic approach to problem-solving and interaction. As defined in the original \\nAgents paper, an agent is an application engineered to achieve specific objectives by \\nperceiving its environment and strategically acting upon it using the tools at its disposal. \\nThe fundamental principle of an agent lies in its synthesis of reasoning, logic, and access to \\nexternal information, enabling it to perform tasks and make decisions beyond the inherent \\ncapabilities of the underlying model. These agents possess the capacity for autonomous \\noperation, independently pursuing their goals and proactively determining subsequent \\nactions, often without explicit instructions.\\nThe future of AI is agentic.', 'Agents Companion\\nFebruary 2025\\n7\\nThe architecture of an agent is composed of three essential elements that drive its behavior \\nand decision-making:\\n• Model: Within the agent\\'s framework, the term \"model\" pertains to the language \\nmodel (LM) that functions as the central decision-making unit, employing instruction-\\nbased reasoning and logical frameworks. The model can vary from general-purpose to \\nmultimodal or fine-tuned, depending on the agent\\'s specific requirements.\\n• Tools: Tools are critical for bridging the divide between the agent\\'s internal capabilities \\nand the external world, facilitating interaction with external data and services. These \\ntools empower agents to access and process real-world information. Tools can include \\nextensions, functions, and data stores. Extensions bridge the gap between an API and \\nan agent, enabling agents to seamlessly execute APIs. Functions are self-contained \\nmodules of code that accomplish specific tasks. Data stores provide access to dynamic \\nand up-to-date information, ensuring a model’s responses remain grounded in factuality \\nand relevance.\\n• Orchestration layer: The orchestration layer is a cyclical process that dictates how the \\nagent assimilates information, engages in internal reasoning, and leverages that reasoning \\nto inform its subsequent action or decision. This layer is responsible for maintaining \\nmemory, state, reasoning, and planning. It employs prompt engineering frameworks to \\nsteer reasoning and planning, facilitating more effective interaction with the environment \\nand task completion. Reasoning techniques such as ReAct, Chain-of-Thought (CoT), and \\nTree-of-Thoughts (ToT) can be applied within this layer.\\nBuilding on these foundational concepts, this companion paper is designed for developers \\nand serves as a \"102\" guide to more advanced topics. It offers in-depth explorations of agent \\nevaluation methodologies and practical applications of Google agent products for enhancing \\nagent capabilities in solving complex, real-world problems.', \"Agents Companion\\nFebruary 2025\\n8\\nWhile exploring these theoretical concepts, we'll examine how they manifest in real-world \\nimplementations, with a particular focus on automotive AI as a compelling case study. The \\nautomotive domain exemplifies the challenges and opportunities of multi-agent architectures \\nin production environments. Modern vehicles demand conversational interfaces that function \\nwith or without connectivity, balance between on-device and cloud processing for both \\nsafety and user experience, and seamlessly coordinate specialized capabilities across \\nnavigation, media control, messaging, and vehicle systems. Through this automotive lens, \\nwe'll see how different coordination patterns -- hierarchical, collaborative, and peer-to-\\npeer -- come together to create robust, responsive user experiences in environments with \\nsignificant constraints. This case study illustrates the practical application of multi-agent \\nsystems that businesses across industries can adapt to their specific domains.\\n Anyone who has built with gen AI quickly realizes it’s easy to get from an idea to a proof of \\nconcept, but it can be quite difficult to ensure high quality results and get to production - gen \\nAI agents are no exception. Quality and Reliability are the most cited concerns for deploying \\nto production, and the “Agent Ops” process is a solution to optimize agent building.\\nAgent Ops\\nOver the past two years, the field of Generative AI (GenAI) has undergone significant \\nchanges, with enterprise customers focusing on how to operationalize related solutions. This \\nhas resulted in various terms describing the operationalization of GenAI, such as MLOps for \\nGenAI, LLMOps, FMOps, and GenAIOps.\\nAgent and Operations (AgentOps) is a subcategory of GenAIOps that focuses on the efficient \\noperationalization of Agents. Its main additional components include internal and external \\ntool management, agent brain prompt (goal, profile, instructions) and orchestration, memory, \\nand task decomposition.\", 'Agents Companion\\nFebruary 2025\\n9\\nFigure 1. Relationship between DevOps, MLOps, and AgentOps. 13\\nEach of these “Ops” requires capabilities like version control, automated deployments \\nthrough CI/CD, testing, logging, security and (critically) metrics. Each system often \\nimplements some form of optimization based on metrics – measuring what your system \\nis and isn’t doing, measuring the outcomes and business metrics, and automating the \\nprocesses for more holistic metrics, and incrementally improving step by step. This practice \\nmight be called “A/B experimentation” or “ML Ops” or “Metrics Driven development\", but \\nthey derive from the same general approach and we will rely on those principles for Agent \\nOps as well.\\nRemember that new practices don\\'t replace the old. DevOps and MLOps best practices \\nare still necessary for AgentOps, as they are dependencies. For example, Agent tool use, \\nwhere APIs are invoked based on agent orchestration, often uses the same APIs you would', 'Agents Companion\\nFebruary 2025\\n10\\ninvoke with non-agentic software. Authentication and secret management, security, privacy, \\nexception handling, throttling, quotas, and scalability are still critical and require careful API \\ndesign in addition to Agent design.\\nLet’s go ahead and define these “ops” terms to help distinguish between them:\\n• Development and Operations (DevOps) is the practice of efficiently productionizing \\ndeterministic software applications by integrating the elements of people, processes, and \\ntechnology. DevOps serves as the foundation for all the following terms.\\n• Machine Learning Operations (MLOps) builds upon the capabilities of DevOps and \\nconcentrates on the efficient productionization of ML models. The primary distinction is \\nthat the output of an ML model is non-deterministic and relies on the input data (garbage \\nin, garbage out).\\n• Foundation Model Operations (FMOps) expands upon the capabilities of MLOps \\nand focuses on the efficient productionization of pre-trained (trained from scratch) or \\ncustomized (fine-tuned) FMs.\\n• Prompt and Operations (PromptOps) is a subcategory of GenAIOps that focuses \\non operationalizing prompts effectively. Its main additional capabilities include prompt \\nstorage, lineage, metadata management (including evaluation scores), a centralized \\nprompt template registry, and a prompt optimizer.\\n• RAG and Operations (RAGOps) is a subcategory of GenAIOps that centers on efficiently \\noperationalizing RAG solutions. Its primary additional capabilities include the retrieval \\nprocess through offline data preparation (encompassing cleaning, chunking, vectorization, \\nsimilarity search, and re-ranking) and the generation process through prompt \\naugmentation and grounding.', 'Agents Companion\\nFebruary 2025\\n11\\n• Agent and Operations (AgentOps) is a subcategory of GenAIOps that focuses on the \\nefficient operationalization of Agents. Its main additional components include internal \\nand external tool management, agent brain prompt (goal, profile, instructions) and \\norchestration, memory, and task decomposition.\\nFigure 2. Each of these “Ops” are about technology, processes, and people 14\\nAll of these “Ops” are, in essence, the harmonious blend of people, processes, and \\ntechnologies working together to efficiently deploy machine learning solutions into a \\nlive production environment. It’s crucial to recovgnize that Ops extends beyond mere \\ntechnology; it’s not simply about constructing and delivering a ML pipeline. Successful Ops \\nimplementations delve deeper, considering the customer’s operational model, their existing \\nbusiness units, and their overall organizational structure. This holistic approach ensures that \\nthe technology is tailored to their specific needs, seamlessly integrating into the business \\nand maximizing value.', 'Agents Companion\\nFebruary 2025\\n12\\nThe next section will cover Agent Evaluation in detail, which is a significant part of the story \\nfor Agent Ops and automation to capture useful metrics. Before we go there, let’s start with a \\nthought experiment; imagine setting up an A/B experiment in production for your new Agent.  \\nThe treatment arm gets your new agent and the control arm does not.  In that scenario, what \\nmetrics are you measuring to determine if the treatment arm is doing better? What metrics \\nare you measuring to determine ROI for the project? Is it a goal being accomplished, or \\nsales totals, or a set of critical steps in a user journey? Those metrics must be understood, \\ninstrumented and easily analyzed in addition to more detailed Agent Evaluation metrics.\\nAgent Success Metrics\\nMetrics are critical to building, monitoring, and comparing revisions of Agents. Business \\nmetrics, like revenue or user engagement, are probably outside of the scope of the agent \\nitself but these should be the north star metric for your agents.\\nMost Agents are designed around accomplishing goals, so goal completion rate is a key \\nmetric to track. Similarly, a goal might be broken down into a few critical tasks or critical \\nuser interactions. Each of these critical tasks and interactions should be independently \\ninstrumented and measured.\\nSo before we get into the details of the Agent itself, we already have several metrics \\nidentified which you should be able to easily track on a dashboard. Each business metric, \\ngoal, or critical interaction, will be aggregated in a familiar fashion: attempts, successes, \\nrates, etc. Additionally, metrics you should be able to get from any application telemetry \\nsystem are very important to track for agents as well, metrics like latency, errors, etc.', 'Agents Companion\\nFebruary 2025\\n13\\nNone of these metrics are specific to Agents, you could track them for any software, but they \\nare even more important for Agent builders. Deterministic code does only what you tell it to \\ndo, whereas an agent can do a lot more, relying on LLMs which are trained on huge amounts \\nof data. Instrumentation of these high level metrics is an important part of observability. \\nYou can think of them as Key Performance Indicators (KPI) for the agent, and they allow for \\nobservability in the aggregate, a higher level perspective of your agents.\\nHuman feedback is one of the more critical metrics to track as well. A simple 👍👎  or user \\nfeedback form, within the context of an agent or task can go a long way to understanding \\nwhere your agent does well and where it needs improvement. This feedback can come from \\nend users of a consumer system, but also employees, QA testers, and process or domain \\nexperts reviewing the agent.\\nMore detailed observability is also very important for agent building, being able to see and \\nunderstand what the agent is doing and why it’s doing that. An agent can be instrumented \\nwith “trace” to log all of the inner workings of the agent, not only the critically important \\ntasks and user interactions. You could conceptually measure every internal step as metrics, \\nbut that is rarely done. Instead these detailed traces are used to debug an agent when \\nmetrics or manual testing show a problem, you can dig into details and see what went wrong.', \"Agents Companion\\nFebruary 2025\\n14\\nFigure 3: An example of Cloud Observability showing traces for an agent with tools and LLM \\nOpenTelemetry spans. 15\\nSo far we’ve been talking about business metrics, goals, tasks, human feedback, and traces \\n– those are all ways of understanding the actions and impact of your agents, in production.  \\nWhile developing an agent, in addition to manual testing, automated testing will be much \\nmore efficient in the long run and provide greater insights into the behavior of agents.\\nAgent Evaluation\\nTo bridge the gap between a proof-of-concept and a production-ready AI agent, a robust \\nand automated evaluation framework is essential. Unlike evaluating generative models, where \\nthe focus is primarily on the final output, agent evaluation requires a deeper understanding of \\nthe decision-making process. Agent evaluation can be broken down into three components \\nthat we discuss in this chapter:\\n1. Assessing Agent Capabilities: Evaluating an agent's core abilities, such as its capacity to \\nunderstand instructions and reason logically.\", \"Agents Companion\\nFebruary 2025\\n15\\n2. Evaluating Trajectory and Tool Use: Analyzing the steps an agent takes to reach a \\nsolution, including its choice of tools, strategies, and the efficiency of its approach.\\n3. Evaluating the Final Response: Assessing the quality, relevance, and correctness of the \\nagent's final output.\\nAssessing Agent Capabilities\\nBefore evaluating your specific agentic use cases, publicly available benchmarks and \\ntechnical reports can provide insight into core capabilities and limitations to consider when \\nbuilding out your agentic use cases. Public benchmarks exist for most fundamental agentic \\ncapabilities like model performance, hallucinations, tool calling and planning. For example, \\ntool calling, the ability to select and use appropriate tools, is demonstrated by benchmarks \\nlike the Berkeley Function-Calling Leaderboard (BFCL)16 and τ-bench17 that also outlines \\ncommon mistakes. Another example, PlanBench18 aims to assess planning and reasoning, \\nacross several domains and specific capabilities. \\nBut tool calling and planning is not the only capability you should consider. Agents inherit \\nbehaviors from their LLMs and each of their other components. Likewise, agent and user \\ninteractions have a history in traditional conversational design systems and workflow \\nsystems, and therefore can inherit the set of metrics and measurements that are used to \\ndetermine the efficacy of these systems.\", 'Agents Companion\\nFebruary 2025\\n16\\nFigure 4: A high level diagram of AgentBench. 19\\nHolistic agent benchmarks like AgentBench19 try to capture end to end performance across \\nseveral scenarios. This is more realistic, when the scenario reflects your agent use case, \\nbut not if it’s testing capabilities your agent doesn’t implement. It is difficult to simulate \\nthe environment, tools, instructions, and use case requirements in ways that are both \\nspecific and general at the same time. Companies and organizations are setting up public \\nbenchmarks for specialized use cases, like Adyen’s Data Analyst leaderboard DBAStep20 \\nwhich may give you a more targeted evaluation - if you understand both the evaluation \\napproach and the agents who are on the leaderboard.\\nPublic benchmarks are a valuable starting point, to get a feeling for what is possible and \\nidentify pitfalls to look out for. Most benchmark assessments include discussions of common \\nfailure modes that can guide you in setting up your own, use-case specific evaluation \\nframework.', 'Agents Companion\\nFebruary 2025\\n17\\nBeyond public evaluations, you will want to evaluate the behavior of your agent across a \\nvariety of scenarios to ensure it does what you want it to do. You are simulating interactions \\nwith your agent and evaluating how it responds. This includes the evaluating final response \\nand also the set of steps it takes along the way (trajectory). Those are the 2 most common \\nand practical approaches we recommend you start with. There are many other evaluation \\ntechniques you can use beyond these, either for finer details on sub-components or \\nbroader approaches.\\nSoftware engineers will compare this to automated testing of code.  Investing in automated \\ntests saves you time and gives you confidence in the software you are building. With agents, \\nthis automation pays off faster, in both time and confidence. Curating the evaluation data \\nset will be extremely important for accurately representing the use case your agent will \\nencounter, even more so than in software testing. \\nEvaluating Trajectory and Tool Use\\nAn agent usually does several actions before it responds back to the user.  It might compare \\nthe user input with session history to disambiguate a term, or lookup a policy document, \\nsearch a knowledge base or invoke an API to save a ticket. Each of those actions is a step on \\na path, also called a “trajectory” of actions. Every time your agent does something, there’s a \\ntrajectory of actions under the hood.\\nComparing the trajectory that you expect the agent to take vs the trajectory that the agent \\nactually took, is particularly useful for developers who want to debug their application, \\nidentifying errors or inefficiencies, and ultimately improving performance.', 'Agents Companion\\nFebruary 2025\\n18\\nFigure 5: LangSmith diagram of final evaluation and trajectory. 21  \\nThe following six ground-truth-based automated trajectory evaluations provide different \\nlenses to assess the performance of an agent:\\n1. Exact match: Requires the AI agent to produce a sequence of actions (a \"trajectory\") that \\nperfectly mirrors the ideal solution. This is the most rigid metric, allowing no deviation from \\nthe expected path. \\n2. In-order match: This metric assesses an agent\\'s ability to complete the expected \\ntrajectory, while accommodating extra, unpenalized actions. Success is defined by \\ncompleting the core steps in order, with flexibility for additional actions.\\n3. Any-order match: Compared to in-order match, this metric now disregards the order. It \\nasks if the agent included all necessary actions, but does not look into the order of actions \\ntaken and also allows for extra steps.', \"Agents Companion\\nFebruary 2025\\n19\\n4. Precision: How many of the tool calls in the predicted trajectory are actually relevant or \\ncorrect according to the reference trajectory?\\n5. Recall: How many of the essential tool calls from the reference trajectory are actually \\ncaptured in the predicted trajectory?\\n6. Single-tool use: Understand if a specific action is within the agent's trajectory. This \\nmetric is useful to understand if the agent has learned to utilize a particular tool yet.\\nFigure 6: A radar chart plotting a single trajectory evaluation with a few metrics. 24\", \"Agents Companion\\nFebruary 2025\\n20\\nConsider these metrics as different lenses for analyzing and debugging your agent's \\ntrajectory. Each metric offers a unique perspective, but not all will be relevant to every \\nsituation. For instance, some use cases demand strict adherence to the ideal trajectory, while \\nothers allow for more creative deviations. A clear limitation of this evaluation approach is that \\nyou need to have a reference trajectory in place for this to work. While ground-truth-based \\nautomated trajectory evaluations that are discussed here are prevalent in popular libraries. \\nResearch is advancing the use of agent autoraters for more efficient evaluation, for example \\nAgent as a Judge, 202422.\\nEvaluating the Final Response\\nThe final response evaluation boils down to a simple question: Does your agent achieve its \\ngoals? You can define custom success criteria, tailored to your specific needs, to measure \\nthis. For example, you could assess whether a retail chatbot accurately answers product \\nquestions, or whether a research agent effectively summarizes findings with the appropriate \\ntone and style. To automate this process, you can use autorater. An autorater is an LLM \\nthat acts as a judge. Given the input prompts and the generated response, it mirrors \\nhuman evaluation by assessing the response against a set of user-provided criteria. For \\nthis evaluation to work, it is crucial to consider that given the absence of ground-truth, you \\nneed to be very precise in defining your evaluation criteria, as this is the core of what your \\nevaluation is looking at. You find a number of predefined criteria in various libraries, treat \\nthem as a starting point and tweak them to provide your definition of good.\", \"Agents Companion\\nFebruary 2025\\n21\\nHuman-in-the-Loop Evaluation\\nThe fields of agent development and agent evaluation are rapidly evolving. Evaluating \\nAI agents presents significant challenges, including defining clear objectives, designing \\nrealistic environments, managing stochastic behavior, and ensuring fairness and bias \\nmitigation, particularly in socially impactful applications. Therefore, it's crucial to incorporate \\na human-in-the-loop approach alongside the automated evaluations discussed previously \\n(which involve predefined metrics and autoraters). Human-in-the-loop is valuable for tasks \\nrequiring subjective judgment or creative problem-solving, it can also serve to calibrate and \\ndouble-check if your automated evaluation approaches actually work and align with your \\npreferences. Key benefits include:\\n• Subjectivity: Humans can evaluate qualities that are difficult to quantify, such as \\ncreativity, common sense, and nuance.\\n• Contextual Understanding: Human evaluators can consider the broader context of the \\nagent's actions and their implications.\\n• Iterative Improvement: Human feedback provides valuable insights for refining the \\nagent's behavior and learning process.\\n• Evaluating the evaluator: Human feedback can provide a signal to calibrate and refine \\nyour autoraters. \\nTo implement human-in-the-loop evaluation, consider these methods:\\n• Direct Assessment: Human experts directly rate or score the agent's performance on \\nspecific tasks.\\n• Comparative Evaluation: Experts compare the agent's performance to that of other \\nagents or your previous iterations.\", \"Agents Companion\\nFebruary 2025\\n22\\n• User Studies: Participants interact with the agent and provide feedback on its behavior, \\nusability, and overall effectiveness.\\nMore about Agent Evaluation\\nIn this section we cover agent evaluation from the practical perspective. But this is just the tip \\nof the iceberg.  Agent evaluation presents many challenges. Evaluation data for your agents \\nmay be difficult to find. While synthetic data or LLMs as judges can be used, evaluations \\nmay still be incomplete. Also, LLM-as-a-Judge metrics, for example, may prioritize final \\noutcomes over the agent's reasoning and intermediate actions, potentially missing key \\ninsights. Additionally, as evaluations for agent systems have a history in conversational and \\nworkflow systems, there is so much to explore on how to inherit methods and metrics to \\nevaluate agent's capabilities, such as the ability to improve task performance over multiple \\ninteractions. Evaluations for multi-modal generations pose additional complexities; images, \\naudio, and video evaluations require their own evaluation methods and metrics. Finally, real-\\nworld environments pose further challenges, as they are dynamic and unpredictable, making \\nit difficult to evaluate agents in controlled settings.\\nLooking ahead, to solve these open challenges, the field of agent evaluation is evolving \\nrapidly. Key trends include a shift towards process-based evaluation, prioritizing the \\nunderstanding of agent reasoning; an increase in AI-assisted evaluation methods for \\nimproved scalability; and a stronger focus on real-world application contexts. The \\ndevelopment of new standardized benchmarks is also gaining traction, facilitating objective \\ncomparisons between agents, while increased emphasis on explainability and interpretability \\naims to provide deeper insights into agent behavior.\", 'Agents Companion\\nFebruary 2025\\n23\\nTable 1: A table comparing strengths and weaknesses of automated evaluations for Agents.\\nAt this point it should be clear that only by continually refining evaluation methods, we will \\nensure that AI agents are developed and deployed responsibly, effectively, and ethically in \\nthe coming future. \\nMultiple Agents & Their Evaluation\\nAgent evaluation, which assesses the effectiveness, reliability, and adaptability of \\nautonomous AI agents, as seen in the previous section, has emerged as a key focus area. \\nWe have seen a significant evolution in AI systems, transitioning towards multi-agent \\narchitectures—where multiple specialized agents collaborate to achieve complex objectives. \\nA multi-agent system is like a team of experts, each specializing in a particular area, working \\ntogether to solve a complex problem. Each agent is an independent entity, potentially using a \\ndifferent LLM, and with its own unique role and context. Agents communicate and collaborate \\nto achieve a common goal. This approach differs from traditional single-agent systems, \\nwhere one LLM handles all aspects of a task.\\nEvaluation Method  Strengths  Weaknesses\\nHuman Evaluation\\nCaptures nuanced behavior, \\nconsiders human factors\\nSubjective, time-consuming, \\nexpensive, difficult to scale\\nLLM-as-a-Judge Scalable, efficient, consistent\\nMay overlook intermediate steps, \\nlimited by LLM capabilities\\nAutomated Metrics Objective, scalable, efficient\\nMay not capture full capabilities, \\nsusceptible to gaming', \"Agents Companion\\nFebruary 2025\\n24\\nMulti-agent systems offer several advantages over single-agent systems:\\n• Enhanced Accuracy: Agents can cross-check each other's work, leading to more \\naccurate results.\\n• Improved Efficiency: Agents can work in parallel, speeding up task completion.\\n• Better Handling of Complex Tasks: Large tasks can be broken down into smaller, more \\nmanageable subtasks, with each agent focusing on a specific aspect.\\n• Increased Scalability: The system can be easily scaled by adding more agents with \\nspecialized capabilities.\\n• Improved Fault Tolerance: If one agent fails, others can take over its responsibilities.\\n• Reduced Hallucinations and Bias: By combining the perspectives of multiple agents, \\nthe system can reduce the effects of hallucinations and bias, leading to more reliable and \\ntrustworthy outputs.\\nUnderstanding Multi-Agent Architectures\\nUnlike traditional monolithic AI systems, multi-agent architectures break down a problem \\ninto distinct tasks handled by specialized agents. Each agent operates with defined roles, \\ninteracting dynamically with others to optimize decision-making, knowledge retrieval, and \\nexecution. These architectures enable more structured reasoning, decentralized problem-\\nsolving, and scalable task automation, offering a paradigm shift from single-agent workflows.\\nAt their core, multi-agent systems leverage principles of modularity, collaboration, and \\nhierarchy to create a robust AI ecosystem. Agents within these systems can be categorized \\nbased on their function for example:\", 'Agents Companion\\nFebruary 2025\\n25\\n• Planner Agents: Responsible for breaking down high-level objectives into \\nstructured sub-tasks.\\n• Retriever Agents: Optimize knowledge acquisition by dynamically fetching relevant data \\nfrom external sources.\\n• Execution Agents: Perform computations, generate responses, or interact with APIs.\\n• Evaluator Agents: Monitor and validate responses, ensuring coherence and alignment \\nwith objectives.\\nThrough these components, multi-agent architectures extend beyond simple prompt-based \\ninteractions, enabling AI-driven workflows that are adaptive, explainable, and efficient.\\nMulti-Agent Design Patterns and Their Business Impact\\nTo design effective multi-agent architectures, specific design patterns have emerged. These \\npatterns define interaction protocols, delegation mechanisms, and role distributions, allowing \\nbusinesses to implement AI-driven automation in structured ways. Some common design \\npatterns include:', 'Agents Companion\\nFebruary 2025\\n26\\nTable 2: A table comparing types of multi-agent systems.\\nType of Multi-Agent System Description Example\\nSequential\\nAgents work in a sequential \\nmanner, with each agent \\ncompleting its task before \\npassing the output to the \\nnext agent.\\nAn assembly line, where each \\nworker performs a specific \\noperation before passing the \\nproduct to the next worker.\\nHierarchical\\nAgents are organized in a \\nhierarchical structure, with a \\n\"manager\" agent coordinating the \\nworkflow and delegating tasks to \\n\"worker\" agents.\\nA system with a leader agent \\nresponsible for making strategic \\ndecisions and follower agents \\nexecuting tasks based on the \\nleader\\'s instructions.\\nCollaborative\\nAgents work together \\ncollaboratively, sharing \\ninformation and resources to \\nachieve a common goal.\\nA team of researchers working \\non a project, where each member \\ncontributes their expertise \\nand insights.\\nCompetitive\\nAgents may compete with \\neach other to achieve the \\nbest outcome.\\nLLMs act as individual players in a \\ngame like Overcooked-AI, where \\nthey must coordinate their actions \\nto achieve a shared goal while \\ncompeting for resources and \\noptimizing individual performance.', 'Agents Companion\\nFebruary 2025\\n27\\nFigure 7: An image depicting different multi-agent topologies, from LangGraph documentation. 23\\nThe choice of design pattern depends on the specific application and the desired level of \\ninteraction between agents.\\nBusinesses benefit from these patterns by reducing operational bottlenecks, improving \\nknowledge retrieval, and enhancing automation reliability. Multi-agent systems enable \\ncompanies to scale AI deployments while ensuring agility in decision-making and \\nworkflow execution.', 'Agents Companion\\nFebruary 2025\\n28\\nImportant components of Agents\\nThe architecture of LLM-based AI agents consists of several interrelated components \\nessential for autonomous operation and intelligent interaction:\\n• Interaction Wrapper: This component serves as the interface between the agent \\nand its environment, managing communication and adapting to various input and \\noutput modalities.\\n• Memory Management: This includes short-term working memory for immediate context, \\ncache, and sessions . It also can include long-term storage for learned patterns and \\nexperiences, as episodes, examples, skills or reference data. It also includes “reflection” \\nto decide which short term items (eg: user preference) should be copied into long term \\nmemory (eg: user profile), and if that can be shared across agents, tasks, or sessions.\\n• Cognitive Functionality: This is often underpinned by Chain-of-Thought (CoT), ReACT, \\nreasoning, thinking, or a planner subsystem - it allows agents to decompose complex \\ntasks into logical steps and engage in self-correction. In some cases this also includes \\nuser intent refinement, to ask a clarifying question if uncertain.\\n• Tool Integration: This subsystem enables agents to utilize external tools, expanding \\ntheir capabilities beyond natural language processing. Dynamic tool registries allowing \\ndiscovery, registration, and “Tool RAG”.\\n• Flow / Routing: This governs connections with other agents, facilitating dynamic neighbor \\ndiscovery and efficient communication within the multi-agent system.  This might be \\nimplemented as a delegation of a task to a background agent, or handoff of the user \\ninteraction to an agent, or the use of an agent as a tool.', 'Agents Companion\\nFebruary 2025\\n29\\n• Feedback Loops / Reinforcement Learning: These enable continuous learning and \\nadaptation by processing interaction outcomes and refining decision-making strategies.  \\nFor gen AI agents this rarely takes the form of traditional RL training, but the performance \\nmetrics of the past can be incorporated into future decision making.\\n• Agent Communication: Effective communication between agents is crucial for the \\nsuccess of multi-agent systems. The Agent to Agent communication protocol facilitates \\nstructured and efficient communication among agents, enabling them to achieve \\nconsensus and address complex problems collaboratively\\n• Remote Agent Communication: Agent to Agent communication within an organization \\nis critical to allows agents to share messages, tasks, and knowledge.  Once your multi-\\nagent system includes a remote agent, the communication protocol becomes even more \\nimportant. Asynchronous tasks and sessions need to be durable, and updated with \\nnotifications while end users are not in session.  Negotiations between Agents must allow \\nfor bringing a user into session and for supported UX capabilities.  \\n• Agent & Tool Registry (mesh): As you go beyond a handful of tools or a handful of \\nagents, you need a robust system to discover, register, administer, select and utilize from \\na “mesh” of tools or agents.  Critically important is the ontology and description of the \\ntools and agents, their capabilities and requirements, and their performance metrics. Your \\nagents will make a plan and choose which tool or which agent to use from such a system, \\nand those choices are informed by the data in the system', 'Agents Companion\\nFebruary 2025\\n30\\nFigure 8: An image demonstrating the process of a user interacting with multiple, self-coordinating agents. 24  \\nThese architectural elements provide a robust foundation for the autonomous operation and \\ncollaborative capabilities of LLM agents within multi-agent systems.', 'Agents Companion\\nFebruary 2025\\n31\\nChallenges in Multi-Agent systems \\nWhile multi-agent systems offer numerous advantages, they also face several challenges:\\n• Task Communication: Today most agent frameworks communicate in messages, not \\nstructured async tasks.\\n• Task Allocation: Efficiently dividing complex tasks among different agents can be \\nchallenging, and feedback loops are often left up to the developer to implement.\\n• Coordinating Reasoning: Getting agents to debate and reason together effectively \\nrequires sophisticated coordination mechanisms.\\n• Managing Context: Keeping track of all the information, tasks, and conversations \\nbetween agents can be overwhelming.\\n• Time and Cost: Multi-agent interactions can be computationally expensive and time-\\nconsuming.  This results in higher runtime prices and more user latency.\\n• Complexity: In the same way that microservice architecture allows each microservice \\nmore flexibility and simplicity, the system as a whole usually becomes more complex.  \\nAddressing these challenges is crucial for developing robust and efficient \\nmulti-agent systems.', 'Agents Companion\\nFebruary 2025\\n32\\nMulti-Agent Evaluation\\nLuckily, the evaluation of multi-agent systems is a clear progression of evaluating single \\nagent systems. Agent Success Metrics are unchanged, business metrics as your north star, \\ngoals and critical task success metrics, application telemetry metrics like latency and errors.  \\nInstrumenting the multi-agent system with trace will help debug and understand what is \\nhappening during complex interactions.\\nIn the Agent Evaluation section we discussed Evaluating Trajectories and Evaluating the Final \\nResponse as the 2 best approaches to automated evaluation of an agent, and this remains \\nthe case for multi-agent systems. For a multi-agent system, a trajectory of actions might \\ninclude several or even all of your agents. Even though several agents may collaborate on a \\ntask, a single final answer is returned to the user at the end and can be evaluated in isolation.\\nBecause a multi-agent system probably has more steps, you can drill down and evaluate at \\nevery step.  You can evaluate each of your agents in isolation and the system as a whole.  \\nTrajectory evaluations are a scalable approach to do exactly this.\\nThere are some questions you need to ask, which are unique to multi-agent \\nsystems, including:\\n• Cooperation and Coordination: How well do agents work together and coordinate their \\nactions to achieve common goals?\\n• Planning and Task Assignment: Did we come up with the right plan, and did we stick to \\nit?  Did child agents deviate from the main plan or get lost in a cul-de-sac?\\n• Agent Utilization: How effectively do agents select the right agent and choose to use the \\nagent as a tool, delegate a background task, or transfer the user?', \"Agents Companion\\nFebruary 2025\\n33\\n• Scalability: Does the system's quality improve as more agents are added?  Does the \\nlatency go down?  Are we being more efficient or less?\\nThese types of questions can guide developers to identify areas for improvement in the \\nmulti-agent system. You will answer these questions using the same tools you use for single \\nagent systems, but the complexity of the analysis goes up.\\nAgentic RAG: A Critical Evolution in \\nRetrieval-Augmented Generation\\nA significant advancement in multi-agent architectures is Agentic Retrieval-Augmented \\nGeneration (Agentic RAG). Traditional RAG pipelines rely on a static approach—retrieving \\nknowledge from vector databases and feeding it into an LLM for synthesis. However, this \\napproach often fails when dealing with ambiguous, multi-step, or multi-perspective queries.\\nAgentic RAG introduces autonomous retrieval agents that actively refine their search \\nbased on iterative reasoning. These agents enhance retrieval in the following ways:\\n• Context-Aware Query Expansion: Instead of relying on a single search pass, agents \\ngenerate multiple query refinements to retrieve more relevant and comprehensive results.\\n• Multi-Step Reasoning: Agents decompose complex queries into smaller logical steps, \\nretrieving information sequentially to build structured responses.\\n• Adaptive Source Selection: Instead of fetching data from a single vector database, \\nretrieval agents dynamically select the best knowledge sources based on context.\\n• Validation and Correction: Evaluator agents cross-check retrieved knowledge for \\nhallucinations and contradictions before integrating it into the final response.\", \"Agents Companion\\nFebruary 2025\\n34\\nThis approach significantly improves response accuracy, explainability, and adaptability, \\nmaking it a crucial innovation for enterprises dealing with complex knowledge retrieval tasks \\nin areas like legal research, scientific discovery, and business intelligence.\\nAgentic RAG and its Importance\\nAgentic RAG (Retrieval-Augmented Generation) is an advanced approach that combines the \\nstrengths of RAG with the autonomy of AI agents. Traditional RAG systems retrieve relevant \\ninformation from external knowledge sources to enhance LLM responses. Agentic RAG \\ntakes this a step further by employing intelligent agents to orchestrate the retrieval process, \\nevaluate the retrieved information, and make decisions about how to best utilize it.\\nAgentic RAG offers several advantages over traditional RAG:\\n• Improved Accuracy: Agents can evaluate the quality of retrieved information and make \\ndecisions about which sources to trust, leading to more accurate and reliable responses.\\n• Enhanced Contextual Understanding: Agents can consider the context of the user's \\nquery and the retrieved information to generate more relevant and meaningful responses.\\n• Increased Adaptability: Agents can adapt to changing information needs and \\ndynamically adjust their retrieval strategies to provide the most up-to-date and relevant \\ninformation. This adaptability is crucial in complex domains where information is constantly \\nevolving, such as healthcare, finance, and legal research.\", 'Agents Companion\\nFebruary 2025\\n35\\nFigure 9: A diagram of Agentic RAG from Vectorize.io. 25\\nAgentic RAG is particularly valuable in complex domains where information is constantly \\nevolving, such as healthcare, finance, and legal research. For example, in healthcare, agentic \\nRAG can help navigate complicated medical databases, research papers, and patient \\nrecords, providing doctors with comprehensive and accurate information.', 'Agents Companion\\nFebruary 2025\\n36\\nBetter Search, Better RAG\\nAlmost all RAG approaches require a search engine to index and retrieve relevant data. The \\nintroduction of agents allows for refinement of query, filtering, ranking, and the final answer.  \\nAgentic RAG agents are executing several searches to retrieve information.\\nFor developers who are trying to optimize existing RAG implementations, it is usually most \\nvaluable to improve search results (measured in recall) prior to introducing agents.  Some of \\nthe main techniques to improve search performance are:\\n• Parse source documents and chunk them: Vertex AI Layout Parser can handle complex \\ndocument layouts, embedded tables, and embedded images like charts, and uses a \\nsemantic chunker to keep chunks on topic with a hierarchy of headings.\\n• Add metadata to your chunks: synonyms, keywords, authors, dates, tags and categories \\nallow your searches to boost, bury, and filter; these allow your users or your agents more \\ncontrol over search results.\\n• Fine tune the embedding model or add a search adaptor which changes embedding \\nspace: these allow the searchable index of vectors to represent your domain better than a \\ngeneral purpose embedding model.\\n• A faster vector database can improve search quality: to search embeddings, you must \\nmake a tradeoff between speed and accuracy, upgrading to an ultra-fast Vertex AI \\nVector Search can improve both latency and quality\\n• Use a ranker: vector searches are fast but approximate, they should return dozens or \\nhundreds of results which need to be re-ranked by a more sophisticated system to ensure \\nthe top few results are the most relevant or best answer.\\n• Implement check grounding: as a safeguard on grounded generation, you can ensure \\neach phrase is actually citable by retrieved chunks.', 'Agents Companion\\nFebruary 2025\\n37\\nFigure 10: A diagram of common RAG and search components, showing Vertex AI Search 26 , search builder \\nAPIs 27 , and RAG Engine. 28\\nVertex AI Search26 is a powerful search engine providing Google quality search for your \\ndata and can be used with any RAG or Agentic RAG implementation. Each of the above \\ncomponents is automatically available within Vertex AI Search, without any development \\ntime at all.  For developers who want to build their own search engine, each of the above \\ncomponents is exposed as a stand alone API27, and RAG Engine28 can orchestrate the whole \\npipeline easily, with a LlamaIndex like python based interface.', 'Agents Companion\\nFebruary 2025\\n38\\nAgents in the enterprise\\nManager of agents\\n2025 is the year of the agents, and this will particularly be true in the context of enterprises \\nthat will develop and use agents that will assist employees to perform specific tasks \\nor autonomously run in the background performing automation. Business analysts can \\neffortlessly uncover industry trends and create compelling, data-driven presentations \\nfueled by AI-generated insights. HR teams can revolutionize the employee experience with \\nstreamlined onboarding, even for complex tasks like 401k selection. Software engineers \\ncan proactively identify and resolve bugs, enabling them to build and iterate with greater \\nefficiency, and accelerate deployment cycles. Marketers can unlock deeper performance \\nanalysis, optimize content recommendations, and fine-tune campaigns effortlessly to achieve \\nbetter results. \\nWe see two types of agents emerging:\\n1. “Assistants”: Agents that interact with the user, take a task, execute it, and come back \\nto the user. Conversational agents popularized by frameworks like Gems or GPTs usually \\nbelong to this category. Assistants can be general - able to help on a variety of tasks - or \\nspecialized to a particular domain, or tasks. Examples include agents that help schedule \\nmeetings, analyze data, write code, write marketing press releases, help sellers with \\nsales opportunities, or even agents that perform deep research on a particular topic \\nas requested by the user. These agents can be synchronous and return the requested \\ninformation or perform the requested task fast, or they run for a longer period of time \\nbefore returning (like the deep research agents).', 'Agents Companion\\nFebruary 2025\\n39\\n2. \"Automation agents”: Agents that run in the background, listen to events, monitor \\nchanges in systems or data, and then make smart decisions and act. Action might include \\nacting on backend systems, performing some tests to validate the observation, fixing \\nproblems, notifying the right employees, etc. This can really be seen as the backbone of \\nthe automation of the future. While in the past we had to write special code to specify \\nthe logic of automations, now we can start relying on smart and general decision making \\nabilities of AI agents.\\nRather than simply invoking agents to perform a task and wait for the output, knowledge \\nworkers will increasingly become managers of agents. They will be assigning tasks to multiple \\nagents and manage them, check if any of agents need help or require approval to proceed, \\nuse the output of specific agents to start new tasks, monitor execution of long running tasks \\nto review and steer them in the right direction. Novel user interfaces to allow this type of \\nvirtual team management will emerge to allow orchestration, monitoring and managing a \\nmulti-agent system with agents performing tasks, calling (or even creating) other agents.\\nGoogle Agentspace aims at providing this type of experience and allow to:\\n• Create new agents by using a no/low code interface or a full code framework to \\nimplement both specialized assistants and automation agents\\n• Configure and manage the agents access for company administrators\\n• Invoke the right agents when appropriate\\n• Monitor, manage, and orchestrate multiple agents in a UI suited for team management', \"Agents Companion\\nFebruary 2025\\n40\\nGoogle Agentspace\\nGoogle Agentspace29 offers a suite of AI-driven tools designed to elevate enterprise \\nproductivity by facilitating access to pertinent information and automating intricate, \\nagentic workflows. It harnesses the advanced reasoning capabilities of Gemini, the power \\nof Google's search infrastructure, and secure access to enterprise data, irrespective of its \\nphysical location.\\nAgentspace directly addresses the limitations inherent in traditional knowledge management \\nsystems, which frequently fall short in areas such as personalization, automated answer \\ngeneration, contextual comprehension, and comprehensive information retrieval. It \\nempowers employees with efficient information access by consolidating disparate content \\nsources, generating grounded and personalized responses, and streamlining operational \\nworkflows. Key functionalities include the capacity to ingest a wide variety of data formats, \\nsynchronize data from Software-as-a-Service (SaaS) platforms, deliver access-controlled \\nsearch results and AI-generated answers, and integrate AI assistance and intelligent agents \\ninto cohesive workflows.\\nThe architecture of Agentspace Enterprise is founded upon several core principles.  \\nParamount among these is built-in trust, which prioritizes security, explainability, and \\ngovernance through features such as single sign-on (SSO) authentication, an integrated \\npermissions model, and user-level access controls. Google's advanced intelligence is \\nleveraged to discern user behavior and content patterns through machine learning, resulting \\nin the delivery of highly relevant results via semantic understanding, knowledge graphs, \\nand LLMs. Universal connectivity is achieved by connecting to a diverse array of enterprise \\nsystems with on-demand and automated data refreshes, thereby eliminating information \\nsilos. Enterprise-level customization and user-level personalization are facilitated through \\ngranular controls for search functionality, recommendations, LLMs, and knowledge graphs, \\nproviding tailored experiences based on individual user roles, permissions, and search\", 'Agents Companion\\nFebruary 2025\\n41\\nhistory. Real-time feedback and adaptation mechanisms enable the continuous refinement \\nof results through machine learning and user input. Blended Retrieval Augmented Generation \\n(RAG) allows for customizable data blending, powering generative AI applications grounded \\nin enterprise data. Finally, scalability is a critical design consideration, with the system \\nengineered to accommodate growth across geographical regions, languages, and peak \\nusage demands.\\nSecurity is always top of mind. Google Agentspace is built on the Google Cloud secure-by-\\ndesign infrastructure, giving you the peace of mind to confidently deploy AI agents across \\nyour organization. It provides granular IT controls, including role-based access control \\n(RBAC), VPC Service Controls, and IAM integration, ensuring your data remains protected \\nand compliant at all times.Security is a foundational principle of Agentspace. Built upon the \\nsecure infrastructure of Google Cloud, it provides a robust environment for the deployment \\nof AI agents. Granular IT controls, encompassing role-based access control (RBAC), \\nVirtual Private Cloud (VPC) Service Controls, and Identity and Access Management (IAM) \\nintegration, guarantee data protection and regulatory compliance. These security measures \\nare essential for the safeguarding of sensitive information and give users the peace of mind \\nto confidently deploy AI agents across their organization.\\nNotebookLM Enterprise\\nNotebookLM30 is a research and learning tool designed to streamline the process of \\nunderstanding and synthesizing complex information. It empowers users to upload various \\nsource materials, including documents, notes, and other relevant files, and leverages the \\npower of artificial intelligence to facilitate deeper comprehension. Imagine researching a \\nmultifaceted topic; NotebookLM allows you to consolidate all your scattered resources into', 'Agents Companion\\nFebruary 2025\\n42\\na single, organized workspace. In essence, NotebookLM functions as a dedicated research \\nassistant, accelerating the research process and enabling users to move beyond mere \\ninformation collection to genuine understanding.\\nFigure 11: NotebookLM Enterprise 30  configured with a few mock earning reports.\\nNotebookLM Plus builds upon this foundation, offering a premium tier with enhanced \\nfeatures and capabilities. While the core functionality of uploading sources, asking questions, \\nand generating summaries remains, NotebookLM Plus typically adds features like increased \\nstorage for source materials, enabling work with larger and more complex projects. It', 'Agents Companion\\nFebruary 2025\\n43\\nmay also include more sophisticated AI-powered analysis tools, such as more nuanced \\nsummarization options, enhanced question-answering capabilities, or the ability to identify \\nconnections and relationships between different sources more effectively.\\nBuilding upon the foundation of NotebookLM Plus, NotebookLM Enterprise30 brings these \\npowerful capabilities to the enterprise environment, streamlining how employees interact \\nwith and derive insights from their data. This enterprise-grade version enables users to not \\nonly upload and synthesize information, but also to uncover hidden patterns and engage with \\ndata in innovative ways. A prime example is the AI-generated audio summary feature, which \\nenhances comprehension and facilitates efficient knowledge absorption by allowing users to \\n\"listen\" to their research. \\nTechnically, NotebookLM, both in its consumer and enterprise forms, employs LLMs to \\nprocess uploaded documents, extract key concepts, and generate summaries. The audio \\nsummary feature uses text-to-speech (TTS) technology incorporating advanced prosody \\ncontrol to ensure clarity and naturalness. Critically, NotebookLM Enterprise incorporates \\nenterprise-grade security and privacy features, ensuring that sensitive company data is \\nhandled with the utmost care and protected in accordance with organizational policies.`\\nGoogle AgentSpace Enterprise\\nGoogle Agentspace furnishes employees with a unified, company-branded, multimodal \\nsearch agent that serves as the definitive source of enterprise information. Drawing upon \\nGoogle\\'s extensive search capabilities, Agentspace offers unparalleled conversational \\nassistance. Employees get answers to complex queries, proactive recommendations, \\nand unified access to information from any source. This functionality extends to both \\nunstructured data, such as documents and emails, and structured data, like tables.  \\nIntegrated translation capabilities ensure comprehensive understanding, regardless of', \"Agents Companion\\nFebruary 2025\\n44\\nthe original language of the information. Pre-built connectors for widely used third-party \\napplications, including Confluence, Google Drive, Jira, Microsoft SharePoint, ServiceNow, \\nand others, empower employees to seamlessly access and query relevant data sources, \\nfacilitating more informed decision-making.\\nFigure 12: Agentspace 29  customized for Cymbal bank, connected to several data stores.\\nMuch more than just information retrieval, agents can take actions in the real world, manage \\nasync tasks and workflows, and help employees get work done. A gallery of agents can be \\nconfigured based on an organization's unique needs and will help with deep research, idea \\ngeneration and merit based refinement, creative asset generation, data analytics and more.\", 'Agents Companion\\nFebruary 2025\\n45\\nAgentspace Enterprise Plus facilitates the creation of custom AI agents tailored to specific \\nbusiness functions. This platform enables the development and deployment of contextually \\naware generative AI agents, empowering employees across departments—marketing, \\nfinance, legal, engineering, and more—to conduct more effective research, rapidly generate \\ncontent, and automate repetitive tasks, including multi-step workflows. A centralized \\ninterface streamlines the discovery and access of these specialized agents, promoting \\nscalable AI adoption. Custom agents will connect with internal and external systems and \\ndata, be tailored to company domain and policies, and potentially even utilize machine \\nlearning models trained on proprietary business data. The platform provides builders tools \\nfor agent development, deployment, and lifecycle managemen', 'Agents Companion\\nFebruary 2025\\n46\\nFrom agents to contractors\\nThe common interface to define AI agents across various tools and platforms today is very \\nsimple, and usually includes specifying the goal, some textual instructions, the tools that the \\nagent can use, and a set of examples. While this might be sufficient to prototype demos, it \\nleads to potentially underspecified definitions, and might be one of the leading reasons that \\nAI agents can struggle to get from prototype-to-production. \\nWe propose to evolve the Agent interface to evolve them into ”Contract adhering agents” \\nwhich are aimed at solving complex tasks using AI Agents, more specifically in contexts \\nwhere stakes are high. \\nContracts\\nThe key idea of contractors is to specify and the standard the contracts between the \\nrequester and the agents, making it possible to: \\n1. Define the outcomes as precisely as possible, similarly to what we would do in a \\nreal world scenario when agreeing on a service with a company we are contracting to \\ndo something for us. This allows the agent (contractor) to validate against the desired \\noutcomes and iterate until the desired objective is achieved. \\n2. Make it possible to negotiate the task as well as clarifying and refining it, in order \\nto avoid any ambiguity in the definition, and fill any gap in common understanding of \\nthe goals.\\n3. Define the rules for the contractors to generate new subcontracts needed to solve the \\nbigger one in a standard fashion (cf. section below on subcontracts).', 'Agents Companion\\nFebruary 2025\\n47\\n4. \\nTable 3: An example data model for the contracts.\\nContract, initial definition\\nFields Description Required\\nTask/Project\\ndescription\\nProvide a detailed description of what we expect the\\ncontractor to achieve. It should be as specific and as non-\\nambiguous as possible.\\nYes\\nDeliverables &\\nSpecifications\\nDescribe precisely the expected outcomes and\\ndeliverables from the contractor&#39;s task, including a list of\\nspecifications clarifying what makes the deliverable\\nacceptable as outcome and details on how to verify that\\nthe deliverable is fulfilling the expectation.\\nYes\\nScope\\nClarify the scope of the tasks that the contractor is\\nresponsible for completing, going into separate detail\\nabout every aspect of the task. Also used to clarify what is out \\nof scope.\\nNo \\nExpected Cost\\nGives expectation in terms of cost for the task completion.\\nThis is usually a function of the complexity of the task\\ncombined with what tools will be used.\\nYes\\nExpected Duration Gives expectation in terms of duration for the task completion. Yes\\nInput Sources Specify what input sources can be used and considered to be \\nuseful to complete the task. No\\nReporting and\\nFeedback\\nSpecifies how the feedback loop should look like: how often we \\nexpect updates on the progress, and what mechanism/surface \\nis used to provide feedback (emails, APIs, etc.).\\nYes', 'Agents Companion\\nFebruary 2025\\n48\\nTable 4: An example data model for the iteration of messaging between contractors.\\nContract Iteration: Feedback & Negotiation\\nFields Description Required\\nUnderspecification Highlight aspects that are underspecified or\\nneed clarification from the task initiator. No\\nCost negotiation Cost considered too high to complete the task. No\\nRisk Highlights potential risks in fulfilling the contract. No \\nAdditional input \\nneeded\\nExpress the kinds of additional data or information that would \\nbe useful to fulfill the contract. No', 'Agents Companion\\nFebruary 2025\\n49\\nContract Lifecycle\\nThe following illustration shows the lifecycle of defining, negotiating and executing the \\ncontract: \\nFigure 13: A diagram of the agent as contractor lifecycle from Agentspace.\\nContract execution\\nThis requires the contractor runtime to be able to fulfill the contracts and solve the \\ntasks behind contracts according to their defined specifications. Prioritizing quality and \\ncompleteness over latency enables to fully leverage the capabilities of LLMs, for example', 'Agents Companion\\nFebruary 2025\\n50\\nby generating different solutions, and reviewing, scoring, and evolving them. An example of \\nthis methodology is shown below in the Co-Scientist study. The engine can iterate and self-\\nvalidate the results and deliverables based on the provided expectations, and can improve \\nand self-correct until the validators are fulfilled. The ability to concretely validate a solution \\nagainst a set of objective criterias has proven to work extremely well in the context of AI and \\nhas been at the core of successful AI systems such as for example Alpha-Code.\\nContract Negotiation\\nOne core hypothesis behind contracts in the context of automation agents specifically is that \\nmany tasks in the enterprise world can draw significant benefit from leveraging the power of \\nLLMs when used in a less-constrained manner (latency and cost wise).\\nBeing able to tackle more and more complex tasks and making it possible for customers to \\nbe able to rely on and trust the results of contractors will ensure real value for companies. \\nEven that being said, we need to have a notion of relative priority in order to make sure that \\ntasks are appropriately prioritized as well as resources fairly allocated. We thus introduce a \\nnotion of cost (typically relative per customer or contract initiator) which can be discussed \\nand negotiated between the contract initiator and the contractor, in order to make sure that \\nthe contract receives the adequate resources relative to the other contracts initiated by the \\ncontract initiator. The contractors can also negotiate other aspects of the contracts, for \\nexample in terms of specification and deliverables (cf. also section below on feedback).', 'Agents Companion\\nFebruary 2025\\n51\\nContract Feedback\\nContracts give a vehicle to provide feedback and in particular resolve ambiguities. As tasks \\nbecome more and more complex, it is critical to be able to raise ambiguities or other issues \\nrelated to the tasks specifications as early as possible.  Contractors can give feedback on the \\ncontract just after having received the contract (initial contract assessment), and then at a \\nfrequency predefined in the contract.\\nThis feedback will contain clarification requests, or other types of feedback about the \\nunderspecification or misspecification of tasks (inconsistencies, conflicting specs, \\nclarification, etc.).\\nSubcontracts\\nAlthough not part of the contract definition and specification directly, the ability to \\ndecompose a task into subtasks by generating subcontracts is a core concept that will be \\nused to power the contractors’ engine.\\nWhen a task is considered too complex to be tackled directly, contractors can decide to \\ndecompose the task into smaller and easier tasks, which will be added to the execution \\nqueue for solving.  This is made possible only through the contract formalization described \\nabove, which makes it possible for the contractors to generate, process and manipulate \\nother contracts in a uniform and standardized way.', 'Agents Companion\\nFebruary 2025\\n52\\nGoogle\\'s Co-Scientist: A Case Study in \\nMulti-Agent Intelligence\\nGoogle\\'s AI co-scientist is a prime example of a multi-agent LLM system applied to scientific \\nresearch. This system utilizes a team of specialized agents, each with its own role and \\nexpertise, to accelerate the pace of scientific discovery. These agents collaborate to \\ngenerate, evaluate, and refine hypotheses, mirroring the iterative process of scientific inquiry.\\nThe co-scientist system employs a \"generate, debate, and evolve\" approach, drawing \\ninspiration from the scientific method. This approach involves generating diverse hypotheses, \\ncritically evaluating their potential, and refining them through ongoing feedback and analysis. \\nThe system leverages the strengths of different LLMs, each specializing in a particular aspect \\nof the research process, to achieve a more comprehensive and robust outcome.\\nFor instance, in a study on liver fibrosis treatments, the co-scientist not only identified \\nexisting drugs but also proposed new mechanisms and promising drug candidates, \\ndemonstrating its potential to generate novel insights. Some of its major components are: \\n• Data Processing Agents: aggregate and structure large volumes of experimental data.\\n• Hypothesis Generators: propose potential explanations based on existing research and \\nnew findings.\\n• Validation Agents: run simulations and verify results before presenting them \\nto researchers.\\n• Collaboration Agents: communicate findings across different research teams, enhancing \\ninterdisciplinary cooperation.', 'Agents Companion\\nFebruary 2025\\n53\\nFigure 14: A process diagram of the AI co-scientist system 31\\nThis approach exemplifies how multi-agent architectures move beyond static AI interactions, \\ninstead fostering dynamic, evolving intelligence capable of handling sophisticated tasks in \\nresearch, enterprise automation, and knowledge management.', 'Agents Companion\\nFebruary 2025\\n54\\nAutomotive AI: Real World Use of \\nMulti-Agent Architecture\\nTo illustrate how multi-agent concepts manifest in practice, let\\'s examine a comprehensive \\nmulti-agent system designed for automotive conversational AI, where several specialized \\nagents collaborate to provide an intuitive and seamless in-car experience.\\nFirst, we’ll look at some examples of specialized agents, followed by some common patterns \\nfor combining them to form a multi-agent system.\\nSpecialized Agents\\nLet’s first look at some examples of specialized agents. \\nConversational Navigation Agent\\nThe Conversational Navigation Agent is specialized in helping users find locations, suggest \\nplaces, and navigate using APIs like Google Places and Maps. This agent understands \\nnavigation intents like \"Find me a sushi restaurant nearby,\" queries the Places API for relevant \\nresults, re-ranks these results based on user preferences and historical data, and sends \\nnavigation commands to the car\\'s built-in system.', 'Agents Companion\\nFebruary 2025\\n55\\nExample Interaction:\\nUser: \"I need to take a break for lunch. What are good restaurants on my route \\nto Munich?\"\\nNavigation Agent: \"I found three well-rated restaurants along your route on the A9. In 25 \\nkilometers, there\\'s \\'Gasthaus Alte Mühle\\' at the Ingolstadt exit with 4.5 stars - reviewers \\npraise their traditional Bavarian dishes and beer garden. In 45 kilometers, \\'La Strada\\' at \\nthe service area has 4.1 stars with good reviews for quick Italian meals. Or if you can wait \\n70 kilometers, \\'Zeitlos\\' near Allershausen has 4.7 stars - it\\'s a bit fancier with farm-to-table \\ncuisine. Which one interests you most?\"\\nUser: \"The Bavarian place sounds good. Is there convenient parking there, and how far is \\nit from the motorway?\"\\nNavigation Agent: \"Gasthaus Alte Mühle has a dedicated parking area with about 40 \\nspaces. It\\'s only 1.2 kilometers from the A9 exit at Ingolstadt-Nord - about a 2-minute drive. \\nThey\\'re known for their Schweinebraten and homemade dumplings. It gets busy around \\nnoon, but it\\'s currently showing moderate occupancy. Would you like me to set this as your \\ndestination and guide you there? You\\'ll arrive around 12:15 if we head there now.\"\\nUser: \"The Bavarian place sounds good. Is there convenient parking there, and how far is \\nit from the motorway?\"\\nNavigation Agent: \"Gasthaus Alte Mühle has a dedicated parking area with about 40 \\nspaces. It\\'s only 1.2 kilometers from the A9 exit at Ingolstadt-Nord - about a 2-minute drive. \\nThey\\'re known for their Schweinebraten and homemade dumplings. It gets busy around \\nnoon, but it\\'s currently showing moderate occupancy. Would you like me to set this as your \\ndestination and guide you there? You\\'ll arrive around 12:15 if we head there now.\"', 'Agents Companion\\nFebruary 2025\\n56\\nConversational Media Search Agent\\nThe Conversational Media Search Agent is specialized in helping users find and play music, \\naudiobooks, and podcasts. This agent retrieves media from local databases or streaming \\nservices, uses web search for additional content when necessary, suggests media based on \\ncontextual factors like mood, weather, and time of day, and handles voice commands such \\nas \"Play a relaxing playlist for my drive.\" It also identifies similar artists or content when users \\nask questions like \"Who sounds like John Coltrane?\"\\nExample Interaction:\\nUser: \"Play something upbeat to help me stay alert on this long motorway drive.\"\\nMedia Search Agent: \"I\\'ll play an upbeat driving playlist to help keep you energized. \\nStarting with \\'Don\\'t Stop Me Now\\' by Queen, followed by some Daft Punk and Stromae \\ntracks. I\\'ve added some popular European radio hits that match your preferences. I\\'ve \\nadjusted the volume for optimal road awareness. Just say \\'next\\' if you want to skip \\na song.\"\\nMessage Composition Agent\\nThe Message Composition Agent is specialized in helping users draft, summarize, and send \\nmessages or emails while driving. This agent parses voice commands like \"Tell Alex I\\'m \\nrunning late,\" uses natural language models to compose contextually appropriate messages, \\noffers draft previews with user modification options, and integrates with various messaging \\napps including SMS, WhatsApp, and email.', 'Agents Companion\\nFebruary 2025\\n57\\nExample Interaction:\\nUser: \"There\\'s heavy traffic on the E40. Let my team know I\\'ll be 20 minutes late to the \\nmorning standup.\"\\nMedia Search Agent: \"I\\'ve prepared a message to your work team: \\'Stuck in traffic on \\nthe E40. Will be about 20 minutes late to our morning standup. Please start without me.\\' \\nShould I send this to your \\'Work Team\\' chat group, or would you like to edit it first?\"\\nCar Manual Agent\\nThe Car Manual Agent is specialized in providing answers to car-related questions by \\nleveraging a Retrieval-Augmented Generation (RAG) system. This agent retrieves relevant \\nsections from the car manual based on user queries, summarizes and contextualizes this \\ninformation using an LLM, and links users to detailed documentation or instructional videos \\nwhen appropriate.\\nExample Interaction:\\nUser: \"How do I turn off the lane-keeping assist feature in this Volkswagen?\"\\nMedia Search Agent: \"To turn off lane-keeping assist in your Volkswagen, press the \\nbutton with the car-and-lane icon located on the left side of your steering wheel. You\\'ll \\nsee a confirmation in the driver display when it\\'s deactivated. You can also disable \\nit permanently in the infotainment system under Settings > Driver Assistance > Lane \\nKeeping. Would you like me to walk you through the settings menu instead?\"', 'Agents Companion\\nFebruary 2025\\n58\\nGeneral Knowledge Agent\\nThe General Knowledge Agent is specialized in answering factual questions about the world, \\nhistory, science, culture, and other general topics. This agent accesses a broad knowledge \\nbase to answer factual questions, provides biographical information about people, offers \\ncontextual explanations of concepts and phenomena, maintains grounding in factual \\ninformation while avoiding hallucinations, and responds to follow-up questions by maintaining \\ncontextual awareness.\\nExample Interaction:\\nUser: \"Tell me an interesting fact about Salzburg.\"\\nMedia Search Agent: \"Salzburg, Austria is the birthplace of Mozart, one of history\\'s most \\nfamous composers. Beyond that well-known fact, Salzburg\\'s old town is a UNESCO World \\nHeritage site, and parts of the beloved musical \\'The Sound of Music\\' were filmed there in \\n1964. The Salzburg Festival, one of the world\\'s most prestigious music and drama festivals, \\nhas been held there annually since 1920.\"\\nPatterns in Use\\nLet’s look at the some common ways used to combine these agents.\\nHierarchical Pattern\\nA central Orchestrator Agent classifies queries and routes them to specialized agents. For \\nexample, when a user asks \"Find me a sushi restaurant nearby,\" the Orchestrator detects \\nthis as a navigation request and routes it to the Navigation Agent, which handles location \\nsearches and interaction with maps APIs.', 'Agents Companion\\nFebruary 2025\\n59\\nFigure 15. Hierarchical pattern.\\nThe Orchestrator Agent is specialized in routing queries to the most appropriate specialized \\nagents and managing the overall conversation flow. This agent analyzes user queries to \\ndetermine domain and intent, directs requests to the most suitable specialized agent, \\nmaintains conversation context across multiple turns, and manages fallback strategies when \\nspecialized agents cannot provide satisfactory responses.\\nExample Interaction:\\nFirst Turn:\\nUser: \"Is there a service area ahead with clean toilets?\"\\nOrchestrator Agent: [internally routes to Navigation Agent]\\nNavigation Agent: \"There\\'s a service area 20 kilometers ahead on the E19. It\\'s Aire \\nde Waremme, which was renovated last year and has good ratings for facilities and \\ncleanliness. It also has a Paul bakery and a quick-service restaurant.\"\\nSecond Turn:', 'Agents Companion\\nFebruary 2025\\n60\\nUser: \"Will it rain at my destination this evening?\"\\nOrchestrator Agent: [recognizes weather query and routes to Weather \\nInformation Agent]\\nWeather Information Agent: \"At your destination in Brussels, the forecast shows a 60% \\nchance of light rain in the evening, starting around 19:00. Temperatures will be around \\n14°C. I\\'d recommend bringing an umbrella if you plan to be outdoors.\"\\nDiamond Pattern\\nIn this variation of the hierarchical pattern, responses from specialized agents pass \\nthrough a central moderation agent before reaching the user. For instance, the Navigation \\nAgent generates a factual response about a nearby restaurant, which then goes through \\na Rephraser Agent that adjusts the tone and style based on user preferences before \\ntext-to-speech synthesis.\\nThe Rephraser Agent is specialized in adapting the tone, style, and presentation of responses \\nto match user preferences and context. This agent transforms technical information into \\nconversational language, adjusts response length based on driving conditions and urgency, \\npersonalizes communication style to match user preferences, and formats complex \\ninformation for easier comprehension while driving.\\nExample Interaction:\\n[Raw response from Car Manual Agent before rephrasing]: \"In the event of a tyre \\npressure monitoring system alert, the vehicle has detected that one or more tyres have \\nfallen below the recommended inflation pressure of 2.2 bar. The affected tyre\\'s pressure is \\napproximately 1.8 bar, which is 18.75% below recommended levels. This does not constitute \\nan immediate safety hazard, but prolonged driving with underinflated tyres can lead to', 'Agents Companion\\nFebruary 2025\\n61\\ndecreased fuel efficiency, reduced tyre longevity, and potentially compromised handling \\ncharacteristics. It is advisable to proceed to a service station at your earliest convenience \\nto reinflate the tyre to proper specifications.\"\\nRephraser Agent: [Adapts for driving context, adjusts information density and urgency]\\nFinal Response to User: \"Your front right tyre is a bit low - down to 1.8 bar. It\\'s safe to \\nkeep driving, but you\\'ll want to add some air at the next petrol station. Low tyre pressure \\ncan affect your fuel economy and make the tyre wear out faster. I can help you find a \\nnearby place to fill up if you\\'d like.\"\\nFigure 16. Diamond pattern.', 'Agents Companion\\nFebruary 2025\\n62\\nExample transformation:\\nInitial Response (factual & neutral): \"I found a highly-rated sushi restaurant nearby. It\\'s \\ncalled Sakura Sushi, located at 123 Main Street. It has a 4.7-star rating and is open until 10 \\nPM. Would you like me to start navigation?\"\\nAfter rephrasing (playful style): \"Sushi craving? Say no more! Head over to Sakura Sushi \\nat 123 Main Street, where the fish is fresh and the soy sauce flows freely! Rated 4.7 stars, \\nopen till 10 PM. Ready for an umami adventure?”\\nPeer-to-Peer\\nAgents can hand off queries to one another when they detect that the orchestration \\nmade a routing mistake. This creates a more resilient system that can recover from \\ninitial misclassifications.\\nFigure 17. Peer-to-peer.', 'Agents Companion\\nFebruary 2025\\n63\\nExample scenario:\\n1. User asks: \"Find a place to eat sushi nearby\"\\n2. The Orchestrator correctly routes this to the Conversational Navigation Agent, which \\nprovides information about nearby sushi restaurants.\\n3. User follows up: \"How big is New York\\'s Central Park?\"\\n4. The Orchestrator might initially route this to the Conversational Navigation Agent again \\n(based on the previous navigation-related conversation).\\n5. However, the Conversational Navigation Agent recognizes this as a general knowledge \\nquestion rather than a navigation request, and hands it off to the General Knowledge \\nAgent, which can provide factual information about Central Park\\'s size.\\nAdvantages of peer-to-peer hand-off compared to centralized orchestration:\\n1. Resilience to misclassification: Even if the central orchestrator makes an error in \\nrouting, specialized agents can recognize when a query falls outside their domain and \\nredirect appropriately.\\n2. Domain expertise in routing: Specialized agents often have better understanding of the \\nboundaries of their own domains. The Media Search Agent knows exactly what kinds of \\nmusic-related queries it can handle better than a general orchestrator would.\\n3. Reduced orchestration complexity: The central orchestrator doesn\\'t need perfect \\naccuracy in initial routing, reducing the complexity of its decision-making logic.', \"Agents Companion\\nFebruary 2025\\n64\\nCollaborative Pattern\\nThe Collaborative Pattern involves multiple agents working on complementary aspects of \\nthe same task, with a Response Mixer Agent that combines elements from different agent \\nresponses to create a comprehensive answer. This approach recognizes that different agents \\ncontribute valuable pieces to a complete solution based on their specialized expertise. The \\npattern is particularly valuable when:\\n1. Different aspects of a query require different types of expertise (e.g., technical \\nspecifications, practical advice, and conceptual explanations)\\n2. No single agent has complete information to fully address the user's needs\\n3. The user would benefit from multiple perspectives on the same question\\n4. Different specialized agents have access to distinct knowledge bases or \\nreasoning capabilities\\nUnlike the competitive approach where responses compete, the collaborative pattern \\nassumes that responses from different agents are complementary rather than redundant. \\nThe Response Mixer Agent identifies the most valuable information from each source and \\nsynthesizes it into a cohesive answer that leverages the unique strengths of each specialist.\\nFor example, when asked about handling hydroplaning, the Car Manual Agent contributes \\nvehicle-specific safety system information, the Driving Tips Agent provides practical driving \\ntechniques, and the General Knowledge Agent explains the physics behind the phenomenon. \\nTogether, they create a more complete and useful response than any single agent could \\nprovide alone.\", 'Agents Companion\\nFebruary 2025\\n65\\nFigure 18. Collaborative pattern.', 'Agents Companion\\nFebruary 2025\\n66\\nResponse Mixer Agent\\nThe Response Mixer is specialized in picking the best responses from multiple agents \\nand merging them when needed. This agent evaluates answers based on their accuracy \\nand relevance, removes incorrect information, and combines useful parts from different \\nresponses to create the most helpful answer for the user.\\nExample Interaction:\\nUser: \"How do I handle aquaplaning?\"\\n[Behind the scenes, multiple agents generate responses:]\\nCar Manual Agent Response: \"Your Audi is equipped with Electronic Stability Control \\n(ESC) and Anti-lock Braking System (ABS) which help prevent aquaplaning by maintaining \\ntraction control.\" [Confidence score: 71%]\\nGeneral Knowledge Agent Response: \"Aquaplaning occurs when your tyres lose contact \\nwith the road surface due to water buildup between the tyres and the road.\" [Confidence \\nscore: 65%]\\nSafety Tips Agent Response: \"If you experience aquaplaning, ease off the accelerator, \\navoid sudden braking or steering, and gently steer in the direction you want to go until you \\nregain traction.\" [Confidence score: 94%]\\nResponse Mixer Agent: [Picks the most relevant parts and merges them into one \\ncomplete answer]\\nFinal Response to User: \"If you experience aquaplaning, which happens when your \\ntyres lose contact with the road due to water buildup, ease off the accelerator, avoid \\nsudden braking or steering, and gently steer in the direction you want to go until you \\nregain traction. Your Audi\\'s Electronic Stability Control and ABS systems will help maintain \\ncontrol, but proper driving technique is still essential.\"', 'Agents Companion\\nFebruary 2025\\n67\\nThis competitive approach ensures that critical safety information isn\\'t missed due to \\nmisrouting, and the user receives the most helpful possible response even when their query \\nhas multiple valid interpretations.\\nAdaptive Loop Pattern\\nThe Adaptive Loop Pattern involves iterative refinement through repeated attempts, \\nprogressively improving results until they meet desired criteria.\\nThe Conversational Navigation Agent can implement a loop pattern to iteratively improve \\nsearch results when initial queries don\\'t yield satisfactory outcomes.\\nExample Interaction:\\n1. User asks: \"Find a nice Italian restaurant that serves vegan options\"\\n2. The Navigation Agent queries Places API with these parameters\\n3. The agent evaluates the results and finds no restaurants meeting all criteria\\n4. Instead of returning \"no results,\" the agent automatically reformulates the query:\\n• First loop: Searches for \"Italian restaurants with vegetarian options\"\\n• Second loop: Broadens to \"Italian restaurants\" and then filters for those mentioning \\nplant-based options\\n• Third loop: If still unsuccessful, searches for \"vegan restaurants\" and filters for \\nItalian-influenced cuisine\\n5. The agent presents the best matches from these progressive searches, explaining how \\nthey relate to the original request', \"Agents Companion\\nFebruary 2025\\n68\\nThis looping behavior enables more robust search capabilities that adapt to availability and \\ncontext, providing useful results even when exact matches aren't available.\\nAdvantages of Multi-Agent Architecture for Automotive AI\\nMulti-agent systems bring powerful advantages to automotive AI by breaking down complex \\ntasks into specialized roles. Think of it as assembling a team of experts rather than relying on \\none generalist.\\nIn this approach, each agent specializes in what it does best. The Navigation Agent focuses \\nsolely on finding locations and planning routes. The Media Search Agent becomes an \\nexpert in music and podcasts. The Car Manual Agent specializes in vehicle features and \\ntroubleshooting. By focusing on specific domains, each agent develops deeper capabilities in \\nits area.\\nThis specialization makes the entire system more efficient. When an agent handles a \\nnarrower set of tasks, it becomes simpler to optimize its performance. The result is higher \\nquality responses delivered more quickly and at lower computational cost. Performance \\nimproves because the system can match the right resources to each task. Simple requests \\nuse minimal processing power, while complex questions tap into more powerful resources \\nonly when needed.\\nSpeed matters in a vehicle, and multi-agent systems deliver where it counts. Critical \\nfunctions like adjusting climate controls or opening windows run on fast, on-device \\nagents for immediate response. Meanwhile, less urgent tasks like finding restaurant \\nrecommendationscan use cloud-based agents with more extensive knowledge. This \\nseparation ensures that essential vehicle controls remain responsive regardless of what else \\nthe system is doing.\", 'Agents Companion\\nFebruary 2025\\n69\\nThis design also creates natural resilience. If internet connectivity drops the essential \\nfunctions running on on-device agents continue working. You might temporarily lose \\nrestaurant recommendations, but climate control and basic media playbacks still \\nfunction perfectly.\\nAgent Builder\\nVertex AI Agent Builder is a collection of products and services for developers. We have \\nput together a comprehensive platform for you to build and connect agents. The engineering \\nexcellence and security from Google Cloud, the AI research from Google Deepmind, and the \\nbest practices of Agent Ops. Google Cloud is building our own agents on top of this platform, \\nand now you can too. Expect many more exciting announcements 2025 targeting developers \\nof agents.\\nVertex AI Agent Engine streamlines development, relying on Google engineering managed \\nintegrations with popular open source agent libraries. It provides a managed autoscaling \\nruntime and many services agents will need (eg: session, examples, trace, evals). This is the \\nvery low effort and high value way to safely deploy agents you have written in any framework.\\nVertex AI Eval Service provides all of the evaluation tools discussed in this whitepaper, and \\nmore. LLMs, RAG, and Agent evals are stable and scalable via the Vertex AI Eval Service, with \\nconvenient integrations into monitoring and experimentation offerings.\\nA large portfolio of agent tools, with more to watch out for:\\n• Retrieval via Vertex AI Search26 or RAG Engine28.\\n• Non-search based retrieval from DBs via Gen AI Toolbox for Databases32', 'Agents Companion\\nFebruary 2025\\n70\\n• Application integrations33 with hundreds of APIs supporting full ACLs\\n• Turn any API into a managed, enterprise ready tool with Apigee Hub34\\nAnd of course the best LLMs for agents, with access to Vertex AI Model Garden35 and also \\nthe Gemini family of models36 which will power the agentic era.\\nSummary\\nThis whitepaper (a companion to our earlier whitepaper on Agents) has explored the rapidly \\nevolving landscape of generative AI agents, from their fundamental architecture to advanced \\nevaluation techniques and the transformative potential of multi-agent systems. \\nKey Takeaways for Developers:\\n1. Agent Ops is Essential: Building successful agents goes far beyond the initial proof-\\nof-concept. Embrace Agent Ops principles, integrating best practices from DevOps and \\nMLOps, but also focusing on agent-specific elements like tool management, orchestration, \\nmemory, and task decomposition.\\n2. Metrics Drive Improvement: Start with business-level KPIs (like goal completion, user \\nengagement, or revenue) as your \"north star.\" Then, instrument your agents to track \\ngranular metrics related to critical tasks, user interactions, and agent actions (traces).  \\nHuman feedback (👍👍, user surveys) is invaluable.\\n3. Automated Evaluation is Key: Don\\'t rely solely on manual testing.  Implement automated \\nevaluation frameworks that assess agent capabilities, trajectory (the steps taken), and the \\nfinal response. Leverage techniques like exact match, in-order match, precision/recall for \\ntrajectory evaluation, and autoraters (LLMs as judges) for final response quality.', 'Agents Companion\\nFebruary 2025\\n71\\n4. Human-in-the-Loop is Crucial: Automated metrics are powerful, but human evaluation \\nprovides essential context, especially for subjective aspects like creativity, common sense, \\nand nuance. Use human feedback to calibrate and validate your automated evaluation \\nmethods. Don’t outsource the domain knowledge.\\n5. Multi-Agent Systems Offer Advantages: Consider multi-agent architectures for \\ncomplex tasks. They can improve accuracy, efficiency, scalability, and fault tolerance. \\nUnderstand different design patterns (sequential, hierarchical, collaborative, competitive) \\nand choose the right one for your application.\\n6. Agentic RAG Improves Relevance: Move beyond traditional RAG by incorporating \\nagents that actively refine search queries, evaluate retrieved information, and adapt to \\nevolving knowledge. This leads to more accurate and contextually relevant responses.\\n7. Search Optimization is Foundational to RAG: Before diving into complex agentic RAG, \\noptimize your underlying search engine. Techniques like semantic chunking, metadata \\nenrichment, fine-tuning embedding models, and using rankers can significantly improve \\nretrieval quality.\\n8. Agent and Tool Registries are Important: As the number of Agents or Tools you \\nare using grow, a registry to manage the capabilities, ontology, and performance \\nbecomes essential.\\n9. Security is Paramount: When deploying agents, especially within an enterprise, prioritize \\nsecurity. Leverage platforms like Google Agentspace that offer built-in security features \\nlike RBAC, VPC Service Controls, and IAM integration.\\n10. Efficient use of developer cycles: The classic build vs buy design choices remain front \\nof mind, as the industry of gen AI agents is rapidly evolving. Consider platforms and \\nproducts as alternatives to building everything from scratch. This will buffer some of the \\nchurn of a fast changing industry and allow you to focus on your data, domain, and users.', 'Agents Companion\\nFebruary 2025\\n72\\n11. Agents in the enterprise: Agents are transforming the way we work by making us much \\nmore productinve, and the way automation can be accomplished. Knowledge workers will \\nincreasingly be managing fleets of agents and novel UX will emerge. Google Agentspace \\nis a powerful tool allowing to put Enterprise Search, AI and AI Agents on top of company’s \\ndata and workflows\\nFuture Directions for Agent Research and Development: The field of AI agents is \\nundergoing rapid evolution.  Key areas of ongoing research and development include:\\n• Advanced Evaluation Methods: Developing more robust and scalable evaluation \\ntechniques, including process-based evaluation (focusing on reasoning), AI-assisted \\nevaluation, and standardized benchmarks.\\n• Multi-Agent Coordination: Improving the coordination and communication mechanisms \\nwithin multi-agent systems to enable more effective collaboration, task handling, \\nand reasoning.\\n• Real-World Adaptation: Creating agents that can adapt and learn in dynamic, \\nunpredictable real-world environments. Production systems like automotive AI \\nillustrate how agents must balance between on-device performance for critical \\nfunctions and cloud-based capabilities for complex tasks, often adapting to changing \\nconnectivity conditions.\\n• Explainability and Interpretability: Making agent behavior more transparent and \\nunderstandable, allowing developers and users to gain deeper insights into their \\ndecision-making processes.\\n• Long-Term Memory and Learning: Developing more sophisticated memory mechanisms \\nthat allow agents to retain and utilize information over extended periods, enabling \\ncontinuous learning and adaptation.', 'Agents Companion\\nFebruary 2025\\n73\\n• Agent Communication Protocols: Better defining how agents share tasks, knowledge, \\nand messages, especially across remote systems which are opaque.\\n• From Agents to contractors: In order for agents to get to next level of reliability and \\nutility, we will need to step up the definition of tasks, making them into contracts with clear \\ndeliverables, validation mechanisms, and ability to negotiate ambiguities, similarly to how \\nwe contract work from other companies.\\nCall to Action:\\nThe future of AI is agentic.  We encourage developers to embrace these concepts and \\nbegin building the next generation of intelligent applications.  Start experimenting with the \\ntools and techniques discussed in this whitepaper. Explore the resources available, such \\nas Google Agentspace, NotebookLM Enterprise, Vertex Eval Service, Cloud Observability, \\nand Vertex AI Search, to accelerate your development process. Dive into the provided code \\nexamples, tutorials, and documentation to gain hands-on experience. Build, evaluate, iterate, \\nand contribute to the growing community of agent developers. The possibilities are limitless, \\nand the time to build is now! Specifically, get started with the code and Colab notebooks in \\nthe references.', \"Agents Companion\\nFebruary 2025\\n74\\nEndnotes\\n1. Shafran, I., Cao, Y. et al., 2022, 'ReAct: Synergizing Reasoning and Acting in Language Models'.  \\nAvailable at: https://arxiv.org/abs/2210.03629 .\\n2. Wei, J., Wang, X. et al., 2023, 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models'. \\nAvailable at: https://arxiv.org/pdf/2201.11903.pdf .\\n3. Wang, X. et al., 2022, 'Self-Consistency Improves Chain of Thought Reasoning in Language Models'.  \\nAvailable at: https://arxiv.org/abs/2203.11171 .\\n4. Diao, S. et al., 2023, 'Active Prompting with Chain-of-Thought for Large Language Models'.  \\nAvailable at: https://arxiv.org/pdf/2302.12246.pdf .\\n5. Zhang, H. et al., 2023, 'Multimodal Chain-of-Thought Reasoning in Language Models'.  \\nAvailable at: https://arxiv.org/abs/2302.00923 .\\n6. Yao, S. et al., 2023, 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models'.  \\nAvailable at: https://arxiv.org/abs/2305.10601 .\\n7. Long, X., 2023, 'Large Language Model Guided Tree-of-Thought'.  \\nAvailable at: https://arxiv.org/abs/2305.08291 .\\n8. Google. ';Google Gemini Application'. Available at: http://gemini.google.com .\\n9. Swagger. 'OpenAPI Specification'. Available at: https://swagger.io/specification/ .\\n10. Xie, M., 2022, 'How does in-context learning work? A framework for understanding the differences \\nfromtraditional supervised learning'. Available at: https://ai.stanford.edu/blog/understanding-incontext/ .\\n11. Google Research. 'ScaNN (Scalable Nearest Neighbors)'.  \\nAvailable at: https://github.com/google-research/google-research/tree/master/scann.\\n12. LangChain. 'LangChain'. Available at: https://python.langchain.com/v0.2/docs/introduction/ .\\n13. Sokratis Kartakis, 2024, ‘GenAI in Production: MLOps or GenAIOps?’. Available \\nat: https://medium.com/google-cloud/genai-in-production-mlops-or-genaiops-25691c9becd0 .\\n14. Sokratis Kartakis, 2024 ‘Gen AI Ops, Operationalize Generative AI, A practical Guide’. Available at: \\nhttps://medium.com/google-cloud/genaiops-operationalize-generative- ai-a-practical-  \\nguide-d5bedaa59d78.\", 'Agents Companion\\nFebruary 2025\\n75\\n15. Cloud Trace overview. Available at: https://cloud.google.com/trace/docs/overview .\\n16. Berkeley Function-Calling Leaderboard (BFCL). Available at: \\nhttps://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html .\\n17. Karthik Narasimhan, et al. 2024, ‘τ-bench’. Available at https://arxiv.org/abs/2406.12045 .\\n18. Karthik Valmeekam, et al., 2023, ‘PlanBench’. Available at: https://arxiv.org/abs/2206.10498 .\\n19. Xiao Liu, et al., 2023, ‘AgentBench’. Available at: https://arxiv.org/abs/2308.03688 .\\n20. Martin Iglesias, et al., 2025, `DBAStep ` Available at: https://huggingface.co/spaces/adyen/DABstep.\\n21. LangSmith platform for agent observability.  \\nAvailable at: https://docs.smith.langchain.com/evaluation/concepts#agents .\\n22. Mingchen Zhuge, et al., 2024, ‘Agent-as-a-Judge: Evaluate Agents with Agents’.  \\nAvailable at: https://arxiv.org/abs/2410.10934 .\\n23. Multi-agent documentation from LangGraph.  \\nAvailable at: https://langchain-ai.github.io/langgraph/concepts/multi_agent/ .\\n24. LangChain blog 2024, ‘Multi-agent workflows’.  \\nAvailable at: https://blog.langchain.dev/langgraph-multi-agent-workflows/ .\\n25. Vectorize blog 2024, ‘How I finally got agentic RAG to work right’.  \\nAvailable at: https://vectorize.io/how-i-finally-got-agentic-rag-to-work-right/ .\\n26. Vertex AI Search, product documentation. Available at: https://cloud.google.com/enterprise-search .\\n27. Vertex AI Search Builder APIs, product documentation.   \\nAvailable at: https://cloud.google.com/generative-ai-app-builder/docs/builder-apis .\\n28. Vertex AI RAG Engine, product documentation.  \\nAvailable at: https://cloud.google.com/vertex-ai/generative-ai/docs/rag-overview .\\n29. Agentspace product documentation.  \\nAvailable at: https://cloud.google.com/agentspace/agentspace-enterprise/docs/overview .\\n30. NotebookLM Enterprise product documentation.  \\nAvailable at: https://cloud.google.com/agentspace/notebooklm-enterprise/docs/overview .', 'Agents Companion\\nFebruary 2025\\n76\\n31. Juraj Gottweis, et. al., 2025, ‘Accelerating scientific breakthroughs with an AI co-scientist’. Available at:  \\nhttps://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/ .\\n32. Hamsa Buvaraghan, et al. 2025,  ‘Announcing public beta of Gen AI Toolbox for Databases’. Available at: \\nhttps://cloud.google.com/blog/products/ai-machine-learning/announcing-gen-ai-toolbox  \\n-for-databases-get-started-today?e=48754805 .\\n33. Google Cloud Integration Connectors, product documentation.  \\nAvailable at: https://cloud.google.com/integration-connectors/docs .\\n34. Apigee API Hub, product documentation.  \\nAvailable at: https://cloud.google.com/apigee/docs/apihub/what-is-api-hub .\\n35. Vertex AI Model Garden, product documentation.  \\nAvailable at: https://cloud.google.com/model-garden .\\n36. Gemini family of LLMs, product documentation.  \\nAvailable at: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models .\\n37. Get Started Evaluating Agents with the Vertex Eval Service. Available at: \\nhttps://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-agents .\\n38. Irina Sigler, Ivan Nardini. Jan 2025 ‘Introducing Agent Evaluation in Vertex AI’. Available at: \\nhttps://cloud.google.com/blog/products/ai-machine-learning/introducing-agent-evaluation-in-vertex  \\n-ai-gen-ai-evaluation-service?e=48754805 .\\n39. Review sample agent evaluation notebooks for LangGraph, CrewAI, and LangChain.  \\nAvailable at: https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/ .\\n40. Review many sample agents, primarily beginner and intermediate level.  \\nAvailable at: https://github.com/GoogleCloudPlatform/generative-ai/ .\\n41. Review many sample agents, intermediate and advanced levels.  \\nAvailable at: https://github.com/GoogleCloudPlatform/applied-ai-engineering-samples .', 'Agents\\nAuthors: Julia Wiesinger, Patrick Marlow  \\nand Vladimir Vuskovic', 'Agents\\n2\\nFebruary 2025\\nAcknowledgements\\nContent contributors\\nEvan Huang\\nEmily Xue\\nOlcan Sercinoglu\\nSebastian Riedel\\nSatinder Baveja\\nAntonio Gulli\\nAnant Nawalgaria\\nCurators and Editors\\nAntonio Gulli\\nAnant Nawalgaria\\nGrace Mollison \\nTechnical Writer\\nJoey Haymaker\\nDesigner\\nMichael Lanning', 'Introduction 4\\nWhat is an agent? 5\\nThe model 6\\nThe tools 7\\nThe orchestration layer 7\\nAgents vs. models 8\\nCognitive architectures: How agents operate  8\\nTools: Our keys to the outside world 12\\nExtensions  13\\nSample Extensions  15\\nFunctions  18\\nUse cases 21\\nFunction sample code 24\\nData stores 27\\nImplementation and application 28\\nTools recap 32\\nEnhancing model performance with targeted learning 33\\nAgent quick start with LangChain 35\\nProduction applications with Vertex AI agents 38\\nSummary 40\\nEndnotes 42\\nTable of contents', \"Agents\\n4\\nFebruary 2025\\nIntroduction\\nHumans are fantastic at messy pattern recognition tasks. However, they often rely on tools \\n- like books, Google Search, or a calculator - to supplement their prior knowledge before \\narriving at a conclusion. Just like humans, Generative AI models can be trained to use tools \\nto access real-time information or suggest a real-world action. For example, a model can \\nleverage a database retrieval tool to access specific information, like a customer's purchase \\nhistory, so it can generate tailored shopping recommendations. Alternatively, based on a \\nuser's query, a model can make various API calls to send an email response to a colleague \\nor complete a financial transaction on your behalf. To do so, the model must not only have \\naccess to a set of external tools, it needs the ability to plan and execute any task in a self-\\ndirected fashion. This combination of reasoning, logic, and access to external information \\nthat are all connected to a Generative AI model invokes the concept of an agent, or a \\nprogram that extends beyond the standalone capabilities of a Generative AI model. This \\nwhitepaper dives into all these and associated aspects in more detail.\\nThis combination of reasoning, \\nlogic, and access to external \\ninformation that are all connected \\nto a Generative AI model invokes \\nthe concept of an agent.\", 'Agents\\n5\\nFebruary 2025\\nWhat is an agent?\\nIn its most fundamental form, a Generative AI agent can be defined as an application that \\nattempts to achieve a goal by observing the world and acting upon it using the tools that it \\nhas at its disposal. Agents are autonomous and can act independently of human intervention, \\nespecially when provided with proper goals or objectives they are meant to achieve. Agents \\ncan also be proactive in their approach to reaching their goals. Even in the absence of \\nexplicit instruction sets from a human, an agent can reason about what it should do next to \\nachieve its ultimate goal. While the notion of agents in AI is quite general and powerful, this \\nwhitepaper focuses on the specific types of agents that Generative AI models are capable of \\nbuilding at the time of publication.\\nIn order to understand the inner workings of an agent, let’s first introduce the foundational \\ncomponents that drive the agent’s behavior, actions, and decision making. The combination \\nof these components can be described as a cognitive architecture, and there are many \\nsuch architectures that can be achieved by the mixing and matching of these components. \\nFocusing on the core functionalities, there are three essential components in an agent’s \\ncognitive architecture as shown in Figure 1.', 'Agents\\n6\\nFebruary 2025\\nFigure 1. General agent architecture and components\\nThe model\\nIn the scope of an agent, a model refers to the language model (LM) that will be utilized as \\nthe centralized decision maker for agent processes. The model used by an agent can be one \\nor multiple LM’s of any size (small / large) that are capable of following instruction based \\nreasoning and logic frameworks, like ReAct, Chain-of-Thought, or Tree-of-Thoughts. Models \\ncan be general purpose, multimodal or fine-tuned based on the needs of your specific agent \\narchitecture. For best production results, you should leverage a model that best fits your \\ndesired end application and, ideally, has been trained on data signatures associated with the \\ntools that you plan to use in the cognitive architecture. It’s important to note that the model is \\ntypically not trained with the specific configuration settings (i.e. tool choices, orchestration/\\nreasoning setup) of the agent. However, it’s possible to further refine the model for the \\nagent’s tasks by providing it with examples that showcase the agent’s capabilities, including \\ninstances of the agent using specific tools or reasoning steps in various contexts.', 'Agents\\n7\\nFebruary 2025\\nThe tools\\nFoundational models, despite their impressive text and image generation, remain constrained \\nby their inability to interact with the outside world. Tools bridge this gap, empowering agents \\nto interact with external data and services while unlocking a wider range of actions beyond \\nthat of the underlying model alone. Tools can take a variety of forms and have varying \\ndepths of complexity, but typically align with common web API methods like GET, POST, \\nPATCH, and DELETE. For example, a tool could update customer information in a database \\nor fetch weather data to influence a travel recommendation that the agent is providing to \\nthe user. With tools, agents can access and process real-world information. This empowers \\nthem to support more specialized systems like retrieval augmented generation (RAG), \\nwhich significantly extends an agent’s capabilities beyond what the foundational model can \\nachieve on its own. We’ll discuss tools in more detail below, but the most important thing \\nto understand is that tools bridge the gap between the agent’s internal capabilities and the \\nexternal world, unlocking a broader range of possibilities.\\nThe orchestration layer\\nThe orchestration layer describes a cyclical process that governs how the agent takes in \\ninformation, performs some internal reasoning, and uses that reasoning to inform its next \\naction or decision. In general, this loop will continue until an agent has reached its goal or a \\nstopping point. The complexity of the orchestration layer can vary greatly depending on the \\nagent and task it’s performing. Some loops can be simple calculations with decision rules, \\nwhile others may contain chained logic, involve additional machine learning algorithms, or \\nimplement other probabilistic reasoning techniques. We’ll discuss more about the detailed \\nimplementation of the agent orchestration layers in the cognitive architecture section.', 'Agents\\n8\\nFebruary 2025\\nAgents vs. models\\nTo gain a clearer understanding of the distinction between agents and models, consider the \\nfollowing chart:\\nModels Agents\\nKnowledge is limited to what is available in their \\ntraining data.\\nKnowledge is extended through the connection \\nwith external systems via tools\\nSingle inference / prediction based on the \\nuser query. Unless explicitly implemented for \\nthe model, there is no management of session \\nhistory or continuous context. (i.e. chat history)\\nManaged session history (i.e. chat history) to \\nallow for multi turn inference / prediction based \\non user queries and decisions made in the \\norchestration layer. In this context, a ‘turn’ is \\ndefined as an interaction between the interacting \\nsystem and the agent. (i.e. 1 incoming event/\\nquery and 1 agent response)\\nNo native tool implementation. Tools are natively implemented in agent \\narchitecture.\\nNo native logic layer implemented. Users can \\nform prompts as simple questions or use \\nreasoning frameworks (CoT, ReAct, etc.) to \\nform complex prompts to guide the model in \\nprediction.\\nNative cognitive architecture that uses reasoning \\nframeworks like CoT, ReAct, or other pre-built \\nagent frameworks like LangChain.\\nCognitive architectures: How agents operate \\nImagine a chef in a busy kitchen. Their goal is to create delicious dishes for restaurant \\npatrons which involves some cycle of planning, execution, and adjustment.', 'Agents\\n9\\nFebruary 2025\\n• They gather information, like the patron’s order and what ingredients are in the pantry \\nand refrigerator.\\n• They perform some internal reasoning about what dishes and flavor profiles they can \\ncreate based on the information they have just gathered.\\n• They take action to create the dish: chopping vegetables, blending spices, searing meat.\\nAt each stage in the process the chef makes adjustments as needed, refining their plan as \\ningredients are depleted or customer feedback is received, and uses the set of previous \\noutcomes to determine the next plan of action. This cycle of information intake, planning, \\nexecuting, and adjusting describes a unique cognitive architecture that the chef employs to \\nreach their goal.\\nJust like the chef, agents can use cognitive architectures to reach their end goals by \\niteratively processing information, making informed decisions, and refining next actions \\nbased on previous outputs. At the core of agent cognitive architectures lies the orchestration \\nlayer, responsible for maintaining memory, state, reasoning and planning. It uses the rapidly \\nevolving field of prompt engineering and associated frameworks to guide reasoning and \\nplanning, enabling the agent to interact more effectively with its environment and complete \\ntasks. Research in the area of prompt engineering frameworks and task planning for \\nlanguage models is rapidly evolving, yielding a variety of promising approaches. While not an \\nexhaustive list, these are a few of the most popular frameworks and reasoning techniques \\navailable at the time of this publication:\\n• ReAct, a prompt engineering framework that provides a thought process strategy for \\nlanguage models to Reason and take action on a user query, with or without in-context \\nexamples. ReAct prompting has shown to outperform several SOTA baselines and improve \\nhuman interoperability and trustworthiness of LLMs.', 'Agents\\n10\\nFebruary 2025\\n• Chain-of-Thought (CoT), a prompt engineering framework that enables reasoning \\ncapabilities through intermediate steps. There are various sub-techniques of CoT including \\nself-consistency, active-prompt, and multimodal CoT that each have strengths and \\nweaknesses depending on the specific application.\\n• Tree-of-thoughts (ToT),, a prompt engineering framework that is well suited for \\nexploration or strategic lookahead tasks. It generalizes over chain-of-thought prompting \\nand allows the model to explore various thought chains that serve as intermediate steps \\nfor general problem solving with language models.\\nAgents can utilize one of the above reasoning techniques, or many other techniques, to \\nchoose the next best action for the given user request. For example, let’s consider an agent \\nthat is programmed to use the ReAct framework to choose the correct actions and tools for \\nthe user query. The sequence of events might go something like this:\\n1. User sends query to the agent\\n2. Agent begins the ReAct sequence\\n3. The agent provides a prompt to the model, asking it to generate one of the next ReAct \\nsteps and its corresponding output:\\na. Question: The input question from the user query, provided with the prompt\\nb. Thought: The model’s thoughts about what it should do next\\nc. Action: The model’s decision on what action to take next\\ni. This is where tool choice can occur\\nii. For example, an action could be one of [Flights, Search, Code, None], where the first \\n3 represent a known tool that the model can choose, and the last represents “no \\ntool choice”', 'Agents\\n11\\nFebruary 2025\\nd. Action input: The model’s decision on what inputs to provide to the tool (if any)\\ne. Observation: The result of the action / action input sequence\\ni. This thought / action / action input / observation could repeat N-times as needed\\nf. Final answer: The model’s final answer to provide to the original user query\\n4. The ReAct loop concludes and a final answer is provided back to the user\\nFigure 2. Example agent with ReAct reasoning in the orchestration layer\\nAs shown in Figure 2, the model, tools, and agent configuration work together to provide \\na grounded, concise response back to the user based on the user’s original query. While \\nthe model could have guessed at an answer (hallucinated) based on its prior knowledge, \\nit instead used a tool (Flights) to search for real-time external information. This additional \\ninformation was provided to the model, allowing it to make a more informed decision based \\non real factual data and to summarize this information back to the user.', 'Agents\\n12\\nFebruary 2025\\nIn summary, the quality of agent responses can be tied directly to the model’s ability to \\nreason and act about these various tasks, including the ability to select the right tools, and \\nhow well that tools has been defined. Like a chef crafting a dish with fresh ingredients and \\nattentive to customer feedback, agents rely on sound reasoning and reliable information to \\ndeliver optimal results. In the next section, we’ll dive into the various ways agents connect \\nwith fresh data. \\nTools: Our keys to the outside world\\nWhile language models excel at processing information, they lack the ability to directly \\nperceive and influence the real world. This limits their usefulness in situations requiring \\ninteraction with external systems or data. This means that, in a sense, a language model \\nis only as good as what it has learned from its training data. But regardless of how much \\ndata we throw at a model, they still lack the fundamental ability to interact with the outside \\nworld. So how can we empower our models to have real-time, context-aware interaction with \\nexternal systems? Functions, Extensions, Data Stores and Plugins are all ways to provide this \\ncritical capability to the model.\\nWhile they go by many names, tools are what create a link between our foundational models \\nand the outside world. This link to external systems and data allows our agent to perform a \\nwider variety of tasks and do so with more accuracy and reliability. For instance, tools can \\nenable agents to adjust smart home settings, update calendars, fetch user information from \\na database, or send emails based on a specific set of instructions.\\nAs of the date of this publication, there are three primary tool types that Google models are \\nable to interact with: Extensions, Functions, and Data Stores. By equipping agents with tools, \\nwe unlock a vast potential for them to not only understand the world but also act upon it, \\nopening doors to a myriad of new applications and possibilities.', 'Agents\\n13\\nFebruary 2025\\nExtensions \\nThe easiest way to understand Extensions is to think of them as bridging the gap between \\nan API and an agent in a standardized way, allowing agents to seamlessly execute APIs \\nregardless of their underlying implementation. Let’s say that you’ve built an agent with a goal \\nof helping users book flights. You know that you want to use the Google Flights API to retrieve \\nflight information, but you’re not sure how you’re going to get your agent to make calls to this \\nAPI endpoint.\\nFigure 3. How do Agents interact with External APIs?\\nOne approach could be to implement custom code that would take the incoming user query, \\nparse the query for relevant information, then make the API call. For example, in a flight \\nbooking use case a user might state “I want to book a flight from Austin to Zurich.” In this \\nscenario, our custom code solution would need to extract “Austin” and “Zurich” as relevant \\nentities from the user query before attempting to make the API call. But what happens if the \\nuser says “I want to book a flight to Zurich” and never provides a departure city? The API call \\nwould fail without the required data and more code would need to be implemented in order \\nto catch edge and corner cases like this. This approach is not scalable and could easily break \\nin any scenario that falls outside of the implemented custom code.', 'Agents\\n14\\nFebruary 2025\\nA more resilient approach would be to use an Extension. An Extension bridges the gap \\nbetween an agent and an API by:\\n1. Teaching the agent how to use the API endpoint using examples.\\n2. Teaching the agent what arguments or parameters are needed to successfully call the \\nAPI endpoint.\\nFigure 4. Extensions connect Agents to External APIs\\nExtensions can be crafted independently of the agent, but should be provided as part of the \\nagent’s configuration. The agent uses the model and examples at run time to decide which \\nExtension, if any, would be suitable for solving the user’s query. This highlights a key strength \\nof Extensions, their built-in example types, that allow the agent to dynamically select the \\nmost appropriate Extension for the task. \\n \\nFigure 5. 1-to-many relationship between Agents, Extensions and APIs', 'Agents\\n15\\nFebruary 2025\\nThink of this the same way that a software developer decides which API endpoints to use \\nwhile solving and solutioning for a user’s problem. If the user wants to book a flight, the \\ndeveloper might use the Google Flights API. If the user wants to know where the nearest \\ncoffee shop is relative to their location, the developer might use the Google Maps API. In \\nthis same way, the agent / model stack uses a set of known Extensions to decide which one \\nwill be the best fit for the user’s query. If you’d like to see Extensions in action, you can try \\nthem out on the Gemini application by going to Settings > Extensions and then enabling any \\nyou would like to test. For example, you could enable the Google Flights extension then ask \\nGemini “Show me flights from Austin to Zurich leaving next Friday.”\\nSample Extensions \\nTo simplify the usage of Extensions, Google provides some out of the box extensions that \\ncan be quickly imported into your project and used with minimal configurations. For example, \\nthe Code Interpreter extension in Snippet 1 allows you to generate and run Python code from \\na natural language description.', 'Agents\\n16\\nFebruary 2025\\nPython\\nimport vertexai\\nimport pprint\\nPROJECT_ID = \"YOUR_PROJECT_ID\"\\nREGION = \"us-central1\"\\nvertexai.init(project=PROJECT_ID, location=REGION)\\nfrom vertexai.preview.extensions import Extension\\nextension_code_interpreter = Extension.from_hub(\"code_interpreter\")\\nCODE_QUERY = \"\"\"Write a python method to invert a binary tree in O(n) time.\"\"\"\\nresponse = extension_code_interpreter.execute(\\n  operation_id = \"generate_and_execute\",\\n  operation_params = {\"query\": CODE_QUERY}\\n  )\\nprint(\"Generated Code:\")\\npprint.pprint({response[\\'generated_code\\']})\\n# The above snippet will generate the following code.\\n```\\nGenerated Code:\\nclass TreeNode:\\n  def __init__(self, val=0, left=None, right=None(:\\n    self.val = val\\n    self.left = left\\n    self.right = right\\nContinues next page...', 'Agents\\n17\\nFebruary 2025\\nPython\\ndef invert_binary_tree(root):\\n    \"\"\"\\n    Inverts a binary tree.\\n    Args:\\n        root: The root of the binary tree.\\n    Returns:\\n        The root of the inverted binary tree.\\n    \"\"\"\\n    if not root:\\n        return None\\n    # Swap the left and right children recursively\\n    root.left, root.right =     \\ninvert_binary_tree(root.right), invert_binary_tree(root.left)\\n    return root\\n# Example usage:\\n# Construct a sample binary tree\\nroot = TreeNode(4)\\nroot.left = TreeNode(2)\\nroot.right = TreeNode(7)\\nroot.left.left = TreeNode(1)\\nroot.left.right = TreeNode(3)\\nroot.right.left = TreeNode(6)\\nroot.right.right = TreeNode(9)\\n# Invert the binary tree\\ninverted_root = invert_binary_tree(root)\\n```\\nSnippet 1. Code Interpreter Extension can generate and run Python code', 'Agents\\n18\\nFebruary 2025\\nTo summarize, Extensions provide a way for agents to perceive, interact, and influence the \\noutside world in a myriad of ways. The selection and invocation of these Extensions is guided \\nby the use of Examples, all of which are defined as part of the Extension configuration.\\nFunctions \\nIn the world of software engineering, functions are defined as self-contained modules \\nof code that accomplish a specific task and can be reused as needed. When a software \\ndeveloper is writing a program, they will often create many functions to do various tasks. \\nThey will also define the logic for when to call function_a versus function_b, as well as the \\nexpected inputs and outputs.\\nFunctions work very similarly in the world of agents, but we can replace the software \\ndeveloper with a model. A model can take a set of known functions and decide when to use \\neach Function and what arguments the Function needs based on its specification. Functions \\ndiffer from Extensions in a few ways, most notably:\\n1. A model outputs a Function and its arguments, but doesn’t make a live API call.\\n2. Functions are executed on the client-side, while Extensions are executed on \\nthe agent-side.\\nUsing our Google Flights example again, a simple setup for functions might look like the \\nexample in Figure 7.', 'Agents\\n19\\nFebruary 2025\\nFigure 7. How do functions interact with external APIs?\\nNote that the main difference here is that neither the Function nor the agent interact directly \\nwith the Google Flights API. So how does the API call actually happen?\\nWith functions, the logic and execution of calling the actual API endpoint is offloaded away \\nfrom the agent and back to the client-side application as seen in Figure 8 and Figure 9 below. \\nThis offers the developer more granular control over the flow of data in the application. There \\nare many reasons why a Developer might choose to use functions over Extensions, but a few \\ncommon use cases are:\\n• API calls need to be made at another layer of the application stack, outside of the direct \\nagent architecture flow (e.g. a middleware system, a front end framework, etc.)\\n• Security or Authentication restrictions that prevent the agent from calling an API directly \\n(e.g API is not exposed to the internet, or non-accessible by agent infrastructure)\\n• Timing or order-of-operations constraints that prevent the agent from making API calls in \\nreal-time. (i.e. batch operations, human-in-the-loop review, etc.)', 'Agents\\n20\\nFebruary 2025\\n• Additional data transformation logic needs to be applied to the API Response that the \\nagent cannot perform. For example, consider an API endpoint that doesn’t provide a \\nfiltering mechanism for limiting the number of results returned. Using Functions on the \\nclient-side provides the developer additional opportunities to make these transformations. \\n• The developer wants to iterate on agent development without deploying additional \\ninfrastructure for the API endpoints (i.e. Function Calling can act like “stubbing” of APIs)\\nWhile the difference in internal architecture between the two approaches is subtle as seen in \\nFigure 8, the additional control and decoupled dependency on external infrastructure makes \\nFunction Calling an appealing option for the Developer.\\nFigure 8. Delineating client vs. agent side control for extensions and function calling', 'Agents\\n21\\nFebruary 2025\\nUse cases\\nA model can be used to invoke functions in order to handle complex, client-side execution \\nflows for the end user, where the agent Developer might not want the language model to \\nmanage the API execution (as is the case with Extensions). Let’s consider the following \\nexample where an agent is being trained as a travel concierge to interact with users that want \\nto book vacation trips. The goal is to get the agent to produce a list of cities that we can use \\nin our middleware application to download images, data, etc. for the user’s trip planning. A \\nuser might say something like:\\nI’d like to take a ski trip with my family but I’m not sure where to go.\\nIn a typical prompt to the model, the output might look like the following:\\nSure, here’s a list of cities that you can consider for family ski trips:\\n• Crested Butte, Colorado, USA\\n• Whistler, BC, Canada\\n• Zermatt, Switzerland\\nWhile the above output contains the data that we need (city names), the format isn’t ideal \\nfor parsing. With Function Calling, we can teach a model to format this output in a structured \\nstyle (like JSON) that’s more convenient for another system to parse. Given the same input \\nprompt from the user, an example JSON output from a Function might look like Snippet \\n5 instead.', 'Agents\\n22\\nFebruary 2025\\nfunction_call {\\n  name: \"display_cities\"\\n  args: {\\n    \"cities\": [\"Crested Butte\", \"Whistler\", \"Zermatt\"],\\n    \"preferences\": \"skiing\"\\n    }\\n}\\nSnippet 5. Sample Function Call payload for displaying a list of cities and user preferences\\nThis JSON payload is generated by the model, and then sent to our Client-side server to do \\nwhatever we would like to do with it. In this specific case, we’ll call the Google Places API to \\ntake the cities provided by the model and look up Images, then provide them as formatted \\nrich content back to our User. Consider this sequence diagram in Figure 9 showing the above \\ninteraction in step by step detail.', \"Agents\\n23\\nFebruary 2025\\n \\nFigure 9. Sequence diagram showing the lifecycle of a Function Call\\nThe result of the example in Figure 9 is that the model is leveraged to “fill in the blanks” with \\nthe parameters required for the Client side UI to make the call to the Google Places API. The \\nClient side UI manages the actual API call using the parameters provided by the model in the \\nreturned Function. This is just one use case for Function Calling, but there are many other \\nscenarios to consider like:\\n• You want a language model to suggest a function that you can use in your code, but you \\ndon't want to include credentials in your code. Because function calling doesn't run the \\nfunction, you don't need to include credentials in your code with the function information.\", \"Agents\\n24\\nFebruary 2025\\n• You are running asynchronous operations that can take more than a few seconds. These \\nscenarios work well with function calling because it's an asynchronous operation.\\n• You want to run functions on a device that's different from the system producing the \\nfunction calls and their arguments.\\nOne key thing to remember about functions is that they are meant to offer the developer \\nmuch more control over not only the execution of API calls, but also the entire flow of data \\nin the application as a whole. In the example in Figure 9, the developer chose to not return \\nAPI information back to the agent as it was not pertinent for future actions the agent might \\ntake. However, based on the architecture of the application, it may make sense to return the \\nexternal API call data to the agent in order to influence future reasoning, logic, and action \\nchoices. Ultimately, it is up to the application developer to choose what is right for the \\nspecific application.\\nFunction sample code\\nTo achieve the above output from our ski vacation scenario, let’s build out each of the \\ncomponents to make this work with our gemini-2.0-flash-001 model. \\nFirst, we’ll define our display_cities function as a simple Python method.\", 'Agents\\n25\\nFebruary 2025\\nPython\\nfrom typing import Optional\\ndef display_cities(cities: list[str], preferences: Optional[str] = None):\\n \"\"\"Provides a list of cities based on the user\\'s search query and preferences.\\n Args:\\n  preferences (str): The user\\'s preferences for the search, like skiing,\\n  beach, restaurants, bbq, etc.\\n  cities (list[str]): The list of cities being recommended to the user.\\n Returns:\\n  list[str]: The list of cities being recommended to the user.\\n \"\"\"\\n return cities\\n \\nSnippet 6. Sample python method for a function that will display a list of cities.\\nNext, we’ll instantiate our model, build the Tool, then pass in our user’s query and tools to \\nthe model. Executing the code below would result in the output as seen at the bottom of the \\ncode snippet.', 'Agents\\n26\\nFebruary 2025\\nPython\\nfrom google.genai import Client, types\\nclient = Client(\\n    vertexai=True,\\n    project=\"PROJECT_ID\",\\n    location=\"us-central1\"\\n    )\\nres = client.models.generate_content(\\n    model=\"gemini-2.0-flash-001\",\\n    model=\"I\\'d like to take a ski trip with my family but I\\'m not sure where \\nto go?\",\\n    config=types.GenerateContentConfig(\\n        tools=[display_cities],\\n     automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True),\\n        tool_config=types.ToolConfig(\\n            function_calling_config=types.FunctionCallingConfig(mode=\\'ANY\\')\\n            )\\n        )\\n)\\nprint(f\"Function Name: {res.candidates[0].content.parts[0].function_call.name}\")\\nprint(f\"Function Args: {res.candidates[0].content.parts[0].function_call.args}\")\\n> Function Name: display_cities\\n> Function Args: {\\'preferences\\': \\'skiing\\', \\'cities\\': [\\'Aspen\\', \\'Park \\nCity\\', \\'Whistler\\']}\\n```\\nSnippet 7. Building a Tool, sending to the model with a user query and allowing the function call to take place', 'Agents\\n27\\nFebruary 2025\\nIn summary, functions offer a straightforward framework that empowers application \\ndevelopers with fine-grained control over data flow and system execution, while effectively \\nleveraging the agent/model for critical input generation. Developers can selectively choose \\nwhether to keep the agent “in the loop” by returning external data, or omit it based on \\nspecific application architecture requirements.\\nData stores\\nImagine a language model as a vast library of books, containing its training data. But unlike \\na library that continuously acquires new volumes, this one remains static, holding only the \\nknowledge it was initially trained on. This presents a challenge, as real-world knowledge is \\nconstantly evolving. Data Stores address this limitation by providing access to more dynamic \\nand up-to-date information, and ensuring a model’s responses remain grounded in factuality \\nand relevance.\\nConsider a common scenario where a developer might need to provide a small amount of \\nadditional data to a model, perhaps in the form of spreadsheets or PDFs.\\nFigure 10. How can Agents interact with structured and unstructured data?', 'Agents\\n28\\nFebruary 2025\\nData Stores allow developers to provide additional data in its original format to an agent, \\neliminating the need for time-consuming data transformations, model retraining, or fine-\\ntuning. The Data Store converts the incoming document into a set of vector database \\nembeddings that the agent can use to extract the information it needs to supplement its next \\naction or response to the user.\\nFigure 11. Data Stores connect Agents to new real-time data sources of various types.\\nImplementation and application\\nIn the context of Generative AI agents, Data Stores are typically implemented as a vector \\ndatabase that the developer wants the agent to have access to at runtime. While we won’t \\ncover vector databases in depth here, the key point to understand is that they store data \\nin the form of vector embeddings, a type of high-dimensional vector or mathematical \\nrepresentation of the data provided. One of the most prolific examples of Data Store usage \\nwith language models in recent times has been the implementation of Retrieval Augmented', 'Agents\\n29\\nFebruary 2025\\nGeneration (RAG) based applications. These applications seek to extend the breadth and \\ndepth of a model’s knowledge beyond the foundational training data by giving the model \\naccess to data in various formats like:\\n• Website content\\n• Structured Data in formats like PDF, Word Docs, CSV, Spreadsheets, etc.\\n• Unstructured Data in formats like HTML, PDF, TXT, etc.\\nFigure 12. 1-to-many relationship between agents and data stores, which can represent various types of \\npre-indexed data\\nThe underlying process for each user request and agent response loop is generally modeled \\nas seen in Figure 13.\\n1. A user query is sent to an embedding model to generate embeddings for the query\\n2. The query embeddings are then matched against the contents of the vector database \\nusing a matching algorithm like SCaNN\\n3. The matched content is retrieved from the vector database in text format and sent back to \\nthe agent\\n4. The agent receives both the user query and retrieved content, then formulates a response \\nor action', 'Agents\\n30\\nFebruary 2025\\n5. A final response is sent to the user\\nFigure 13. The lifecycle of a user request and agent response in a RAG based application\\nThe end result is an application that allows the agent to match a user’s query to a known data \\nstore through vector search, retrieve the original content, and provide it to the orchestration \\nlayer and model for further processing. The next action might be to provide a final answer to \\nthe user, or perform an additional vector search to further refine the results.\\nA sample interaction with an agent that implements RAG with ReAct reasoning/planning can \\nbe seen in Figure 14.', 'Agents\\n31\\nFebruary 2025\\nFigure 14. Sample RAG based application w/ ReAct reasoning/planning', 'Agents\\n32\\nFebruary 2025\\nTools recap\\nTo summarize, extensions, functions and data stores make up a few different tool types \\navailable for agents to use at runtime. Each has their own purpose and they can be used \\ntogether or independently at the discretion of the agent developer. \\nExtensions Function Calling Data Stores\\nExecution Agent-Side Execution Client-Side Execution Agent-Side Execution\\nUse Case • Developer wants \\nagent to control \\ninteractions with the \\nAPI endpoints\\n• Useful when \\nleveraging native pre-\\nbuilt Extensions (i.e. \\nVertex Search, Code \\nInterpreter, etc.)\\n• Multi-hop planning \\nand API calling \\n(i.e. the next agent \\naction depends on \\nthe outputs of the \\nprevious action / \\nAPI call)\\n• Security or \\nAuthentication \\nrestrictions prevent the \\nagent from calling an \\nAPI directly\\n• Timing constraints or \\norder-of-operations \\nconstraints that \\nprevent the agent \\nfrom making API calls \\nin real-time. (i.e. batch \\noperations, human-in-\\nthe-loop review, etc.)\\n• API that is not exposed \\nto the internet, or \\nnon-accessible by \\nGoogle systems\\nDeveloper wants to \\nimplement Retrieval \\nAugmented Generation \\n(RAG) with any of the \\nfollowing data types:\\n• Website Content from \\npre-indexed domains \\nand URLs\\n• Structured Data in \\nformats like PDF, \\nWord Docs, CSV, \\nSpreadsheets, etc.\\n• Relational / Non- \\nRelational Databases\\n• Unstructured Data in \\nformats like HTML, PDF, \\nTXT, etc.', \"Agents\\n33\\nFebruary 2025\\nEnhancing model performance with \\ntargeted learning\\nA crucial aspect of using models effectively is their ability to choose the right tools when \\ngenerating output, especially when using tools at scale in production. While general training \\nhelps models develop this skill, real-world scenarios often require knowledge beyond the \\ntraining data. Imagine this as the difference between basic cooking skills and mastering \\na specific cuisine. Both require foundational cooking knowledge, but the latter demands \\ntargeted learning for more nuanced results.\\nTo help the model gain access to this type of specific knowledge, several approaches exist:\\n• In-context learning: This method provides a generalized model with a prompt, tools, and \\nfew-shot examples at inference time which allows it to learn ‘on the fly' how and when to \\nuse those tools for a specific task. The ReAct framework is an example of this approach in \\nnatural language.\\n• Retrieval-based in-context learning: This technique dynamically populates the model \\nprompt with the most relevant information, tools, and associated examples by retrieving \\nthem from external memory. An example of this would be the ‘Example Store’ in Vertex AI \\nextensions or the data stores RAG based architecture mentioned previously.\\n• Fine-tuning based learning: This method involves training a model using a larger dataset \\nof specific examples prior to inference. This helps the model understand when and how to \\napply certain tools prior to receiving any user queries. \\nTo provide additional insights on each of the targeted learning approaches, let’s revisit our \\ncooking analogy.\", 'Agents\\n34\\nFebruary 2025\\n• Imagine a chef has received a specific recipe (the prompt), a few key ingredients (relevant \\ntools) and some example dishes (few-shot examples) from a customer. Based on this \\nlimited information and the chef’s general knowledge of cooking, they will need to figure \\nout how to prepare the dish ‘on the fly’ that most closely aligns with the recipe and the \\ncustomer’s preferences. This is in-context learning.\\n• Now let’s imagine our chef in a kitchen that has a well-stocked pantry (external data \\nstores) filled with various ingredients and cookbooks (examples and tools). The chef is now \\nable to dynamically choose ingredients and cookbooks from the pantry and better align \\nto the customer’s recipe and preferences. This allows the chef to create a more informed \\nand refined dish leveraging both existing and new knowledge. This is retrieval-based \\nin-context learning.\\n• Finally, let’s imagine that we sent our chef back to school to learn a new cuisine or set of \\ncuisines (pre-training on a larger dataset of specific examples). This allows the chef to \\napproach future unseen customer recipes with deeper understanding. This approach is \\nperfect if we want the chef to excel in specific cuisines (knowledge domains). This is fine-\\ntuning based learning. \\nEach of these approaches offers unique advantages and disadvantages in terms of speed, \\ncost, and latency. However, by combining these techniques in an agent framework, we can \\nleverage the various strengths and minimize their weaknesses, allowing for a more robust and \\nadaptable solution.', 'Agents\\n35\\nFebruary 2025\\nAgent quick start with LangChain\\nIn order to provide a real-world executable example of an agent in action, we’ll build a quick \\nprototype with the LangChain and LangGraph libraries. These popular open source libraries \\nallow users to build customer agents by “chaining” together sequences of logic, reasoning, \\nand tool calls to answer a user’s query. We’ll use our gemini-2.0-flash-001 model and \\nsome simple tools to answer a multi-stage query from the user as seen in Snippet 8.\\nThe tools we are using are the SerpAPI (for Google Search) and the Google Places API. After \\nexecuting our program in Snippet 8, you can see the sample output in Snippet 9.', 'Agents\\n36\\nFebruary 2025\\nPython\\nfrom langgraph.prebuilt import create_react_agent\\nfrom langchain_core.tools import tool\\nfrom langchain_community.utilities import SerpAPIWrapper\\nfrom langchain_community.tools import GooglePlacesTool\\nos.environ[\"SERPAPI_API_KEY\"] = \"XXXXX\"\\nos.environ[\"GPLACES_API_KEY\"] = \"XXXXX\"\\n@tool\\ndef search(query: str):\\n \"\"\"Use the SerpAPI to run a Google Search.\"\"\"\\n search = SerpAPIWrapper()\\n return  search.run(query)\\n@tool\\ndef places(query: str):\\n \"\"\"Use the Google Places API to run a Google Places Query.\"\"\"\\n places = GooglePlacesTool()\\n return  places.run(query)\\nmodel = ChatVertexAI(model=\"gemini-2.0-flash-001\")\\ntools = [search, places]\\nquery = \"Who did the Texas Longhorns play in football last week? What is the \\naddress of the other team\\'s stadium?\"\\nagent = create_react_agent(model, tools)\\ninput = {\"messages\": [(\"human\", query)]}\\nfor s in agent.stream(input, stream_mode=\"values\"):\\n message = s[\"messages\"][-1]\\n if isinstance (message, tuple):\\n  print(message )\\n else:\\n  message.pretty_print ()\\nSnippet 8. Sample LangChain and LangGraph based agent with tools', 'Agents\\n37\\nFebruary 2025\\n============================== Human Message ================================\\nWho did the Texas Longhorns play in football last week? What is the address \\nof the other team\\'s stadium?\\n================================= Ai Message =================================\\nTool Calls: search\\nArgs:\\n query: Texas Longhorns football schedule\\n================================ Tool Message ================================\\nName: search\\n{...Results: \"NCAA Division I Football, Georgia, Date...\"}\\n================================= Ai Message =================================\\nThe Texas Longhorns played the Georgia Bulldogs last week.\\nTool Calls: places\\nArgs:\\n query: Georgia Bulldogs stadium\\n================================ Tool Message ================================\\nName: places\\n{...Sanford Stadium Address: 100 Sanford...}\\n================================= Ai Message =================================\\nThe address of the Georgia Bulldogs stadium is 100 Sanford Dr, Athens, GA \\n30602, USA.\\nSnippet 9. Output from our program in Snippet 8\\nWhile this is a fairly simple agent example, it demonstrates the foundational components \\nof Model, Orchestration, and tools all working together to achieve a specific goal. In the \\nfinal section, we’ll explore how these components come together in Google-scale managed \\nproducts like Vertex AI agents and Generative Playbooks.', 'Agents\\n38\\nFebruary 2025\\nProduction applications with Vertex \\nAI agents\\nWhile this whitepaper explored the core components of agents, building production-grade \\napplications requires integrating them with additional tools like user interfaces, evaluation \\nframeworks, and continuous improvement mechanisms. Google’s Vertex AI platform \\nsimplifies this process by offering a fully managed environment with all the fundamental \\nelements covered earlier. Using a natural language interface, developers can rapidly \\ndefine crucial elements of their agents - goals, task instructions, tools, sub-agents for task \\ndelegation, and examples - to easily construct the desired system behavior. In addition, the \\nplatform comes with a set of development tools that allow for testing, evaluation, measuring \\nagent performance, debugging, and improving the overall quality of developed agents. This \\nallows developers to focus on building and refining their agents while the complexities of \\ninfrastructure, deployment and maintenance are managed by the platform itself. \\nIn Figure 15 we’ve provided a sample architecture of an agent that was built on the Vertex \\nAI platform using various features such as Vertex Agent Builder, Vertex Extensions, Vertex \\nFunction Calling and Vertex Example Store to name a few. The architecture includes many of \\nthe various components necessary for a production ready application.', 'Agents\\n39\\nFebruary 2025\\nFigure 15. Sample end-to-end agent architecture built on Vertex AI platform\\nYou can try a sample of this prebuilt agent architecture from our official documentation.', 'Agents\\n40\\nFebruary 2025\\nSummary\\nIn this whitepaper we’ve discussed the foundational building blocks of Generative AI \\nagents, their compositions, and effective ways to implement them in the form of cognitive \\narchitectures. Some key takeaways from this whitepaper include:\\n1. Agents extend the capabilities of language models by leveraging tools to access real-\\ntime information, suggest real-world actions, and plan and execute complex tasks \\nautonomously. agents can leverage one or more language models to decide when and \\nhow to transition through states and use external tools to complete any number of \\ncomplex tasks that would be difficult or impossible for the model to complete on its own.\\n2. At the heart of an agent’s operation is the orchestration layer, a cognitive architecture that \\nstructures reasoning, planning, decision-making and guides its actions. Various reasoning \\ntechniques such as ReAct, Chain-of-Thought, and Tree-of-Thoughts, provide a framework \\nfor the orchestration layer to take in information, perform internal reasoning, and generate \\ninformed decisions or responses. \\n3. Tools, such as Extensions, Functions, and Data Stores, serve as the keys to the outside \\nworld for agents, allowing them to interact with external systems and access knowledge \\nbeyond their training data. Extensions provide a bridge between agents and external APIs, \\nenabling the execution of API calls and retrieval of real-time information. functions provide \\na more nuanced control for the developer through the division of labor, allowing agents \\nto generate Function parameters which can be executed client-side. Data Stores provide \\nagents with access to structured or unstructured data, enabling data-driven applications.\\nThe future of agents holds exciting advancements and we’ve only begun to scratch the \\nsurface of what is possible. As tools become more sophisticated and reasoning capabilities \\nare enhanced, agents will be empowered to solve increasingly complex problems. \\nFurthermore, the strategic approach of ‘agent chaining’ will continue to gain momentum. By', 'Agents\\n41\\nFebruary 2025\\ncombining specialized agents - each excelling in a particular domain or task - we can create \\na ‘mixture of agent experts’ approach, capable of delivering exceptional results across \\nvarious industries and problem areas.\\nIt’s important to remember that building complex agent architectures demands an iterative \\napproach. Experimentation and refinement are key to finding solutions for specific business \\ncases and organizational needs. No two agents are created alike due to the generative nature \\nof the foundational models that underpin their architecture. However, by harnessing the \\nstrengths of each of these foundational components, we can create impactful applications \\nthat extend the capabilities of language models and drive real-world value.', \"Agents\\n42\\nFebruary 2025\\nEndnotes\\n1. Shafran, I., Cao, Y. et al., 2022, 'ReAct: Synergizing Reasoning and Acting in Language Models'. Available at:  \\nhttps://arxiv.org/abs/2210.03629\\n2. Wei, J., Wang, X. et al., 2023, 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models'. \\nAvailable at: https://arxiv.org/pdf/2201.11903.pdf .\\n3. Wang, X. et al., 2022, 'Self-Consistency Improves Chain of Thought Reasoning in Language Models'.  \\nAvailable at: https://arxiv.org/abs/2203.11171 .\\n4. Diao, S. et al., 2023, 'Active Prompting with Chain-of-Thought for Large Language Models'. Available at:  \\nhttps://arxiv.org/pdf/2302.12246.pdf .\\n5. Zhang, H. et al., 2023, 'Multimodal Chain-of-Thought Reasoning in Language Models'. Available at:  \\nhttps://arxiv.org/abs/2302.00923 .\\n6. Yao, S. et al., 2023, 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models'. Available at:  \\nhttps://arxiv.org/abs/2305.10601 .\\n7. Long, X., 2023, 'Large Language Model Guided Tree-of-Thought'. Available at:  \\nhttps://arxiv.org/abs/2305.08291 .\\n8. Google. 'Google Gemini Application'. Available at: http://gemini.google.com .\\n9. Swagger. 'OpenAPI Specification'. Available at: https://swagger.io/specification/ .\\n10. Xie, M., 2022, 'How does in-context learning work? A framework for understanding the differences from \\ntraditional supervised learning'. Available at: https://ai.stanford.edu/blog/understanding-incontext/ .\\n11. Google Research. 'ScaNN (Scalable Nearest Neighbors)'. Available at:  \\nhttps://github.com/google-research/google-research/tree/master/scann .\\n12. LangChain. 'LangChain'. Available at: https://python.langchain.com/v0.2/docs/introduction/ .\", 'Operationalizing \\nGenerative AI on \\nVertex AI using \\nMLOps\\nAuthors: Anant Nawalgaria,  \\nGabriela Hernandez Larios, Elia Secchi,  \\nMike Styer, Christos Aniftos, \\nOnofrio Petragallo, and \\nSokratis Kartakis', 'Operationalizing Generative AI on Vertex AI using MLOps\\n2\\nFebruary 2025\\nAcknowledgements\\nContent contributors\\nNenshad Bardoliwalla\\nWarren Barkley\\nMikhail Chrestkha\\nChase Lyall\\nLakshmanan Sethu\\nErwan Menard\\nAlan Blount\\nCurators and Editors\\nAntonio Gulli\\nAnant Nawalgaria\\nGrace Mollison \\nTechnical Writer\\nJoey Haymaker\\nDesigner\\nMichael Lanning', 'Introduction 6\\nWhat are DevOps and MLOps? 7\\nLifecycle of a gen AI system 8\\nDiscover 10\\nDevelop and experiment 11\\nThe foundational model paradigm 12\\nThe core component of LLM Systems: A prompted model component 14\\nChain & Augment 17\\nTuning & training 21\\nContinuous Training & Tuning 22\\nData Practices 24\\nEvaluate 28\\nDeploy 31\\nDeployment of gen AI systems 32\\nVersion control 32\\nContinuous integration of gen AI systems 33\\nContinuous delivery of gen AI systems 34\\nDeployment of foundation models 35\\nTable of contents', 'Infrastructure validation 35\\nCompression and optimization 36\\nDeployment, packaging, and serving checklist 37\\nLogging and monitoring 38\\nGovern 43\\nExtend MLOps for gen AI to Agents 44\\nAgent Lifecycle 45\\nTool Orchestration 47\\nTool Types & Environments 48\\nTool Registry 50\\nTool Selection Strategies at Scale 51\\nAgent Evaluation & Optimization 53\\nObservability and Memory 55\\nDeploying an Agent to Production  58\\nOperations: People & Processes 60\\nThe role of an AI platform for gen AI operations 65\\nKey components of Vertex AI for gen AI 66\\nDiscover: Vertex Model Garden 67\\nPrototype: Vertex AI Studio & Notebooks 70\\nCustomize: Vertex AI training & tuning  71\\nTrain  72\\nTune  72', 'Orchestrate 74\\nChain & Augment: Vertex AI Grounding, Extensions, and RAG building blocks 75\\nEvaluate: Vertex AI Experiments, Tensorboard, & evaluation pipelines 79\\nExperiment 80\\nEvaluation 81\\nPredict: Vertex AI endpoints & monitoring 81\\nGovern: Vertex AI Feature Store, Model Registry, and Dataplex 83\\nSummary 85\\nEndnotes 87', 'Operationalizing Generative AI on Vertex AI using MLOps\\n6\\nFebruary 2025\\nIntroduction\\nThe emergence of foundation models and generative AI (gen AI) has introduced a new era \\nfor building AI systems. Selecting the right model from a diverse range of architectures \\nand sizes, curating data, engineering optimal prompts, tuning models for specific tasks, \\ngrounding model outputs in real-world data, optimizing hardware - these are just a few of the \\nnovel challenges that large models introduce.  \\nThis whitepaper delves into the fundamental tenets of MLOps and the necessary adaptations \\nrequired for the domain of gen AI and Foundation Models. We also examine the diverse range \\nof Vertex AI products, specifically tailored to address the unique demands of foundation \\nmodels and gen AI-based applications. Through this exploration we uncover how Vertex AI, \\nwith its solid foundations of AI infrastructure and MLOps tools, expands its capabilities to \\nprovide a comprehensive MLOps platform for gen AI.\\nEmergence of foundation models \\nand generative AI (gen AI) has \\nintroduced a new era for building \\nAI systems.', 'Operationalizing Generative AI on Vertex AI using MLOps\\n7\\nFebruary 2025\\nWhat are DevOps and MLOps?\\nDevOps is a software engineering methodology that aims to bridge the gap between \\ndevelopment (Dev) and operations (Ops). It promotes collaboration, automation, and \\ncontinuous improvement to streamline the software development lifecycle, introducing \\npractices such as continuous integration and continuous delivery. \\nMLOps builds upon DevOps principles to address the unique challenges of operationalizing \\nMachine Learning systems rapidly and reliably. In particular, MLOps tackles the experimental \\nnature of ML through practices like:\\n• Data validation: Ensuring the quality and integrity of training data.\\n• Model evaluation: Rigorously assessing model performance with appropriate metrics.\\n• Model monitoring: Tracking model behavior in production to detect and mitigate drift.\\n• Tracking & reproducibility: Maintaining meticulous records for experiment tracking and \\nresult reproduction.\\nFigure 1. Machine learning workflow', \"Operationalizing Generative AI on Vertex AI using MLOps\\n8\\nFebruary 2025\\nLifecycle of a gen AI system\\nImagine deploying your first chatbot after months of dedicated work, and it's now interacting \\nwith users and answering questions. Behind this seemingly simple interaction lies the \\ncomplex and fascinating life cycle of a gen AI System, which can be broken down into five \\nkey moments.\\nFirst in the discovery phase, developers and AI engineers must navigate the expanding \\nlandscape of available models to identify the most suitable one for their specific gen AI \\napplication. They must consider each model's strengths, weaknesses, and costs to make an \\ninformed decision.\\nNext, development and experimentation become paramount, with prompt engineering \\nplaying a crucial role in crafting and refining input prompts to elicit desired outputs based on \\nan understanding of the model's intricacies. Few-shot learning, where examples are provided, \\ncan further guide model behavior, while additional customization may involve parameter-\\nefficient fine-tuning (PEFT). Most gen AI systems also involve model chaining, which refers to \\norchestrating calls to multiple models in a specific sequence to create a workflow.\\nData engineering practices have a critical role across all development stages, with factual \\ngrounding (ensuring the model's outputs are based on accurate, up-to-date information) and \\nrecent data from internal and enterprise systems being essential for reliable outputs. Tuning \\ndata is often needed to adapt models to specific tasks, styles, or to rectify persistent errors.\\nDeployment needs to manage many new artifacts in the deployment process, including \\nprompt templates, chain definitions, embedding models, retrieval data stores, and fine-tuned \\nmodel adapters among others. These artifacts each have unique governance requirements,\", \"Operationalizing Generative AI on Vertex AI using MLOps\\n9\\nFebruary 2025\\nnecessitating careful management throughout development and deployment. Gen AI system \\ndeployment also needs to account for the technical capabilities of the target infrastructure, \\nensuring that system hardware requirements are fulfilled.\\nContinuous monitoring in production ensures improved application performance and \\nmaintains safety standards through responsible AI techniques, such as ensuring fairness, \\ntransparency, and accountability in the model's outputs.\\nContinuous Improvement  as a concept is still key for Gen AI-based applications, though \\nwith a twist. For most Gen AI applications, instead of training models from scratch, we’re \\ntaking foundation models (FMs) and then adapting them to our specific use case. This means \\nconstantly tweaking these FMs through prompting techniques, swapping them out for newer \\nversions, or even combining multiple models for enhanced performance, cost efficiency, or \\nreduced latency. Traditional continuous training still holds relevance for scenarios when \\nrecurrent fine-tuning or incorporating human feedback loops are still needed.\\nNaturally, this lifecycle assumes that the foundational model powering the gen AI system is \\nalready operationalized. It's important to recognize that not all organizations will be directly \\ninvolved in this part of the process. In particular, the operationalization of foundational \\nmodels is a specialized set of tasks that is typically only relevant for a select few companies \\nwith the necessary resources and expertise.\\nBecause of that, this whitepaper will focus on practices required to operationalize gen AI \\napplications using and adapting existing foundation models, referring to other whitepapers in \\nthe book should you want to deep dive into how foundational models are operationalized. \\nThis includes active areas of research such as model pre-training, alignment (ensuring the \\nmodel's outputs align with the desired goals and values), evaluation or serving.\", 'Operationalizing Generative AI on Vertex AI using MLOps\\n10\\nFebruary 2025\\nFigure 2. Lifecycle of a Foundational Model & gen AI system and relative operationalization practices \\nDiscover\\nAs mentioned before, building foundational models from scratch is resource-intensive. \\nTraining costs and data requirements are substantial, pushing most practitioners towards \\nadapting existing foundation models through techniques like fine-tuning and prompt \\nengineering. This shift highlights a crucial need: efficiently discovering the optimal foundation \\nmodel for a given use case.\\nThese two characteristics of the gen AI landscape make model discovery an essential \\nMLOps practice:\\n1. An abundance of models: The past year has witnessed an explosion of open-source \\nand proprietary foundation models. Navigating this complex landscape, each with varying \\narchitectures, sizes, training datasets, and licenses, requires a systematic approach to \\nidentify suitable candidates for further evaluation.', \"Operationalizing Generative AI on Vertex AI using MLOps\\n11\\nFebruary 2025\\n2. No one-size-fits-all solution : Each use case presents unique requirements, demanding a \\nnuanced analysis of available models across multiple dimensions.\\nHere are some factors to consider when exploring models:\\n1. Quality: Early assessments can involve running test prompts or analyzing public \\nbenchmarks and metrics to gauge output quality.\\n2. Latency & throughput: These factors directly impact user experience. A chatbot \\ndemands lower latency than batch-processed summarization tasks.\\n3. Development & maintenance time: Consider the time investment for both initial \\ndevelopment and ongoing maintenance. Managed models often require less effort than \\nself-deployed open-source alternatives.\\n4. Usage cost: Factor in infrastructure and consumption costs associated with using the \\nchosen model.\\n5. Compliance: Assess the model's ability to adhere to relevant regulations and \\nlicensing terms.\\nBecause the activity of discovery has become so important for gen AI systems, many model \\ndiscoverability platforms were created to support this need. An example of that is Vertex \\nModel Garden,1 which is explored later in this whitepaper.\\nDevelop and experiment\\nThe process of development and experimentation remains iterative and orchestrated \\nwhile building gen AI applications. Each experimental iteration involves a tripartite \\ninterplay between data refinement, foundation model(s) selection and adaptation, and\", 'Operationalizing Generative AI on Vertex AI using MLOps\\n12\\nFebruary 2025\\nrigorous evaluation. Evaluation provides crucial feedback, guiding subsequent iterations \\nin a continuous feedback loop. Subpar performance might call for gathering more data, \\naugmenting data, or further curating the data. Similarly, the adaptation of the foundation \\nmodel itself might need tweaking - optimizing prompts, applying fine-tuning techniques, or \\neven swapping it out for a different one altogether. This iterative refinement cycle, driven by \\nevaluation insights, is just as critical for optimizing gen AI applications as it’s always been for \\ntraditional machine learning.\\nThe foundational model paradigm\\nFoundation models differ from predictive models most importantly because they are multi-\\npurpose models. Instead of being trained for a single purpose, on data specific to that \\ntask, foundation models are trained on broad datasets, and therefore can be applied to \\nmany different use cases. This distinction brings with it several more important differences \\nbetween foundation models and predictive models.\\nFoundation models also exhibit what are known as ‘emergent properties’,2 capabilities that \\nemerge in response to specific input without additional training. Predictive models are \\nonly able to perform the single function they were trained for; a traditional French-English \\ntranslation model, for instance, cannot also solve math problems.\\nFoundation models are also highly sensitive to changes in their input. The output of the \\nmodel and the task it performs are strongly affected, indeed determined, by the input to the \\nmodel. A foundation model can be made to perform translation, generation, or classification \\ntasks simply by changing the input. Even insignificant changes to the input can affect its \\nability to correctly perform that task.', 'Operationalizing Generative AI on Vertex AI using MLOps\\n13\\nFebruary 2025\\nThese new properties of foundation models have created a corresponding paradigm shift \\nin the practices required to develop and operationalize Gen AI systems. While models in \\nthe predictive AI context are self-sufficient and task-specific, gen AI models are multi-\\npurpose and need an additional element beyond the user input to function as part of a \\ngen AI Application: a prompt, and more specifically, a prompt template, defined as a set of \\ninstructions and examples along with placeholders to accommodate user input. A prompt \\ntemplate, along with dynamic data such as user input, can be combined to create a complete \\nprompt, the text that is passed as input to the foundation model.\\nFigure 3.  How Prompt Template and User input can be combined to create a prompt', 'Operationalizing Generative AI on Vertex AI using MLOps\\n14\\nFebruary 2025\\nThe core component of LLM Systems: A prompted \\nmodel component\\nThe presence of the prompt element is a distinguishing feature of gen AI applications. \\nNeither the model nor the prompt is sufficient for the generation of content; gen AI needs the \\ncombination of both. We refer to the combination as a ‘prompted model component’. This \\nis the smallest independent component sufficient to create an LLM application. The prompt \\ndoes not need to be very complicated. It can be a simple instruction, such as “translate \\nthe following sentence from English to French“, followed by the sentence to be translated. \\nWithout that preliminary instruction, though, a foundation model would not perform the \\ndesired translation task. So a prompt, even just a basic instruction, is necessary along with \\nthe input to get the foundation model to do the task required by the application.\\nFigure 4.  Predictive AI unit compared with the gen AI unit\\nThis introduces an important distinction when it comes to MLOps practices for gen AI. In \\nthe development of a gen AI System, experimentation and iteration need to be done in the \\ncontext of a prompted model component, the combination of a model and a prompt. The Gen', \"Operationalizing Generative AI on Vertex AI using MLOps\\n15\\nFebruary 2025\\nAI experimentation cycle typically begins with testing variations of the prompt – changing the \\nwording of the instructions, providing additional context, or including relevant examples, etc., \\nand evaluating the impact of those changes. This practice is commonly referred to as prompt \\nengineering. \\nPrompt engineering involves two iterative steps:\\n1. Prompting: Crafting and refining prompts to elicit desired behaviors from a foundational \\nmodel for a specific use case.\\n2. Evaluation: Assessing the model's outputs, ideally programmatically, to gauge its \\nunderstanding and success in fulfilling the prompt's instructions.\\nFigure 5.  The activity of prompt engineering\\nResults of an evaluation can be optionally registered as part of an experiment, to allow for \\nresult tracking. Since the prompt itself is a core element of the prompt engineering process, \\nit becomes a first class citizen within the artifacts part of the experiment. \\nHowever, we need to identify which type of artifacts they are. In the good old days of \\nPredictive AI, we had clear lines - data was one thing, pipelines and code another. But with \\nthe “Prompt” paradigm in gen AI, those lines get blurry. Think about it: prompts can include \\nanything from context, instructions, examples, guardrails to actual internal or external data \\npulled from somewhere else. So, are prompts data? Are they code?\", 'Operationalizing Generative AI on Vertex AI using MLOps\\n16\\nFebruary 2025\\nTo address these questions, a hybrid approach is needed, recognizing that a prompt has \\ndifferent components and requires different management strategies. Let’s break it down:\\nPrompt as Data: Some parts of the prompt will act just like data. Elements like few-shot \\nexamples, knowledge bases, and user queries are essentially data points. For these \\ncomponents, we need data-centric MLOps practices such as data validation, drift detection, \\nand lifecycle management.\\nPrompt as Code: Other components such as context, prompt templates, guardrails are mode \\ncode-like. They define the structure and rules of the prompt itself. Here, we need code-\\ncentric practices such as approval processes, code versioning, and testing.\\nAs a result, when applying MLOps practices to gen AI, it becomes important to have in place \\nprocesses that give developers easy storage, retrieval, tracking, and modification of prompts. \\nThis allows for fast iteration and principled experimentation. Often one version of a prompt \\nwill work well with a specific version of the model and less well with a different version. In \\ntracking the results of an experiment, both the prompt and its components version, and the \\nmodel version must be recorded and stored along with metrics and output data produced by \\nthe prompted model.\\nThe fact that development and experimentation in gen AI requires working with the prompt \\nand the model together introduces changes in some of the common MLOps practices, \\ncompared to the predictive AI case in which experimentation is done by changing the model \\nalone. Specifically, several of the MLOps practices need to be expanded to consider the \\nprompted model component together as a unit. This includes practices like evaluation, \\nexperiment tracking, model adaptation and deployment, and artifact management, \\nwhich will be discussed below in this whitepaper.', 'Operationalizing Generative AI on Vertex AI using MLOps\\n17\\nFebruary 2025\\nChain & Augment\\nGen AI models, particularly large language models (LLMs), face inherent challenges in \\nmaintaining recency and avoiding hallucinations. Encoding new information into LLMs \\nrequires expensive and data-intensive pre-training, posing a significant hurdle. Additionally, \\nLLMs might be unable to solve complex challenges, especially when step-by-step reasoning \\nis required. Depending on the use case, leveraging only one prompted model to perform \\na particular generation might not be sufficient. To solve this issue, leveraging a divide and \\nconquer approach, several prompted models can be connected together, along with calls \\nto external APIs and logic expressed as code. A sequence of prompted model components \\nconnected together in this way is commonly known as a chain. \\nFigure 6.  Components of a chain and relative development process', \"Operationalizing Generative AI on Vertex AI using MLOps\\n18\\nFebruary 2025\\nTwo common chain-based patterns that have emerged to mitigate recency and \\nhallucinations are retrieval augmented generation (RAG)3 and Agents. \\n• RAG addresses these challenges by augmenting pre-trained models with \\n“knowledge” retrieved from databases, bypassing the need for pre-training. This \\nenables grounding and reduces hallucinations by incorporating up-to-date factual \\ninformation directly into the generation process. \\n• Agents, popularized by the ReAct prompting technique,4 leverage LLMs as mediators \\ninteracting with various tools, including RAG systems, internal or external APIs, \\ncustom extensions, or even with other agents. This enables complex queries and \\nreal-time actions by dynamically selecting and utilizing relevant information sources. \\nThe LLM, acting as an agent, interprets the user’s query, decides which tool to utilize, \\nand how to formulate the response based on the retrieved information.\\nRAG and Agents approaches can be combined to create multi-agent systems connected \\nto large information networks, enabling sophisticated query handling and real-time \\ndecision-making. \\nThe orchestration of different models, logic and APIs is not a novelty of gen AI \\nApplications. For example, recommendation engines have long combined collaborative \\nfiltering models, content-based models, and business rules to generate personalized \\nproduct recommendations for users. Similarly, in fraud detection, machine learning \\nmodels are integrated with rule-based systems and external data sources to identify \\nsuspicious activities.\\nWhat makes these chains of gen AI components different, is that, we can't a priori \\ncharacterize or cover the distribution of component inputs, which makes the individual \\ncomponents much harder to evaluate and maintain in isolation.\", \"Operationalizing Generative AI on Vertex AI using MLOps\\n19\\nFebruary 2025\\nThis results in a paradigm shift in how AI applications are being developed for gen AI.\\nUnlike Predictive AI where it is often possible to iterate on the separate models and \\ncomponents in isolation to then chain in the AI application, in gen AI it’s often easier to \\ndevelop a chain in integration, performing experimentation on the chain end-to-end, iterating \\nover chaining strategies, prompts, the underlying foundational models and other APIs in \\na coordinated manner to achieve a specific goal. No feature engineering, data collection, \\nor further model training cycles is often needed; just changes to the wording of the \\nprompt template.\\nThe shift towards MLOps for gen AI, in contrast to predictive AI, brings forth a new set of \\ndemands. Let's break down these key differences:\\n1. Evaluation: Because of their tight coupling, chains need end-to-end evaluation, not just \\non a per-component basis, to gauge their overall performance and the quality of their \\noutput. In terms of evaluation techniques and metrics, evaluating chains is not dissimilar \\nto evaluating prompted models. Please refer to the below segment on evaluation for more \\ndetails on these approaches.\\n2. Versioning: A chain needs to be managed as a complete artifact in its entirety. The chain \\nconfiguration should be tracked with its own revision history for analysis, reproducibility, \\nand understanding the impact of changes on output. Logging should also include the \\ninputs, outputs, and intermediate states of the chain, and any chain configurations used \\nduring each execution.\\n3. Continuous Monitoring: Establishing proactive monitoring systems is vital for detecting \\nperformance degradation, data drift, or unexpected behavior in the chain. This ensures \\nearly identification of potential issues to maintain the quality of the generated output. The \\nactivity of monitoring Chains is discussed in detail in the section ‘Logging and Monitoring’.\", 'Operationalizing Generative AI on Vertex AI using MLOps\\n20\\nFebruary 2025\\n4. Introspection: The ability to inspect the internal data flows of a chain (inputs and outputs \\nfrom each component) as well as the inputs and outputs of the entire chain is paramount. \\nBy providing visibility into the data flowing through the chain and the resulting content, \\ndevelopers can pinpoint the sources of errors, biases, or undesirable behavior.\\nFigure 7.  Putting together chains, prompted models and model tuning\\nThere are several products in Vertex AI that can support the need for chaining and \\naugmentation, including Grounding as a service,5 Extensions,6 Vector Search,7 Agent Builder,8 \\nand more. We discuss the products in the  section “Role of an AI Platform”. LangChain9 is \\nalso integrated with the Vertex SDK,10 and can be used alongside the core Vertex products to \\ndefine and configure gen AI chained applications.', \"Operationalizing Generative AI on Vertex AI using MLOps\\n21\\nFebruary 2025\\nTuning & training\\nWhen developing a gen AI use case and a specific task that involves LLMs, it can be difficult, \\nespecially for complex tasks, to rely on only prompt engineering and chaining to solve it.      \\nTo improve task performance practitioners often also need to fine-tune the model directly. \\nFine-tuning lets you actively change the layers or a subset of layers of the LLM to optimize \\nthe capability of the model to perform a certain task. Two of the most common ways of \\ntuning a model are:\\n1. Supervised fine-tuning: This is where we train the model in a supervised manner, teaching \\nit to predict the right output sequence for a given input. \\n2. Reinforcement Learning from Human Feedback (RLHF): In this approach, we first train \\na reward model to predict what humans would prefer as a response. Then, we use this \\nreward model to nudge the LLM in the right direction during the tuning process. Like \\nhaving a panel of human judges guiding the model's learning. \\nFigure 8.  Putting together chains, prompted models and model tuning\", 'Operationalizing Generative AI on Vertex AI using MLOps\\n22\\nFebruary 2025\\nWhen viewed through the MLOps lens, fine-tuning shares similar requirements with \\nmodel training:\\n1. The capability to track artifacts being part of the tuning job. This includes for example the \\ninput data or the parameters being used to tune the model.\\n2. The capability to measure the impact of the tuning. This translates into the capability \\nto perform evaluation of the tuned model for the specific tasks it was trained on and to \\ncompare results with previously tuned models or frozen models for the same task.\\nPlatforms like Vertex AI11 (and the Google Cloud platform more broadly) provide a robust \\nsuite of services designed to address these MLOps requirements: Vertex Model Registry,12 \\nfor instance, provides a centralized storage location for all the artifacts created during the \\ntuning job, and Vertex Pipelines13 streamlines the development and management of these \\ntuning jobs. Dataplex,14 meanwhile, provides an organization-wide data fabric for data lineage \\nand governance and integrates well with both Vertex AI and BigQuery.15 What’s more, these \\nproducts provide the same governance capability for both predictive and gen AI applications, \\nmeaning customers do not need separate products or configurations to manage generative \\nversus AI development.\\nContinuous Training & Tuning\\nIn machine learning operations (MLOps), continuous training is the practice of repeatedly \\nretraining machine learning models in a production environment. This is done to ensure \\nthat the model remains up-to-date and performs well as real-world data patterns change \\nover time. For gen AI models, continuous tuning of the models is often more practical than \\nretraining from scratch due to the high data and computational costs involved.', 'Operationalizing Generative AI on Vertex AI using MLOps\\n23\\nFebruary 2025\\nThe approach to continuous tuning depends on the specific use case and goals. For relatively \\nstatic tasks like text summarization, the continuous tuning requirements may be lower. But \\nfor dynamic applications like chatbots that need constant human alignment, more frequent \\ntuning using techniques like RLHF based on human feedback is necessary. \\nTo determine the right continuous tuning strategy, AI practitioners must carefully evaluate \\nthe nature of their use case and how the input data evolves over time. Cost is also a major \\nconsideration, as the compute infrastructure greatly impacts the speed and expense of \\ntuning.  We discuss in detail monitoring of GenAI systems in the Logging and Monitoring \\nsection of this whitepaper.\\nGraphics processing units (GPUs) and tensor processing units (TPUs) are key hardware for \\nfine-tuning. GPUs, known for their parallel processing power, are highly effective in handling \\nthe computationally intensive workloads and often associated with training and running \\ncomplex machine learning models. TPUs, on the other hand, are specifically designed \\nby Google for accelerating machine learning tasks. TPUs excel in handling large matrix \\noperations common in deep learning neural networks.\\nTo manage costs, techniques like model quantization can be applied. This represents model \\nweights and activations using lower-precision 8-bit integers rather than 32-bit floats, which \\nreduces computational and memory requirements.\\nWe discuss in detail the support for tuning in Vertex AI in the Customize: Vertex AI Training & \\nTuning section.', 'Operationalizing Generative AI on Vertex AI using MLOps\\n24\\nFebruary 2025\\nData Practices\\nTraditionally, ML model behavior was dictated solely by its training data. While this still holds \\ntrue for foundation models – trained on massive, multilingual, multimodal datasets – gen AI \\napplications built on top of them introduce a new twist: model behavior is now determined by \\nhow you adapt the model using different types of input data (Figure. 9).\\nFigure 9.  Examples of data spectrum for foundation models – creation (left) vs. adaptation (right)\\nThe key difference between traditional predictive ML and gen AI lies in where you start. In \\npredictive ML, the data is paramount. You spend a lot of time on data engineering, and if you \\ndon’t have the right data, you cannot build an application. Gen AI takes a unique approach to \\nthis matter. You start with a foundation model, some instructions and maybe a few example \\ninputs (in-context learning). You can prototype and launch an application with surprisingly \\nlittle data.', 'Operationalizing Generative AI on Vertex AI using MLOps\\n25\\nFebruary 2025\\nThis ease of prototyping, however, comes with a challenge. Traditional predictive AI relies on \\napriori well-defined dataset(s). In gen AI, a single application can leverage various data types, \\nfrom completely different data sources, all working together (Figure 10). Let’s explore some \\nof these data types:\\n• Conditioning prompts: These are essentially instructions given to the Foundation Model \\n(FM) to guide its output, setting boundaries of what it can generate.\\n• Few-shot examples: A way to show the model what you want to achieve through input-\\noutput pairs. This helps the model grasp the specific task(s) at hand, and in many cases, it \\nboosts performances.\\n• Grounding/augmentation data: Data coming from either external APIs (like Google \\nSearch) or internal APIs and data sources. This data  permits the FM to produce answers \\nfor a specific context, keeping responses current, relevant without retraining the entire \\nFM. This type of data also supports reducing hallucinations.\\n• Task-specific datasets: These are used to fine-tune an existing FM for a particular task, \\nimproving its performance in that specific area.\\n• Human preference datasets: These capture feedback on generated outputs, helping \\nrefine the model’s ability to produce outputs that align with human preferences. \\n• Full pre training corpora: These are massive datasets used to initially train foundation \\nmodels. While application builders may not have access to them nor the tokenizers, \\nthe information encoded in the model itself will influence the application’s output \\nand performance.\\nThis is not an exhaustive list. The variety of data used in gen AI applications is constantly \\ngrowing and evolving.', 'Operationalizing Generative AI on Vertex AI using MLOps\\n26\\nFebruary 2025\\nFigure 10.  Example of high-level data and adaptations landscape for developing gen AI applications using \\nexisting foundation models\\nThis diverse range of data adds another complexity layer in terms of data organization, \\ntracking and lifecycle management. Take a RAG-based application as an example: it might \\ninvolve rewriting user queries, dynamically gathering relevant examples using a curated set \\nof examples, querying a vector database, and combining it all with a prompt template. This \\ninvolves managing multiple data types: user queries,  vector databases with curated few-shot \\nexamples and company information, and prompt templates.', \"Operationalizing Generative AI on Vertex AI using MLOps\\n27\\nFebruary 2025\\nEach data type needs careful organization and maintenance. For example, the vector \\ndatabase requires processing data into embeddings, optimizing chunking strategies, and \\nensuring only relevant information is available. The prompt template itself needs versioning \\nand tracking, the user queries need rewriting, etc. This is where  traditional MLOps and \\nDevOps best practices come into play, with a twist. We need to ensure reproducibility, \\nadaptability, governance, and continuous improvement using all the data required in an \\napplication as a whole but also individually. Think of it this way: in predictive AI, the focus \\nwas on well-defined data pipelines for extraction, transformation, and loading. In gen AI, \\nit's about building pipelines to manage, evolve, adapt and integrate different data types in a \\nversionable, trackable, and reproducible way. \\nAs mentioned earlier, fine-tuning foundation models (FMs) can boost gen AI app \\nperformance, but it needs data. You can get this data by launching your app and gathering \\nreal-world data, generating synthetic data, or a mix of both. Using large models to generate \\nsynthetic data is becoming popular because it speeds things up, but it's still good to have a \\nhuman check the results for quality assurance. Here are few ways to leverage large models \\nfor data engineering purposes:\\n1. Synthetic data generation: This process involves creating artificial data that closely \\nresembles real-world data in terms of its characteristics and statistical properties, often \\nbeing done with a large and capable model. This synthetic data serves as additional \\ntraining data for gen AI, enabling it to learn patterns and relationships even when labeled \\nreal-world data is scarce.\\n2. Synthetic data correction: This technique focuses on identifying and correcting errors \\nand inconsistencies within existing labeled datasets. By leveraging the power of larger \\nmodels, gen AI can flag potential labeling mistakes and propose corrections, improving the \\nquality and reliability of the training data.\", \"Operationalizing Generative AI on Vertex AI using MLOps\\n28\\nFebruary 2025\\n3. Synthetic data augmentation: This approach goes beyond simply generating new \\ndata. It involves intelligently manipulating existing data to create diverse variations while \\npreserving essential features and relationships. Thus, gen AI can encounter a broader \\nrange of scenarios during training, leading to improved generalization and ability to \\ngenerate nuanced and relevant outputs.\\nEvaluating gen AI, unlike predictive AI, is tricky. You don't usually know the training data \\ndistribution of the foundational models. Building a custom evaluation dataset reflecting your \\nuse case is essential. This dataset should cover essential, average, and edge cases. Similar \\nto fine-tuning data, you can leverage powerful language models to generate, curate, and \\naugment data for building robust evaluation datasets.\\nEvaluate\\nEven if only prompt engineering is performed, as any experimental process, it does require \\nevaluation in order to iterate and improve. This makes the evaluation process a core activity \\nof the development of any gen AI systems.\\nIn the context of gen AI systems, evaluation might have different degrees of automation: from \\nentirely driven by humans to entirely automated by a process. \\nIn the early days of a project, when you're still prototyping, evaluation is often a manual \\nprocess. Developers eyeball the model's outputs, getting a qualitative sense of how it's \\nperforming. But as the project matures and the number of test cases balloons, manual \\nevaluation becomes a bottleneck. That's when automation becomes key.\", \"Operationalizing Generative AI on Vertex AI using MLOps\\n29\\nFebruary 2025\\nAutomating evaluation has two big benefits. First, it lets you move faster. Instead of spending \\ntime manually checking each test case, you can let the machines do the heavy lifting. \\nThis means more iterations, more experiments, and ultimately, a better product. Second, \\nautomation makes evaluation more reliable. It takes human subjectivity out of the equation, \\nensuring that results are reproducible.\\nBut automating evaluation for gen AI comes with its own set of challenges. \\nFor one, both the inputs (prompts) and outputs can be incredibly complex. A single prompt \\nmight include multiple instructions and constraints that the model needs to juggle. And the \\noutputs themselves are often high-dimensional - think a generated image or a block of text. \\nCapturing the quality of these outputs in a simple metric is tough.\\nThere are some established metrics, like BLEU for translations and ROUGE for summaries, \\nbut they don't always tell the full story. That's where custom evaluation methods come in. \\nOne approach is to use another foundational model as a judge. For example, you could \\nprompt a large language model to score the quality of generated texts across various \\ndimensions. This is the idea behind techniques like AutoSxS.16\\nAnother challenge is the subjective nature of many evaluation metrics for gen AI. What \\nmakes one output ‘better’ than another can often be a matter of opinion. The key here is to \\nmake sure your automated evaluation aligns with human judgment. You want your metrics \\nto be a reliable proxy for what people would think. And to ensure comparability between \\nexperiments, it's crucial to lock down your evaluation approach and metrics early in the \\ndevelopment process.\\nLack of ground truth data is another common hurdle, especially in the early stages of a \\nproject. One workaround is to generate synthetic data to serve as a temporary ground truth, \\nwhich can be refined over time with human feedback.\", \"Operationalizing Generative AI on Vertex AI using MLOps\\n30\\nFebruary 2025\\nFinally, comprehensive evaluation is essential for safeguarding gen AI applications against \\nadversarial attacks. Malicious actors can craft prompts to try to extract sensitive information \\nor manipulate the model's outputs. Evaluation sets need to specifically address these attack \\nvectors, through techniques like prompt fuzzing (feeding the model random variations on \\nprompts) and testing for information leakage.\\nAutomating the evaluation process ensures speed, scalability and reproducibility\\nAn automation of the evaluation process can be considered a proxy for the \\nhuman judgmen\\nDepending on the use case, the evaluation process will require a high degree \\nof customization.\\nTo ensure comparability it is essential to stabilize the evaluation approach, metrics, \\nand ground truth data as early as possible in the development phase.\\nIt is possible to generate synthetic ground truth data to accommodate for the lack of \\nreal ground truth data.\\nIt is important to include test cases of adversarial prompting as part of the evaluation \\nset to test the reliability of the system itself for these attacks.\\nTable 1.  Key suggestions to approach evaluation of gen AI systems\", 'Operationalizing Generative AI on Vertex AI using MLOps\\n31\\nFebruary 2025\\nDeploy\\nIt should be clear by this point that production gen AI applications are complex systems with \\nmany interacting components. Some of the common components discussed include multiple \\nprompts, models, adapter layers and external data sources. In deploying a gen AI system to \\nproduction, all these components need to be managed and coordinated with the previous \\nstages of gen AI system development. Given the novelty of these systems, best practices \\nfor deployment and management are still evolving, but we can discuss observations and \\nrecommendations for these components and indicate how to address the major concerns.\\nDeploying gen AI solutions necessarily involves multiple steps. For example, a single \\napplication might utilize several large language models (LLMs) alongside a database, all \\nfed by a dynamic data pipeline. Each of these components potentially requires its own \\ndeployment process.\\nFor clarity, we distinguish between two main types of deployment:\\n1. Deployment of gen AI systems: This focuses on operationalizing a complete system \\ntailored for a specific use case. It encompasses deploying all the necessary elements \\n-  the application, chosen LLMs, database, data pipelines, and any other relevant \\ncomponents - to create a functioning end-user solution.\\n2. Deployment of foundational models: This applies to open-weight models, where the \\nmodel weights are publicly available on platforms like Vertex Model Garden or Hugging \\nFace, or privately trained models. Deployment in this scenario centers around making \\nthe foundational model itself accessible to users. Given their multipurpose nature, these \\ndeployments often aim to support various potential use cases.', \"Operationalizing Generative AI on Vertex AI using MLOps\\n32\\nFebruary 2025\\nDeployment of gen AI systems\\nDeployment of gen AI systems is broadly similar to deployment of any other complex \\nsoftware system. Most of the system components – databases, Python applications, etc. – \\nare also found in other non-gen AI applications. As a result, our general recommendation is \\nto manage these components using standard software engineering practices such as version \\ncontrol17 and Continuous Integration / Continuous Delivery (CI/CD).18\\nVersion control\\nGen AI experimentation is an iterative process involving repeated cycles of development, \\nevaluation, and modification. To ensure a structured and manageable approach, it's crucial to \\nimplement strict versioning for all modifiable components. These components include:\\n• Prompt templates: Unless leveraging specific prompt management solutions, version \\nthem through standard version control tools like Git.\\n• Chain definitions: The code defining the chain (including API integrations, database calls, \\nfunctions, etc.) should also be versioned using tools like Git. This provides a clear history \\nand enables easy rollback if needed.\\n• External datasets: In retrieval augmented generation (RAG) systems, external datasets \\nplay a key role. It’s important to track these changes and versions of these datasets for \\nreproducibility. You can do that by leveraging existing data analytics solutions such as \\nBigQuery, AlloyDB, Vertex Feature Store. \\n• Adapter models: The landscape of techniques like LoRA tuning for adapter models is \\nconstantly evolving. . You can leverage established data storage solutions (e.g. cloud \\nstorage) to manage and version these assets effectively.\", \"Operationalizing Generative AI on Vertex AI using MLOps\\n33\\nFebruary 2025\\nContinuous integration of gen AI systems\\nIn a continuous integration framework, every code change goes through automatic testing \\nbefore merging to catch issues early. Here, unit and integration testing are key for quality \\nand reliability. Unit tests act like a microscope, zooming in on individual code pieces, while \\nintegration testing verifies that different components work together.\\nThe benefits of continuous integration in traditional software development are well-\\nunderstood. Implementing a CI system helps to do the following:\\n1. Ensure reliable, high-quality outputs: Rigorous testing increases confidence in the \\nsystem's performance and consistency.  \\n2. Catch bugs early: Identifying issues through testing prevents them from causing bigger \\nproblems downstream. It also makes the system more robust and resilient to edge cases \\nand unexpected inputs.\\n3. Lower maintenance costs: Well-documented test cases simplify troubleshooting and \\nenable smoother modifications in the future, reducing overall maintenance efforts\\nThese benefits are applicable to gen AI Systems as much as any software product. \\nContinuous Integration should be applied to all elements of the system, including the prompt \\ntemplates, the chain and chaining logic, and any embedding models and retrieval systems.\\nHowever, applying CI to gen AI comes with challenges:\\n1. Difficult to generate comprehensive test cases: The complex and open-ended nature of \\ngen AI outputs makes it hard to define and create an exhaustive set of test cases that \\ncover all possibilities.\", 'Operationalizing Generative AI on Vertex AI using MLOps\\n34\\nFebruary 2025\\n2. Reproducibility issues: Achieving deterministic, reproducible results is tricky since \\ngenerative models often have intrinsic randomness and variability in their outputs, even for \\nidentical inputs. This makes it harder to consistently test for specific expected behaviors.\\nThese challenges are closely related to the broader question of how to evaluate gen AI \\nsystems. Many of the same techniques discussed in the Evaluation section above can also \\nbe applied to the development of CI systems for gen AI. This is an ongoing area of research, \\nhowever, and more techniques will undoubtedly emerge in the near future.\\nContinuous delivery of gen AI systems\\nOnce the code is merged, a continuous delivery process begins to move the built and tested \\ncode through environments that closely resemble production for further testing before the \\nfinal deployment. \\nAs mentioned in the \"\\'\"Develop and Experiment\"\\'\" segment, chain elements become one \\nof the main components to deploy, as they fundamentally constitute the gen AI application \\nserving users.\\nThe delivery process of the gen AI application containing the chain may vary depending on \\nthe latency requirements and whether the use case is batch or online:\\n1. Batch use cases require deploying a batch process executed on a schedule in production. \\nThe delivery process should focus on testing the entire pipeline in integration in an \\nenvironment close to production before deployment. As part of the testing process, \\ndevelopers can assert specific requirements around the throughput of the batch process \\nitself and checking that all components of the application are functioning correctly (e.g., \\npermissioning, infrastructure, code dependencies).', 'Operationalizing Generative AI on Vertex AI using MLOps\\n35\\nFebruary 2025\\n2. Online use cases require deploying an API, in this case, the application containing the \\nchain, capable of responding to users at low latency. The delivery process should involve \\ntesting the API in integration in an environment close to production, with tests to assert \\nthat all components of the application are functioning correctly (e.g., permissioning, \\ninfrastructure, code dependencies). Non-functional requirements (e.g., scalability, \\nreliability, performance) can be verified through a series of tests, including load tests.\\nDeployment of foundation models\\nBecause foundation models are so large and complex, deployment and serving of these \\nmodels raises a number of issues – most obviously, the compute and storage resources \\nneeded to run these massive models successfully. At a minimum, a foundation model \\ndeployment needs to include several key considerations: selecting and securing necessary \\ncompute resources, such as GPUs or TPUS; choosing appropriate data storage services \\nlike BigQuery or Google Cloud Storage that can scale to deal with the large datasets; and \\nimplementing model optimization or compression techniques.\\nInfrastructure validation\\nOne technique that can be applied to address the resource requirements of gen AI systems is \\ninfrastructure validation. This refers to the introduction of an additional verification step, prior \\nto deploying the training and serving systems, to check both the compatibility of the model \\nwith the defined serving configuration and the availability of the required hardware. There \\nare a number of optional infrastructure validation layers that can perform some of these \\nchecks automatically. For instance, TFX19 has an infrastructure validation layer that checks', 'Operationalizing Generative AI on Vertex AI using MLOps\\n36\\nFebruary 2025\\nwhether the model will run correctly on a specified hardware configuration, which can help \\ncatch configuration issues before deployment. Nevertheless, the availability of the required \\nhardware still needs to be verified by hand by the engineer or the system administrator.\\nCompression and optimization\\nAnother way of addressing infrastructure challenges is to optimize the model itself. \\nCompressing and/or optimizing the model can often significantly reduce the storage and \\ncompute resources needed for training and serving, and in many cases can also decrease \\nthe serving latency.\\nSome techniques for model compression and optimization include quantization, distillation \\nand model pruning. Quantization reduces the size and computational requirements of the \\nmodel by converting its weights and activations from higher-precision floating-point numbers \\nto lower-precision representations, such as 8-bit integers or 16-bit floating-point numbers. \\nThis can significantly reduce the memory footprint and computational overhead of the model. \\nModel Pruning is a technique for eliminating unnecessary weight parameters or by selecting \\nonly important subnetworks within the model. This reduces model size while maintaining \\naccuracy as high as possible. Finally, distillation trains a smaller model, using the responses \\ngenerated by a larger LLM, to reproduce the output of the larger LLM for a specific domain. \\nThis can significantly reduce the amount of training data, compute, and storage resources \\nneeded for the application.\\nIn certain situations, model distillation can also improve the performance of the model itself \\nin addition to reducing resource requirements. This happens because the smaller model can \\ncombine the knowledge of the larger model with labeled data, which can help it to generalize \\nbetter to new data on a limited use case.The process of distillation usually involves training \\na large foundational LLM (teacher model) and having it generate responses to certain tasks,', \"Operationalizing Generative AI on Vertex AI using MLOps\\n37\\nFebruary 2025\\nand then having the smaller LLM (student model) use a combination of the LLMs knowledge \\nas well as task specific supervised dataset to learn. The size and complexity of the smaller \\nLLM can be adjusted to achieve the desired trade-off between performance and resource \\nrequirements. A technique known as step-by-step distillation20 has proven to achieve \\ngreat results.\\nDeployment, packaging, and serving checklist\\nFollowing are the important steps to take when deploying a model on Vertex AI.\\n □ Configure version control: Implement version control practices for LLM deployments. \\nThis allows you to roll back to previous versions if necessary and track changes made to \\nthe model or deployment configuration.\\n □ Optimize the model: Perform any model optimization (distillation, quantization, pruning, \\netc.) before packaging or deploying the model.\\n □ Containerize the model: Package the trained LLM model into a container. \\n □ Define target hardware requirements: Ensure the target deployment environment \\nmeets the requirements for optimal performance of the LLM model, such as GPUs, as well \\nas TPUs and other specialized hardware accelerators.\\n □ Define model endpoint: Define the endpoint configuration using Vertex AI's endpoint \\ncreation interface or the Vertex AI SDK. Specify the model container, input and output \\nformats, and any additional configuration parameters.\\n □ Allocate resources: Allocate the appropriate compute resources for the endpoint based \\non the expected traffic and performance requirements.\", \"Operationalizing Generative AI on Vertex AI using MLOps\\n38\\nFebruary 2025\\n □ Configure access control: Set up access control mechanisms to restrict access to \\nthe endpoint based on authentication and authorization policies. This ensures that only \\nauthorized users or services can interact with the deployed LLM.\\n □ Create model endpoint: Create a Vertex AI endpoint to deploy21 the LLM as a REST API \\nservice. This allows clients to send requests to the endpoint and receive responses from \\nthe LLM..\\n □ Configure monitoring and logging: Establish monitoring and logging systems to track \\nthe endpoint's performance, resource utilization, and error logs.\\n □ Deploy custom integrations: Integrate the LLM into custom applications or services \\nusing the model's SDK or APIs. This provides more flexibility for integrating the LLM into \\nspecific workflows or frameworks.\\n □ Deploy Real-time Applications: For real-time applications, consider using Cloud \\nFunctions and Cloud Run in combination with LLMs hosted in Vertex AI to create a \\nstreaming pipeline that processes data and generates responses in real time.\\nLogging and monitoring\\nMonitoring gen AI applications and, as a result, their components, presents unique \\nchallenges that require additional techniques and approaches on top of those in traditional \\nMLOps. The use of gen AI requires the chaining of components in order to produce results \\nfor practical applications. Additionally, to your application user, all the components are \\nhidden. Therefore, the interface they have to your application is their input and the final \\noutput. This creates the need to log and monitor your application end-to-end: that is, logging \\nand monitoring the input and output of your application overall as well as the input and \\noutput of every single component.\", 'Operationalizing Generative AI on Vertex AI using MLOps\\n39\\nFebruary 2025\\nLogging is necessary for applying monitoring and debugging on your gen AI system in \\nproduction. An input to the application triggers multiple components. Imagine the output \\nto a given input is factually inaccurate. How can you find out which of the components are \\nthe ones that didn’t perform well? To answer this question it is necessary to apply logging \\non the application level and at the component level. We need lineage in our logging for all \\ncomponents executed. For every component we need to log their inputs and outputs. We \\nalso need to be able to map those with any additional artifacts and parameters they depend \\non so we can easily analyze those inputs and outputs.\\nMonitoring can be applied to the overall gen AI application and to individual components. We \\nprioritize monitoring at the application level. This is because if the application is performant \\nand monitoring proves that, it implies that all components are also performant. You can also \\napply the same practices to each of the prompted model components to get more granular \\nresults and understanding of your application.\\nSkew detection in traditional ML systems refers to training-serving skew that occurs when \\nthe feature data distribution in production deviates from the feature data distribution \\nobserved during model training. In the case of Gen AI systems using pretrained models in \\ncomponents chained together to produce the output, we need to modify our approach. We \\ncan measure skew by comparing the distribution of the input data we used to evaluate our \\napplication (the test set as described under the Data Curation and Principles section above) \\nand the distribution of the inputs to our application in production. Once the two distributions \\ndrift apart,further investigation is needed. The same process can be applied to the output \\ndata as well.', 'Operationalizing Generative AI on Vertex AI using MLOps\\n40\\nFebruary 2025\\nFigure 11.  Drift/skew detection process overview\\nLike skew detection, the drift detection process checks for statistical differences between \\ntwo datasets. However, instead of comparing evaluations and serving inputs, drift looks for \\nchanges in input data. This allows you to check how the inputs and therefore the behavior of \\nyour users changed over time. This is the same as traditional MLOps. \\nGiven that the input to the application is typically text, there are a few approaches to \\nmeasuring skew and drift. In general all the methods are trying to identify significant \\nchanges in production data, both textual (size of input) and conceptual (topics in input), \\nwhen compared to the evaluation dataset. All these methods are looking for changes that \\ncould potentially indicate the application might not be prepared to successfully handle the \\nnature of the new data that are now coming in. Some common approaches are calculating \\nembeddings and distances, counting text length and number of tokens, and tracking \\nvocabulary changes, new concepts and intents, prompts and topics in datasets, as well \\nas statistical approaches such as least-squares density difference,22 maximum mean \\ndiscrepancy (MMD),23 learned kernel MMD,24 or context-aware MMD.25 As gen AI use cases \\nare so diverse, it is often necessary to create additional custom metrics that better capture \\nabnormal changes in your data.', \"Operationalizing Generative AI on Vertex AI using MLOps\\n41\\nFebruary 2025\\nWith the rise of multimodal models, both dedicated image or video generation ones like \\nImagen and Veo, along with Gemini’s ability to process and output other modalities than \\nonly text, there’s a need for additional techniques to monitor and align these outputs to \\nthe creator’s intents. In addition to prompt alignment - assuring that the output from the \\ngenerative model matches the text description provided, there’s also organizational policy \\nalignment that can be tracked, logged, and monitored. As an analog to drift, subjective \\noutput can be measured by reviewing the original prompt and utilizing a multimodal \\ngenerative AI’s capabilities of assessment to determine whether the prompt and the \\noutput are similar, or have alignment. This can be done in multiple ways - a zero-shot, \\nsingle prompt process to get a quick classification - or using the reasoning process of a \\ngenerative model to decompose the prompt into related component parts, assess each one \\nof these component part questions, and aggregate a score, yielding a more explainable or \\ninterpretable score. This process, often called “LLM as a Judge” or a generative Autorater, \\ncan bring some of the organization’s potentially subjective policy guidance into assessable \\nscores. Vertex AI’s Generative AI Evaluation service provides primitives for constructing \\nCustom Metrics and using these as autoraters.\\nContinuous evaluation is another common approach to GenAI application monitoring. In \\na continuous evaluation system, you capture the model's production output and run an \\nevaluation task using that output, to keep track of the model's performance over time. One \\napproach is collecting direct user feedback, such as ratings (for example thumbs up/down), \\nwhich provides immediate insight into the perceived quality of outputs. In parallel, comparing \\nmodel-generated responses against established ground truth, often collected through \\nhuman assessment or as a result of an ensemble AI Model approach, allows for deeper \\nanalysis of performance. Ground truth metrics can be used to generate evaluation metrics \\nas described in the Evaluation section. This process provides a view on how your evaluation \\nmetrics changed from when you developed your model to what you have in production today.\", 'Operationalizing Generative AI on Vertex AI using MLOps\\n42\\nFebruary 2025\\nAs with traditional monitoring in MLOps an alerting process should be deployed for notifying \\napplication owners when a drift, skew or performance decay from evaluation tasks is \\ndetected. This can help you promptly intervene and resolve issues. This is achieved by \\nintegrating alerting and notification tools into your monitoring process.\\nMonitoring expands beyond drift, skew and evaluation tasks. Monitoring in MLOps includes \\nefficiency metrics like resources utilization and latency. Efficiency metrics are as relevant and \\nimportant in gen AI as they are in any other AI application.\\nVertex AI provides a set of tools that can help with monitoring. The Vertex AI Eval Service \\nfor gen AI26 can be used for classification, summarization, question answering, and text \\ngeneration tasks as well as Agent evals based on rubrics and more detailed trajectory \\nevaluations. Vertex Pipelines can be used to allow the recurrent execution of evaluation jobs \\nin production as well as running pipelines for skew and drift detection processes.\\nOne form of logging commonly employed in application development is “tracing”, which \\nsends out logged events to an aggregator from internal steps in a complex system - \\nallowing detailed representations of a complicated application.  This same approach can \\nbe utilized in developing agents, instrumenting steps by emitting traces, and ensuring that \\nwhen you look at the logs you can understand which step for which agent for which user \\nsession. OpenTelemetry is the commonly adopted standard for traces with many possible \\naggregators, including Cloud Observability.  The spec for traces has been changing recently \\nto support larger payloads as needed to describe the behaviors of agents and LLMs.', 'Operationalizing Generative AI on Vertex AI using MLOps\\n43\\nFebruary 2025\\nGovern\\nIn the context of MLOps governance encompasses all the practices, and policies that \\nestablish control, accountability, and transparency over the development, deployment, and \\nongoing management of machine learning (ML) models, including all the activities related to \\nthe code, data and models lifecycle.\\nAs mentioned in the Develop & Experiment section the chain element and the relative \\ncomponents become a new type of assets that need to be governed over the full lifecycle \\nfrom development to deployment, to monitoring. \\nThe governance of the chain element lifecycle extends to lineage tracking practices as well.  \\nWhile for predictive AI systems lineage focuses on tracking and understanding the complete \\njourney of a machine learning model, in gen AI, lineage goes beyond the model artifact \\nextending to all the components in the chain. This includes the data and models used and \\ntheir lineage, the code involved and the relative evaluation data and metrics. This can help \\nauditing, debugging and improvements of the models\\nAlong with these new practices, existing MLOps and DevOps practices still apply to MLOps \\nfor gen AI:\\n1. The need to govern the data lifecycle; see “Data Practices”.\\n2. The need to govern the tuned model lifecycle; see “Tuning and Training”.\\n3. The need to govern the code lifecycle; see “Deployment of GenAI \\nSystem components”.', 'Operationalizing Generative AI on Vertex AI using MLOps\\n44\\nFebruary 2025\\nThe next segment will introduce a set of products that allow developers to perform \\ngovernance of the data, model and code assets. We will discuss products like Google \\nCloud Dataplex, which centralizes the governance of model and data, Vertex ML Metadata \\nand Vertex Experiment, which allows developers to register experiments, their metrics \\nand artifacts.\\nExtend MLOps for gen AI to Agents\\nThe development and operationalization of genAI systems involve intricate workflows, \\noften chaining together multiple models, APIs, and data sources. A particularly compelling \\nand increasingly important facet of this landscape is the rise of Agents. These intelligent \\nsystems, capable of interacting with their environment/tools and making decisions, represent \\na significant advancement in genAI capabilities. While we briefly introduced the concept \\nof agents within the context of chains and augmentation, their unique characteristics \\nand operational demands warrant a deeper dive. This section unfolds the specific MLOps \\nconsiderations for building, deploying, and managing agent-based systems, addressing their \\ndistinct lifecycle, tooling requirements, and the crucial aspects of observability, safety, and \\ncontinuous improvement. For the fundamental knowledge on Agents we recommend to read \\nthe Agent whitepaper by Google.93', \"Operationalizing Generative AI on Vertex AI using MLOps\\n45\\nFebruary 2025\\nAgent Lifecycle\\nFigure 12. Agent Core: Tools, model, and instruction prompt\\nThe core of an agent system comprises three fundamental elements: a Foundation Model, \\nInstructions, and a Tool. The foundation model serves as the cognitive engine, providing \\nthe agent with reasoning and language processing capabilities. Instructions represent the \\nguiding directives or goals that the agent is designed to achieve. These instructions could \\nrange from simple tasks to complex, multi-step objectives. The Tool, rather than being the \\nactual executable code, consists of descriptions of the available functions and their required \\nparameters.  These descriptions provide the foundation model with the information it needs \\nto reason about which tool is most appropriate for a given situation and how to use it.  The \\nfoundation model analyzes the instructions and the descriptions of the available functions, \\ndetermining which function to call and what parameter values to use. The actual execution \\nof the chosen function is then handled separately (by the developer's code or, in some \\nadvanced systems, directly by the model itself). Together, these three components work in \\nconcert, allowing the agent to intelligently interpret instructions, leverage the Foundation \\nModel's knowledge of available tools, and utilize the appropriate tool (via its description) to \\naccomplish its objectives.\", \"Operationalizing Generative AI on Vertex AI using MLOps\\n46\\nFebruary 2025\\nFigure 13. Multi-turn interaction of an agent with tools and memory until the final response\\nThese core components – the Foundation Model, Instructions, and Tool – work in dynamic \\ninterplay throughout the agent's operational lifecycle. The following steps detail how these \\nelements are orchestrated to process information, interact with the environment, and \\nultimately achieve the agent's objectives.\\n1. New Query: The user initiates the interaction by providing a new query or question\\n2. Function Identification: The foundation model, along with the available tools (e.g., APIs, \\ndatabases, specialized software) and instructions, analyzes the query and determines \\nif a function call is necessary. If so, it identifies the appropriate function name and the \\nrequired parameters.\\n3. Function Call Preparation: The foundation model generates a structured function call \\nrequest, specifying the function name and the parameters to be passed.\\n4. Function Call Execution: This step is performed by the developer's code (or, in advanced \\nfoundation models, automatically by the model). The code receives the function call \\nrequest, executes the corresponding function (e.g., making an API call), and retrieves \\nthe result.\", \"Operationalizing Generative AI on Vertex AI using MLOps\\n47\\nFebruary 2025\\n5. Intermediate Response: The result of the function execution (the data retrieved by the \\nfunction) is sent back to the foundation model that produces an intermediate response.\\n6. Iterative Context Update (Short-Term Memory): The developer's code updates the \\nconversation history (short-term memory) with the intermediate response and provides \\nthe updated context to the foundation model (returning to step 2).\\n7. Final Response: Once the foundation determines it has all the necessary information (or \\na maximum number of steps is reached to prevent infinite loops), it generates the final \\nresponse to the user, incorporating the information gathered from all the function calls.\\nThe preceding summary of the core components and operational lifecycle of an agent \\nprovides a foundation for examining the crucial considerations for productionizing these \\nintelligent systems.\\nTool Orchestration\\nAs established, tools are essential for enabling generative AI agents to interact with the world \\nand execute complex tasks.  These tools vary significantly in implementation, accessibility, \\nand capabilities. Understanding these distinctions is crucial for building effective and \\nrobust agents.\", \"Operationalizing Generative AI on Vertex AI using MLOps\\n48\\nFebruary 2025\\nTool Types & Environments\\nIn a cloud environment, tools can be categorized based on origin and access method:\\n• Code Functions: Functions implemented directly within the agent's codebase, granting \\ndirect access to local resources and logic. These can be implemented in various \\nprogramming languages (e.g., Python, Java). On Google Cloud, services like Artifact \\nRegistry (for storing code libraries) and Cloud Code (for repository management) can \\nbe used.\\n• Private REST APIs: APIs hosted within a Virtual Private Cloud (VPC), providing secure \\naccess to internal services and data. Google Cloud offers services like Cloud Run (for \\ndeploying containerized applications, including APIs), API Gateway (for managing and \\nsecuring APIs), and Apigee API Management (for advanced API management).\\n• Public REST APIs: Publicly available APIs from third-party services, offering diverse \\nfunctionalities accessible over the internet. A NAT Gateway can be used to provide secure \\noutbound internet access for resources within a VPC without direct public exposure.\\nBoth Code Functions and Private REST APIs enable secure interactions with internal \\ndatabases, storage systems, other internal services, or even other agents within the \\norganization's infrastructure. A comparison of the tool types follows:\", 'Operationalizing Generative AI on Vertex AI using MLOps\\n49\\nFebruary 2025\\nTable 2. Different types of tools and their feature comparison\\nAs described, tools can be implemented in various ways, from code functions within the \\nagent itself to APIs hosted in VPCs or publicly available services.  Furthermore, these tools \\noften reside in different locations within a cloud environment. Data sources, for example, \\nare frequently managed within dedicated data environments, for example data lake or \\nmesh projects, while APIs and agents used as tools might be deployed within application \\nproduction environments. Code repositories, housing code functions and other artifacts, are \\ntypically maintained in central artifact government environments for auditability purpose. This \\ndistributed nature of tool deployment presents a significant challenge: how do we effectively \\nmanage and utilize this diverse and dispersed collection of tools in a production setting?\\nFeature  \\\\  Tool Type Code Functions Private REST APIs Public REST APIs\\nLatency Very low Medium Potentially High\\nImplementation Easy Medium Relatively Simple\\nOwnership/Control Full Full within the VPC None\\nAuthentication None Enabled Requires API keys, OAuth\\nShareability Limited Sharable within VPC Accessible to all\\nMonitoring Custom Standard APIs Limited\\nVersion Control Repositories (e.g. Git) Repositories (e.g. Git) API provider dependent\\nSecurity Agent environment Strong within VPC Careful consideration\\nInternal Systems Direct access Access via network Generally not direct', 'Operationalizing Generative AI on Vertex AI using MLOps\\n50\\nFebruary 2025\\nTo address this challenge, and given the variety of tool types and their locations, a \\ncentralized catalog becomes essential for managing tools in a production environment.  This \\nis where the concept of a Tool Registry comes into play.\\nTool Registry\\nA Tool Registry, a centralized catalog of all available tools, provides a standardized way \\nto discover, access, and manage these essential components. This centralized approach \\noffers several key advantages.  First, it promotes reusability by enabling easy discovery \\nand reuse of tools by different agents, significantly reducing development time and effort.  \\nSecond, it enhances shareability and visibility, making tools readily available to all \\nauthorized developers, fostering collaboration and knowledge sharing across teams.  Third, \\nit strengthens security and accessibility by enforcing access control and ensuring that \\nonly authorized agents utilize specific tools.  Fourth, the registry promotes standardization \\nin tool implementation and usage, which improves code maintainability and agent \\ninteroperability.  Fifth, it contributes to the robustness of the system through centralized \\nmanagement that allows for better monitoring, evaluation, and version control. Finally, the \\nTool Registry facilitates auditability by providing a clear record of tool usage, which is \\ncrucial for compliance and accountability.\\nTo deliver these advantages, the Tool Registry incorporates several key features. The Tool \\nRegistry stores essential information about each tool, including its name, description, \\nparameters, and output format, effectively serving as a comprehensive tool catalog.  It also \\nprovides robust version control, tracking different versions of tools to ensure compatibility \\nwith various agents.  The registry further simplifies the discovery process through effective \\nsearch and discovery functionalities, allowing developers to easily find the tools they need.  \\nFinally, it manages permissions through access control mechanisms, ensuring that only \\nauthorized agents can access specific tools.', 'Operationalizing Generative AI on Vertex AI using MLOps\\n51\\nFebruary 2025\\nThe Tool Registry mirrors the Model Registry in MLOps, highlighting its importance \\nfor operationalizing generative AI agents, just as Model Registries are essential for \\noperationalizing machine learning models.\\nTool Selection Strategies at Scale\\nThe Tool Registry serves as a comprehensive catalog of all available tools within an \\nenterprise, potentially housing hundreds or even thousands of options. However, simply \\nproviding a foundation model with access to this entire catalog isn\\'t always the most \\neffective approach.  Presenting the model with an excessively long list of tools can actually \\nbe counterproductive.  The model might become overwhelmed or confused, especially \\nif tools have overlapping functionalities or similar descriptions. This can lead to incorrect \\ntool selection, reduced performance, and unpredictable agent behavior.  Imagine trying to \\nchoose the right tool from a massive, disorganized toolbox – it becomes a time-consuming \\nand error-prone process.  Therefore, strategic tool selection is crucial for optimizing agent \\nperformance and reliability.\\nTo address this challenge, we can draw a parallel with microservices architecture. Just as \\nin microservices, where each service is designed for specific tasks, we can view generative \\nAI agents as specialized microservices. Each agent should ideally be equipped with only \\nthe subset of tools directly relevant to its specific responsibilities. This focused collection \\nof tools, a carefully curated \"Tool List,\" is a subset of the broader Tool Registry. Providing \\nagents with a limited toolset offers several key benefits:\\n• Improves performance by reducing the search space for tool selection. \\n• Increases predictability, making the agent\\'s behavior more understandable and easier to \\nanticipate. \\n• Simplifies testing and debugging, as the agent\\'s scope is more clearly defined.', \"Operationalizing Generative AI on Vertex AI using MLOps\\n52\\nFebruary 2025\\n• Enhances security by limiting the agent's potential impact on sensitive data or systems.\\nThree general strategies can be employed: providing full toolset access (creating a generalist \\nagent), providing a limited toolset (creating a specialist agent), or dynamic tool selection. \\nA generalist agent is granted access to the entire Tool Registry, relying on the foundation \\nmodel's reasoning abilities to select the appropriate tool. While this approach offers greater \\nflexibility, it can lead to the performance and predictability issues mentioned earlier. A \\nspecialist agent, conversely, is equipped with a carefully curated Tool List containing only the \\ntools necessary for its specific task. This strategy promotes performance, predictability, and \\nsecurity, but it requires more upfront design effort to define each agent's toolset.\\nThe third strategy, dynamic tool selection, represents a more emergent approach. In this \\nmodel, the agent doesn't have a predefined tool list. Instead, for each incoming task, the \\nagent queries the Tool Registry to identify and select the most relevant tools. This dynamic \\napproach allows the agent to adapt to a wider range of tasks without requiring a massive, \\npre-configured toolset.  However, this increased flexibility comes with its own set of \\nchallenges.  Because the tool selection process occurs at runtime and is dependent on the \\nspecific task, predicting the agent's behavior and ensuring consistent performance can be \\nmore difficult.\\nThe optimal strategy, whether to create generalist, specialist, or dynamically selecting \\nagents, depends on the specific needs and context of the enterprise. Factors to consider \\ninclude the total number of tools in the registry, the complexity of the tasks the agents are \\ndesigned to perform, the desired level of control over agent behavior, and the available \\nresources for agent design and maintenance.  Future work will involve releasing experimental \\nresults comparing these three strategies quantitatively to provide clearer guidance.  In \\nparticular, the dynamic selection strategy, being relatively new, requires further research to \\ndevelop robust testing and evaluation methodologies.\", \"Operationalizing Generative AI on Vertex AI using MLOps\\n53\\nFebruary 2025\\nAgent Evaluation & Optimization\\nAs we have discussed, agents are a new and emerging type of AI system with the potential \\nto revolutionize human-computer interaction.  A crucial aspect of realizing this potential lies \\nin effectively evaluating and optimizing agent performance. While previous sections of this \\nwhite paper have addressed general GenAI evaluation, this section focuses specifically on \\nagent-centric evaluation and optimization techniques.\\nAgent evaluation can be structured into five key stages:\\n1. Tool Unit Testing and Refinement: During development, each tool available to the agent \\nundergoes rigorous unit testing. This process validates the tool's functionality and ensures \\nit performs as expected.  Crucially, this stage also involves refining the descriptions of \\nthe available functions and their parameters, which are what the foundation model uses \\nfor reasoning and selection.  Thorough unit testing and well-defined tool descriptions are \\nfoundational for reliable agent behavior.\\n2. Evaluation Dataset Creation: A representative evaluation dataset of potential \\nagent (multi-turn) scenarios is constructed.  This dataset can be created manually, by \\ncarefully crafting specific interaction sequences, or automatically, by recording real \\nuser interactions with the agent and then incorporating these recorded sessions into \\nthe evaluation set.  This automatic approach allows for continuous enrichment of the \\nevaluation dataset with real-world usage patterns.\\n3. Tool Selection Evaluation: Tool selection evaluation focuses on assessing the agent's \\nproficiency in choosing and utilizing the correct tools for given tasks. For an individual \\ntool, this involves measuring the percentage of successful tool selections, ensuring the \\naccuracy of parameter structuring and population for function calls, and confirming the \\nappropriate handling of scenarios where no tool selection is necessary. Collectively we \\nuse a trajectory evaluation, where we compare the set of steps we expect the agent to\", 'Operationalizing Generative AI on Vertex AI using MLOps\\n54\\nFebruary 2025\\ntake, including the tools the agent should use, against the steps an agent actually took.  \\nFrom a trajectory evaluation we can identify when an agent hallucinates (by not looking \\nup knowledge) or uses the wrong tool or getting stuck in a cul-de-sac.  These metrics \\ncollectively provide a comprehensive view of the agent\\'s effectiveness in leveraging its \\navailable tools.\\n4. Reasoning and Groundedness Evaluation: Beyond the standard gen AI end-to-end \\nevaluation (i.e., assessing the correctness and groundedness of the final answer given \\nan input), agent evaluation also involves assessing the reasoning process.  This includes \\nevaluating how the agent handles situations with missing data, how it recovers from errors, \\nand its ability to maintain context across multiple turns.  This stage examines the agent\\'s \\n\"cognitive\" abilities and its capacity to handle complex, multi-step interactions.\\n5. Operational Metric Evaluation: Finally, the agent\\'s performance is evaluated from \\nan operational perspective.  This involves measuring metrics such as latency (the time \\ntaken to generate a response or interact  with specific tools) and cost (the computational \\nresources consumed).  These metrics are essential for determining the practical viability \\nand scalability of the agent in a production environment.\\nBy systematically addressing these five stages, developers can gain a comprehensive \\nunderstanding of their agent\\'s strengths and weaknesses, enabling targeted optimization \\nand improvement.  This multi-faceted evaluation approach is critical for building robust and \\nreliable agent-based applications.\\nAgent optimization is an iterative process that focuses on refining the agent\\'s components \\nand configuration to enhance its performance and effectiveness. One key aspect of \\noptimization involves selecting the most appropriate set of tools for the agent\\'s tasks and \\nensuring that the function and parameter definitions within those tools are clear, concise, \\nand well-structured. This clarity enables the foundation model to effectively reason about \\nand utilize the available tools. Another crucial factor is selecting the right foundation model', \"Operationalizing Generative AI on Vertex AI using MLOps\\n55\\nFebruary 2025\\nfor the agent's specific needs. Different foundation models may exhibit varying levels of \\nproficiency in reasoning and tool selection, so choosing a model that excels in these areas \\nis essential. Finally, optimizing the instruction prompt provided to the agent can significantly \\nimprove its performance. A well-crafted prompt provides clear guidance, sets expectations, \\nand helps the agent effectively navigate complex tasks. By iteratively refining these three \\nelements—tools, foundation model, and instruction prompt—developers can significantly \\nenhance the agent's capabilities and overall performance.\\nObservability and Memory\\nIn the realm of agent-based systems, observability and explainability are paramount.  These \\nqualities are essential for understanding agent behavior, building trust, and ensuring that \\nagents operate as intended.  Observability refers to the ability to gain insights into an agent's \\ninternal workings and its interactions with the environment.  Explainability, on the other hand, \\nfocuses on understanding the why behind an agent's decisions—why it selected a particular \\ntool or took a specific action.\\nAchieving observability and explainability requires careful consideration of the agent's \\nmemory and decision-making processes.  The agent's short-term and long-term memory \\nplay a crucial role in providing context and enabling traceability.  Being able to see events, \\norganized on a timeline, as related to the steps an agent takes is paramount.\", \"Operationalizing Generative AI on Vertex AI using MLOps\\n56\\nFebruary 2025\\nFigure 14. Example of cloud trace for agents\\nShort-term memory (or conversation history) stores the ongoing conversation within a single \\nuser session. This includes the user's queries, the model's function calls, and the responses \\nfrom those function calls. This context is essential for the model to understand follow-up \\nquestions and maintain coherence throughout the interaction.  By examining the short-term \\nmemory, we can trace the sequence of interactions and understand how the agent arrived at \\na particular state or decision.\\nLong-term memory, on the other hand, stores information about past user interactions \\nacross multiple sessions. This allows the agent to learn user preferences, provide \\npersonalized recommendations, and offer more efficient service over time.  Long-term \\nmemory provides valuable insights into the agent's learning process and how its behavior \\nevolves over time.\\nSeveral implementation options exist for both short-term and long-term memory in agent \\nproductionization, each with its own trade-offs:\", \"Operationalizing Generative AI on Vertex AI using MLOps\\n57\\nFebruary 2025\\nShort-Term Memory Implementation Options:\\n• Logs: Storing interaction history as plain text logs can be sufficient for simple applications.\\n• Cloud Storage/Database: Offers more structured storage for complex applications.\\n• API Session: Managing conversation history on the client-side reduces server-side \\nstorage needs.\\n• Combination: A hybrid approach combining different mechanisms for \\noptimized performance.\\nLong-Term Memory Implementation Options:\\n• Vector Databases: Well-suited for storing and retrieving information based on \\nsemantic similarity.\\n• Metadata Storage/Graphs: Used to store session IDs, timestamps, and other \\nrelevant metadata.\\n• Cloud Storage/Databases: Provides a full record of all interactions.\\n• Combination: Combining different mechanisms for efficient storage and retrieval.\\nThe choice of implementation depends on the specific needs of the application and the \\ncomplexity of the agent's interactions.\\nBy effectively leveraging both short-term and long-term memory, developers can gain a \\ndeeper understanding of the agent's behavior, trace its decision-making process, and \\nidentify areas for improvement.  This granular level of observability and explainability is \\ncrucial for building trust in agent-based systems and ensuring their responsible deployment.\", \"Operationalizing Generative AI on Vertex AI using MLOps\\n58\\nFebruary 2025\\nDeploying an Agent to Production \\nFigure 15. Agent and tool productionization using CI/CD pipelines\\nDeploying an agent to production requires a robust and automated process to ensure \\nconsistency, reliability, and scalability. Building upon a standardized repository structure, we \\ncan implement CI/CD pipelines that automate the tool registration process and streamline \\nthe deployment workflow. Such a pipeline typically involves several key stages. Initially, the \\npipeline validates that the repository structure adheres to defined standards, including \\nthe presence of required directories and files. It then executes code-level tests, such as \\nunit tests and static code analysis, to verify code quality and correctness. If necessary, \\nthe pipeline builds custom container images for the tool or agent, ensuring consistent \\ndeployment across environments.\\nThe pipeline then deploys the agent and its associated tools to a development environment \\nfor early testing and integration. This deployment can leverage infrastructure-as-code \\n(IaC) scripts within the agent's repository for fine-grained control over the environment\", \"Operationalizing Generative AI on Vertex AI using MLOps\\n59\\nFebruary 2025\\nor utilize a pre-defined, standardized infrastructure configuration for consistency and \\nsecurity. A manual approval gate at this stage allows for human oversight and verification \\nbefore proceeding.\\nAfter approval, the pipeline deploys the agent and tools to a staging environment that closely \\nmirrors production, enabling more rigorous testing under realistic conditions.  Here, human \\ntesters can engage with the agent at scale, further validating its performance and potentially \\naugmenting the evaluation data with real-world usage patterns. Automated evaluation scripts \\nand various tests, including integration and stress tests, are also executed, and the results \\nare stored centrally for analysis and governance. Another manual approval gate follows, \\nallowing for final verification based on the comprehensive testing results.\\nOnce approved, the pipeline deploys the agent to the production environment. Crucially, \\nduring this process, the tool registration occurs implicitly. Metadata extracted from the \\nrepository structure is automatically registered in the Tool Registry, eliminating manual \\neffort and ensuring consistency. Post-deployment, the agent's performance is continuously \\nmonitored using predefined scripts and configurations, enabling rapid detection and \\nresolution of any issues.\\nThis entire deployment lifecycle, from initial development to production deployment and \\nmonitoring, forms a continuous loop of improvement.  Insights gained from monitoring and \\nevaluation inform further refinements to the agent's components, tools, and configuration, \\nleading to iterative enhancements and updates that are then redeployed through the same \\npipeline. This iterative approach ensures that agents remain adaptable, effective, and aligned \\nwith evolving needs and objectives.\", 'Operationalizing Generative AI on Vertex AI using MLOps\\n60\\nFebruary 2025\\nThis automated approach, with its standardized structure, rigorous testing, and implicit \\ntool registration, ensures a robust and efficient deployment process. The CI/CD pipeline, \\ncoupled with the centralized Tool Registry, provides a solid foundation for deploying and \\nmanaging agents in production, enabling organizations to leverage the full potential of these \\nintelligent systems.\\nOperations: People & Processes\\nProductionizing machine learning or gen AI solutions, requires more than just technology and \\nservices. It demands a well-orchestrated interplay of diverse individuals, each contributing \\nspecialized skills and expertise, working together seamlessly under standardized processes.  \\nLike a well-oiled machine, the effective operation of ML solutions hinges on the coordinated \\nefforts of various personas, forming a cohesive and efficient workflow.\\nFigure 16. Traditional MLOps landscape of people, process, and environments \\nIn the traditional predictive ML landscape, a typical setup involves several interconnected \\nteams, each with distinct roles and responsibilities:', 'Operationalizing Generative AI on Vertex AI using MLOps\\n61\\nFebruary 2025\\n• Cloud Platform Team: This foundational team comprises cloud architects, administrators, \\nand security specialists. They are responsible for providing and managing the essential \\ncloud infrastructure, ensuring security, defining access control, and overseeing the overall \\ncloud strategy.\\n• Data Engineering Team: This team focuses on building and maintaining the data \\npipelines that fuel the AI models and solutions. The data engineers handle data ingestion, \\npreprocessing, and preparation, ensuring data quality, while data owners are responsible \\nfor the data accessibility for data scientists and other business stakeholders.\\n• Data Science and MLOps Team: This team includes data scientists who experiment with \\nand train models, as well as ML engineers who collaborate with data scientists to automate \\nthe ML pipeline, encompassing data preprocessing, model training, and post-processing \\nat scale. They leverage CI/CD pipelines for seamless model deployment and management. \\nWithin this team, the MLOps Engineer plays a vital role in building and maintaining the \\ninfrastructure (IAM roles and networking layer) for the Cloud Platform team  that supports \\nand standardizes these pipelines.\\n• Machine Learning Governance: This centralized function oversees the entire ML \\nlifecycle, acting as a repository for model metadata, development artifacts, and \\nperformance metrics. It ensures transparency, accountability, and compliance in AI \\ndevelopment and deployment. This is the single point of truth for product owner and \\nauditors to assess the ML solutions and models respectively.\\nThis collaborative structure ensures a smooth and controlled path from initial \\nexperimentation to production-ready ML solutions. However, gen AI introduces a new \\nlayer of complexity to the enterprise organizational landscape. This new layer, focused on \\nbuilding and deploying GenAI applications, necessitates specialized roles and expertise to \\neffectively harness the power of these advanced models.', 'Operationalizing Generative AI on Vertex AI using MLOps\\n62\\nFebruary 2025\\nFigure 17. From MLOps to GenAI application development landscape of people, process, and environments\\n• Prompt Engineers: These individuals wear two hats, blending technical expertise with \\ndomain knowledge.  First, they are skilled in the art of crafting and refining prompts, \\nunderstanding how different inputs influence model behavior and optimizing prompt \\ndesign for specific applications.  Second, they often possess deep subject matter \\nexpertise, allowing them to define the right questions and expected answers for eliciting \\ndesired knowledge from the model.  This dual role enables them to bridge the gap \\nbetween the technical intricacies of gen AI models and the specific needs of various \\ndomains.  We have seen data scientists, ML engineers, and even business analysts with \\ndeep domain knowledge successfully transition into prompt engineering roles.\\n• AI Engineers: AI engineers are responsible for scaling the use of generative models and \\ntransitioning gen AI solutions into production. They focus on building robust backend \\nsystems that incorporate functionalities like evaluation at scale, guardrails to ensure', \"Operationalizing Generative AI on Vertex AI using MLOps\\n63\\nFebruary 2025\\nresponsible AI, and seamless integration with data sources (including RAG, data lakes, \\nand tools).  Their expertise lies in creating a stable and scalable foundation for gen \\nAI applications.\\n• DevOps/App Developers: These individuals leverage their software engineering skills \\nto develop the front-end components of gen AI applications. They work closely with AI \\nengineers to integrate the backend capabilities, ensuring a user-friendly and efficient \\ninterface for interacting with GenAI models.  Their focus is on delivering a seamless and \\nengaging user experience.\\nThe introduction of these specialized roles highlights the growing demand for expertise in \\napplying and deploying gen AI solutions effectively.  Prompt engineers, AI engineers, and \\nDevOps/App developers work in concert, forming a new operational layer dedicated to \\nbuilding and deploying the next generation of AI-powered solutions.\\nIt's important to acknowledge that the scale and structure of an organization will influence \\nthe specific roles and responsibilities within these teams. In smaller organizations, individuals \\nmay wear multiple hats, fulfilling the duties of several personas. As companies grow and their \\nGenAI initiatives mature, more specialized roles and dedicated teams may emerge to handle \\nthe increased complexity.\\nThe diagram below provides a more detailed illustration of the key operations and \\ninteractions within this organizational structure. A deeper dive into this diagram can provide \\nfurther insights into the specific processes and responsibilities associated with each role.\", 'Operationalizing Generative AI on Vertex AI using MLOps\\n64\\nFebruary 2025\\nFigure 18. How multiple team collaborate to operationalize both models and GenAI applications \\nBy effectively coordinating these diverse roles and streamlining their interactions, \\norganizations can establish a robust operational foundation for both traditional ML and \\ngenerative AI initiatives. This collaborative approach, combined with well-defined processes, \\nis essential for successfully navigating the complexities of AI productionization and \\nmaximizing its potential benefits.', 'Operationalizing Generative AI on Vertex AI using MLOps\\n65\\nFebruary 2025\\nThe role of an AI platform for gen \\nAI operations\\nAlongside the explosion of both predictive and gen AI applications, AI platforms, like Vertex \\nAI,11 have emerged as indispensable tools for organizations seeking to leverage the power of \\nArtificial Intelligence (AI). These comprehensive platforms provide a unified environment that \\nstreamlines the entire AI lifecycle, from data preparation and model training to deployment, \\nautomation, continuous integration/continuous delivery (CI/CD), governance, and monitoring.\\nAt the heart of an AI platform lies its ability to support diverse AI development needs. Data \\nscientists and model builders need to create, evaluate, and tune predictive and generative \\nAI tools. Whether you seek to utilize pre-trained AI solutions, adapt existing models through \\ntuning or transfer learning, or embark on training your own large models, AI platforms provide \\nthe infrastructure and tools necessary to support these journeys. Application developers \\nand agent builders also need centralized and scalable access to models and services which \\nsupport them, like RAG, and observability support to bring MLOps to gen AI. The advent \\nof these platforms has revolutionized the way organizations approach AI, enabling them \\nto productionize AI applications in a secure, enterprise-ready, responsible, controlled and \\nscalable manner. These platforms accelerate innovation as well as foster reproducibility and \\ncollaboration while reducing costs and maximizing Return on Investment (ROI).\\nThe new gen AI paradigm discussed in prior sections demands a robust and reliable AI \\nplatform that can seamlessly integrate and orchestrate a wide range of functionalities.   \\nThese functionalities include model tuning for specific tasks; leveraging paradigms like \\nretrieval augmented generation3 (RAG) to connect to internal and external data sources; \\nand pre-training or instruction fine-tuning large models from scratch. Complex applications', 'Operationalizing Generative AI on Vertex AI using MLOps\\n66\\nFebruary 2025\\nalso often require chaining with other models, such as classifiers to route inputs to the \\nappropriate LLM/ML model, extraction of customer information from a knowledge base, \\ninclusion of safety checks, or even creation of caching systems for cost optimization.  \\nFigure 19.  Key components of Vertex AI for gen AI\\nKey components of Vertex AI for gen AI\\nVertex AI eliminates the complexities of managing the entire infrastructure required for AI \\ndevelopment and deployment. Instead, Vertex AI offers a user-centric approach, providing \\non-demand access to the needed resources. This flexibility empowers organizations to \\nfocus on innovation and collaboration, rather than infrastructure management, and up-\\nfront hardware purchase. The features of Vertex AI that support gen AI development can be \\ngrouped into eight areas.', 'Operationalizing Generative AI on Vertex AI using MLOps\\n67\\nFebruary 2025\\nDiscover: Vertex Model Garden\\nAs discussed before, there is already a wide variety of available foundation models, trained \\non a broad range of datasets, and the cost of training a new foundation model can be \\nprohibitive. Thus it often makes sense for companies to adapt existing foundation models \\nrather than creating their own from scratch. As a result, a platform facilitating seamless \\ndiscovery and integration of diverse model types is critical.\\nVertex AI Model Garden1 supports these needs, offering a curated collection of over \\n150 Machine Learning and gen AI models from Google, Google partners, and the open-\\nsource community. It simplifies the discovery, customization, and deployment of both \\nGoogle’s proprietary foundational models and diverse open-source models across a \\nvast spectrum of modalities, tasks, and features. This comprehensive repository permits \\ndevelopers to leverage the collective research on artificial intelligence models within a single \\nstreamlined environment.\\nModel Garden encompasses a diverse range of modalities such as Language, Vision, Tabular, \\nDocument, Speech, Video, and Multimodal data. This broad coverage enables developers \\nto tackle a multitude of tasks, including generation, classification, regression, extraction, \\nrecognition, segmentation, tracking, translation, and embedding. Model Garden houses \\nGoogle’s proprietary and foundational models (like Gemini,27 PaLM 2,28 Imagen29) alongside \\nnumerous popular open source and third-party partner models like like Llama 3,30 T5 Flan,31 \\nBERT,32 Stable Diffusion,33 Claude 3 (Anthropic),34 and Mistral AI.35 Additionally, it offers task-\\nspecific models for occupancy analysis, watermark detection, text-moderation, text-to-video, \\nhand-gesture recognition, product identification, and tag recognition, among others. Every \\nmodel36 in Vertex Model Garden has a model card which includes a description of the model, \\nthe main use cases that can cover, and the option (if available) to tune the model or deploy \\nit directly.', 'Operationalizing Generative AI on Vertex AI using MLOps\\n68\\nFebruary 2025\\nModel Garden fosters experimentation by facilitating access to Google’s proprietary \\nfoundational models through the Vertex AI Studio UI,37 a playground where you can play \\naround with prompts, models, and open-source models using provided Colab notebooks. \\nOne-click deployment is available for some external models, and there are more than 40 \\nmodels available for fine-tuning for specific needs. Furthermore, the platform allows users to \\nleverage technologies like vLLM38 and quantization techniques for optimizing deployments for \\nefficiency and reduced costs. We present below an overview of some of the models in Model \\nGarden. For an up-to-date list, please visit.36', \"Operationalizing Generative AI on Vertex AI using MLOps\\n69\\nFebruary 2025\\nModel Type Description Details\\nFirst-party models Foundation models\\nLeverage multimodal models \\nfrom Google across vision, \\ndialog, code generation, and \\ncode completion.\\nGemini39 family of models supporting \\ntext, image, audio, video streaming, \\ngiant cacheable context windows, \\ngrounding and function calling Imagen \\nfor text-to-image41\\nChip for high-quality  \\nspeech-to-text43\\nVeo for high quality text-to-video 43\\nCodey for code generation \\nand completion 42\\nFirst-party models Pre-trained APIs\\nBuild and deploy AI applications \\nfaster with our pre-trained APIs \\npowered by the best Google AI \\nresearch and technology.\\nText-to-Speech44\\nNatural Language processing 45\\nTranslation46\\nVision47\\nOpen  models Open source models\\nAccess a wide variety of \\nenterprise-ready open \\nsource models\\nGoogle’s Gemma, 48 family of \\nmodels including PaliGemma, 16 \\nCodeGemma 49, etc\\nMeta's Llama30\\nTII's Falcon50\\nMistral AI51\\nBERT,32 T-5 FLAN,31 ViT,52 EfficientNet53\\nThird-party models Third-party models\\nModel Garden will support third-\\nparty models from partners with \\nfoundation models.\\nAnthropic’s Claude 3 Haiku,  Sonnet \\nand Opus54,55\\nTable 3.  An overview of some of the models in Model Garden [Last Updated: March 18th, 2024]\", \"Operationalizing Generative AI on Vertex AI using MLOps\\n70\\nFebruary 2025\\nPrototype: Vertex AI Studio & Notebooks\\nRapid development and prototyping capabilities are also essential for developing gen AI \\napplications. Vertex AI prioritizes inclusivity and flexibility in its development environments, \\ncatering to a wide range of developer preferences and proficiency levels. This platform \\nprovides options for both console-driven and programmatic development workflows. Users \\ncan leverage the intuitive web interface for end-to-end application creation or utilize various \\nAPIs for deeper customization and control. These include the REST API56 and dedicated \\nSDKs for Python,57 NodeJS58 and Java,59 ensuring compatibility with diverse programming \\nlanguages and ecosystems. Developers can choose to use the tools and IDEs of their \\nchoice for interacting with the platform, or take advantage of Vertex-native tools like Vertex \\nColab Enterprise or Vertex Workbench to explore and experiment with code within familiar \\nnotebook environments.\\nVertex AI Studio60 provides a unified console-driven entry point to access and leverage the \\nfull spectrum of Vertex AI's gen AI services. It facilitates exploration and experimentation with \\nvarious Google first party foundation models (for example, PaLM 2, Gemini, Codey, Imagen, \\nand Universal Speech Model). Additionally, it offers prompt examples and functionalities \\nfor testing distinct prompts and models with diverse parameters. It’s also possible to adapt \\nexisting models through various techniques like supervised fine-tuning (SFT), reinforcement \\nlearning tuning techniques, and Distillation, and deploy gen AI applications in just a few \\nclicks. Vertex AI Studio considerably simplifies and democratizes gen AI adoption, catering \\nto a variety of users, from business analysts to machine learning engineers. You can see the \\nhomepage of Vertex AI Studio in Figure 13.\", 'Operationalizing Generative AI on Vertex AI using MLOps\\n71\\nFebruary 2025\\nFigure 20. Vertex AI Studio - Homepage\\nCustomize: Vertex AI training & tuning \\nWhile prompt engineering and augmentation are sufficient for some gen AI use cases, other \\ncases require training, tuning and adapting the models to get the best results. Vertex AI \\nprovides a comprehensive platform for training and adapting LLMs, supporting a range of \\ntechniques and approaches from prompt engineering to training models from scratch.', 'Operationalizing Generative AI on Vertex AI using MLOps\\n72\\nFebruary 2025\\nTrain \\nFor full-scale LLM training, TPUs and GPUs are vital because of their superior processing \\npower and memory capacity compared to CPUs. GPUs excel at parallel processing, enabling \\nfaster model training. TPUs, specifically designed for machine learning tasks, offer even \\nfaster processing and higher energy efficiency. This makes them ideal for large-scale, \\ncomplex models. Google Cloud provides a range of offerings to support LLM training, \\nincluding TPU VMs with various configurations, pre-configured AI platforms like Vertex AI, \\nand dedicated resources like Cloud TPU Pods for scaling up training. These offerings allow \\nusers to choose the right infrastructure for their needs, accelerating LLM development and \\nenabling cutting-edge research and applications.\\nTune \\nVertex AI also provides a comprehensive solution for adapting pre-trained LLMs. It supports \\na spectrum of techniques from a non-technical prompt engineering playground at inference \\ntime, to data-driven approaches involving tuning, reinforcement learning and distillation \\nmethods during the development or adaptation phase. The following five techniques – many \\nof which are unique to Vertex AI – enable users to explore and implement them effectively. \\nThis applies to both proprietary and open-source LLMs, allowing you to achieve superior \\nresults while optimizing for costs and latency requirements.\\n• Prompt engineering61 leverages carefully crafted natural language prompts, potentially \\nchained and enriched with external knowledge and examples, to nudge the LLM towards \\ndesired outputs without necessitating further training. Vertex AI through Vertex AI Studio \\noffers a dedicated playground for crafting, testing, comparing and managing diverse \\nprompts and techniques. Users can access various pre-built prompt templates within the \\nplatform and leverage public prompting guidelines62 for Google’s proprietary large models.', 'Operationalizing Generative AI on Vertex AI using MLOps\\n73\\nFebruary 2025\\n• Supervised fine-tuning (SFT) 63 on Vertex AI facilitates model adaptation by leveraging a \\nset of labeled examples (even a few hundred is enough) to tune a model on specific tasks \\nand contexts within domain-specific datasets. The required examples resemble the one-\\nshot example structure employed in the construction of a prompt. This effectively extends \\nthe few-shot learning approach for enhanced optimization. This focused tuning enables \\nthe model to encode additional parameters in the model necessary for mimicking desired \\nbehaviors such as improved complex prompt comprehension, adaptation to specific \\noutput formats, correcting errors, and learning new tasks. The SFT tuning approach on \\nVertex AI, minimizes computational overhead and time while yielding an updated model \\nthat integrates the newly acquired parameters with the original model’s core parameters.\\n• Reinforcement learning with human feedback (RLHF) ,64 available on Vertex AI for \\nfoundational models like PaLM 2,and open-source models like T5 (s-xxl) and Llama2, \\nleverages human feedback to train large models to align with human preferences. This \\ntechnique is well-suited in complex tasks involving preference modeling and optimizes \\nLLMs on intricate, sequence-level objectives not easily addressed by traditional \\nsupervised fine-tuning. The process involves first training a reward model using a human \\npreference dataset, then utilizing it to score the output from the LLM, and finally applying \\nreinforcement learning to optimize the LLM. This approach is recognized as a key driver of \\nsuccess in conversational large language models.\\n• Distillation step-by-step20 is an advanced distillation technique transferring knowledge \\nfrom a significantly larger model (known as teacher model) to a smaller task-specific \\nmodel (known as student model), preserving important information while reducing model \\nsize. Step-by-Step Distillation20 surpasses common techniques by requiring significantly \\nless data. This method, accessible on Vertex AI,65 significantly reduces inference costs and \\nlatencies while minimizing performance impact in the resulting smaller LLM.66', 'Operationalizing Generative AI on Vertex AI using MLOps\\n74\\nFebruary 2025\\nOrchestrate\\nAny training or tuning job you run can be orchestrated and then operationalized using Vertex \\nPipelines,13 a service that aims to simplify and automate the deployment, management, and \\nscaling of your ML workflows. \\nIt provides a platform for building, orchestrating, scheduling and monitoring complex and \\ncustom ML pipelines, enabling you to efficiently translate your models from prototypes \\nto production.\\nVertex Pipelines is also the platform behind all the managed tuning and evaluation services \\nfor the Google Foundation Models on Vertex AI. This ensures consistency as you can \\nconsume and extend those pipelines easily, without having to familiarize yourself with \\nmany services.\\nGetting started with Vertex Pipelines is simple: you define the pipeline’s step sequence in \\na Python file utilizing Kubeflow SDK.67 For further details and comprehensive onboarding, \\nconsult the official documentation.68', 'Operationalizing Generative AI on Vertex AI using MLOps\\n75\\nFebruary 2025\\nChain & Augment: Vertex AI Grounding, Extensions, and RAG \\nbuilding blocks\\nBeyond training, tuning and adapting models and prompts directly, Vertex AI offers a \\ncomprehensive ecosystem for augmenting LLMs, to address the challenges of factual \\ngrounding and hallucination. The platform incorporates emerging techniques like RAG and \\nagent-based approaches.\\nLLM function calling69 empowers users by enhancing the capabilities of language models \\n(LLMs). It enables LLMs to access real-time data and interact with external systems, \\nproviding users with more accurate and up-to-date information. To do that, users need to \\nprovide function definitions such as description, inputs, outputs to the gen AI model. Instead \\nof directly executing functions, the LLM intelligently analyzes user requests and generates \\nstructured data outputs. These outputs propose which function to call and what arguments \\nto use. Gemini models regularly top function calling leaderboards and support multi-modal \\nand compositional function calling. Not all LLMs support function calling.\\nLLM sandbox & code interpreter allow LLMs to write code (usually python) and execute \\nthat code in a sandbox runtime on the server.  This can allow a model to build its own tools \\non the fly and execute tools in a different fashion than the function calling feature.  In some \\ncases this may outperform function calling.\\nLLM structured output allows LLMs to return outputs in JSON or adhering to a specific \\nschema of fields and allowed value types. This is extremely important to application and \\nagent developers who are stitching together multiple LLM outputs across the steps of an \\napplication, to enforce a data model and have the LLM respect it.', 'Operationalizing Generative AI on Vertex AI using MLOps\\n76\\nFebruary 2025\\nLLM context caching and context window are extremely important aspects of LLM \\nselection for any agent builder. A large context window allows a lot of flexibility, bringing \\nmore information to the LLM, about the session or task in scope, and about the skill or \\nknowledge in scope.  Caching a large context window allows a developer to provide \\nknowledge to LLM without having to consume all of the input tokens at runtime for every \\nexecution - resulting in faster and cheaper execution.  If you need more information than can \\nfit into your context window, or need speed or pricing optimizations beyond context caching, \\nyou need to consider RAG and Grounding.\\nRAG overcomes limitations by enriching prompts with data retrieved from vector databases, \\ncircumventing pre-training requirements and ensuring the integration of up-to-date \\ninformation. Agent-based approaches, popularized by ReAct prompting, leverage LLMs as \\nmediators interacting with tools like RAG systems, APIs, and custom extensions. Vertex AI \\nfacilitates this dynamic information source selection, enabling complex queries, real-time \\nactions, and the creation of multi-agent systems connected to vast information networks for \\nsophisticated query processing and real-time decision-making.\\nVertex AI Grounding5 helps users connect large models with verifiable information by \\ngrounding them to internal data corpora on Vertex AI Agent Builder70 or external sources \\nusing Google Search. This enables two key functionalities: verifying model-generated outputs \\nagainst internal or external sources and creating RAG systems using Google’s advanced \\nsearch capabilities that produce quality content grounded in your own or web search data. \\nVertex AI extensions6 manage the whole lifecycle of tools for agents and foundation \\nmodels.  Instead of function calling where the developer invokes an API as recommended by \\nthe LLM, using Vertex AI extensions allows the model to do so automatically. Google offers \\na set of 1st party extensions like Vertex AI Search7 (RAG) and Code Interpreter,71 (sandbox). \\nApigee Cloud Hub allows you to automatically turn any API spec into an extension.', 'Operationalizing Generative AI on Vertex AI using MLOps\\n77\\nFebruary 2025\\nVertex AI Agent Builder70 is a suite of products that allows you to quickly build gen AI \\napplications and agents, streamlining setup of Google quality search on your data wherever \\nit is stored, providing RAG to agents and applications. Agent Builder is integrated with \\nConversational Agents (playbooks) which is a no-code, natural language agent building \\nproduct.  All agents, no matter how they are built, will require knowledge provided by RAG \\nand search.\\nVertex AI Search  is an out-of-the-box, fully managed search and RAG provider built on \\nGoogle Search technologies, for your data. With it you are be able to easily ground your \\nagents by indexing a diverse range of data sources, including structured datastores such \\nus BigQuery, Spanner, Cloud SQL, unstructured sources like website content crawling and \\ncloud storage as well as connectors to Google drive and dozens of third party APIs. Vertex AI \\nSearch brings together a robust foundation of Google Search technologies, encompassing \\nsemantic search, hybrid search, semantic chunking, embedded image & table annotations, \\nranking, algorithms, query rewriting and user intent understanding. Under the hood it \\noptimizes each of these steps, embedding models, and ranking strategies. It abstracts away \\nthese complexities and allows users to simply specify their data source to initiate the gen \\nAI-powered agent.This approach is ideal for organizations seeking to build robust search \\nexperiences for standard use cases without extensive technical expertise, and this product \\nimplements search ready for any scale.\\nRAG Engine is a companion product which can orchestrate any set of components using \\nLlamaIndex like syntax which should be familiar for many developers.  This can assemble \\na bespoke RAG pipeline to build your own search engine, and still utilize many of the same \\nunderlying components which Vertex AI Search uses like the Document AI Layout Parser \\nor the Ranking API or Check Grounding API.  This approach is much more flexible, but still \\nprovides the orchestration and runtime for a DIY RAG implementation on any components in \\nany configuration.', 'Operationalizing Generative AI on Vertex AI using MLOps\\n78\\nFebruary 2025\\nVector databases are specialized systems for managing multi-dimensional data. This data, \\nencompassing images, text, audio, video, and other structured or unstructured formats, \\nis represented as vectors capturing its semantic meaning. Vector databases accelerate \\nsearching and retrieval within these high-dimensional spaces, enabling efficient tasks like \\nfinding similar images from billions or extracting relevant text snippets based on various \\ninputs. For a deeper dive into these topics, refer to 4 and 19. Vertex AI offers three flexible \\nsolutions for storing and serving embeddings at scale, catering to diverse use cases and \\nuser profiles.\\nVertex AI Vector Search7 is a highly scalable low-latency similarity search and fully \\nmanaged vector database scaling to billions of vector embeddings with auto-scaling. This \\ntechnology, built upon ScaNN72 (a Google-developed technology used in products like \\nSearch, YouTube, and Play), allows you to search from billions of semantically similar or \\nrelated items within your stored data. In the context of gen AI, the most common use cases \\nwhere Vertex Vector Search can be used are:\\n1. Finding similar items (either text or image) based solely on their semantic meaning, in \\nconjunction with an embedding model.\\n2. Creating a hybrid search approach that combines semantic and keyword or metadata \\nsearch to refine the results.\\n3. Extracting relevant information from the database to feed into LLMs, enabling them to \\ngenerate more accurate and informed responses.\\nVertex AI Vector Search primarily functions as a vector database for storing pre-generated \\nembeddings. These embeddings must be created beforehand using separate models like \\nVertex Embedding models73 (namely textembedding-gecko, text-embedding-gecko-\\nmultilingual, or multimodalembedding). Choosing Vertex Vector Search is optimal \\nwhen you require control over aspects like the chunk, retrieval, query and models strategy.', 'Operationalizing Generative AI on Vertex AI using MLOps\\n79\\nFebruary 2025\\nThis includes fine-tuning an embedding model for your specific data. However, if your use \\ncase is a standard one requiring little customization, a readily available solution like Vertex \\nSearch might be a better choice.\\nVertex AI Feature Store74 is a centralized and fully managed repository for ML features \\nand embedding. It enables teams to share, serve, and reuse machine learning features and \\nembeddings effortlessly alongside other data. Its native BigQuery23 integration eliminates \\nduplication, simplifies lineage tracking and preserves data governance. Vertex AI Feature \\nStore supports offline retrieval and an easy and fast online serving for machine learning \\nfeatures and embeddings. Vertex AI Feature Store is a good choice when you want to iterate \\nand maintain different embedding versions alongside other machine learning features in a \\nsingle place.\\nVertex AI offers the flexibility to seamlessly create and connect various products to build \\nyour own custom grounding, RAG, and Agent systems. This includes utilizing diverse \\nembedding models (multimodal, multilingual), various vector stores (Vector Search, Feature \\nStore) and search engines like Vertex AI Agent Builder, extensions, grounding, and even SQL \\nquery generation for complex natural language queries. Moreover, Vertex AI provides SDK \\nintegration with LangChain9 to easily build and prototype applications using the umbrella \\nof Vertex AI products. For further details and integration information, consult the official \\ndocumentation75 and official examples.76\\nEvaluate: Vertex AI Experiments, Tensorboard, & \\nevaluation pipelines\\nIn the dynamic world of gen AI, experimentation and evaluation are the cornerstones of \\niterative development and continuous improvement. With a multitude of variables influencing \\nGen AI models (prompt engineering, model selection, data interaction, pretraining,', 'Operationalizing Generative AI on Vertex AI using MLOps\\n80\\nFebruary 2025\\nand tuning), evaluation goes hand-in-hand with experimentation. The more seamlessly \\nexperiments and evaluations can be integrated into the development process, the \\nsmoother and more efficient the overall development becomes. Vertex AI provides cohesive \\nexperimentation and evaluation products permitting connected iterations over applications \\nand models alongside their evaluations.\\nExperiment\\nThe process of selecting, creating, and customizing machine learning (including large \\nmodels) and its applications involves significant experimentation, collaboration, and iteration. \\nVertex AI seamlessly integrates experimentation and collaboration into the development \\nlifecycle of AI/ML and gen AI models and applications. Its Workbench Instances77 provide \\nJupyter-based development environments for the entire data science workflow, connected \\nto other Google Cloud services and with GitHub synchronization capabilities. Vertex Colab \\nEnterprise78 accelerates the AI workflow by enabling collaborative coding and leveraging \\ncode completion and generation features.\\nVertex AI also provides two tools for tracking and visualizing the output of many experiment \\ncycles and training runs. Vertex AI Experiments79 facilitates meticulous tracking and \\nanalysis of model architectures, hyperparameters, and training environments. It logs \\nexperiments, artifacts, and metrics, enabling comparison and reproducibility across multiple \\nruns. This comprehensive tracking permits data scientists to select the optimal model \\nand architecture for their specific use case. Vertex AI TensorBoard80 complements the \\nexperimentation process by providing detailed visualizations for tracking, visualizing, and \\nsharing ML experiments. It offers a range of visualizations, including loss and accuracy \\nmetrics tracking, model computational graph visualization, and weight and bias histograms,', 'Operationalizing Generative AI on Vertex AI using MLOps\\n81\\nFebruary 2025\\nwhich - for example - can be used for tracking various metrics pertaining to training and \\nevaluation of gen AI models with different prompting and tuning strategies. It also projects \\nembeddings to lower-dimensional space, and displays image, text, and audio samples.\\nEvaluation\\nVertex AI also provides a comprehensive set of evaluation tools for gen AI, from ground truth \\nmetrics to using LLMs as raters. \\nFor Ground Truth-based metrics, Automatic Metrics in Vertex AI81 lets you evaluate a model \\nbased on a defined task and “ground truth” dataset. For LLM-based evaluation, pairwise and \\npointwise model-based evaluation in Vertex AI82 uses a large model to evaluate the output \\nof multiple models or configurations being tested, helping to augment human evaluation at \\nscale. \\nIn addition to that, users can also leverage a set of pre-built metrics for evaluating gen AI \\napplications and relative SDK, integrated into the Vertex AI Python SDK for rapid and flexible, \\nnotebook-based, prototyping. To get started with Evaluation Vertex AI SDK see example in \\nthe official documentation.83\\nPredict: Vertex AI endpoints & monitoring\\nOnce developed, a production gen AI application must be deployed, including all its model \\ncomponents. If the application uses any models that have been trained or adapted, those \\nmodels need to be deployed to their own serving endpoints. You can serve any model in the \\nModel Garden through Vertex AI Endpoints21,which acts as the gateway for deploying your \\ntrained machine learning models. They allow you to serve online predictions with low latency,', 'Operationalizing Generative AI on Vertex AI using MLOps\\n82\\nFebruary 2025\\nmanage access controls, and monitor model performance easily through Model Monitoring. \\nEndpoints also offer scaling options to handle varying traffic demands, ensuring optimal user \\nexperience and reliability.\\nAlong with the prediction service, Vertex AI offers the following features for all Google \\nmanaged models:\\n• Citation checkers: Gen AI on Vertex performs Citation checks71. Citations are important \\nfor LLMs and gen AI for several reasons. Citing sources ensures proper acknowledgment \\nof sources and prevents plagiarism and demonstrates transparency and accountability. \\nCiting sources is essential for LLMs and gen AI also because they help identify, \\nunderstand potential biases, and enable reproducibility and verification. For example in \\nGoogle Cloud,84 the gen AI models are designed to produce original content, limiting the \\npossibility of copying existing contents. If this happens, Google Cloud provides quotes for \\nwebsites and code repositories. \\n• Safety scores: Safety attributes are crucial for LLMs and gen AI to mitigate potential \\nrisks like bias, lack of explainability, and misuse. These attributes help detect and mitigate \\nbiased outputs and mitigate misuse, enabling these tools to be used responsibly. As \\nLLMs and gen AI evolve, incorporating safety attributes will be increasingly essential for \\nresponsible and ethical use. For example, Google Cloud added safety scores in Vertex \\nAI PaLM API and Vertex AI Gemini API85: content processed through the API is checked \\nagainst a list of safety attributes, including \"harmful categories\" and sensitive topics. Each \\nattribute has a confidence score between 0.0 and 1.0, indicating the likelihood of the \\ninput belonging to that category. These safety filters can be used in conjunction with all \\nmodels: be it proprietary ones like Palm2 and Gemini or OSS ones like the ones available in \\nModel garden.', 'Operationalizing Generative AI on Vertex AI using MLOps\\n83\\nFebruary 2025\\n• Watermarking: With AI-based tools becoming increasingly popular for creation of \\ncontent, it’s very important to identify if an image has been created using AI. Vertex AI \\noffers digital watermarking and verification for AI-generated images86 using the algorithm \\nSynthID87 developed by Google DeepMind.\\n• Content moderation and bias detection: By using the Content moderation88 and Bias89 \\ndetection tools on Vertex AI, you can add an extra layer of security on the responses \\nof the LLMs to mitigate the risk that the model training and tuning may sway a model to \\ngenerate outputs that aren’t fair or appropriate for the task.\\nGovern: Vertex AI Feature Store, Model Registry, \\nand Dataplex\\nAddressing the multifaceted requirements of data and model lineage and governance in \\ngen AI requires a comprehensive strategy that tackles both conventional challenges and \\nnovel regulatory or technical complexities associated with large models. By adopting robust \\ngovernance, observability, and lineage practices in the development of gen AI solutions, \\norganizations can ensure comprehensive tracking, iteration, and evolution of data. They \\ncan also track the large models used, prompt adaptations, tuning, and other artifacts. This \\nfacilitates reproducibility of results, transparency and understanding of generated content \\nsources, troubleshooting, compliance enforcement, and enhanced reliability and security. \\nThese practices collectively enable the ethical and responsible development and deployment \\nof gen AI solutions. This fosters internal and external trust and fairness in gen AI models and \\npractices. Vertex AI and Google Cloud offer the following comprehensive suite of tools for \\nunified lineage, governance and monitoring, effectively addressing these critical concerns.\\nIn the context of governance and lineage, Vertex AI Feature Store74 offers:\\n• Track feature and embeddings versions and lineage, ensuring transparency', 'Operationalizing Generative AI on Vertex AI using MLOps\\n84\\nFebruary 2025\\n• Monitor feature (prompt) and embedding, response drift, and identify potential \\nissues proactively\\n• Store feature formulas and discover relevant features or embeddings for different \\nuse cases\\n• Utilize feature selection algorithms to optimize model performance\\n• Consolidate and unify all machine learning data within a singular repository encompassing \\nnumerical data, categorical data, textual data, and embeddings representations\\nVertex AI Model Registry12 serves as a centralized repository for comprehensive lifecycle \\nmanagement of both Google proprietary foundational and open-source Machine Learning \\nmodels. This includes gen AI models in addition to predictive models. This unified platform \\nenables registration, storage, and version control of diverse model types, including various \\niterations of tuning for large models. Vertex AI Model Registry seamlessly integrates with \\nVertex Pipelines,13 facilitating orchestration and management of training and tuning jobs \\nwhile leveraging lineage capabilities for recording and documenting the lineage from \\ndatasets to models and associated artifacts. It also couples with Vertex AI Experiments79 \\nand Vertex AI Model Evaluation,90 enabling performance monitoring and comparison of \\ndifferent model versions alongside their artifacts – all within a single interface. Furthermore, \\nVertex AI Model Registry bolsters observability by providing integrated configuration and \\naccess to Vertex AI Model Monitoring91 and logging functionalities. This enables proactive \\nidentification and mitigation of both training-serving skew and prediction drift, ensuring \\nreliability and accuracy of deployed models. Users can directly assign desired model versions \\nto endpoints for one-click deployment from Vertex Model Registry or leverage aliases for \\nsimplified deployment.\\nGoogle Cloud Dataplex14 provides an organization-wide lineage across product boundaries \\nin Google Cloud. Within the domains of AI and gen AI (and more broadly across data analytics \\nand AI/ML) Dataplex seamlessly integrates with BigQuery and Vertex AI. Dataplex facilitates', 'Operationalizing Generative AI on Vertex AI using MLOps\\n85\\nFebruary 2025\\nthe unification, management, discovery, and governance of both data and models. Through \\ncomprehensive data lineage, quality, and metadata management capabilities it provides \\nactionable insights for comprehensive data and model understanding. This promotes \\ncompliance, facilitates data analysis, and guarantees the training of machine learning \\nmodels on trusted data sources. This in turn leads to enhanced accuracy and reliability. This \\nintegration permits users across an organization to identify ‘champion models’ and ‘golden \\ndatasets and features’ across projects and regions in a secure way by adhering to identity \\naccess management (IAM)92 boundaries. In short, Dataplex encapsulates a framework within \\nan organization that governs the interaction between people, processes and technology \\nacross all the products in Google Cloud.\\nSummary\\nThe explosion of gen AI in the last several years introduced fundamental changes in the way \\nAI applications are developed – but far from upending the MLOps discipline, these changes \\nhave only reinforced its basic principles and processes. As we have seen, the principles of \\nMLOps that emphasize reliability, repeatability, and dependability in ML systems development \\nare comfortably extended to include the innovations of gen AI. Some of the necessary \\nchanges are deeper and more far-reaching than others, but nowhere do we find any change \\nthat MLOps cannot accommodate.\\nAs a result, many tools and processes built to support traditional MLOps can also support \\nthe requirements of gen AI. Vertex AI, for instance, is a powerful platform that can be used to \\nbuild and deploy machine learning models and AI applications. It provides a comprehensive \\nsuite of functions for developing both Predictive and gen AI systems, encompassing data \\npreparation, pre-trained APIs, AutoML capabilities, training and serving hardware,  advanced \\nfine-tuning techniques and deployment tools, and a diverse selection of proprietary and', 'Operationalizing Generative AI on Vertex AI using MLOps\\n86\\nFebruary 2025\\nopen-source foundation models. It also offers evaluation methods, monitoring capabilities, \\nand governance tools, all unified within a single platform to streamline the AI development \\nlifecycle. It’s built on Google Cloud Platform, which provides a scalable, reliable, secure and \\ncompliant infrastructure for machine learning. It’s a good choice for organizations that want \\nto build and deploy machine learning models and AI applications. \\nThe next few years will undoubtedly see gen AI extended in directions that today are \\nunimaginable. Regardless of the direction these developments take, it will continue to \\nbe important to build on solid engineering processes that embody the basic principles \\nof MLOps. These principles support the development of scalable, robust production AI \\napplications today, and no doubt will continue to do so into the future.', 'Operationalizing Generative AI on Vertex AI using MLOps\\n87\\nFebruary 2025\\nEndnotes\\n1. Model Garden on Vertex AI. Available at: https://cloud.google.com/model-garden\\n2. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, \\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, \\nJeff Dean, William Fedus. 2022. Emergent Abilities of Large Language Models. Available at: https://arxiv.org/\\npdf/2206.07682.pdf\\n3. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich \\nKüttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela. 2022. Retrieval-Augmented \\nGeneration for Knowledge-Intensive NLP Tasks. Available at: https://arxiv.org/pdf/2005.11401.pdf\\n4. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, Department of \\nComputer Science, Princeton University, Google Research, Brain team, REACT: SYNERGIZING REASONING AND \\nACTING IN LANGUAGE MODELS. Available at: https://arxiv.org/pdf/2210.03629.pdf\\n5. Grounding in Vertex AI. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/grounding/\\nground-language-models\\n6. Vertex Extensions. Connect models to APIs by using extensions. Available at: https://cloud.google.com/\\nvertex-ai/docs/generative-ai/extensions/overview\\n7. Overview of Vertex AI Vector Search. Available at: https://cloud.google.com/vertex-ai/docs/vector-search/\\noverview\\n8. What is Vertex AI Agent Builder? Available at: https://cloud.google.com/generative-ai-app-builder/docs/\\nintroduction\\n9. LangChain. Get your LLM application from prototype to production. Available at: https://www.langchain.\\ncom/\\n10. Introduction to the Vertex AI SDK for Python. Available at: https://cloud.google.com/vertex-ai/docs/python-\\nsdk/use-vertex-ai-python-sdk\\n11. Introduction to Vertex AI. Available at: https://cloud.google.com/vertex-ai/docs/start/introduction-unified-\\nplatform\\n12. Introduction to Vertex AI Model Registry. Available at: https://cloud.google.com/vertex-ai/docs/model-\\nregistry/introduction', 'Operationalizing Generative AI on Vertex AI using MLOps\\n88\\nFebruary 2025\\n13. Introduction to Vertex AI Pipelines. Available at: https://cloud.google.com/vertex-ai/docs/pipelines/\\nintroduction\\n14. Dataplex. Available at: https://cloud.google.com/dataplex\\n15. BigQuery. Available at: https://cloud.google.com/bigquery?hl=en\\n16. PaLi-Gemma model card. Available at: https://ai.google.dev/gemma/docs/paligemma/model-card\\n17. Version Control. Available at: https://en.wikipedia.org/wiki/Version_control\\n18. Continuous integration. Available at: https://wikipedia.org/wiki/Continuous_integration\\n19. TFX is an end-to-end platform for deploying production ML pipelines. Available at: https://www.tensorflow.\\norg/tfx\\n20. Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay \\nKrishna, Chen-Yu Lee, Tomas Pfister. 2023. Distilling Step-by-Step! Outperforming Larger Language Models with \\nLess Training Data and Smaller Model Sizes. Available at: https://arxiv.org/pdf/2305.02301.pdf\\n21. Vertex Endpoints. Use private endpoints for online prediction. Available at: https://cloud.google.com/vertex-\\nai/docs/predictions/using-private-endpoints\\n22. Tuan Duong Nguyen, Marthinus Christoffel du Plessis, Takafumi Kanamori, Masashi Sugiyama, 2014. \\nConstrained Least-Squares Density-Difference Estimation. Available at: https://www.ms.k.u-tokyo.ac.jp/\\nsugi/2014/CLSDD.pdf\\n23. Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Schölkopf, Alexander Smola, 2012. A Kernel \\nTwo-Sample Test. Available at: https://jmlr.csail.mit.edu/papers/v13/gretton12a.html\\n24. Oliver Cobb, Arnaud Van Looveren, 2022. Context-Aware Drift Detection. Available at: https://arxiv.org/\\npdf/2203.08644.pdf\\n25. Google Gemma Model. Available at: https://gemini.google.com/\\n26. Perform metrics-based evaluation. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/\\nmodels/evaluate-models\\n27. Gemini Team, Google, 2023. Gemini: A Family of Highly Capable Multimodal Models. Available at: https://\\nstorage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf\\n28. Anil, Dai et al., 2023. PaLM 2 Technical Report. Available at: https://arxiv.org/abs/2305.10403', 'Operationalizing Generative AI on Vertex AI using MLOps\\n89\\nFebruary 2025\\n29. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed \\nGhasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David \\nJ Fleet, Mohammad Norouzi, 2022. Photorealistic Text-to-Image Diffusion Models with Deep Language \\nUnderstanding. Available at: https://arxiv.org/abs/2205.11487\\n30. Build the future of AI with Meta Llama 3. Available at: https://llama.meta.com/llama3\\n31. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, \\nMostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun \\nChen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, \\nGaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff \\nDean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, Jason Wei. 2022. Scaling Instruction-Finetuned \\nLanguage Models. Available at: https://arxiv.org/abs/2210.11416\\n32. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 2018. BERT: Pre-training of Deep \\nBidirectional Transformers for Language Understanding. Available at: https://arxiv.org/abs/1810.04805\\n33. Stable Diffusion. Available at: https://github.com/CompVis/stable-diffusion\\n34. Vertex AI Function Calling. Available at: https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/\\nfunction-calling\\n35. Mistral AI. Available at: https://mistral.ai/\\n36. Models available in Model Garden. Available at: https://cloud.google.com/vertex-ai/docs/start/explore-\\nmodels#available-models\\n37. Vertex AI Studio. Customize and deploy generative models. Available at: https://cloud.google.com/\\ngenerative-ai-studio\\n38. vLLM. Easy, fast, and cheap LLM serving for everyone. Available at: https://github.com/vllm-project/vllm\\n39. Overview of multimodal models. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/\\nmultimodal/overview\\n40. Text models. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text\\n41. Imagen on Vertex AI | AI Image Generator. Available at: https://cloud.google.com/vertex-ai/docs/\\ngenerative-ai/image/overview\\n42. Code models overview. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-\\nmodels-overview', 'Operationalizing Generative AI on Vertex AI using MLOps\\n90\\nFebruary 2025\\n43. Convert speech to text. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/speech/\\nspeech-to-text\\n44. Text-to-Speech AI. Available at: https://cloud.google.com/text-to-speech\\n45. Natural Language AI. Available at: https://cloud.google.com/natural-language\\n46. Translate docs, audio, and videos in real time with Google AI. Available at: https://cloud.google.com/\\ntranslate\\n47. Vision AI. Available at: https://cloud.google.com/vision\\n48. Git. Available at: https://git-scm.com/\\n49. CodeGemma model card. Available at: https://ai.google.dev/gemma/docs/codegemma/model_card\\n50. TII’s Falcon. Available at: https://falconllm.tii.ae/\\n51. Mistral AI. Available at: https://mistral.ai/\\n52. Hugging Face, 2024. Vision Transformer (ViT) Documentation. Hugging Face, [online] Available at:   \\nhttps://huggingface.co/docs/transformers/en/model_doc/vit\\n53. Mingxing Tan, Quoc V. Le, 2019. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. \\nAvailable at: https://arxiv.org/abs/1905.11946\\n54. Anthropic Claude 3. Available at: https://www.anthropic.com/news/claude-3-haiku\\n55. Anthropic Claude 3 on Google Cloud Model Garden. Available at: https://cloud.google.com/blog/products/\\nai-machine-learning/announcing-anthropics-claude-3-models-in-google-cloud-vertex-ai\\n56. Vertex AI API. Available at: https://cloud.google.com/vertex-ai/docs/reference/rest\\n57. Vertex AI: Python SDK. Available at: https://cloud.google.com/python/docs/reference/aiplatform/latest/\\nvertexai\\n58. Vertex AI: Node.js Client. Available at: https://cloud.google.com/nodejs/docs/reference/aiplatform/latest/\\noverview\\n59. Vertex AI for Java. Available at: https://cloud.google.com/java/docs/reference/google-cloud-aiplatform/\\nlatest/overview\\n60. Customize and deploy generative models. Available at: https://cloud.google.com/generative-ai-studio', 'Operationalizing Generative AI on Vertex AI using MLOps\\n91\\nFebruary 2025\\n61. Design text prompts. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/text/text-\\nprompts\\n62. Introduction to prompt design. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/learn/\\nintroduction-prompt-design\\n63. Supervised tuning. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-\\nmodels#supervised-tuning\\n64. RLHF model tuning. Available at: https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-text-\\nmodels-rlhf\\n65. Vertex AI Distilation. Available at: https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-\\ntext-models\\n66. Create distilled text models. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/models/\\ndistill-text-models\\n67. Pipeline Basics. Available at: https://www.kubeflow.org/docs/components/pipelines/v2/pipelines/pipeline-\\nbasics/\\n68. Build a pipeline. Available at: https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline\\n69. Vertex AI Search extension. Available at: https://cloud.google.com/vertex-ai/generative-ai/docs/\\nextensions/vertex-ai-search\\n70. What is Vertex AI Agent Builder? Available at: https://cloud.google.com/generative-ai-app-builder/docs/\\nintroduction\\n71. Generative AI on Vertex AI, Citation Check. Available at: https://cloud.google.com/vertex-ai/generative-ai/\\ndocs/learn/overview#citation_check\\n72. Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar, 2020. \\nAccelerating Large-Scale Inference with Anisotropic Vector Quantization. Available at: https://arxiv.org/\\npdf/1908.10396.pdf\\n73. Get text embeddings. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/\\nget-text-embeddings\\n74. About Vertex AI Feature Store. Available at: https://cloud.google.com/vertex-ai/docs/featurestore/latest/\\noverview', 'Operationalizing Generative AI on Vertex AI using MLOps\\n92\\nFebruary 2025\\n75. Google Cloud Vertex AI. Available at: https://python.langchain.com/docs/integrations/llms/google_vertex_\\nai_palm\\n76. Generative AI - Language - LangChain. Available at: https://github.com/GoogleCloudPlatform/generative-\\nai/tree/main/language/orchestration/langchain\\n77. Introduction to Vertex AI Workbench, Workbench Instances. Available at: https://cloud.google.com/vertex-\\nai/docs/workbench/introduction\\n78. Introduction to Colab Enterprise. Available at: https://cloud.google.com/colab/docs/introduction\\n79. Introduction to Vertex AI Experiments. Available at: https://cloud.google.com/vertex-ai/docs/experiments/\\nintro-vertex-ai-experiments\\n80. Vertex AI TensorBoard Introduction to Vertex AI TensorBoard. Available at https://cloud.google.com/vertex-\\nai/docs/experiments/tensorboard-introduction\\n81. Perform metrics-based evaluation. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/\\nmodels/evaluate-models\\n82. Perform automatic side-by-side evaluation. Available at: https://cloud.google.com/vertex-ai/docs/\\ngenerative-ai/models/side-by-side-eval\\n83. Rapid Evaluation Vertex AI. Available at: https://cloud.google.com/vertex-ai/generative-ai/docs/models/\\nrapid-evaluation\\n84. Citation metadata. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-\\nai#citation_metadata\\n85. Responsible AI. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-\\nai#filters-palm-api\\n86. Imagen on Vertex AI | AI Image Generator. Available at: https://cloud.google.com/vertex-ai/docs/\\ngenerative-ai/image/overview\\n87. SynthID. Identifying AI-generated content with SynthID. Available at: https://deepmind.google/technologies/\\nsynthid/\\n88. Moderate text. Available at: https://cloud.google.com/natural-language/docs/moderating-text\\n89. Model bias metrics for Vertex AI. Available at: https://cloud.google.com/vertex-ai/docs/evaluation/model-\\nbias-metrics', 'Operationalizing Generative AI on Vertex AI using MLOps\\n93\\nFebruary 2025\\n90. Model evaluation in Vertex AI. Available at: https://cloud.google.com/vertex-ai/docs/evaluation/\\nintroduction\\n91. Introduction to Vertex AI Model Monitoring. Available at: https://cloud.google.com/vertex-ai/docs/model-\\nmonitoring/overview\\n92. Identity and Access Management (IAM). Available at: https://cloud.google.com/iam/docs\\n93. Agents. Available at: https://www.kaggle.com/whitepaper-agents', 'Prompt  \\nEngineering\\nAuthor: Lee Boonstra', 'Prompt Engineering\\nFebruary 2025\\n2\\nAcknowledgements\\nContent contributors\\nMichael Sherman\\nYuan Cao\\nErick Armbrust\\nAnant Nawalgaria\\nAntonio Gulli\\nSimone Cammel\\nCurators and Editors\\nAntonio Gulli\\nAnant Nawalgaria\\nGrace Mollison \\nTechnical Writer\\nJoey Haymaker\\nDesigner\\nMichael Lanning', 'Introduction 6\\nPrompt engineering 7\\nLLM output configuration 8\\nOutput length 8\\nSampling controls 9\\nTemperature 9\\nTop-K and top-P 10\\nPutting it all together 11\\nPrompting techniques 13\\nGeneral prompting / zero shot 13\\nOne-shot & few-shot 15\\nSystem, contextual and role prompting 18\\nSystem prompting 19\\nRole prompting 21\\nContextual prompting 23\\nTable of contents', 'Step-back prompting 25\\nChain of Thought (CoT) 29\\nSelf-consistency 32\\nTree of Thoughts (ToT) 36\\nReAct (reason & act) 37\\nAutomatic Prompt Engineering 40\\nCode prompting 42\\nPrompts for writing code 42\\nPrompts for explaining code 44\\nPrompts for translating code 46\\nPrompts for debugging and reviewing code 48\\nWhat about multimodal prompting? 54\\nBest Practices 54\\nProvide examples 54\\nDesign with simplicity 55\\nBe specific about the output 56\\nUse Instructions over Constraints 56\\nControl the max token length 58\\nUse variables in prompts 58\\nExperiment with input formats and writing styles 59\\nFor few-shot prompting with classification tasks, mix up the classes 59\\nAdapt to model updates 60\\nExperiment with output formats 60', 'JSON Repair 61\\nWorking with Schemas 62\\nExperiment together with other prompt engineers 63\\nCoT Best practices 64\\nDocument the various prompt attempts 64\\nSummary 66\\nEndnotes 68', 'Prompt Engineering\\nFebruary 2025\\n6\\nIntroduction\\nWhen thinking about a large language model input and output, a text prompt (sometimes \\naccompanied by other modalities such as image prompts) is the input the model uses \\nto predict a specific output. You don’t need to be a data scientist or a machine learning \\nengineer – everyone can write a prompt. However, crafting the most effective prompt can be \\ncomplicated. Many aspects of your prompt affect its efficacy: the model you use, the model’s \\ntraining data, the model configurations, your word-choice, style and tone, structure, and \\ncontext all matter. Therefore, prompt engineering is an iterative process. Inadequate prompts \\ncan lead to ambiguous, inaccurate responses, and can hinder the model’s ability to provide \\nmeaningful output.\\nYou don’t need to be a data \\nscientist or a machine learning \\nengineer – everyone can write \\na prompt.', 'Prompt Engineering\\nFebruary 2025\\n7\\nWhen you chat with the Gemini chatbot,1 you basically write prompts, however this \\nwhitepaper focuses on writing prompts for the Gemini model within Vertex AI or by using  \\nthe API, because by prompting the model directly you will have access to the configuration \\nsuch as temperature etc.\\nThis whitepaper discusses prompt engineering in detail. We will look into the various \\nprompting techniques to help you getting started and share tips and best practices to \\nbecome a prompting expert. We will also discuss some of the challenges you can face  \\nwhile crafting prompts.\\nPrompt engineering\\nRemember how an LLM works; it’s a prediction engine. The model takes sequential text as \\nan input and then predicts what the following token should be, based on the data it was \\ntrained on. The LLM is operationalized to do this over and over again, adding the previously \\npredicted token to the end of the sequential text for predicting the following token. The next \\ntoken prediction is based on the relationship between what’s in the previous tokens and what \\nthe LLM has seen during its training. \\nWhen you write a prompt, you are attempting to set up the LLM to predict the right sequence \\nof tokens. Prompt engineering is the process of designing high-quality prompts that guide \\nLLMs to produce accurate outputs. This process involves tinkering to find the best prompt, \\noptimizing prompt length, and evaluating a prompt’s writing style and structure in relation \\nto the task. In the context of natural language processing and LLMs, a prompt is an input \\nprovided to the model to generate a response or prediction.', 'Prompt Engineering\\nFebruary 2025\\n8\\nThese prompts can be used to achieve various kinds of understanding and generation \\ntasks such as text summarization, information extraction, question and answering, text \\nclassification, language or code translation, code generation, and code documentation \\nor reasoning.\\nPlease feel free to refer to Google’s prompting guides2,3 with simple and effective \\nprompting examples.\\nWhen prompt engineering, you will start by choosing a model. Prompts might need to be \\noptimized for your specific model, regardless of whether you use Gemini language models in \\nVertex AI, GPT, Claude, or an open source model like Gemma or LLaMA.\\nBesides the prompt, you will also need to tinker with the various configurations of a LLM.\\nLLM output configuration\\nOnce you choose your model you will need to figure out the model configuration. Most LLMs \\ncome with various configuration options that control the LLM’s output. Effective prompt \\nengineering requires setting these configurations optimally for your task.\\nOutput length\\nAn important configuration setting is the number of tokens to generate in a response. \\nGenerating more tokens requires more computation from the LLM, leading to higher energy \\nconsumption, potentially slower response times, and higher costs.', 'Prompt Engineering\\nFebruary 2025\\n9\\nReducing the output length of the LLM doesn’t cause the LLM to become more stylistically \\nor textually succinct in the output it creates, it just causes the LLM to stop predicting more \\ntokens once the limit is reached. If your needs require a short output length, you’ll also \\npossibly need to engineer your prompt to accommodate.\\nOutput length restriction is especially important for some LLM prompting techniques, like \\nReAct, where the LLM will keep emitting useless tokens after the response you want.\\nBe aware, generating more tokens requires more computation from the LLM, leading \\nto higher energy consumption and potentially slower response times, which leads to \\nhigher costs.\\nSampling controls\\nLLMs do not formally predict a single token. Rather, LLMs predict probabilities for what the \\nnext token could be, with each token in the LLM’s vocabulary getting a probability. Those \\ntoken probabilities are then sampled to determine what the next produced token will be. \\nTemperature, top-K, and top-P are the most common configuration settings that determine \\nhow predicted token probabilities are processed to choose a single output token.\\nTemperature\\nTemperature controls the degree of randomness in token selection. Lower temperatures \\nare good for prompts that expect a more deterministic response, while higher temperatures \\ncan lead to more diverse or unexpected results. A temperature of 0 (greedy decoding) is', 'Prompt Engineering\\nFebruary 2025\\n10\\ndeterministic: the highest probability token is always selected (though note that if two tokens \\nhave the same highest predicted probability, depending on how tiebreaking is implemented \\nyou may not always get the same output with temperature 0).\\nTemperatures close to the max tend to create more random output. And as temperature gets \\nhigher and higher, all tokens become equally likely to be the next predicted token.\\nThe Gemini temperature control can be understood in a similar way to the softmax function \\nused in machine learning. A low temperature setting mirrors a low softmax temperature (T), \\nemphasizing a single, preferred temperature with high certainty. A higher Gemini temperature \\nsetting is like a high softmax temperature, making a wider range of temperatures around \\nthe selected setting more acceptable. This increased uncertainty accommodates scenarios \\nwhere a rigid, precise temperature may not be essential like for example when experimenting \\nwith creative outputs.\\nTop-K and top-P\\nTop-K and top-P (also known as nucleus sampling)4 are two sampling settings used in LLMs \\nto restrict the predicted next token to come from tokens with the top predicted probabilities.  \\nLike temperature, these sampling settings control the randomness and diversity of \\ngenerated text.\\n• Top-K sampling selects the top K most likely tokens from the model’s predicted \\ndistribution. The higher top-K, the more creative and varied the model’s output; the \\nlower top-K, the more restive and factual the model’s output. A top-K of 1 is equivalent to \\ngreedy decoding.', 'Prompt Engineering\\nFebruary 2025\\n11\\n• Top-P sampling selects the top tokens whose cumulative probability does not exceed \\na certain value (P). Values for P range from 0 (greedy decoding) to 1 (all tokens in the \\nLLM’s vocabulary).\\nThe best way to choose between top-K and top-P is to experiment with both methods (or \\nboth together) and see which one produces the results you are looking for. \\nPutting it all together\\nChoosing between top-K, top-P, temperature, and the number of tokens to generate, \\ndepends on the specific application and desired outcome, and the settings all impact one \\nanother. It’s also important to make sure you understand how your chosen model combines \\nthe different sampling settings together.\\nIf temperature, top-K, and top-P are all available (as in Vertex Studio), tokens that meet \\nboth the top-K and top-P criteria are candidates for the next predicted token, and then \\ntemperature is applied to sample from the tokens that passed the top-K and top-P criteria. If \\nonly top-K or top-P is available, the behavior is the same but only the one top-K or P setting \\nis used. \\nIf temperature is not available, whatever tokens meet the top-K and/or top-P criteria are then \\nrandomly selected from to produce a single next predicted token.\\nAt extreme settings of one sampling configuration value, that one sampling setting either \\ncancels out other configuration settings or becomes irrelevant.', 'Prompt Engineering\\nFebruary 2025\\n12\\n• If you set temperature to 0, top-K and top-P become irrelevant–the most probable \\ntoken becomes the next token predicted. If you set temperature extremely high (above \\n1–generally into the 10s), temperature becomes irrelevant and whatever tokens make \\nit through the top-K and/or top-P criteria are then randomly sampled to choose a next \\npredicted token.\\n• If you set top-K to 1, temperature and top-P become irrelevant. Only one token passes the \\ntop-K criteria, and that token is the next predicted token. If you set top-K extremely high, \\nlike to the size of the LLM’s vocabulary, any token with a nonzero probability of being the \\nnext token will meet the top-K criteria and none are selected out.\\n• If you set top-P to 0 (or a very small value), most LLM sampling implementations will then \\nonly consider the most probable token to meet the top-P criteria, making temperature and \\ntop-K irrelevant. If you set top-P to 1, any token with a nonzero probability of being the \\nnext token will meet the top-P criteria, and none are selected out.\\nAs a general starting point, a temperature of .2, top-P of .95, and top-K of 30 will give you \\nrelatively coherent results that can be creative but not excessively so. If you want especially \\ncreative results, try starting with a temperature of .9, top-P of .99, and top-K of 40. And if you \\nwant less creative results, try starting with a temperature of .1, top-P of .9, and top-K of 20. \\nFinally, if your task always has a single correct answer (e.g., answering a math problem), start \\nwith a temperature of 0.\\nNOTE: With more freedom (higher temperature, top-K, top-P, and output tokens), the LLM \\nmight generate text that is less relevant.\\nWARNING: Have you ever seen a response ending with a large amount of filler words? This \\nis also known as the \"repetition loop bug\", which is a common issue in Large Language \\nModels where the model gets stuck in a cycle, repeatedly generating the same (filler) word, \\nphrase, or sentence structure, often exacerbated by inappropriate temperature and top-k/', 'Prompt Engineering\\nFebruary 2025\\n13\\ntop-p settings. This can occur at both low and high temperature settings, though for different \\nreasons. At low temperatures, the model becomes overly deterministic, sticking rigidly to the \\nhighest probability path, which can lead to a loop if that path revisits previously generated \\ntext.  Conversely, at high temperatures, the model\\'s output becomes excessively random, \\nincreasing the probability that a randomly chosen word or phrase will, by chance, lead back \\nto a prior state, creating a loop due to the vast number of available options.  In both cases, \\nthe model\\'s sampling process gets \"stuck,\" resulting in monotonous and unhelpful output \\nuntil the output window is filled. Solving this often requires careful tinkering with temperature \\nand top-k/top-p values to find the optimal balance between determinism and randomness.\\nPrompting techniques\\nLLMs are tuned to follow instructions and are trained on large amounts of data so they can \\nunderstand a prompt and generate an answer. But LLMs aren’t perfect; the clearer your \\nprompt text, the better it is for the LLM to predict the next likely text. Additionally, specific \\ntechniques that take advantage of how LLMs are trained and how LLMs work will help you get \\nthe relevant results from LLMs\\nNow that we understand what prompt engineering is and what it takes, let’s dive into some \\nexamples of the most important prompting techniques.\\nGeneral prompting / zero shot\\nA zero-shot5 prompt is the simplest type of prompt. It only provides a description of a task \\nand some text for the LLM to get started with. This input could be anything: a question, a \\nstart of a story, or instructions. The name zero-shot stands for ’no examples’.', 'Prompt Engineering\\nFebruary 2025\\n14\\nLet’s use Vertex AI Studio (for Language) in Vertex AI,6 which provides a playground to test \\nprompts. In Table 1, you will see an example zero-shot prompt to classify movie reviews. \\nThe table format as used below is a great way of documenting prompts. Your prompts will \\nlikely go through many iterations before they end up in a codebase, so it’s important to keep \\ntrack of your prompt engineering work in a disciplined, structured way. More on this table \\nformat, the importance of tracking prompt engineering work, and the prompt development \\nprocess is in the Best Practices section later in this chapter (“Document the various prompt \\nattempts”).  \\nThe model temperature should be set to a low number, since no creativity is needed, and we \\nuse the gemini-pro default top-K and top-P values, which effectively disable both settings \\n(see ‘LLM Output Configuration’ above). Pay attention to the generated output. The words \\ndisturbing and masterpiece should make the prediction a little more complicated, as both \\nwords are used in the same sentence.', 'Prompt Engineering\\nFebruary 2025\\n15\\nName 1_1_movie_classification\\nGoal Classify movie reviews as positive, neutral or negative.\\nModel gemini-pro\\nTemperature 0.1 Token Limit 5\\nTop-K N/A Top-P 1\\nPrompt Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE. \\nReview: \"Her\" is a disturbing study revealing the direction \\nhumanity is headed if AI is allowed to keep evolving, \\nunchecked. I wish there were more movies like this masterpiece. \\nSentiment:\\nOutput POSITIVE\\nTable 1. An example of zero-shot prompting\\nWhen zero-shot doesn’t work, you can provide demonstrations or examples in the prompt, \\nwhich leads to “one-shot” and “few-shot” prompting. General prompting / zero shot\\nOne-shot & few-shot\\nWhen creating prompts for AI models, it is helpful to provide examples. These examples can \\nhelp the model understand what you are asking for. Examples are especially useful when you \\nwant to steer the model to a certain output structure or pattern. \\nA one-shot prompt, provides a single example,  hence the name one-shot. The idea is the \\nmodel has an example it can imitate to best complete the task.\\nA few-shot prompt 7 provides multiple examples to the model. This approach shows the  \\nmodel a pattern that it needs to follow. The idea is similar to one-shot, but multiple examples \\nof the desired pattern increases the chance the model follows the pattern.', 'Prompt Engineering\\nFebruary 2025\\n16\\nThe number of examples you need for few-shot prompting depends on a few factors, \\nincluding the complexity of the task, the quality of the examples, and the capabilities of the \\ngenerative AI (gen AI) model you are using. As a general rule of thumb, you should use at \\nleast three to five examples for few-shot prompting. However, you may need to use more \\nexamples for more complex tasks, or you may need to use fewer due to the input length \\nlimitation of your model.\\nTable 2 shows a few-shot prompt example, let’s use the same gemini-pro model \\nconfiguration settings as before, other than increasing the token limit to accommodate the \\nneed for a longer response.\\nGoal Parse pizza orders to JSON\\nModel gemini-pro\\nTemperature 0.1 Token Limit 250\\nTop-K N/A Top-P 1\\nPrompt Parse a customer\\'s pizza order into valid JSON:\\nEXAMPLE:\\nI want a small pizza with cheese, tomato sauce, and pepperoni.\\nJSON Response:\\n```\\n{\\n\"size\": \"small\",\\n\"type\": \"normal\",\\n\"ingredients\": [[\"cheese\", \"tomato sauce\", \"peperoni\"]]\\n}\\n```\\nContinues next page...', 'Prompt Engineering\\nFebruary 2025\\n17\\nPrompt EXAMPLE:\\nCan I get a large pizza with tomato sauce, basil and mozzarella \\n{\\n\"size\": \"large\",\\n\"type\": \"normal\",\\n\"ingredients\": [[\"tomato sauce\", \"bazel\", \"mozzarella\"]]\\n}\\nNow, I would like a large pizza, with the first half cheese and \\nmozzarella. And the other tomato sauce, ham and pineapple.\\nJSON Response:\\nOutput ```\\n{\\n\"size\": \"large\",\\n\"type\": \"half-half\",\\n\"ingredients\": [[\"cheese\", \"mozzarella\"], [\"tomato sauce\", \\n\"ham\", \"pineapple\"]]\\n}\\n```\\nTable 2. An example of few-shot prompting\\nWhen you choose examples for your prompt, use examples that are relevant to the task you \\nwant to perform. The examples should be diverse, of high quality, and well written. One small \\nmistake can confuse the model and will result in undesired output.\\nIf you are trying to generate output that is robust to a variety of inputs, then it is important to \\ninclude edge cases in your examples. Edge cases are inputs that are unusual or unexpected, \\nbut that the model should still be able to handle.', 'Prompt Engineering\\nFebruary 2025\\n18\\nSystem, contextual and role prompting\\nSystem, contextual and role prompting are all techniques used to guide how LLMs generate \\ntext, but they focus on different aspects:\\n• System prompting sets the overall context and purpose for the language model. It \\ndefines the ‘big picture’ of what the model should be doing, like translating a language, \\nclassifying a review etc.\\n• Contextual prompting provides specific details or background information relevant to \\nthe current conversation or task. It helps the model to understand the nuances of what’s \\nbeing asked and tailor the response accordingly.\\n• Role prompting assigns a specific character or identity for the language model to adopt. \\nThis helps the model generate responses that are consistent with the assigned role and its \\nassociated knowledge and behavior.\\nThere can be considerable overlap between system, contextual, and role prompting. E.g. a \\nprompt that assigns a role to the system, can also have a context.\\nHowever, each type of prompt serves a slightly different primary purpose:\\n• System prompt: Defines the model’s fundamental capabilities and overarching purpose.\\n• Contextual prompt: Provides immediate, task-specific information to guide the response. \\nIt’s highly specific to the current task or input, which is dynamic.\\n• Role prompt: Frames the model’s output style and voice. It adds a layer of specificity \\nand personality.', 'Prompt Engineering\\nFebruary 2025\\n19\\nDistinguishing between system, contextual, and role prompts provides a framework for \\ndesigning prompts with clear intent, allowing for flexible combinations and making it easier to \\nanalyze how each prompt type influences the language model’s output.\\nLet’s dive into these three different kinds of prompts.\\nSystem prompting\\nTable 3 contains a system prompt, where I specify additional information on how to return the \\noutput. I increased the temperature to get a higher creativity level, and I specified a higher \\ntoken limit. However, because of my clear instruction on how to return the output the model \\ndidn’t return extra text.\\nGoal Classify movie reviews as positive, neutral or negative.\\nModel gemini-pro\\nTemperature 1 Token Limit 5\\nTop-K 40 Top-P 0.8\\nPrompt Classify movie reviews as positive, neutral or negative. Only \\nreturn the label in uppercase.\\nReview: \"Her\" is a disturbing study revealing the direction \\nhumanity is headed if AI is allowed to keep evolving, \\nunchecked. It\\'s so disturbing I couldn\\'t watch it.\\nSentiment:\\nOutput NEGATIVE\\nTable 3. An example of system prompting\\nSystem prompts can be useful for generating output that meets specific requirements. The \\nname ‘system prompt’ actually stands for ‘providing an additional task to the system’. For \\nexample, you could use a system prompt to generate a code snippet that is compatible with \\na specific programming language, or you could use a system prompt to return a certain \\nstructure. Have a look into Table 4, where I return the output in JSON format.', 'Prompt Engineering\\nFebruary 2025\\n20\\nGoal Classify movie reviews as positive, neutral or negative, return JSON.\\nModel gemini-pro\\nTemperature 1 Token Limit 1024\\nTop-K 40 Top-P 0.8\\nPrompt Classify movie reviews as positive, neutral or negative. Return \\nvalid JSON:\\nReview: \"Her\" is a disturbing study revealing the direction \\nhumanity is headed if AI is allowed to keep evolving, \\nunchecked. It\\'s so disturbing I couldn\\'t watch it.\\nSchema:\\n```\\nMOVIE:\\n{\\n\"sentiment\": String \"POSITIVE\" | \"NEGATIVE\" | \"NEUTRAL\",\\n\"name\": String\\n}\\nMOVIE REVIEWS:\\n{\\n\"movie_reviews\": [MOVIE]\\n}\\n```\\nJSON Response:\\nOutput ```\\n{\\n  \"movie_reviews\": [\\n    {\\n      \"sentiment\": \"NEGATIVE\",\\n      \"name\": \"Her\"\\n    }\\n  ]\\n}\\n```\\nTable 4. An example of system prompting with JSON format', 'Prompt Engineering\\nFebruary 2025\\n21\\nThere are some benefits in returning JSON objects from a prompt that extracts data. In \\na real-world application I don’t need to manually create this JSON format, I can already \\nreturn the data in a sorted order (very handy when working with datetime objects), but most \\nimportantly, by prompting for a JSON format it forces the model to create a structure and \\nlimit hallucinations.\\nSystem prompts can also be really useful for safety and toxicity. To control the output, simply \\nadd an additional line to your prompt like: ‘You should be respectful in your answer.’.\\nRole prompting\\nRole prompting is a technique in prompt engineering that involves assigning a specific role to \\nthe gen AI model. This can help the model to generate more relevant and informative output, \\nas the model can craft its responses to the specific role that it has been assigned.\\nFor example, you could role prompt a gen AI model to be a book editor, a kindergarten \\nteacher, or a motivational speaker. Once the model has been assigned a role, you can then \\ngive it prompts that are specific to that role. For example, you could prompt a teacher to \\ncreate a lesson plan that they can then review. Have a look into Table 5, where the model \\ntakes the role of a travel guide.', 'Prompt Engineering\\nFebruary 2025\\n22\\nGoal Act as travel guide and provide 3 travel suggestions\\nModel gemini-pro\\nTemperature 1 Token Limit 1024\\nTop-K 40 Top-P 0.8\\nPrompt I want you to act as a travel guide. I will write to you \\nabout my location and you will suggest 3 places to visit near \\nme. In some cases, I will also give you the type of places I \\nwill visit.\\nMy suggestion: \"I am in Amsterdam and I want to visit \\nonly museums.\"\\nTravel Suggestions:\\nOutput 1. Rijksmuseum: World-renowned museum, known for Dutch masterpieces \\nlike Rembrandt’s “The Night Watch.” Don’t miss the incredible collection of \\npaintings and artifacts showcasing Dutch art and history.\\n2. Van Gogh Museum: Immerse yourself in the captivating works of Vincent \\nvan Gogh. The museum houses the largest collection of his paintings and \\ndrawings, including “The Starry Night” and “Sunflowers.”\\n3. Stedelijk Museum Amsterdam: Discover modern and contemporary art \\nfrom around the world. The museum’s collection includes works by Picasso, \\nKandinsky, and Rietveld, housed in a striking modern building.\\nTable 5. An example of role prompting\\nThe above example shows an example of taking the role of a travel agent. When you \\nwould change the role to a teacher of geography, you would notice that you will receive a \\ndifferent response.\\nDefining a role perspective for an AI model gives it a blueprint of the tone, style, and focused \\nexpertise you’re looking for to improve the quality, relevance, and effectiveness of your \\noutput.', 'Prompt Engineering\\nFebruary 2025\\n23\\nHere are some styles you can choose from which I find effective:\\nConfrontational, Descriptive, Direct, Formal, Humorous, Influential, Informal, \\nInspirational, Persuasive\\nLet’s change our prompt in Table 6 to include a humorous and inspirational style.\\nPrompt I want you to act as a travel guide. I will write to you about \\nmy location and you will suggest 3 places to visit near me in \\na humorous style.\\nMy suggestion: \"I am in Manhattan.\"\\nTravel Suggestions:\\nOutput 1. Behold the Empire State of Mind: Ascend to the dizzying heights of the Empire \\nState Building and bask in the glory of Manhattan\\'s skyline. Prepare to feel like \\nKing Kong atop the Big Apple, minus the giant ape-sized banana.\\n2. Get Artsy-Fartsy at MoMA: Unleash your inner art aficionado at the Museum \\nof Modern Art (MoMA). Gaze upon masterpieces that will boggle your \\nmind and make you question whether your stick-figure drawings have any \\nartistic merit.\\n3. Shop ‘Til You Drop on Fifth Avenue: Indulge in some retail therapy on the \\niconic Fifth Avenue. Brace yourself for sticker shock as you window-shop (or \\nactually shop) at designer boutiques that will make your wallet cry. But hey, \\nyou’re in Manhattan, so you might as well embrace the fabulousness!\\nTable 6. An example of role prompting with a humorous tone and style\\nContextual prompting\\nBy providing contextual prompts, you can help ensure that your AI interactions are as \\nseamless and efficient as possible. The model will be able to more quickly understand your \\nrequest and be able to generate more accurate and relevant responses, as you can see in the \\nexample of Table 7.', \"Prompt Engineering\\nFebruary 2025\\n24\\nGoal Suggest articles for a blog about retro games\\nModel gemini-pro\\nTemperature 1 Token Limit 1024\\nTop-K 40 Top-P 0.8\\nPrompt Context: You are writing for a blog about retro 80's arcade \\nvideo games.\\nSuggest 3 topics to write an article about with a few lines of \\ndescription of what this article should contain.\\nOutput 1. **The Evolution of Arcade Cabinet Design:** \\nThis article would explore the evolution of arcade cabinet designs, from the \\nearly wood and metal cabinets of the 1970s to the sleek, neon-lit designs of \\nthe 1980s and beyond.\\n2. **Blast From The Past: Iconic Arcade Games of The 80’s:** \\n- This article would feature a glimpse into the history, impact, and legacy \\nof some of the most iconic and influential arcade games of the 1980s, \\nhighlighting their innovations, popular mechanics, and enduring charm.\\n3. **The Rise and Retro Revival of Pixel Art:** \\n- This article would delve into the evolution of pixel art as a defining visual \\nstyle of early arcade games, tracing its roots, exploring its techniques, and \\ndiscussing the resurgence of pixel art in modern games and digital art.\\nTable 7. An example of contextual prompting\", 'Prompt Engineering\\nFebruary 2025\\n25\\nStep-back prompting\\nStep-back8 prompting is a technique for improving the performance by prompting the LLM \\nto first consider a general question related to the specific task at hand, and then feeding the \\nanswer to that general question into a subsequent prompt for the specific task. This ‘step \\nback’ allows the LLM to activate relevant background knowledge and reasoning processes \\nbefore attempting to solve the specific problem.\\nBy considering the broader and underlying principles, LLMs can generate more accurate and \\ninsightful responses. Step-back prompting encourages LLMs to think critically and apply their \\nknowledge in new and creative ways. It changes the final prompt doing the task by utilizing \\nmore knowledge in the LLM’s parameters than would otherwise come into play when the LLM \\nis prompted directly.\\nIt can help to mitigate biases in LLM responses, by focusing on general principles instead of \\nspecific details, step-back prompting.\\nLet’s have a look into these examples to understand how step-back prompting can improve \\nthe results. Let’s first review a traditional prompt (Table 8) before we compare it to a step \\nback prompt (Table 9):', 'Prompt Engineering\\nFebruary 2025\\n26\\nGoal Write a storyline for a level of a first-person shooter video game.\\nModel gemini-pro\\nTemperature 1 Token Limit 1024\\nTop-K 40 Top-P 0.8\\nPrompt Write a one paragraph storyline for a new level of a first-\\nperson shooter video game that is challenging and engaging.\\nOutput The level begins with the player’s squad being ambushed in a dense urban area \\nby a relentless enemy faction. Players must fight their way through a maze of \\nback alleys and dilapidated buildings, utilizing stealth and precision to eliminate \\nenemy forces and avoid detection. The player must use their environment to their \\nadvantage, setting traps, utilizing cover, and making calculated moves to survive. \\nAlong the way, they uncover intel suggesting the enemy is planning a devastating \\nattack, and the player must race against time to gather evidence and relay it back \\nto their headquarters before the enemy can carry out their plans.\\nTable 8. A traditional prompt before we compare it with a step back prompt\\nWhen you set the temperature to 1, you might end up with all kinds of creative writings for a \\nstoryline, but it’s also quite random and generic. So let’s take a step back:', 'Prompt Engineering\\nFebruary 2025\\n27\\nPrompt Based on popular first-person shooter action games, what are \\n5 fictional key settings that contribute to a challenging and \\nengaging level storyline in a first-person shooter video game?\\nOutput 1. **Abandoned Military Base**: A sprawling, post-apocalyptic military complex \\ncrawling with mutated soldiers and rogue robots, ideal for challenging \\nfirearm combat.\\n2. **Cyberpunk City**: A neon-lit, futuristic urban environment with towering \\nskyscrapers and dense alleyways, featuring cybernetically enhanced enemies \\nand hacking mechanics.\\n3. **Alien Spaceship**: A vast alien vessel stranded on Earth, with eerie \\ncorridors, zero-gravity sections, and extraterrestrial creatures to encounter.\\n4. **Zombie-Infested Town**: A desolate town overrun by hordes of aggressive \\nzombies, featuring intense close-quarters combat and puzzle-solving to find \\nsafe passage.\\n5. **Underwater Research Facility**: A deep-sea laboratory flooded with water, \\nfilled with mutated aquatic creatures, and requiring stealth and underwater \\nexploration skills to survive.\\nTable 9. An example of prompting for self consistency\\nYeah those topics seem like a good fit for a first-person video game. Let’s go back to the \\noriginal prompt, but this time we include the answer of the step back as context and see what \\nit will return.', 'Prompt Engineering\\nFebruary 2025\\n28\\nPrompt Context: 5 engaging themes for a first person shooter video game:\\n1. **Abandoned Military Base**: A sprawling, post-apocalyptic \\nmilitary complex crawling with mutated soldiers and rogue \\nrobots, ideal for challenging firearm combat.\\n2. **Cyberpunk City**: A neon-lit, futuristic urban environment \\nwith towering skyscrapers and dense alleyways, featuring \\ncybernetically enhanced enemies and hacking mechanics.\\n3. **Alien Spaceship**: A vast alien vessel stranded on \\nEarth, with eerie corridors, zero-gravity sections, and \\nextraterrestrial creatures to encounter.\\n4. **Zombie-Infested Town**: A desolate town overrun by hordes of \\naggressive zombies, featuring intense close-quarters combat and \\npuzzle-solving to find safe passage.\\n5. **Underwater Research Facility**: A deep-sea laboratory flooded \\nwith water, filled with mutated aquatic creatures, and requiring \\nstealth and underwater exploration skills to survive.\\nTake one of the themes and write a one paragraph storyline \\nfor a new level of a first-person shooter video game that is \\nchallenging and engaging.\\nOutput In the heart of a murky abyss, lies a dilapidated underwater research facility, \\nstanding as a testament to human ambition and its disastrous consequences. \\nShrouded in darkness, pulsating with the hum of malfunctioning machinery, and \\nhaunted by the echoes of drowned souls, the facility is now a breeding ground for \\na nightmare borne from the deep sea. The player, an elite marine equipped with \\nadvanced diving gear and experimental weaponry, is tasked with venturing into \\nthis aquatic realm of terror. Armed with resilience and resourcefulness, they must \\nnavigate treacherous corridors, solve cryptic puzzles, and confront gruesome sea \\nmonstrosities that lurk in the depths. From grappling with the crushing pressure \\nof the deep sea to outsmarting cunning aquatic predators, every moment in this \\nuncharted underworld tests the limits of human endurance and courage.\\nTable 10. An example of prompting for self consistency\\nThat looks like an interesting video game! By using step back prompting techniques you can \\nincrease the accuracy of your prompts.', 'Prompt Engineering\\nFebruary 2025\\n29\\nChain of Thought (CoT)\\nChain of Thought (CoT) 9 prompting is a technique for improving the reasoning capabilities \\nof LLMs by generating intermediate reasoning steps. This helps the LLM generate more \\naccurate answers. You can combine it with few-shot prompting to get better results on more \\ncomplex tasks that require reasoning before responding as it’s a challenge with a zero-shot \\nchain of thought.\\nCoT has a lot of advantages. First of all, it’s low-effort while being very effective and works \\nwell with off-the-shelf LLMs (so no need to finetune). You also get interpretability with CoT \\nprompting, as you can learn from the LLM’s responses and see the reasoning steps that were \\nfollowed. If there’s a malfunction, you will be able to identify it. Chain of thought appears \\nto improve robustness when moving between different LLM versions. Which means the \\nperformance of your prompt should drift less between different LLMs than if your prompt \\ndoes not use reasoning chains. Of course there are also disadvantages, but they are \\nsomewhat intuitive.\\nThe LLM response includes the chain of thought reasoning, which means more output \\ntokens, which means predictions cost more money and take longer.\\nTo explain the following example in Table 11, let’s first try to create a prompt that is not using \\nCoT prompting to showcase the flaws of a large language model.\\nPrompt When I was 3 years old, my partner was 3 times my age. Now, I \\nam 20 years old. How old is my partner?\\nOutput 63 years old\\nTable 11. An example of a prompt which is trying to solve a mathematical problem', \"Prompt Engineering\\nFebruary 2025\\n30\\nYikes. That’s obviously the wrong answer. As a matter of fact, LLMs often struggle with \\nmathematical tasks and can provide incorrect answers – even for a task as simple as \\nmultiplying two numbers. This is because they are trained on large volumes of text and math \\nmay require a different approach. So let’s see if intermediate reasoning steps will improve \\nthe output.\\nPrompt When I was 3 years old, my partner was 3 times my age. Now, \\nI am 20 years old. How old is my partner? Let's think step \\nby step.\\nOutput 1. Let’s assume my current age is “x” years.\\n2. When I was 3 years old, my age was 3 years.\\n3. My partner’s age at that time was 3 times my age, which means my partner’s \\nage was 3 * 3 = 9 years.\\n4. Now, I am 20 years old, which means my age has increased by 20 - 3 = 17 \\nyears since I was 3 years old.\\n5. Since my partner’s age was 9 years when I was 3 years old, and my age has \\nincreased by 17 years, my partner’s age must have also increased by 17 years.\\n6. Therefore, my partner’s current age is 9 + 17 = 26 years.\\nSo, my partner is 26 years old.\\nTable 12. An example of Chain of Thought prompting\\nAlright, now the final answer is correct. That’s because we clearly instructed the LLM to \\nexplain each step instead of just returning an answer. It’s interesting to see that the model \\nadds up the 17 increased years. In my head I would take the difference of years between my \\npartner and me and add those up. (20+(9-3)). Let’s help the model to think a little bit more \\nlike me.\", \"Prompt Engineering\\nFebruary 2025\\n31\\nTable 12 is an example of ‘zero-shot’ Chain of thought. Chain of thought prompting can be \\nvery powerful when combined with a single-shot or few-shot, as what you can see in Table 13:\\nPrompt Q: When my brother was 2 years old, I was double his age. Now \\nI am 40 years old. How old is my brother? Let's think step \\nby step.\\nA: When my brother was 2 years, I was 2 * 2 = 4 years old. \\nThat's an age difference of 2 years and I am older. Now I am 40 \\nyears old, so my brother is 40 - 2  = 38 years old. The answer \\nis 38.\\nQ: When I was 3 years old, my partner was 3 times my age. Now, \\nI am 20 years old. How old is my partner? Let's think step \\nby step.\\nA:\\nOutput When I was 3 years old, my partner was 3 * 3 = 9 years old. That’s an age \\ndifference of 6 years and my partner is older. Now I am 20 years old, so my \\npartner is 20 + 6 = 26 years old. The answer is 26.\\nTable 13. An example of chain of thought prompting with a single-shot\\nChain of thought can be useful for various use-cases. Think of code generation, for breaking \\ndown the request into a few steps, and mapping those to specific lines of code. Or for \\ncreating synthetic data when you have some kind of seed like “The product is called XYZ, \\nwrite a description guiding the model through the assumptions you would make based on \\nthe product given title.” Generally, any task that can be solved by ‘talking through is a good \\ncandidate for a chain of thought. If you can explain the steps to solve the problem, try chain \\nof thought.\\nPlease refer to the notebook10 hosted in the GoogleCloudPlatform Github repository which \\nwill go into further detail on CoT prompting:\", 'Prompt Engineering\\nFebruary 2025\\n32\\nIn the best practices section of this chapter, we will learn some best practices specific to \\nChain of thought prompting.\\nSelf-consistency\\nWhile large language models have shown impressive success in various NLP tasks, their \\nability to reason is often seen as a limitation that cannot be overcome solely by increasing \\nmodel size. As we learned in the previous Chain of Thought prompting section, the model can \\nbe prompted to generate reasoning steps like a human solving a problem. However CoT uses \\na simple ‘greedy decoding’ strategy, limiting its effectiveness. Self-consistency11 combines \\nsampling and majority voting to generate diverse reasoning paths and select the most \\nconsistent answer. It improves the accuracy and coherence of responses generated by LLMs.\\nSelf-consistency gives a pseudo-probability likelihood of an answer being correct, but \\nobviously has high costs.\\nIt follows the following steps:\\n1. Generating diverse reasoning paths: The LLM is provided with the same prompt multiple \\ntimes. A high temperature setting encourages the model to generate different reasoning \\npaths and perspectives on the problem.\\n2. Extract the answer from each generated response.\\n3. Choose the most common answer.\\nLet’s look into an example of an email classification system, which classifies an email as \\nIMPORTANT or NOT IMPORTANT. A zero-shot chain of thought prompt will be sent to the LLM \\nmultiple times, to see if the responses differ after each submit. Notice the friendly tone, the \\nword choice and the sarcasm that’s been used in the email. All this could trick the LLM.', \"Prompt Engineering\\nFebruary 2025\\n33\\nPrompt EMAIL:\\n```\\nHi,\\nI have seen you use Wordpress for your website. A great open \\nsource content management system. I have used it in the past \\ntoo. It comes with lots of great user plugins. And it's pretty \\neasy to set up.\\nI did notice a bug in the contact form, which happens when \\nyou select the name field. See the attached screenshot of me \\nentering text in the name field. Notice the JavaScript alert \\nbox that I inv0k3d.\\nBut for the rest it's a great website. I enjoy reading it. Feel \\nfree to leave the bug in the website, because it gives me more \\ninteresting things to read.\\nCheers,\\nHarry the Hacker.\\n```\\nClassify the above email as IMPORTANT or NOT IMPORTANT. Let's \\nthink step by step and explain why.\\nContinues next page...\", 'Prompt Engineering\\nFebruary 2025\\n34\\nOutput \\nAttempt 1\\n**Step 1: Identify the purpose of the email.**\\nThe purpose of the email is to inform the recipient of a bug in the contact form \\non their website. The sender also mentions that they enjoy reading the website \\nand that they are leaving the bug in place for the time being.\\n**Step 2: Assess the potential impact of the bug.**\\nThe bug in the contact form could allow an attacker to execute arbitrary \\nJavaScript code on the website. This could lead to the attacker being able to take \\ncontrol of the website, steal sensitive information, or deface the website.\\n**Step 3: Consider the credibility of the sender.**\\nThe sender of the email claims to be a hacker named Harry. However, there is no \\nway to verify the sender’s identity. It is possible that the sender is a legitimate \\nhacker who is trying to help the recipient, but it is also possible that the sender \\nis a malicious actor who is trying to trick the recipient into clicking on a malicious \\nlink or downloading a malicious file.\\n**Conclusion: IMPORTANT**\\nBased on the potential impact of the bug and the credibility of the sender, \\nthe email should be classified as **IMPORTANT**. The recipient should take \\nimmediate action to fix the bug and protect their website from attack.\\nOutput \\nAttempt 2\\n**Explanation:**\\n1. **Lack of urgency:** The email does not convey any sense of urgency or \\ntime-sensitivity. It starts with a casual greeting and presents the information \\nin a matter-of-fact tone.\\n2. **Non-critical bug report:** The bug reported in the contact form is \\ndescribed as interesting but not critical. The email sender even suggests that \\nthe website owner can leave the bug in place.\\n3. **Lack of personal impact:** The email does not indicate any direct or \\npersonal impact on the website owner. It simply mentions that the sender \\nenjoys reading the website.\\nContinues next page...', 'Prompt Engineering\\nFebruary 2025\\n35\\nOutput \\nAttempt 2\\n4. **Absence of action request:** The email does not explicitly request any \\naction from the website owner. It does not ask for the bug to be fixed or \\nprovide any specific suggestions for improvement.\\n5. **Sender’s intent:** The sender’s intention seems to be more focused on \\nsharing an observation and expressing interest in the website rather than \\nraising a serious concern or requesting assistance.\\n**Conclusion: NOT IMPORTANT**\\nOutput \\nAttempt 3\\n**Step 1: Identify the purpose of the email.**\\nThe purpose of the email is to inform the recipient of a bug in the contact form on \\ntheir website. The sender also mentions that they enjoy reading the website and \\nthat they are leaving the bug in place for the time being.\\n**Step 2: Assess the potential impact of the bug.**\\nThe bug in the contact form could allow an attacker to execute arbitrary \\nJavaScript code on the website. This could lead to the attacker being able to take \\ncontrol of the website, steal sensitive information, or deface the website.\\n**Step 3: Consider the credibility of the sender.**\\nThe sender of the email claims to be a hacker named Harry. However, there is no \\nway to verify the sender’s identity. It is possible that the sender is a legitimate \\nhacker who is trying to help the recipient, but it is also possible that the sender is \\na malicious actor who is trying to exploit the bug for their own gain.\\n**Conclusion:**\\nBased on the above factors, the email should be classified as **IMPORTANT**. \\nThe bug in the contact form is a serious security risk, and the sender’s credibility \\nis unknown. The recipient should take immediate action to fix the bug and \\ninvestigate the sender’s claims.\\nTable 14. An example of prompting for self consistency\\nYou can use the above prompt and try to see if it returns a consistent classification. \\nDepending on the model that you are using and the temperature configuration, it might \\nreturn “IMPORTANT” or “NOT IMPORTANT”.', 'Prompt Engineering\\nFebruary 2025\\n36\\nBy generating many Chains of Thoughts, and taking the most commonly occurring answer \\n(“IMPORTANT”), we can get a more consistently correct answer from the LLM.\\nThis example shows how self-consistency prompting can be used to improve the accuracy \\nof an LLM’s response by considering multiple perspectives and selecting the most \\nconsistent answer.\\nTree of Thoughts (ToT)\\nNow that we are familiar with chain of thought and self-consistency prompting, let’s review \\nTree of Thoughts (ToT).12 It generalizes the concept of CoT prompting because it allows LLMs \\nto explore multiple different reasoning paths simultaneously, rather than just following a \\nsingle linear chain of thought. This is depicted in Figure 1.\\nFigure 1. A visualization of chain of thought prompting on the left versus. Tree of Thoughts prompting on \\nthe right', 'Prompt Engineering\\nFebruary 2025\\n37\\nThis approach makes ToT particularly well-suited for complex tasks that require exploration. It \\nworks by maintaining a tree of thoughts, where each thought represents a coherent language \\nsequence that serves as an intermediate step toward solving a problem. The model can then \\nexplore different reasoning paths by branching out from different nodes in the tree. \\nThere’s a great notebook, which goes into a bit more detail showing The Tree of Thought \\n(ToT) which is based on the paper ‘Large Language Model Guided Tree-of-Thought’.9\\nReAct (reason & act)\\nReason and act (ReAct) [10]13 prompting is a paradigm for enabling LLMs to solve complex \\ntasks using natural language reasoning combined with external tools (search, code \\ninterpreter etc.) allowing the LLM to perform certain actions, such as interacting with external \\nAPIs to retrieve information which is a first step towards agent modeling.\\nReAct mimics how humans operate in the real world, as we reason verbally and can \\ntake actions to gain information. ReAct performs well against other prompt engineering \\napproaches in a variety of domains.\\nReAct prompting works by combining reasoning and acting into a thought-action loop. The \\nLLM first reasons about the problem and generates a plan of action. It then performs the \\nactions in the plan and observes the results. The LLM then uses the observations to update \\nits reasoning and generate a new plan of action. This process continues until the LLM \\nreaches a solution to the problem.\\nTo see this in action, you need to write some code. In code Snippet 1 I am using the langchain \\nframework for Python, together with VertexAI (google-cloud-aiplatform) and the \\ngoogle-search-results pip packages.', 'Prompt Engineering\\nFebruary 2025\\n38\\nTo run this sample you must create a (free) SerpAPI key from https://serpapi.com/manage-\\napi-key and set an environment variable SERPAPI_API_KEY.\\nNext let’s write some Python code, with the task for the LLM to figure out: How many children \\nhave a famous dad that performs in the band Metallica.\\nPython\\nfrom langchain.agents import load_tools\\nfrom langchain.agents import initialize_agent\\nfrom langchain.agents import AgentType\\nfrom langchain.llms import VertexAI\\nprompt = \"How many kids do the band members of Metallica have?\"\\nllm = VertexAI(temperature=0.1)\\ntools = load_tools([\"serpapi\"], llm=llm)\\nagent = initialize_agent(tools, llm,  \\nagent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\\nagent.run(prompt)\\nSnippet 1. Creating a ReAct Agent with LangChain and VertexAI\\nCode Snippet 2 shows the result. Notice that ReAct makes a chain of five searches. In fact, \\nthe LLM is scraping Google search results to figure out the band names. Then, it lists the \\nresults as observations and chains the thought for the next search.', 'Prompt Engineering\\nFebruary 2025\\n39\\n> Entering new AgentExecutor chain...\\nMetallica has 4 members. \\nAction: Search\\nAction Input: How many kids does James Hetfield have?\\nObservation: three children\\nThought: 1/4 Metallica band members have 3 children\\nAction: Search\\nAction Input: How many kids does Lars Ulrich have?\\nObservation: 3\\nThought: 2/4 Metallica band members have 6 children\\nAction: Search\\nAction Input: How many kids does Kirk Hammett have?\\nObservation: Hammett has been married to his second wife Lani since 1998. \\nThey have two sons, Angel (b. September 29, 2006) and Vincenzo (b. June \\n28, 2008).\\nThought: 3/4 Metallica band members have 8 children\\nAction: Search\\nAction Input: How many kids does Robert Trujillo have?\\nObservation: 2\\nThought: 4/4 Metallica band members have 10 children\\nFinal Answer: 10\\nSnippet 2. Executing ReAct Agent in the terminal\\nCode Snippet 2 figures out that the band Metallica has four band members. Then it searches \\neach band member to request the total of children and add up the total. Finally, it returns the \\ntotal number of children as the final answer.\\nReAct prompting in practice requires understanding that you continually have to resend the \\nprevious prompts/responses (and do trimming of the extra generated content) as well as set \\nup the model with appropriate examples/instructions. Please refer to the notebook14 hosted \\nin the GoogleCloudPlatform Github repository, which goes into a bit more detail showing the \\nactual LLM inputs and outputs with a more elaborate example.', 'Prompt Engineering\\nFebruary 2025\\n40\\nAutomatic Prompt Engineering\\nAt this point you might realize that writing a prompt can be complex. Wouldn’t it be nice to \\nautomate this (write a prompt to write prompts)? Well, there’s a method: Automatic Prompt \\nEngineering (APE). This method15 not only alleviates the need for human input but also \\nenhances the model’s performance in various tasks.\\nYou will prompt a model to generate more prompts. Evaluate them, possibly alter the good \\nones. And repeat.\\nFor example, you can use automatic prompt engineering that will help with training a chatbot \\nfor a merchandise t-shirt webshop. We want to figure out all the various ways customers \\ncould phrase their order for buying a band merchandise t-shirt.\\n1. Write the prompt which will generate the output variants. In this example, I am using \\ngemini-pro to generate 10 instructions. See Table 15:', 'Prompt Engineering\\nFebruary 2025\\n41\\nPrompt We have a band merchandise t-shirt webshop, and to train a \\nchatbot we need various ways to order: \"One Metallica t-shirt \\nsize S\". Generate 10 variants, with the same semantics but keep \\nthe same meaning.\\nOutput 1. I’d like to purchase a Metallica t-shirt in size small.\\n2. Can I order a small-sized Metallica t-shirt?\\n3. I’d like to buy one Metallica t-shirt in small.\\n4. One Metallica shirt, size small, please.\\n5. I want one Metallica t-shirt in size small.\\n6. I’d like to place an order for one Metallica t-shirt in small.\\n7. Small Metallica t-shirt, one please.\\n8. I’m looking for one Metallica t-shirt in small.\\n9. Can you get me one Metallica t-shirt in small?\\n10. One Metallica small t-shirt, please.\\nTable 15. Use an LLM to generate prompts\\n2. Evaluate all of the instruction candidates by scoring the candidates based on a chosen \\nmetric, For example, you can use. BLEU (Bilingual Evaluation Understudy) or ROUGE \\n(Recall-Oriented Understudy for Gisting Evaluation).\\n3. Select the instruction candidate with the highest evaluation score. This candidate will be \\nthe final prompt you can use in your software application or chatbot. You can also tweak \\nthe select prompt and evaluate again.', 'Prompt Engineering\\nFebruary 2025\\n42\\nCode prompting\\nGemini focuses primarily on text-based prompts, which also includes writing prompts for \\nreturning code. Let’s go to the Vertex AI Studio and test these prompts to look at some \\ncoding examples.\\nPrompts for writing code\\nGemini can also be a developer and help you with writing code in any programming language \\nof choice. As a developer this can help you speed up the process of writing code.\\nImagine a folder on your machine with hundreds of files that needs to be renamed. \\nRenaming each file would take you a lot of time. You know a little Bash, and could write \\na script to automate this, but that may also take awhile. So let’s write a prompt. You can \\nwrite the prompt in the public consumer chatbot Gemini, or if you are more concerned \\nabout confidentiality, you can write these prompts within your Google Cloud account and \\nopen Vertex AI Studio. The advantage of Vertex AI Studio is that you can configure the \\ntemperature etc.', 'Prompt Engineering\\nFebruary 2025\\n43\\nName 1_bash_rename_files_in_folder\\nGoal Write a prompt to write code in Bash to rename files in a folder.\\nModel gemini-pro\\nTemperature 0.1 Token Limit 1024\\nTop-K N/A Top-P 1\\nPrompt Write a code snippet in Bash, which asks for a folder name. \\nThen it takes the contents of the folder and renames all the \\nfiles inside by prepending the name draft to the file name.\\nOutput ```bash\\n#!/bin/bash\\n# Ask for the folder name\\necho \"Enter the folder name: \"\\nread folder_name\\n# Check if the folder exists\\nif [ ! -d \"$folder_name\" ]; then\\necho \"Folder does not exist.\"\\nexit 1\\nfi\\n# Get the list of files in the folder\\nfiles=( \"$folder_name\"/* )\\n# Rename each file by prepending \"draft\" to the file name\\nfor file in \"${files[@]}\"; do\\nnew_file_name=\"draft_$(basename \"$file\")\"\\nmv \"$file\" \"$new_file_name\"\\ndone\\necho \"Files renamed successfully.\"\\n```\\nTable 16. Using Gemini to program code\\nThat seems like good code to me - it’s even documented! However, since LLMs can’t reason, \\nand repeat training data, it’s essential to read and test your code first. \\nThe moment we are all waiting for, does it really work?', 'Prompt Engineering\\nFebruary 2025\\n44\\nLet’s try it first with a test folder with only a few files in it, which will need to be renamed from \\nfilename.txt to draft_filename.txt. \\n1. Copy the output from Table 16 (without the ```bash ``` text wrapper), and paste it in a new \\nfile called: “rename_files.sh”. \\n2. Open a terminal window and type: . rename_files.sh. It will ask to enter a folder name, e.g. \\ntest. and hit enter. \\n3. The script seems to run fine. You will see the message: Files renamed successfully. \\nWhen you look into the test folder, notice that all the files were perfectly renamed to \\ndraft_filename.txt. \\nIt worked!\\nPrompts for explaining code\\nAs a developer when you work in teams you have to read someone else’s code. Gemini can \\nhelp you with this as well. Let’s take the code output from Table 16, remove the comments \\nand ask the large language model to explain what’s happening, See Table 17:', 'Prompt Engineering\\nFebruary 2025\\n45\\nName 1_bash_explain_the_code\\nGoal Write a prompt to explain Bash code\\nModel gemini-pro\\nTemperature 0.1 Token Limit 1024\\nTop-K N/A Top-P 1\\nPrompt Explain to me the below Bash code:\\n```\\n#!/bin/bash\\necho \"Enter the folder name: \"\\nread folder_name\\nif [ ! -d \"$folder_name\" ]; then\\necho \"Folder does not exist.\"\\nexit 1\\nfi\\nfiles=( \"$folder_name\"/* )\\nfor file in \"${files[@]}\"; do\\nnew_file_name=\"draft_$(basename \"$file\")\"\\nmv \"$file\" \"$new_file_name\"\\ndone\\necho \"Files renamed successfully.\"\\n```\\nOutput This Bash script performs the following tasks:\\n1. **User Input**: \\n- It prompts the user to enter a folder name using the `echo` and `read` \\ncommands. The entered folder name is stored in the `folder_name` variable.\\n2. **Folder Existence Check**: \\n- It checks if the folder with the specified name exists using the `[ ! -d \\n“$folder_name” ]` condition. If the folder does not exist, it displays an error \\nmessage (“Folder does not exist.”) and exits the script with an exit code of 1.\\nContinues next page...', 'Prompt Engineering\\nFebruary 2025\\n46\\nOutput 3. **File Listing**: \\n- It lists all the files inside the specified folder using the `files=( “$folder_\\nname”/* )` command. This creates an array called `files` containing the paths \\nto all the files in the folder.\\n4. **File Renaming**: \\n- It iterates through each file in the `files` array using a `for` loop. \\n- For each file, it generates a new file name using the `new_file_\\nname=”draft_$(basename “$file”)” ` command. This new file name is created \\nby prefixing the original file name with the string “draft_”. \\n- It then renames the file using the `mv “$file” “$new_file_name”` command, \\nwhich moves the file to the new file name.\\n5. **Success Message**: \\n- After renaming all the files, it displays a success message (“Files renamed \\nsuccessfully.”) to indicate that the renaming operation was successful\\nTable 17. A prompt for explaining code\\nPrompts for translating code\\nThe bash code from Table 16 seems to work fine. However this script could really be reusable \\nwhen it would prompt me for file names, ideally it should work as a separate application with \\nan UI. As a starting point, Python would be a better language for a (web) application than \\nBash. LLMs can help with translating code from one language to another. See the example in \\nTable 18:', 'Prompt Engineering\\nFebruary 2025\\n47\\nName 1_python_rename_files\\nGoal Write a prompt to translate Bash code to Python\\nModel gemini-pro\\nTemperature 0.1 Token Limit 1024\\nTop-K N/A Top-P 1\\nPrompt Translate the below Bash code to a Python snippet.\\n```bash\\n#!/bin/bash\\necho \"Enter the folder name: \"\\nread folder_name\\nif [ ! -d \"$folder_name\" ]; then\\necho \"Folder does not exist.\"\\nexit 1\\nfi\\nfiles=( \"$folder_name\"/* )\\nfor file in \"${files[@]}\"; do\\nnew_file_name=\"draft_$(basename \"$file\")\"\\nmv \"$file\" \"$new_file_name\"\\ndone\\necho \"Files renamed successfully.\"\\n```\\nOutput ```python\\nimport os\\nimport shutil\\n# Get the folder name from the user\\nfolder_name = input(\"Enter the folder name: \")\\n# Check if the folder exists\\nif not os.path.isdir(folder_name):\\nprint(\"Folder does not exist.\")\\n    exit(1)\\nContinues next page...', 'Prompt Engineering\\nFebruary 2025\\n48\\nOutput # Get a list of all files in the folder\\nfiles = os.listdir(folder_name)\\n# Iterate over the files\\nfor file in files:\\n    # Create a new file name with the prefix \"draft_\"\\n    new_file_name = f\"draft_{file}\"\\n# Move the file to the new name\\nshutil.move(os.path.join(folder_name, file), \\nos.path.join(folder_name, new_file_name))\\n# Print a success message\\nprint(\"Files renamed successfully.\")\\n```\\nTable 18. A prompt for translating code from Bash to Python\\nRead and review the code. Copy the output from the prompt and paste it into a new file: \\nfile_renamer.py. Test the code by opening a Terminal window, and execute the following \\ncommand python file_renamer.py. \\nNOTE: When prompting for (Python) code in the Language Studio in Vertex AI, you will have \\nto click on the ‘Markdown’ button. Otherwise you will receive plain text which is missing the \\nproper indenting of lines, which is important for running Python code.\\nPrompts for debugging and reviewing code\\nLet’s manually write some edits to the code of Table 18. It should prompt the user for the \\nfilename prefix, and write this prefix in upper case characters. See the example code in \\nSnippet *3, but what a bummer. It now returns Python errors!', 'Prompt Engineering\\nFebruary 2025\\n49\\nPython\\nimport os\\nimport shutil\\nfolder_name = input(\"Enter the folder name: \")\\nprefix = input(\"Enter the string to prepend to the filename: \")\\ntext = toUpperCase(prefix)\\nif not os.path.isdir(folder_name):\\n    print(\"Folder does not exist.\")\\n    exit(1)\\nfiles = os.listdir(folder_name)\\nfor file in files:\\n    new_filename = f\"{text}_{file}\"\\nshutil.move(os.path.join(folder_name, file), \\nos.path.joi(folder_name, new_file_name))\\nprint(\"Files renamed successfully.\")\\nSnippet 3. A broken Python script\\nDoh! That looks like a bug:\\nThe below Python code gives an error:\\nTraceback (most recent call last):\\n  File “/Users/leeboonstra/Documents/test_folder/rename_files.py”, line 7, \\nin <module>\\n    text = toUpperCase(prefix)\\nNameError: name ‘toUpperCase’ is not defined\\nSnippet 4. I broke the Python code\\nLet’s see if we can ask the large language model to debug and review code. Have a look into \\nTable 19:', 'Prompt Engineering\\nFebruary 2025\\n50\\nName 1_python_debug_code\\nGoal Write a prompt to debug and review Python code.\\nModel gemini-pro\\nTemperature 0.1 Token Limit 1024\\nTop-K N/A Top-P 1\\nPrompt The below Python code gives an error:\\nTraceback (most recent call last):\\n                                                   File \"/\\nUsers/leeboonstra/Documents/test_folder/rename_files.py\", line \\n7, in <module>\\n    text = toUpperCase(prefix)\\nNameError: name \\'toUpperCase\\' is not defined\\nDebug what\\'s wrong and explain how I can improve the code.\\n```python\\nimport os\\nimport shutil\\n# Get the folder name from the user\\nfolder_name = input(\"Enter the folder name: \")\\nprefix = input(\"Enter the string to prepend to the filename: \")\\ntext = toUpperCase(prefix)\\n# Check if the folder exists\\nif not os.path.isdir(folder_name):\\n    print(\"Folder does not exist.\")\\n    exit(1)\\n# Get a list of all files in the folder\\nfiles = os.listdir(folder_name)\\n# Iterate over the files\\nfor file in files:\\nContinues next page...', 'Prompt Engineering\\nFebruary 2025\\n51\\nPrompt # Create a new file name with the prefix \"draft_\"\\n    new_filename = f\"{text}_{file}\"\\n# Move the file to the new name\\nshutil.move(os.path.join(folder_name, file), \\nos.path.join(folder_name, new_file_name))\\n# Print a success message\\nprint(\"Files renamed successfully.\")\\n```\\nOutput The code calls the `toUpperCase` function to convert `prefix` to uppercase, but \\nthat function is not defined. To fix this issue, you can use the `upper()` method of \\nthe string class which converts a given string into uppercase. The modified code \\nis shown below:\\n```python\\nimport os\\nimport shutil\\n# Get the folder name from the user\\nfolder_name = input(\"Enter the folder name: \")\\nprefix = input(\"Enter the string to prepend to the filename: \")\\ntext = prefix.upper()\\n# Check if the folder exists\\nif not os.path.isdir(folder_name):\\n    print(\"Folder does not exist.\")\\n    exit(1)\\n# Get a list of all files in the folder\\nfiles = os.listdir(folder_name)\\n# Iterate over the files\\nfor file in files:\\n    # Create a new file name with the prefix \"draft_\"\\n    new_filename = f\"{text}_{file}\"\\n# Move the file to the new name\\nshutil.move(os.path.join(folder_name, file),\\nContinues next page...', 'Prompt Engineering\\nFebruary 2025\\n52\\nOutput os.path.join(folder_name, new_filename))\\n# Print a success message\\nprint(\"Files renamed successfully.\")\\n```\\nAdditionally, there are a few other improvements that can be made to the code:\\n1. The file extension of the new filenames is not kept. It’s better to extract the \\nextension from the original filename and append it to the new filename.\\n2. The code assumes that there are no spaces in the folder name. However, \\nit is a good practice to handle spaces gracefully by replacing them with \\nunderscores or hyphens.\\n3. Using the `f` string syntax for string interpolation is more readable and \\nconcise than the traditional `+` operator.\\n4. The code doesn’t handle errors that might occur during the renaming process. \\nIt would be better to wrap the `shutil.move` call in a `try...except` block to \\ncatch any potential errors.\\nHere is the improved code with these suggestions:\\n```python\\nimport os\\nimport shutil\\n# Get the folder name from the user\\nfolder_name = input(\"Enter the folder name: \")\\nprefix = input(\"Enter the string to prepend to the filename: \")\\n# Check if the folder exists\\nif not os.path.isdir(folder_name):\\n    print(\"Folder does not exist.\")\\n    exit(1)\\n# Get a list of all files in the folder\\nfiles = os.listdir(folder_name)\\n# Iterate over the files\\nfor file in files:\\nContinues next page...', 'Prompt Engineering\\nFebruary 2025\\n53\\nOutput     # Get the file extension\\n    ext = os.path.splitext(file)[1]\\n# Create a new file name with the prefix and the file extension\\n    new_filename = f\"{prefix}_{os.path.splitext(file)[0]}{ext}\"\\n# Build the full paths for the original and new files\\n    old_path = os.path.join(folder_name, file)\\n    new_path = os.path.join(folder_name, new_filename)\\n# Try to move the file\\n    try:\\n        shutil.move(old_path, new_path)\\nexcept Exception as e:\\n        print(f\"An error occurred while moving the file \\n{file}: {e}\")\\n# Print a success message\\nprint(\"Files renamed successfully.\")\\n```\\nThese improvements make the code more robust and flexible while also making \\nit easier to read and understand... (The response was truncated because \\nit has reached the token limit. Try to increase the token limit if you need a \\nlonger response.)\\nTable 19. A prompt for debugging and reviewing Python code\\nThat’s great. Not only did it tell me how to solve the problem, it also figured out that my code \\nhad more bugs and how to solve them, too. The last part of the prompt gave suggestions to \\nimprove the code in general.', 'Prompt Engineering\\nFebruary 2025\\n54\\nWhat about multimodal prompting?\\nPrompting for code still uses the same regular large language model. Multimodal prompting \\nis a separate concern, it refers to a technique where you use multiple input formats to guide \\na large language model, instead of just relying on text. This can include combinations of text, \\nimages, audio, code, or even other formats, depending on the model’s capabilities and the \\ntask at hand. \\nBest Practices\\nFinding the right prompt requires tinkering. Language Studio in Vertex AI is a perfect place to \\nplay around with your prompts, with the ability to test against the various models.\\nUse the following best practices to become a pro in prompt engineering.\\nProvide examples\\nThe most important best practice is to provide (one shot / few shot) examples within a \\nprompt. This is highly effective because it acts as a powerful teaching tool. These examples \\nshowcase desired outputs or similar responses, allowing the model to learn from them \\nand tailor its own generation accordingly. It’s like giving the model a reference point or \\ntarget to aim for, improving the accuracy, style, and tone of its response to better match \\nyour expectations.', \"Prompt Engineering\\nFebruary 2025\\n55\\nDesign with simplicity\\nPrompts should be concise, clear, and easy to understand for both you and the model. As a \\nrule of thumb, if it’s already confusing for you it will likely be also confusing for the model. Try \\nnot to use complex language and don’t provide unnecessary information. \\nExamples:\\nBEFORE:\\nI am visiting New York right now, and I'd like to hear more about great \\nlocations. I am with two 3 year old kids. Where should we go during \\nour vacation?\\nAFTER REWRITE:\\nAct as a travel guide for tourists. Describe great places to visit in \\nNew York Manhattan with a 3 year old.\\nTry using verbs that describe the action. Here’s a set of examples:\\nAct, Analyze, Categorize, Classify, Contrast, Compare, Create, Describe, Define, \\nEvaluate, Extract, Find, Generate, Identify, List, Measure, Organize, Parse, Pick, \\nPredict, Provide, Rank, Recommend, Return, Retrieve, Rewrite, Select, Show, Sort, \\nSummarize, Translate, Write.\", 'Prompt Engineering\\nFebruary 2025\\n56\\nBe specific about the output\\nBe specific about the desired output. A concise instruction might not guide the LLM enough \\nor could be too generic. Providing specific details in the prompt (through system or context \\nprompting) can help the model to focus on what’s relevant, improving the overall accuracy.\\nExamples:\\nDO: \\nGenerate a 3 paragraph blog post about the top 5 video game consoles. \\nThe blog post should be informative and engaging, and it should be \\nwritten in a conversational style.\\nDO NOT: \\nGenerate a blog post about video game consoles.\\nUse Instructions over Constraints\\nInstructions and constraints are used in prompting to guide the output of a LLM. \\n• An instruction provides explicit instructions on the desired format, style, or content of the \\nresponse. It guides the model on what the model should do or produce.\\n• A constraint is a set of limitations or boundaries on the response. It limits what the model \\nshould not do or avoid.', 'Prompt Engineering\\nFebruary 2025\\n57\\nGrowing research suggests that focusing on positive instructions in prompting can be more \\neffective than relying heavily on constraints. This approach aligns with how humans prefer \\npositive instructions over lists of what not to do. \\nInstructions directly communicate the desired outcome, whereas constraints might leave the \\nmodel guessing about what is allowed. It gives flexibility and encourages creativity within the \\ndefined boundaries, while constraints can limit the model’s potential. Also a list of constraints \\ncan clash with each other.\\nConstraints are still valuable but in certain situations. To prevent the model from generating \\nharmful or biased content or when a strict output format or style is needed.\\nIf possible, use positive instructions: instead of telling the model what not to do, tell it what to \\ndo instead. This can avoid confusion and improve the accuracy of the output. \\nDO: \\nGenerate a 1 paragraph blog post about the top 5 video game consoles. \\nOnly discuss the console, the company who made it, the year, and total \\nsales. \\nDO NOT: \\nGenerate a 1 paragraph blog post about the top 5 video game consoles. \\nDo not list video game names.\\nAs a best practice, start by prioritizing instructions, clearly stating what you want the model \\nto do and only use constraints when necessary for safety, clarity or specific requirements. \\nExperiment and iterate to test different combinations of instructions and constraints to find \\nwhat works best for your specific tasks, and document these.', 'Prompt Engineering\\nFebruary 2025\\n58\\nControl the max token length\\nTo control the length of a generated LLM response, you can either set a max token limit in the \\nconfiguration or explicitly request a specific length in your prompt. For example:\\n\"Explain quantum physics in a tweet length message.\"\\nUse variables in prompts\\nTo reuse prompts and make it more dynamic use variables in the prompt, which can be \\nchanged for different inputs. E.g. as shown in Table 20, a prompt which gives facts about a \\ncity. Instead of hardcoding the city name in the prompt, use a variable. Variables can save \\nyou time and effort by allowing you to avoid repeating yourself. If you need to use the same \\npiece of information in multiple prompts, you can store it in a variable and then reference \\nthat variable in each prompt. This makes a lot of sense when integrating prompts into your \\nown applications.\\nPrompt VARIABLES\\n{city} = \"Amsterdam\"\\nPROMPT\\nYou are a travel guide. Tell me a fact about the city: {city}\\nOutput Amsterdam is a beautiful city full of canals, bridges, and narrow streets. It’s a \\ngreat place to visit for its rich history, culture, and nightlife.\\nTable 20. Using variables in prompts', 'Prompt Engineering\\nFebruary 2025\\n59\\nExperiment with input formats and writing styles\\nDifferent models, model configurations, prompt formats, word choices, and submits can yield \\ndifferent results. Therefore, it’s important to experiment with prompt attributes like the style, \\nthe word choice, and the type prompt (zero shot, few shot, system prompt).\\nFor example a prompt with the goal to generate text about the revolutionary video game \\nconsole Sega Dreamcast, can be formulated as a question, a statement or an instruction, \\nresulting in different outputs:\\n• Question: What was the Sega Dreamcast and why was it such a revolutionary console?\\n• Statement: The Sega Dreamcast was a sixth-generation video game console released by \\nSega in 1999. It...\\n• Instruction: Write a single paragraph that describes the Sega Dreamcast console and \\nexplains why it was so revolutionary.\\nFor few-shot prompting with classification tasks, mix up \\nthe classes\\nGenerally speaking, the order of your few-shots examples should not matter much. However, \\nwhen doing classification tasks, make sure you mix up the possible response classes in the \\nfew shot examples. This is because you might otherwise be overfitting to the specific order of \\nthe examples. By mixing up the possible response classes, you can ensure that the model is \\nlearning to identify the key features of each class, rather than simply memorizing the order of \\nthe examples. This will lead to more robust and generalizable performance on unseen data.', 'Prompt Engineering\\nFebruary 2025\\n60\\nA good rule of thumb is to start with 6 few shot examples and start testing the accuracy \\nfrom there.\\nAdapt to model updates\\nIt’s important for you to stay on top of model architecture changes, added data, and \\ncapabilities. Try out newer model versions and adjust your prompts to better leverage new \\nmodel features. Tools like Vertex AI Studio are great to store, test, and document the various \\nversions of your prompt.\\nExperiment with output formats\\nBesides the prompt input format, consider experimenting with the output format. For non-\\ncreative tasks like extracting, selecting, parsing, ordering, ranking, or categorizing data try \\nhaving your output returned in a structured format like JSON or XML. \\nThere are some benefits in returning JSON objects from a prompt that extracts data. In \\na real-world application I don’t need to manually create this JSON format, I can already \\nreturn the data in a sorted order (very handy when working with datetime objects), but most \\nimportantly, by prompting for a JSON format it forces the model to create a structure and \\nlimit hallucinations.\\nIn summary, benefits of using JSON for your output:\\n• Returns always in the same style\\n• Focus on the data you want to receive', \"Prompt Engineering\\nFebruary 2025\\n61\\n• Less chance for hallucinations\\n• Make it relationship aware\\n• You get data types\\n• You can sort it\\nTable 4 in the few-shot prompting section shows an example on how to return \\nstructured output.\\nJSON Repair\\nWhile returning data in JSON format offers numerous advantages, it's not without its \\ndrawbacks. The structured nature of JSON, while beneficial for parsing and use in \\napplications, requires significantly more tokens than plain text, leading to increased \\nprocessing time and higher costs. Furthermore, JSON's verbosity can easily consume the \\nentire output window, becoming especially problematic when the generation is abruptly cut \\noff due to token limits. This truncation often results in invalid JSON, missing crucial closing \\nbraces or brackets, rendering the output unusable. Fortunately, tools like the json-repair \\nlibrary (available on PyPI) can be invaluable in these situations. This library intelligently \\nattempts to automatically fix incomplete or malformed JSON objects, making it a crucial \\nally when working with LLM-generated JSON, especially when dealing with potential \\ntruncation issues.\", 'Prompt Engineering\\nFebruary 2025\\n62\\nWorking with Schemas\\nUsing structured JSON as an output is a great solution, as we\\'ve seen multiple times in this \\npaper. But what about input? While JSON is excellent for structuring the output the LLM \\ngenerates, it can also be incredibly useful for structuring the input you provide. This is where \\nJSON Schemas come into play. A JSON Schema defines the expected structure and data \\ntypes of your JSON input. By providing a schema, you give the LLM a clear blueprint of the \\ndata it should expect, helping it focus its attention on the relevant information and reducing \\nthe risk of misinterpreting the input. Furthermore, schemas can help establish relationships \\nbetween different pieces of data and even make the LLM \"time-aware\" by including date or \\ntimestamp fields with specific formats.\\nHere\\'s a simple example:\\nLet\\'s say you want to use an LLM to generate descriptions for products in an e-commerce \\ncatalog. Instead of just providing a free-form text description of the product, you can use a \\nJSON schema to define the product\\'s attributes:\\nSnippet 5. Definition of the structured output schema\\n{\\n  \"type\": \"object\",\\n  \"properties\": {\\n    \"name\": { \"type\": \"string\", \"description\": \"Product name\" },\\n    \"category\": { \"type\": \"string\", \"description\": \"Product category\" },\\n    \"price\": { \"type\": \"number\", \"format\": \"float\", \"description\": \"Product \\nprice\" },\\n    \"features\": {\\n      \"type\": \"array\",\\n      \"items\": { \"type\": \"string\" },\\n      \"description\": \"Key features of the product\"\\n    },\\n    \"release_date\": { \"type\": \"string\", \"format\": \"date\", \"description\": \\n\"Date the product was released\"}\\n  },', 'Prompt Engineering\\nFebruary 2025\\n63\\nThen, you can provide the actual product data as a JSON object that conforms to \\nthis schema:\\nSnippet 6. Structured output from the LLM\\nBy preprocessing your data and instead of providing full documents only providing both the \\nschema and the data, you give the LLM a clear understanding of the product\\'s attributes, \\nincluding its release date, making it much more likely to generate an accurate and relevant \\ndescription. This structured input approach, guiding the LLM\\'s attention to the relevant fields, \\nis especially valuable when working with large volumes of data or when integrating LLMs into \\ncomplex applications.\\nExperiment together with other prompt engineers\\nIf you are in a situation where you have to try to come up with a good prompt, you might \\nwant to find multiple people to make an attempt. When everyone follows the best practices \\n(as listed in this chapter) you are going to see a variance in performance between all the \\ndifferent prompt attempts.\\n{\\n  \"name\": \"Wireless Headphones\",\\n  \"category\": \"Electronics\",\\n  \"price\": 99.99,\\n  \"features\": [\"Noise cancellation\", \"Bluetooth 5.0\", \"20-hour battery life\"],\\n  \"release_date\": \"2023-10-27\"\\n}', 'Prompt Engineering\\nFebruary 2025\\n64\\nCoT Best practices\\nFor CoT prompting, putting the answer after the reasoning is required because the \\ngeneration of the reasoning changes the tokens that the model gets when it predicts the \\nfinal answer.\\nWith CoT and self-consistency you need to be able to extract the final answer from your \\nprompt, separated from the reasoning. \\nFor CoT prompting, set the temperature to 0.\\nChain of thought prompting is based on greedy decoding, predicting the next word in a \\nsequence based on the highest probability assigned by the language model. Generally \\nspeaking, when using reasoning, to come up with the final answer, there’s likely one single \\ncorrect answer. Therefore the temperature should always set to 0. \\nDocument the various prompt attempts\\nThe last tip was mentioned before in this chapter, but we can’t stress enough how important \\nit is: document your prompt attempts in full detail so you can learn over time what went well \\nand what did not. \\nPrompt outputs can differ across models, across sampling settings, and even across different \\nversions of the same model. Moreover, even across identical prompts to the same model, \\nsmall differences in output sentence formatting and word choice can occur. (For example, as \\nmentioned previously, if two tokens have the same predicted probability, ties may be broken \\nrandomly. This can then impact subsequent predicted tokens.).', 'Prompt Engineering\\nFebruary 2025\\n65\\nWe recommend creating a Google Sheet with Table 21 as a template. The advantages of \\nthis approach are that you have a complete record when you inevitably have to revisit your \\nprompting work–either to pick it up in the future (you’d be surprised how much you can \\nforget after just a short break), to test prompt performance on different versions of a model, \\nand to help debug future errors.\\nBeyond the fields in this table, it’s also helpful to track the version of the prompt (iteration), \\na field to capture if the result was OK/NOT OK/SOMETIMES OK, and a field to capture \\nfeedback. If you’re lucky enough to be using Vertex AI Studio, save your prompts (using the \\nsame name and version as listed in your documentation) and track the hyperlink to the saved \\nprompt in the table. This way, you’re always one click away from re-running your prompts.\\nWhen working on a retrieval augmented generation system, you should also capture the \\nspecific aspects of the RAG system that impact what content was inserted into the prompt, \\nincluding the query, chunk settings, chunk output, and other information.\\nOnce you feel the prompt is close to perfect, take it to your project codebase. And in the \\ncodebase, save prompts in a separate file from code, so it’s easier to maintain. Finally, ideally \\nyour prompts are part of an operationalized system, and as a prompt engineer you should \\nrely on automated tests and evaluation procedures to understand how well your prompt \\ngeneralizes to a task.\\nPrompt engineering is an iterative process. Craft and test different prompts, analyze, \\nand document the results. Refine your prompt based on the model’s performance. Keep \\nexperimenting until you achieve the desired output. When you change a model or model \\nconfiguration, go back and keep experimenting with the previously used prompts.', 'Prompt Engineering\\nFebruary 2025\\n66\\nName [name and version of your prompt]\\nGoal [One sentence explanation of the goal of this attempt]\\nModel [name and version of the used model]\\nTemperature [value between 0 - 1] Token Limit [number]\\nTop-K [number] Top-P [number]\\nPrompt [Write all the full prompt]\\nOutput [Write out the output or multiple outputs]\\nTable 21. A template for documenting prompts\\nSummary\\nThis whitepaper discusses prompt engineering. We learned various prompting techniques, \\nsuch as:\\n• Zero prompting\\n• Few shot prompting\\n• System prompting\\n• Role prompting\\n• Contextual prompting\\n• Step-back prompting\\n• Chain of thought\\n• Self consistency\\n• Tree of thoughts', 'Prompt Engineering\\nFebruary 2025\\n67\\n• ReAct\\nWe even looked into ways how you can automate your prompts. \\nThe whitepaper then discusses the challenges of gen AI like the problems that can happen \\nwhen your prompts are insufficient. We closed with best practices on how to become a better \\nprompt engineer.', 'Prompt Engineering\\nFebruary 2025\\n68\\nEndnotes\\n1. Google, 2023, Gemini by Google. Available at: https://gemini.google.com .\\n2. Google, 2024, Gemini for Google Workspace Prompt Guide. Available at:  \\nhttps://inthecloud.withgoogle.com/gemini-for-google-workspace-prompt-guide/dl-cd.html .\\n3. Google Cloud, 2023, Introduction to Prompting. Available at:  \\nhttps://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/introduction-prompt-design .\\n4. Google Cloud, 2023, Text Model Request Body: Top-P & top-K sampling methods. Available at:  \\nhttps://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text#request_body .\\n5. Wei, J., et al., 2023, Zero Shot - Fine Tuned language models are zero shot learners. Available at:  \\nhttps://arxiv.org/pdf/2109.01652.pdf .\\n6. Google Cloud, 2023, Google Cloud Model Garden. Available at: https://cloud.google.com/model-garden .\\n7. Brown, T., et al., 2023, Few Shot - Language Models are Few Shot learners. Available at:  \\nhttps://arxiv.org/pdf/2005.14165.pdf.\\n8. Zheng, L., et al., 2023, Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models. \\nAvailable at: https://openreview.net/pdf?id=3bq3jsvcQ1\\n9. Wei, J., et al., 2023, Chain of Thought Prompting. Available at: https://arxiv.org/pdf/2201.11903.pdf .\\n10. Google Cloud Platform, 2023, Chain of Thought and React. Available at: https://github.com/ \\nGoogleCloudPlatform/generative-ai/blob/main/language/prompts/examples/chain_of_thought_react.ipynb . \\n11. Wang, X., et al., 2023, Self Consistency Improves Chain of Thought reasoning in language models.  \\nAvailable at: https://arxiv.org/pdf/2203.11171.pdf .\\n12. Yao, S., et al., 2023, Tree of Thoughts: Deliberate Problem Solving with Large Language Models.  \\nAvailable at: https://arxiv.org/pdf/2305.10601.pdf .\\n13. Yao, S., et al., 2023, ReAct: Synergizing Reasoning and Acting in Language Models. Available at:  \\nhttps://arxiv.org/pdf/2210.03629.pdf.\\n14. Google Cloud Platform, 2023, Advance Prompting: Chain of Thought and React. Available at: \\nhttps://github.com/GoogleCloudPlatform/applied-ai-engineering-samples/blob/main/genai-  \\non-vertex-ai/advanced_prompting_training/cot_react.ipynb .\\n15. Zhou, C., et al., 2023, Automatic Prompt Engineering - Large Language Models are Human-Level Prompt \\nEngineers. Available at: https://arxiv.org/pdf/2211.01910.pdf .', 'Solving  \\nDomain-Specific \\nProblems Using \\nLLMs\\nAuthors: Christopher Semturs,  \\nShekoofeh Azizi, Scott Coull,  \\nUmesh Shankar  \\nand Wieland Holfelder', 'Solving Domain-Specific Problems Using LLMs\\n2\\nFebruary 2025\\nAcknowledgements\\nContent contributors\\nJohan Schalkwyk\\nSami Lachgar\\nDale Webster\\nYun Liu\\nConnie Fan\\nChris Grier\\nAntonio Gulli\\nAnant Nawalgaria\\nCurators and Editors\\nAntonio Gulli\\nAnant Nawalgaria\\nGrace Mollison \\nTechnical Writer\\nKaren Lin\\nDesigners\\nMichael Lanning\\nLuxi Chen', 'Introduction 4\\nSecLM and the future of cybersecurity 5\\n Challenges in cybersecurity  6\\n How GenAI can tackle the challenges in cybersecurity 7\\n SecLM: An API for cybersecurity tasks 10\\n  Security-focused large language models 12\\n\\t \\t A\\tflexible\\tplanning\\tand\\treasoning\\tframework 16\\nMedLM and the future of health tech 20\\n\\t The\\tpotential\\tfor\\tGenAI\\tin\\tmedical\\tQ&A 20\\n\\t The \\topportunities 21\\n\\t The\\tscientific\\tstarting\\tpoint 22\\n  How to evaluate: quantitative and qualitative 24\\n Evaluation in real clinical environments 30\\n\\t Task-\\tvs.\\tdomain-specific\\tmodels 31\\n Training strategies for Med-PaLM 2 32\\nSummary 35\\nEndnotes 36\\nTable of contents', 'Solving Domain-Specific Problems Using LLMs\\n4\\nFebruary 2025\\nIntroduction\\nLarge\\tlanguage\\tmodels\\t(LLMs)\\thave\\temerged\\tas\\tpowerful\\ttools\\tfor\\ttackling\\tcomplex\\t\\nchallenges\\tin\\tnumerous\\tdomains.\\tWhile\\tearly\\titerations\\tfocused\\ton\\tgeneral-purpose\\ttasks,\\t\\nrecent\\tdevelopments\\thave\\thighlighted\\tthe\\tpotential\\tof\\tfine-tuning\\tLLMs\\tto\\taddress\\tspecific\\t\\nproblems\\twithin\\tspecialized\\tfields.\\tThis\\twhitepaper\\texplores\\tthese\\tconcepts\\tin\\ttwo\\tdistinct\\t\\ndomains: cybersecurity and medicine. Each showcases the unique ability of LLMs to enhance \\nexisting\\tworkflows\\tand\\tunlock\\tnew\\tpossibilities.\\nCybersecurity\\tpresents\\ta\\tnumber\\tof\\tunique\\tchallenges\\tfor\\tLLMs,\\tincluding\\ta\\tscarcity\\tof\\t\\npublicly\\tavailable\\tdata,\\ta\\twide\\tdiversity\\tof\\thighly\\ttechnical\\tconcepts,\\tand\\tinformation\\tabout\\t\\nthreats\\tthat\\tchange\\ton\\ta\\tdaily\\tbasis.\\tAdditionally,\\tsensitive\\tuse\\tcases,\\tlike\\tmalware\\tanalysis,\\t\\nnecessitate\\tspecific\\tconsiderations\\tfor\\tmodel\\tdevelopment.\\tWe\\taddress\\tthese\\tchallenges\\t\\n... recent developments \\nhave highlighted the potential \\nof fine-tuning LLMs \\nto address specific problems \\nwithin specialized fields.', 'Solving Domain-Specific Problems Using LLMs\\n5\\nFebruary 2025\\nby\\tfocusing\\ton\\tcybersecurity-specific\\tcontent\\tand\\ttasks,\\tpairing\\tsecurity-focused\\tlanguage\\t\\nmodels\\twith\\ta\\tsuite\\tof\\tsupporting\\ttechniques\\tto\\toffer\\timproved\\tperformance\\tfor\\tvital\\ttasks\\t\\nlike\\tthreat\\tidentification\\tand\\trisk\\tanalysis.\\nIn\\tthe\\tfield\\tof\\tmedicine,\\tLLMs\\tface\\ta\\tdifferent\\tset\\tof\\tobstacles,\\tsuch\\tas\\tthe\\tvast\\tand\\tever-\\nevolving\\tnature\\tof\\tmedical\\tknowledge\\tand\\tthe\\tneed\\tto\\tapply\\tsaid\\tknowledge\\tin\\ta\\tcontext-\\ndependent\\tmanner\\tthat\\tmakes\\taccurate\\tdiagnosis\\tand\\ttreatment\\ta\\tcontinual\\tchallenge.\\t\\nLLMs\\tlike\\tMed-PaLM,\\tcustomized\\tfor\\tmedical\\tapplications,\\tdemonstrate\\tthe\\tability\\tto\\tanswer\\t\\ncomplex\\tmedical\\tquestions\\tand\\tprovide\\tinsightful\\tinterpretations\\tof\\tmedical\\tdata,\\tshowing\\t\\npotential\\tfor\\tsupporting\\tboth\\tclinicians\\tand\\tpatients.\\nThrough\\tthe\\tlens\\tof\\tthese\\ttwo\\tdistinct\\tdomains,\\tin\\tthis\\twhitepaper\\twe\\twill\\texplore\\tthe\\t\\nchallenges\\tand\\topportunities\\tpresented\\tby\\tspecialized\\tdata,\\ttechnical\\tlanguage,\\tand\\t\\nsensitive\\tuse\\tcases.\\tBy\\texamining\\tthe\\tunique\\tpaths\\ttaken\\tby\\tSecLM\\tand\\tMed-PaLM,\\twe\\t\\nprovide\\tinsights\\tinto\\tthe\\tpotential\\tof\\tLLMs\\tto\\trevolutionize\\tvarious\\tareas\\tof\\texpertise.\\nSecLM and the future of cybersecurity\\nSecurity\\tpractitioners\\tface\\ta\\tmyriad\\tof\\tchallenges,\\tincluding\\tnew\\tand\\tevolving\\tthreats,\\t\\noperational\\ttoil,\\tand\\ta\\ttalent\\tshortage.\\tSpecialized\\tGenerative\\tAI\\t(Gen\\tAI)\\tcan\\thelp\\taddress\\t\\nthese\\tchallenges\\tby\\tautomating\\trepetitive\\ttasks,\\tfreeing\\tup\\ttime\\tfor\\tmore\\tstrategic\\tactivities,\\t\\nand\\tproviding\\tnew\\topportunities\\tto\\taccess\\tknowledge.', 'Solving Domain-Specific Problems Using LLMs\\n6\\nFebruary 2025\\nChallenges in cybersecurity \\nIn\\tthe\\tmovies,\\twe\\toften\\tsee\\tinformation\\tsecurity\\treduced\\tto\\tthe\\tcaricature\\tof\\thoodie-clad\\t\\nand\\theadset-wearing\\thackers\\twith\\till\\tintent,\\tarmed\\twith\\truggedized\\tlaptops,\\ttapping\\taway\\t\\nfuriously until we hear the two magic words: “I’m in.” \\nTo\\tthe\\textent\\tthat\\tyou\\teven\\tsee\\tthe\\tdefenders,\\tthey\\tare\\tin\\treactive\\tmode-think\\twar\\trooms,\\t\\nempty\\tcoffee\\tcups,\\tpeople\\tbarking\\torders,\\tand\\tmonitors\\tshowing\\tthe\\tattacker’s\\tevery\\tmove\\t\\nin real-time. \\nThat is Hollywood; we live in the real world.\\nIn\\treality,\\tthe\\tpeople\\twho\\tpractice\\tcybersecurity\\t-\\tthe\\tdevelopers,\\tsystem\\tadministrators,\\t\\nSREs,\\tand\\tmany\\tjunior\\tanalysts\\tto\\twhom\\tour\\twork\\there\\tis\\tdedicated\\t-\\thave\\tthe\\tSisyphean\\t\\ntask\\tof\\tkeeping\\tup\\twith\\tthe\\tlatest\\tthreats\\tand\\ttrying\\tto\\tprotect\\tcomplex\\tsystems\\tagainst\\t\\nthem.\\tMany\\tpractitioners’\\tdays\\tare\\tlargely\\tfilled\\twith\\trepetitive\\tor\\tmanual\\ttasks,\\tsuch\\tas\\t\\nindividually\\ttriaging\\thundreds\\tof\\talerts,\\tthat\\ttake\\tvaluable\\ttime\\taway\\tfrom\\tdeveloping\\tmore\\t\\nstrategic\\tdefenses.\\tThe\\tmomentum\\tis\\tdefinitely\\tnot\\tin\\tthe\\tdefender’s\\tfavor;\\tattackers\\tare\\t\\nadopting\\tadvanced\\ttechnologies,\\tincluding\\tartificial\\tintelligence,1 to extend their reach and \\nquicken\\tthe\\tpace\\tof\\texploitation.\\tAnd\\tthere\\tare\\tdefinitely\\tno\\tmonitors\\tshowing\\tthe\\tattacker’s\\t\\nevery move!\\nBased\\ton\\tour\\texperience\\tworking\\twith\\tusers\\tand\\tpartners,\\twe\\tsee\\tthree\\tmajor\\tchallenges\\tin\\t\\nthe security industry today: threats,\\ttoil,\\tand\\ttalent. \\n• New and evolving threats:\\tThe\\tthreat\\tlandscape\\tis\\tconstantly\\tchanging,\\twith\\tnew\\t\\nand\\tincreasingly\\tsophisticated\\tattacks\\temerging\\tall\\tthe\\ttime.\\tThis\\tmakes\\tit\\tdifficult\\tfor\\t\\ndefenders\\tto\\tkeep\\tup\\twith\\tthe\\tlatest\\tinformation,\\tand\\tconversely\\tfor\\tpractitioners\\tto\\tsift\\t\\nthrough\\tthat\\tflood\\tof\\tdata\\tto\\tidentify\\twhat’s\\trelevant\\tto\\tthem\\tand\\ttake\\taction.', 'Solving Domain-Specific Problems Using LLMs\\n7\\nFebruary 2025\\n• Operational toil:\\tPeople\\tworking\\tin\\tsecurity\\toperations\\tor\\tDevOps\\troles\\toften\\tspend\\ta\\t\\nsignificant\\tamount\\tof\\ttime\\ton\\trepetitive\\tmanual\\ttasks\\tthat\\tcould\\tbe\\tautomated\\tor\\tassisted.\\t\\nThis leads to overload and takes away time from more strategic activities. Excessive focus \\non\\tminutiae\\talso\\tprevents\\tanalysts\\tand\\tengineers\\tfrom\\tseeing\\tthe\\tbigger\\tpicture\\tthat\\tis\\t\\nkey\\tto\\tsecuring\\ttheir\\torganizations.\\n• Talent shortage :\\tThere\\tis\\ta\\tshortage\\tof\\tskilled\\tsecurity\\tprofessionals,\\tmaking\\tit\\tdifficult\\t\\nfor\\torganizations\\tto\\tfind\\tthe\\tpeople\\tthey\\tneed\\tto\\tprotect\\ttheir\\tdata\\tand\\tsystems.\\tOften,\\t\\npeople\\tenter\\tsecurity-focused\\troles\\twithout\\tmuch\\ttraining\\tand\\twith\\tlittle\\tspare\\ttime\\tto\\t\\nexpand\\ttheir\\tskills\\ton\\tthe\\tjob.\\nWithout\\tthe\\tability\\tto\\taddress\\tthese\\tthree\\tchallenges,\\tit\\twill\\tbe\\tdifficult\\tto\\tkeep\\tup\\twith\\tthe\\t\\ndemands of modern cybersecurity systems.\\nHow GenAI can tackle the challenges in cybersecurity\\nWe\\tenvision\\ta\\tworld\\twhere\\tnovices\\tand\\tsecurity\\texperts\\talike\\tare\\tpaired\\twith\\tAI\\texpertise\\t\\nto\\tfree\\tthemselves\\tfrom\\trepetition\\tand\\ttoil,\\taccomplish\\ttasks\\tthat\\tseem\\timpossible\\tto\\tus\\t\\ntoday,\\tand\\tprovide\\tnew\\topportunities\\tto\\tshare\\tknowledge.\\tLarge\\tlanguage\\tmodels\\t(LLMs)\\t\\nand\\tadjacent\\tGenAI\\ttechniques\\tcan\\tmeaningfully\\timprove\\tthe\\tworking\\tlives\\tof\\tboth\\tsecurity\\t\\nnovices\\tand\\texperienced\\tpractitioners.\\tIndeed,\\tin\\tmany\\tcases,\\twe\\thave\\talready\\tfound\\tthat\\t\\nGenAI\\tis\\tuseful\\tto\\tsolve\\ta\\tnumber\\tof\\treal-world\\tsecurity\\tproblems\\tin\\tour\\tchallenge\\tareas:', 'Solving Domain-Specific Problems Using LLMs\\n8\\nFebruary 2025\\nPersona(e) Challenges faced How Gen AI can help\\nSecurity analyst\\nAnalysts not familiar with \\neach\\ttool’s\\tbespoke\\tschema\\t\\nand query language.\\nTranslate natural-language \\nqueries\\tinto\\ta\\tdomain-specific\\t\\nsecurity event query language and \\nrules language.\\nInvestigating,\\tclustering,\\tand\\t\\ntriaging\\tincoming\\talerts\\tis\\t\\ntime-consuming and requires \\nmultiple\\tsteps\\tand\\ttools.\\nAutonomous\\tcapabilities\\tto\\t\\nperform\\tinvestigation,\\tgrouping,\\t\\nand\\tclassification,\\tincorporating\\t\\ncontext and real-time tool use.\\nHard to assemble the right \\nseries\\tof\\ttailored\\tsteps\\tto\\t\\nremediate an issue.\\nPersonalized,\\tcase-specific\\t\\nremediation\\tplanning\\tin\\t\\nuser environments.\\nThreat Researcher or \\nSystem Administrator\\nAn unknown and obfuscated \\nartifact\\t(such\\tas\\ta\\tscript\\t\\nor binary) is discovered \\nand can’t be easily \\nanalyzed\\tmanually.\\nAutomated reverse engineering \\nwith\\tLLM-powered\\tcode\\tanalysis\\t\\nwith tool use for de-obfuscation \\nand\\tdecompilation.\\tExplain,\\t\\nanalyze,\\tand\\tclassify\\tpotentially\\t\\nmalicious\\tartifacts.\\nCISO team\\nManual work required to \\nidentify\\tand\\tsummarize\\tthe\\t\\nmost likely threats facing \\nthe\\torganization.\\nGenerate a readable document \\nor\\tslide\\tdeck,\\tapplying\\tthe\\tlatest\\t\\nthreat\\tintelligence\\tand\\tfindings\\t\\nfrom security tools to the \\nspecific\\torganization.\\nIT Administrator \\nDedicated Security Team\\nHard to understand all the \\nways\\tan\\tattacker\\tcould\\t\\naccess sensitive resources.\\nIdentify\\tpotential\\tor\\tactual\\tattack\\t\\npaths,\\thighlighting\\tkey\\telements\\t\\nand remediations.', 'Solving Domain-Specific Problems Using LLMs\\n9\\nFebruary 2025\\nApplication Developers\\nChallenging to determine \\nthe\\tright\\tplaces\\tto\\tfuzz-test\\t\\nan\\tapplication.\\nIdentify which locations to \\nfuzz-test\\tand\\tgenerate\\tthe\\t\\nappropriate\\tcode.\\nApplication Developers & \\nIT Administrators\\nKeep\\taccess\\tpolicies\\t\\naligned\\tto\\tthe\\tprinciple\\tof\\t\\nleast\\tprivilege.\\nGiven\\thistorical\\taccess\\tpatterns\\t\\nand\\tcurrent\\tconfiguration,\\t\\nconstruct\\ta\\tconfiguration\\tfile\\t\\nmodification\\tthat\\tgrants\\ta\\tmore\\t\\nminimal set of roles.\\nA person responsible for an \\napplication or system\\nPeople\\tdon’t\\talways\\t\\nunderstand\\tsecurity\\tconcepts\\t\\nor\\thow\\tto\\tapply\\tthem\\tto\\ttheir\\t\\nenvironments; they have to \\nknow\\thow\\tto\\tbreak\\ta\\tproblem\\t\\ndown,\\task\\tquestions\\tin\\tmany\\t\\nplaces,\\tand\\tthen\\tcombine\\t\\nthem to obtain an answer.\\nGive\\tan\\tanswer\\tthat\\treflects\\t\\nauthoritative\\tsecurity\\texpertise\\t\\nand,\\tusing\\tintegrations,\\tis\\trelevant\\t\\nto the user’s working environment.\\nTo\\ttackle\\tthese\\tproblems\\tin\\ta\\tmeaningful\\tand\\tholistic\\tway,\\thowever,\\twe\\tneed\\ta\\t\\nmulti-layered\\tapproach:\\n• Top layer: existing\\tsecurity\\ttools\\tthat\\tunderstand\\tthe\\trelevant\\tcontext\\tand\\tdata,\\tand\\t\\ncan actuate necessary changes;\\n• Middle layer: a\\tsecurity-specialized\\tmodel\\tAPI\\twith\\tadvanced\\treasoning\\tand\\t\\nplanning\\tcapabilities;\\n• Bottom layer: datastores of authoritative security intelligence and \\noperational\\texpertise\\nNotably,\\tone\\tof\\tthe\\tkey\\tbenefits\\tof\\tLLMs\\tis\\ttheir\\tability\\tto\\tprocess\\tand\\tsynthesize\\tvast\\t\\namounts\\tof\\theterogenous\\tdata\\t–\\tan\\timportant\\tcapability\\tin\\tthe\\tincreasingly\\tsiloed\\tworld\\t\\nof\\tcybersecurity\\tdata.\\tWe\\tseek\\tto\\tleverage\\tthat\\tcapability\\tto\\tsolve\\tchallenging\\tsecurity', 'Solving Domain-Specific Problems Using LLMs\\n10\\nFebruary 2025\\nproblems,\\twhether\\tby\\tassisting\\thuman\\tanalysts\\tor\\tthrough\\tautonomous\\tagents,\\tby\\tcombining\\t\\nrelevant\\tcontext\\tand\\tauthoritative\\tsources\\twith\\ta\\tflexible\\tplanning\\tframework\\tin\\ta\\tsingle\\tAPI,\\t\\nwhich we call SecLM.\\nThis\\tAPI\\toffers\\trich\\tplanning\\tcapabilities\\tthat\\tcombine\\tLLMs\\tand\\tother\\tML\\tmodels,\\tRetrieval-\\nAugmented\\tGeneration\\t(RAG)\\tto\\tground\\tresults\\tin\\tauthoritative\\tdata,\\tand\\ttool\\tuse\\tto\\tperform\\t\\nactions\\tor\\tlook\\tup\\trelevant\\tinformation.\\tWe\\targue\\tthat\\tthis\\tholistic\\tapproach\\tis\\tcritical\\t\\nbecause\\taccuracy\\tis\\tso\\timportant\\tin\\tsecurity\\tand\\tLLMs\\talone\\tcannot\\tinherently\\tsolve\\tall\\t\\nsecurity\\tproblems.\\nSecLM: An API for cybersecurity tasks\\nOur\\tvision\\tof\\tthe\\tSecLM\\tAPI\\tis\\tto\\tprovide\\ta\\t‘one-stop\\tshop’\\tfor\\tgetting\\tanswers\\tto\\tsecurity\\t\\nquestions,\\tregardless\\tof\\ttheir\\tlevel\\tof\\tcomplexity.\\tThat\\tis,\\tthe\\tengineer\\tor\\tanalyst\\tcan\\tpose\\t\\nquestions\\tand\\trefer\\tto\\tdata\\tsources\\twith\\tnatural\\tlanguage,\\tand\\texpect\\tan\\tanswer\\tthat\\t\\nautomatically\\tincorporates\\tthe\\tnecessary\\tinformation.\\tHowever,\\tsecurity\\tproblems\\toften\\t\\nrequire\\ta\\tlot\\tof\\tinformation\\tto\\tbe\\tgathered\\tand\\tanalyzed\\tusing\\tdomain-specific\\treasoning,\\t\\noften\\tby\\texperts\\tacross\\tseveral\\tdisciplines.\\nIdeally,\\tone\\tcan\\task\\tthe\\tSecLM\\tAPI\\ta\\tquestion\\tin\\ta\\tzero-shot\\tmanner\\tand\\tget\\ta\\thigh-quality\\t\\nresponse\\twithout\\tfussing\\tover\\tprompting\\tor\\tmanually\\tintegrating\\texternal\\tdata.\\tIn\\torder\\tto\\t\\nachieve\\tthis\\tin\\ta\\tcoherent\\tand\\tseamless\\tmanner,\\tit\\tis\\timportant\\tto\\thave\\ta\\twell-designed\\tAPI\\t\\nthat\\tinteracts\\twith\\tLLMs\\tand\\ttraditional\\tML\\tmodels,\\tthe\\tuser’s\\tdata,\\tand\\tother\\tservices\\tto\\t\\naccurately\\tcomplete\\tthe\\ttask\\tat\\thand.\\tDue\\tto\\tthe\\tcomplex\\tnature\\tof\\tthese\\tsecurity\\tproblems,\\t\\nwe must aim to address the following key requirements:', 'Solving Domain-Specific Problems Using LLMs\\n11\\nFebruary 2025\\n• Freshness: The model should be able to access the latest threat and vulnerability \\ndata,\\twhich\\tchanges\\ton\\ta\\tdaily\\tbasis.\\tDue\\tto\\tits\\tcost\\tand\\tduration\\t(often\\tdays),\\t\\nretraining\\tthe\\tmodel\\ton\\ta\\tdaily\\tor\\thourly\\tbasis\\tto\\tincorporate\\tthe\\tlatest\\tdata\\tis\\tnot\\ta\\t\\nfeasible\\tapproach.\\n• User-specific data: The\\tmodel\\tshould\\tbe\\table\\tto\\toperate\\ton\\tthe\\tuser’s\\town\\tsecurity\\t\\ndata\\twithin\\tthe\\tuser’s\\tenvironment\\twithout\\tthe\\trisk\\tof\\texposing\\tthat\\tsensitive\\tdata\\t\\nto\\tothers\\tor\\tthe\\tinfrastructure\\tprovider.\\tThis\\trules\\tout\\tany\\tcentralized\\ttraining\\ton\\t\\nuser data.\\n• Security expertise: The model should be able to understand high-level security \\nconcepts\\tand\\tterminology,\\tand\\tbreak\\tthem\\tinto\\tmanageable\\tpieces\\tthat\\tare\\tuseful\\t\\nwhen\\tsolving\\tthe\\tproblem.\\tFor\\tinstance,\\tdecomposing\\ta\\thigh-level\\tattack\\tstrategy\\t(e.g.,\\t\\nlateral\\tmovement)\\tinto\\tits\\tconstituent\\tcomponents\\tfor\\tsearch\\tor\\tdetection.\\n• User-specific data: The\\tmodel\\tshould\\tbe\\table\\tto\\treason\\tabout\\tthe\\tprovided\\tsecurity\\t\\ndata\\tin\\ta\\tmulti-step\\tfashion\\tby\\tcombining\\tdifferent\\tdata\\tsources,\\ttechniques,\\tand\\t\\nspecialized\\tmodels\\tto\\tsolve\\tsecurity\\tproblems.\\nSecLM\\taddresses\\tthese\\tchallenges\\tthrough\\tthe\\tuse\\tof\\tsecurity-specialized\\tLLMs,\\ttraditional\\t\\nML\\tmodels,\\tand\\ta\\tflexible\\tplanning\\tframework\\tthat\\tenables\\tdynamic\\tuse\\tof\\ttools\\tand\\t\\ninteraction\\tamong\\tmultiple\\tdomain-specialized\\tagents\\tto\\treason\\tover\\tthe\\tprovided\\tdata.\\t\\nHere,\\twe\\twill\\tbriefly\\tdiscuss\\tour\\tapproach\\tto\\ttraining\\tsecurity-specialized\\tmodels\\tand\\t\\ndesigning\\tthe\\tplanning\\tframework\\tthat\\tdrives\\tthe\\tSecLM\\tAPI.', 'Solving Domain-Specific Problems Using LLMs\\n12\\nFebruary 2025\\nSecurity-focused large language models\\nOne\\tof\\tthe\\tthings\\twe\\tobserved\\tin\\tapplying\\tLLMs\\tto\\tsecurity\\tis\\tthat\\tgeneral-purpose\\tmodels\\t\\ndidn’t\\tpeform\\tas\\twell\\tas\\twe\\tneeded\\ton\\tsome\\tsecurity\\ttasks.\\tThe\\treasons\\tfor\\tthis\\tfall\\tinto\\t\\nthree categories:\\n• Lack of publicly available security data:\\tLLMs\\tare\\tdata-hungry,\\trequiring\\tlarge\\tpre-\\ntraining\\tcorpora\\tfor\\tbest\\tresults.\\tAt\\tthe\\tsame\\ttime,\\tsecurity\\tdata\\tis\\tsensitive\\tso\\twe\\tcannot\\t\\nuse\\treal\\tsecurity\\tdata\\tin\\ttraining.\\tMoreover,\\twhat\\tlittle\\tdata\\tis\\tavailable\\tpublicly\\tis\\tusually\\t\\nconcentrated\\ton\\ta\\tsmall\\tnumber\\tof\\tthe\\tmost\\tpopular\\tsecurity\\tproducts\\tor\\ton\\tgeneric\\t\\nsecurity\\tcontent\\tthat\\tlacks\\tconnection\\tto\\tconcrete\\tapplication.\\t\\n• Limited depth of security content:\\tSimilarly,\\tthere\\tis\\ta\\tcertain\\thighly\\ttechnical\\tlanguage\\t\\nthat\\tis\\tused\\tto\\ttalk\\tabout\\tsecurity\\tor\\texpress\\tsecurity\\tinsights,\\toften\\tcrossing\\tdisciplines\\t\\nfrom\\tlow-level\\tcomputer\\tscience\\tconcepts\\tto\\thigh-level\\tpolicy\\tand\\tintelligence\\tanalysis.\\t\\nTo\\tbe\\teffective,\\tsecurity\\tLLMs\\tmust\\tseamlessly\\tblend\\tthis\\tlanguage,\\tconnect\\tthem\\tto\\t\\ntheir\\tunderlying\\ttechnical\\tconcepts,\\tand\\tsynthesize\\trelevant,\\taccurate\\toutput\\tfor\\tsecurity\\t\\nanalysts\\tand\\tengineers\\tto\\tconsume.\\tWhile\\tthere\\tare\\tsome\\thigh-quality,\\tin-depth\\tarticles\\t\\nthat\\texplain\\thow\\tto\\taddress\\twell-known\\tvulnerabilities\\tor\\tattacks,\\tthousands\\tof\\tnew\\t\\nthreats emerge each year.\\n• Sensitive use cases:\\tThere\\tare\\tsome\\tuse\\tcases\\tin\\tsecurity\\tthat\\tgeneral\\tpurpose\\tmodels\\t\\ndo\\tnot\\thandle\\tby\\tdesign\\tsuch\\tas\\tabuse\\tareas\\tlike\\tmalware\\tor\\tphishing.\\tIn\\tmost\\tcases,\\t\\ngeneral-purpose\\tLLMs\\twould\\tactively\\twork\\tto\\tavoid\\tincorporating\\tsuch\\ttasks\\tor\\trelated\\t\\ndata\\tfor\\tfear\\tof\\tincreasing\\trisk\\tof\\tmisuse\\tor\\tabuse.\\tHowever,\\tthese\\tcases\\tare\\tcrucial\\tfor\\t\\nsecurity\\tpractitioners\\tlooking\\tto\\tsecure\\ttheir\\tsystems,\\tto\\tanalyze\\tartifacts,\\tor\\teven\\tfor\\t\\ntesting\\tpurposes.\\t\\nTaken\\ttogether,\\tthese\\tchallenges\\tmotivate\\tthe\\tdevelopment\\tof\\tsecurity-focused\\tLLMs\\t\\nthat\\toperate\\tacross\\tas\\tmany\\tsecurity\\tplatforms\\tand\\tenvironments\\tas\\tthe\\thumans\\tthey\\twill\\t\\nultimately\\tsupport.\\tTo\\tthis\\tend,\\twe\\tdevelop\\tspecialized\\tLLMs\\tthat\\thave\\tbeen\\ttrained\\ton\\ta\\t\\nvariety\\tof\\tcybersecurity-specific\\tcontent\\tand\\ttasks.', 'Solving Domain-Specific Problems Using LLMs\\n13\\nFebruary 2025\\nThis\\tbroad\\tset\\tof\\tsupported\\ttasks\\tmeans\\tthat\\twe\\thave\\tto\\ttake\\tinto\\taccount\\tmultiple\\tuse\\t\\ncases\\tand\\tenvironments\\twhen\\tmaking\\tdesign\\tdecisions,\\tsuch\\tas\\tchoosing\\tthe\\tmodel\\tsize\\tand\\t\\ncomposition\\tof\\ttraining\\ttasks.\\tFor\\texample,\\tan\\tLLM\\twith\\thundreds\\tof\\tbillions\\tof\\tparameters\\t\\nmay\\tmaximize\\treasoning\\tand\\tabstraction\\tcapabilities,\\tbut\\tmight\\tnot\\tbe\\tideal\\tfor\\tlatency-\\nsensitive\\tor\\thigh-volume\\ttasks,\\tlike\\tsummarizing\\tand\\tcategorizing\\tsecurity\\tevents.\\nTo\\tensure\\tthe\\tmodel\\tgeneralizes\\tto\\tnew\\ttasks\\tand\\tsecurity\\tproducts\\tnot\\tdirectly\\tvisible\\tin\\tthe\\t\\ntraining\\tdata,\\twe\\thave\\tto\\tbe\\tvery\\tcareful\\twith\\tthe\\ttraining\\tregime\\tused\\tto\\tcreate\\tthe\\tmodels.\\t\\nAs\\tan\\texample,\\tconsider\\tthat\\tfor\\tmany\\ttask\\tareas,\\tsuch\\tas\\ttranslating\\tnatural\\tlanguage\\tinto\\ta\\t\\ndomain-specific\\tquery\\tlanguage,\\tit\\tis\\thighly\\tlikely\\tthat\\tany\\ttraining\\tdata\\twe\\thave\\twill\\tcontain\\t\\nonly\\ta\\tfraction\\tof\\tthe\\teventual\\ttargets\\tfor\\tour\\tusers.\\tIn\\tthis\\tcase,\\twithout\\tcareful\\tcuration\\t\\nof\\tthe\\ttraining\\tdata,\\twe\\tmay\\tinadvertently\\teliminate\\tthe\\tability\\tof\\tthe\\tmodel\\tto\\tgeneralize\\t\\nto\\tnew\\ttasks\\tor\\tdata\\tsources\\tthat\\tare\\timportant\\tto\\tusers.\\tLikewise,\\tsome\\tdata\\tsources\\tare\\t\\nparticularly\\tsensitive\\tor\\tproprietary\\tand\\tshould\\tnot\\tbe\\tincluded\\tin\\tgeneralized\\ttraining\\tof\\t\\nthe\\tmodel.\\tInstead,\\tthese\\tdata\\tsources\\tshould\\tbe\\tincorporated\\tinto\\ta\\tspecialized\\tderivative\\t\\nmodel\\t(using\\ta\\tlightweight,\\tparameter-efficient\\tprocess)\\tthat\\tdoes\\tnot\\tdegrade\\tthe\\toverall\\t\\nperformance\\tof\\tthe\\tcore\\tsecurity-specialized\\tmodel.\\nThe\\ttraining\\tprocess,\\tshown\\tin\\tFigure\\t1,\\tdemonstrates\\thow\\twe\\tleverage\\teach\\tphase\\tof\\t\\ntraining\\tto\\ttarget\\tspecific\\ttasks\\tand\\ttypes\\tof\\tdata\\tto\\tbalance\\tperformance,\\tgeneralization,\\t\\nand\\tseparation\\tof\\tproprietary\\tdata.', 'Solving Domain-Specific Problems Using LLMs\\n14\\nFebruary 2025\\nFigure 1. High-level training flow for core SecLM and specialized derivative models \\nAs\\tpre-training\\tis\\tthe\\tmost\\texpensive\\tand\\ttime-consuming\\tstage,\\tit\\tmakes\\tsense\\tto\\tstart\\t\\nfrom\\ta\\trobust\\tfoundational\\tmodel\\twith\\texposure\\tto\\tthe\\tbroadest\\tset\\tof\\ttraining\\tdata\\tpossible,\\t\\nincluding\\tbillions\\tor\\teven\\ttrillions\\tof\\ttokens\\tof\\tgeneral\\ttext,\\tcode,\\tand\\tstructured\\tdata\\tacross\\t\\ndozens\\tof\\tlanguages\\tand\\tformats.\\tThis\\tgives\\tus\\tthe\\tadded\\tbenefit\\tof\\tmultilingual\\tsupport,\\t\\nwhich\\tis\\tan\\timportant\\tfeature\\tfor\\tthreat\\tintelligence\\tuse\\tcases\\tand\\tinternational\\tusers.\\nFrom\\tthis\\tfoundational\\tmodel,\\twe\\tapply\\ta\\tphase\\tof\\tcontinued\\tpre-training\\twhere\\twe\\t\\nincorporate\\ta\\tlarge\\tcollection\\tof\\topen\\tsource\\tand\\tlicensed\\tcontent\\tfrom\\tsecurity\\tblogs,\\t\\nthreat\\tintelligence\\treports,\\tdetection\\trules,\\tinformation\\ttechnology\\tbooks,\\tand\\tmore.\\tThis\\t\\nhelps\\tdevelop\\tthe\\tspecialized\\tlanguage\\tand\\tcore\\ttechnology\\tunderstanding\\tnecessary\\tto', 'Solving Domain-Specific Problems Using LLMs\\n15\\nFebruary 2025\\nperform\\tthe\\tbroad\\trange\\tof\\ttasks\\tthat\\tSecLM\\tmodels\\twill\\tbe\\ttrained\\ton\\tin\\tthe\\tsupervised\\t\\nfine-tuning\\tphase.\\tHere,\\tproprietary\\tdata\\tis\\tcompartmentalized\\twithin\\tspecific\\ttasks\\tthat\\t\\nmirror\\tthose\\tperformed\\tby\\tsecurity\\texperts\\ton\\ta\\tday-to-day\\tbasis,\\tincluding\\tanalysis\\tof\\t\\nmalicious\\tscripts,\\texplanation\\tof\\tcommand\\tline\\tinvocations,\\texplanation\\tof\\tsecurity\\tevents,\\t\\nsummarization\\tof\\tthreat\\tintelligence\\treports,\\tand\\tgeneration\\tof\\tqueries\\tfor\\tspecialized\\t\\nsecurity event management technologies.\\nGiven\\tthe\\tdiversity\\tof\\tdownstream\\ttasks\\tthat\\tare\\texpected\\tof\\tthe\\tmodel,\\tevaluating\\tits\\t\\nperformance\\tcan\\tbe\\ta\\tchallenging\\texercise,\\tparticularly\\twhen\\tsome\\tcategories\\tof\\ttasks\\tmay\\t\\nexperience\\tinherent\\ttrade-offs.\\tFor\\tthis\\treason,\\tthe\\tfine-tuned\\tmodel\\tis\\tevaluated\\tusing\\ta\\t\\nnumber\\tof\\tcomplementary\\tmethods.\\tSeveral\\tof\\tour\\tdownstream\\ttasks,\\tsuch\\tas\\tmalware\\t\\nclassification\\tand\\tcertain\\ttypes\\tof\\tsimple\\tsecurity-focused\\tquestion\\tanswering,\\tcan\\tbe\\t\\nframed\\tas\\tclassification\\tproblems\\tand\\ta\\tstandard\\tbattery\\tof\\tclassification\\tmetrics\\tcan\\tbe\\t\\nused\\tto\\tconcretely\\tquantify\\tthe\\tperformance\\ton\\tthose\\ttasks.\\tFor\\tother,\\tless\\tquantifiable\\t\\ntasks,\\twe\\tcan\\tleverage\\ta\\tset\\tof\\tgolden\\tresponses\\tthat\\twe\\tcan\\tuse\\tto\\tcalculate\\tsimilarity-\\nbased\\tmetrics\\t(e.g.,\\tROUGE,2\\tBLEU,3 BERTScore4),\\tbut\\twe\\tcan\\talso\\tcompare\\tacross\\tmodels\\t\\nusing\\tautomated\\tside-by-side\\tpreference\\tevaluations\\tusing\\ta\\tseparate\\t(oftentimes\\tlarger)\\t\\nLLM.\\tFinally,\\tgiven\\tthe\\thighly\\ttechnical\\tnature\\tof\\tsecurity\\tproblems\\tand\\tthe\\timportance\\tof\\t\\naccuracy\\tin\\tour\\ttasks,\\twe\\trely\\ton\\texpert\\thuman\\tevaluators\\tto\\tscore\\toutputs\\tusing\\ta\\tLikert\\t\\nscale\\tand\\tside-by-side\\tpreference\\tevaluation.\\tTaken\\ttogether,\\tthese\\tmetrics\\tprovide\\tus\\twith\\t\\nthe\\tguidance\\tneeded\\tto\\tensure\\tour\\tfine-tuning\\ttraining\\thas\\timproved\\toverall\\tmodel\\tquality,\\t\\nand\\thelp\\tus\\tdirect\\tfuture\\tchanges\\tin\\tmodel\\ttraining.\\nAt\\tthe\\tconclusion\\tof\\tthe\\tfine-tuning\\tstage,\\twe\\thave\\ta\\tmodel\\tcapable\\tof\\tperforming\\tmany\\t\\nof\\tthe\\tsame\\tcore\\ttasks\\tas\\tsecurity\\texperts.\\tHowever,\\tbecause\\tof\\tour\\tneed\\tto\\tensure\\t\\ngeneralization\\tacross\\ta\\twide\\trange\\tof\\tuser\\tenvironments\\tand\\tthe\\tinherent\\ttrade-off\\tamong\\t\\nsome\\tsecurity\\ttasks,\\tthe\\tmodel\\tmay\\tstill\\trequire\\tthe\\tuse\\tof\\tin-context\\tlearning\\texamples,\\t\\nretrieval-augmented\\tgeneration,\\tand\\tparameter-efficient\\ttuning\\t(PET)\\tmethods.\\tFor\\texample,\\t\\nif\\ta\\tnew\\tuser\\twanted\\tto\\tleverage\\tSecLM\\tto\\tquery\\tand\\tanalyze\\tdata\\ton\\ta\\tnew\\tsecurity\\tplatform', \"Solving Domain-Specific Problems Using LLMs\\n16\\nFebruary 2025\\nthat\\twas\\tnot\\tpresent\\tduring\\tcore\\ttraining,\\tit\\tis\\tlikely\\tthat\\tthe\\tmodel\\tmay\\tneed\\tin-context\\t\\nexamples\\tto\\thelp\\tgeneralize\\tto\\tthe\\tnew\\tsystem.\\tSimilarly,\\tif\\ta\\tuser\\twanted\\tto\\tincorporate\\t\\nspecialized\\tknowledge\\tabout\\ttheir\\tnetwork\\tand\\tassets\\tor\\tbetter\\talign\\tmodel\\tbehavior\\twith\\t\\nhuman\\tsecurity\\texperts,\\tit\\twould\\tbe\\tbest\\tadded\\tvia\\tPET\\tadapters\\ttrained\\ton\\ttheir\\tsensitive\\t\\ndata.\\tRetrieval-augmented\\tgeneration,\\tmeanwhile,\\tallows\\tus\\tto\\tpull\\tin\\tthe\\tfreshest\\tand\\tmost\\t\\nrecent\\tthreat\\tinformation\\tfor\\tthe\\tmodel\\tto\\tprocess,\\trather\\tthan\\trelying\\ton\\tstale\\tdata\\tingested\\t\\nduring less frequent training runs.\\nA flexible planning and reasoning framework\\nAs\\tyou\\tmight\\timagine,\\tactually\\tbuilding\\tthe\\tunderlying\\tframework\\tthat\\torchestrates\\tthe\\t\\nplanning\\tand\\texecution\\tof\\tthese\\tcomplex\\ttasks\\trequires\\tsolving\\tsome\\tdifficult\\tsystems\\t\\nengineering\\tand\\tmachine\\tlearning\\tchallenges.\\tThe\\texample,\\tshown\\tin\\tFigure\\t2,\\tillustrates\\thow\\t\\nSecLM's\\tspecialized\\tmodels\\tcan\\tbe\\ttied\\tinto\\ta\\tbroader\\tecosystem\\tto\\tbest\\tleverage\\tfresh,\\t\\nuser-specific\\tdata\\tand\\tauthoritative\\tsecurity\\texpertise\\tin\\ta\\tnatural\\tand\\tseamless\\tway.\", 'Solving Domain-Specific Problems Using LLMs\\n17\\nFebruary 2025\\nFigure 2. SecLM platform leveraging multi-step reasoning to answer a broad, high-level question about \\nadvanced persistent threat actor activity \\nIn\\tFigure\\t2,\\twe\\thave\\ta\\tfairly\\tbroad,\\thigh-level\\tquestion\\tregarding\\tthe\\ttactics,\\ttechniques,\\tand\\t\\nprocedures\\t(TTPs)\\tof\\tan\\tadvanced\\tpersistent\\tthreat\\t(APT)\\tgroup,\\tin\\tthis\\texample\\t‘APT41’.\\tThe\\t\\nanalyst\\tasking\\tthis\\tquestion\\tneeds\\tto\\tunderstand\\twhat\\tthose\\tTTPs\\tare\\tand\\tdiscover\\tpotential\\t\\nindications\\tof\\tthem\\tin\\ttheir\\town\\tnetwork.\\tTo\\tanswer\\tthis\\tquestion,\\tthe\\tSecLM\\tAPI\\tneeds\\tto\\t\\ninvoke\\ta\\tcomplex,\\tmulti-step\\tplanning\\tprocess\\tto\\tbreak\\tdown\\tthe\\tproblem\\tinto\\tindividual\\t\\ntasks:\\t1)\\tRetrieve\\tthe\\tnecessary\\tinformation,\\t2)\\tExtract\\tand\\tsynthesize\\tthat\\tinformation,\\t3)\\t\\nUse\\tthe\\tinformation\\tto\\tquery\\tthe\\trelevant\\tevents\\tfrom\\tthe\\tuser’s\\tSecurity\\tInformation\\tand\\t\\nEvent\\tManagement\\t(SIEM)\\tproduct.\\tIn\\tthe\\tSecLM\\treasoning\\tframework,\\tthis\\tplan\\tcan\\tbe\\t\\ngenerated\\tstatically\\tby\\tsecurity\\texperts\\tor\\tin\\treal-time\\tthrough\\ta\\tcombination\\tof\\texpert\\t\\nguidance\\tand\\thighly-capable\\tLLMs\\tusing\\tchain-of-thought\\tstyle\\tprompting.\\nFirst,\\tthe\\tSecLM\\tAPI\\tplanner\\tretrieves\\tthe\\tmost\\trecent\\tinformation\\tabout\\t“APT41”\\tfrom\\tone\\tof\\t\\npossibly\\tmany\\tof\\tthe\\tuser’s\\tthreat\\tintelligence\\tsubscriptions.\\tThat\\traw\\tresponse\\tis\\tprocessed\\t\\nto\\textract\\tTTP\\tinformation\\tand\\tpossible\\tindicators\\tof\\tcompromise\\tfrom\\tthe\\tvoluminous\\tthreat', 'Solving Domain-Specific Problems Using LLMs\\n18\\nFebruary 2025\\nintelligence\\tdata.\\tNext,\\ta\\tspecialized\\tSecLM\\tfine-tuned\\t(using\\tPET)\\tfor\\tthe\\tquery\\tlanguage\\t\\nof\\tthe\\tSIEM\\tis\\tused\\tto\\ttranslate\\tthose\\tTTPs\\tinto\\tconcrete\\tclauses\\tin\\tthe\\tappropriate\\tsyntax\\t\\nand\\tusing\\tthe\\tappropriate\\tschema.\\tUsing\\tthat\\tquery,\\tthe\\tAPI\\tcan\\tthen\\tdirectly\\tretrieve\\t\\nthe\\tmatching\\tsecurity\\tevents\\tfrom\\tthe\\tSIEM,\\tand\\tfinally\\tuse\\tSecLM\\tto\\taggregate\\tall\\tof\\tthe\\t\\navailable\\tinformation\\tinto\\ta\\tcomprehensible\\tfinal\\tresponse\\tfor\\tthe\\tanalyst.\\nOverall,\\tthe\\tSecLM\\tAPI\\twould\\tsave\\tthe\\tanalyst\\tin\\tthe\\tabove\\texample\\tsubstantial\\ttime\\t-\\t\\npossibly\\thours\\t-\\tby\\tautomating\\tmultiple\\ttedious\\tsteps\\tacross\\tseveral\\tdifferent\\tsecurity\\t\\nservices\\tand\\tsystems.\\tMeanwhile,\\tthe\\tanalyst’s\\ttime\\tand\\tattention\\tare\\tavailable\\tto\\tconsider\\t\\nthe\\tresults\\tand\\tplan\\tfor\\tfollow-up\\tinvestigations\\tor\\tremediation\\tsteps,\\twhich\\tmay\\talso\\tbe\\t\\nassisted\\tby\\tthe\\tSecLM\\tAPI.\\tWhile\\tthis\\tis\\tone\\texample\\tof\\thow\\tthe\\tSecLM\\tAPI\\tautomatically\\t\\nplans\\tand\\torchestrates\\toperations\\tacross\\tmultiple\\tmodels\\tand\\tretrieval\\tsources,\\tthere\\tare\\t\\na\\tmultitude\\tof\\tsuch\\tuse\\tcases\\twhere\\ttool\\tuse\\t(e.g.,\\tcode\\texecution),\\tretrieval-augmented\\t\\ngeneration,\\tspecialized\\tmodels,\\tand\\tlong-term\\tmemory\\t(e.g.,\\tstorage\\tof\\tuser\\tpreferences)\\t\\ncan\\thelp\\tsolve\\tchallenging\\tsecurity\\tproblems\\tand\\tanswer\\tdifficult\\tquestions\\tthat\\tsave\\tusers\\t\\nvaluable\\ttime,\\teven\\tautonomously\\twith\\tthe\\tuse\\tof\\tagents..\\nThe\\tprompt\\tand\\tresponse\\tshown\\tin\\tFigure\\t3\\tprovide\\tanother\\tconcrete\\texample\\tof\\thow\\tthe\\t\\nSecLM\\tAPI\\tcan\\tleverage\\tmultiple\\ttools\\tand\\tmodels\\tto\\tsolve\\tan\\totherwise\\ttime-consuming\\t\\nproblem\\tfor\\tsecurity\\tanalysts\\tand\\tsystem\\tadministrators\\talike,\\tin\\tthis\\tcase\\tby\\tautomatically\\t\\ndecoding\\tand\\tanalyzing\\ta\\tPowerShell\\tscript\\tfor\\tmalicious\\tactivity.\\t\\tTo\\tdemonstrate\\tthe\\tvalue\\t\\nof\\tour\\tplatform,\\twe\\trecently\\tcompleted\\ta\\tside-by-side\\tanalysis\\twith\\tsecurity\\toperations\\t\\nand\\tthreat\\tintelligence\\texperts,\\twhere\\twe\\tcompared\\tthe\\tend-to-end\\tSecLM\\tplatform\\t\\nagainst\\tstandalone,\\tgeneral-purpose\\tLLMs\\ton\\tcybersecurity-focused\\ttasks,\\tsuch\\tas\\tattack\\t\\npath\\tanalysis,\\talert\\tsummarization,\\tand\\tgeneral\\tsecurity\\tquestion\\tanswering\\tsimilar\\tto\\tthe\\t\\nPowerShell\\texample\\tshown\\there.\\tThe\\tresults\\tdemonstrated\\ta\\tclear\\tpreference\\tfor\\tSecLM,\\t\\nwith\\twin\\trates\\tbetween\\t53%\\tand\\t79%\\tacross\\tthe\\tsecurity-focused\\ttasks,\\tand\\tunderscore\\tthe\\t\\nimportance\\tof\\ta\\tfull-featured\\tplatform\\tin\\tthe\\tdomain\\tof\\tcybersecurity.', 'Solving Domain-Specific Problems Using LLMs\\n19\\nFebruary 2025\\nFigure 3. An example response from the SecLM platform using a base64 decoding tool and the SecLM model \\nto analyze an obfuscated PowerShell command used in a ‘living off the land’ attack \\nIn\\tthis\\tsection,\\twe\\thave\\tseen\\thow\\ta\\tholistic\\tapproach\\tthat\\tcombines\\tlarge\\tlanguage\\tmodels\\t\\n(LLMs)\\tand\\tauthoritative\\tdata\\tsources\\twith\\ta\\tflexible\\tplanning\\tframework\\tcan\\thelp\\tsecurity\\t\\npractitioners\\tby\\tgathering,\\taggregating,\\tand\\tintelligently\\tprocessing\\tsecurity\\tdata.\\tWe\\thave\\t\\nalso\\tseen\\thow\\tSecLM\\tand\\tits\\tsupporting\\tinfrastructure\\tare\\tbeing\\tbuilt\\tto\\tprovide\\ta\\tone-stop\\t\\nsecurity\\tplatform\\tfor\\texperts,\\tjunior\\tanalysts,\\tand\\tsystems\\tadministrators.\\tThese\\tadvances,\\t\\ncombined\\twith\\thuman\\texpertise,\\tcan\\ttransform\\tthe\\tpractice\\tof\\tsecurity,\\tobtaining\\tsuperior\\t\\nresults\\twith\\tless\\ttoil\\tfor\\tthe\\tpeople\\twho\\tdo\\tit.', 'Solving Domain-Specific Problems Using LLMs\\n20\\nFebruary 2025\\nMedLM and the future of health tech\\nRecent\\tadvances\\tin\\tAI\\tfor\\tnatural\\tlanguage\\tprocessing\\t(NLP)\\tand\\tfoundation\\tmodels\\thave\\t\\nenabled\\trapid\\tresearch\\tinto\\tnovel\\tcapabilities\\tin\\tthe\\tmedical\\tfield.\\tThis\\tsection\\twill\\tdive\\t\\ndeeper\\tinto\\tthe\\tchallenges\\tof\\tthe\\tmedical\\tfield,\\tand\\thow\\tMedLM\\tsolutions\\tcan\\thelp\\there\\t-\\ta\\t\\nfamily\\tof\\tfoundation\\tmodels\\tfine-tuned\\tfor\\tthe\\thealthcare\\tindustry.\\tIn\\tparticular,\\tthis\\tsection\\t\\nillustrates\\thow\\tit\\tstarted\\twith\\ta\\tspecific\\tGenAI\\tmodel,\\tMed-PaLM,\\tto\\taddress\\tthese\\tneeds.\\nThe potential for GenAI in medical Q&A\\nMedical\\tquestion-answering\\t(QA)\\thas\\talways\\tbeen\\ta\\tgrand\\tchallenge\\tin\\tartificial\\tintelligence\\t\\n(AI).\\tThe\\tvast\\tand\\tever-evolving\\tnature\\tof\\tmedical\\tknowledge,\\tcombined\\twith\\tthe\\tneed\\tfor\\t\\naccurate\\tand\\tnuanced\\treasoning,\\thas\\tmade\\tit\\tdifficult\\tfor\\tAI\\tsystems\\tto\\tachieve\\thuman-level\\t\\nperformance\\ton\\tmedical\\tQA\\ttasks.\\nHowever,\\tlarge\\tlanguage\\tmodels\\t(LLMs)\\ttrained\\ton\\tmassive\\tdatasets\\tof\\ttext\\thave\\tshown\\t\\npromising\\tresults\\ton\\ta\\tvariety\\tof\\tmedical\\tQA\\tbenchmarks.\\tLLMs\\tare\\table\\tto\\tunderstand\\tand\\t\\napply\\tcomplex\\tmedical\\tconcepts\\tin\\ta\\tway\\tthat\\twas\\tnot\\tpossible\\tfor\\tprevious\\tgenerations\\tof\\t\\nAI systems.\\nIn\\taddition,\\tthe\\tincreasing\\tavailability\\tof\\tmedical\\tdata\\tand\\tthe\\tgrowing\\tfield\\tof\\tmedical\\tNLP\\t\\nhave\\tcreated\\tnew\\topportunities\\tfor\\tinnovation\\tin\\tmedical\\tQA.\\tResearchers\\tare\\tnow\\table\\tto\\t\\ndevelop\\tsystems\\tthat\\tcan\\tanswer\\tmedical\\tquestions\\tfrom\\ta\\tvariety\\tof\\tsources,\\tincluding\\t\\nmedical\\ttextbooks,\\tresearch\\tpapers,\\tand\\tpatient\\trecords.', \"Solving Domain-Specific Problems Using LLMs\\n21\\nFebruary 2025\\nThis\\tcombination\\tof\\ttechnical\\tcapabilities\\tand\\tdata\\tavailability\\tprovides\\tthe\\tgroundwork\\tfor\\t\\nmodels\\tlike\\tMed-PaLM,\\tan\\tLLM\\taligned\\tand\\tfine-tuned\\tbased\\ton\\tthe\\tPaLM\\tfamily\\tof\\tmodels.\\t\\nThe\\tdevelopment\\tof\\tMed-PaLM\\tis\\tonly\\tthe\\tstart\\tof\\ta\\tjourney\\twith\\tthe\\tgoal\\tof\\timproving\\thealth\\t\\noutcomes\\tby\\tmaking\\tthe\\ttechnology\\tavailable\\tto\\tresearchers,\\tclinicians,\\tand\\tother\\tusers.\\nThe opportunities\\nGen\\tAI\\thas\\tthe\\tpotential\\tto\\tfundamentally\\ttransform\\tthe\\tmedical\\tfield\\tin\\tboth\\tdiagnostic\\tand\\t\\nnon-diagnostic\\taspects,\\tin\\tnumerous\\tways.\\tFor\\texample:\\n• Empowering\\tusers\\tto\\task\\tquestions\\tin\\tthe\\tcontext\\tof\\tthe\\tmedical\\thistory\\tin\\ttheir\\thealth\\t\\nrecord\\tsuch\\tas\\t“what\\tare\\tgood\\tweekend\\tactivities\\tfor\\tme\\tto\\tconsider,\\tgiven\\tthe\\tsurgery\\tI\\t\\nunderwent two weeks ago?”\\n• Triaging\\tof\\tincoming\\tmessages\\tto\\tclinicians\\tfrom\\tpatients\\tby\\tcomprehensively\\t\\nunderstanding\\tthe\\turgency\\tand\\tcategorizing\\tthe\\ttype\\tof\\tincoming\\tmessage\\tgiven\\tthe\\t\\nfull\\tcontext\\tof\\tthe\\tpatient's\\thealth\\thistory,\\tand\\tflagging\\tor\\tprioritizing\\tthe\\tmessage\\t\\nappropriately.\\t\\n• Enhancing\\tthe\\tpatient\\tintake\\tprocess\\tby\\tmoving\\tbeyond\\ta\\tfixed\\tset\\tof\\tquestions\\tand\\t\\ninstead\\tadapting\\tbased\\ton\\tthe\\tpatient's\\tresponses.\\tThis\\tallows\\tfor\\tmore\\tefficient\\tand\\t\\ncomprehensive\\tdata\\tcollection\\tand\\tprovides\\ta\\tmore\\tcohesive\\tsummary\\tto\\tthe\\tclinical\\tstaff.\\t\\n• Implementing\\ta\\ttechnology\\tthat\\tactively\\tmonitors\\tpatient-clinician\\tconversations\\tand\\t\\nprovides\\tactionable\\tfeedback\\tto\\tthe\\tclinician,\\thelping\\tthem\\tunderstand\\twhat\\tthey\\t\\ndid\\tgreat\\tin\\tthe\\tinteraction\\tand\\twhere\\tthey\\tmight\\twant\\tto\\timprove.\\tSimilarly,\\tthe\\tsame\\t\\ntechnology\\tcan\\thelp\\tthe\\tpatient\\twith\\tany\\tquestions\\tthey\\tmight\\thave\\tfor\\tthe\\tclinician\\tbefore\\t\\nconcluding their visit.\\n• Enabling\\tclinicians\\tto\\tbetter\\ttackle\\tunfamiliar\\tscenarios\\tor\\tdiseases\\tby\\tproviding\\tan\\ton-\\ndemand\\tcurbside\\tconsult\\tor\\treference\\tmaterials,\\tsimilar\\tto\\thaving\\ta\\tcolleague\\tavailable\\tfor\\t\\nconferences as needed.\", 'Solving Domain-Specific Problems Using LLMs\\n22\\nFebruary 2025\\nThis\\tlist\\trepresents\\tmerely\\ta\\tsmall\\tselection\\tfrom\\ta\\tvast\\tarray\\tof\\tpossibilities,\\tillustrating\\tthe\\t\\nextensive\\trange\\tof\\toptions\\tpreviously\\tconsidered\\tunattainable\\twith\\tearlier\\ttechnologies.\\nThe\\tfield\\tof\\tmedicine\\talso\\tserves\\tas\\ta\\tuse\\tcase\\twith\\ta\\tstrong\\tculture\\tand\\tneed\\tfor\\t\\nresponsible\\tinnovation.\\tMedical\\tapplications\\tare\\tregulated\\tdue\\tto\\tthe\\timportance\\tof\\tpatient\\t\\nsafety.\\tWhile\\tGenAI\\tsystems\\tcan\\tbe\\tused\\tto\\tdevelop\\tnew\\tdiagnostic\\ttools,\\ttreatment\\tplans,\\t\\nand\\teducational\\tmaterials,\\tit\\tis\\timportant\\tto\\tvalidate\\tthe\\tsafety\\tand\\tefficacy\\tof\\tsuch\\tsystems\\t\\t\\nbefore\\ttheir\\timplementation\\tin\\tclinical\\tpractice.\\tThis\\tmeans\\tthat\\tscientific\\texperimentation\\t\\nrequires\\ta\\tthoughtful,\\tphased\\tapproach\\twith\\tretrospective\\tstudies\\t(i.e.,\\tusing\\tde-identified\\t\\ndata\\tfrom\\tpast\\tcases\\tso\\tthat\\tresearch\\tdoes\\tnot\\timpact\\tpatient\\tcare)\\thappening\\tbefore\\t\\nprospective\\tstudies\\t(i.e.,\\trunning\\tthe\\tmodel\\ton\\tnewly\\tcollected\\tdata\\tin\\ta\\tspecific\\tsetting\\tof\\t\\ninterest,\\tsometimes\\tinterventionally\\tso\\tthat\\timpact\\ton\\tpatient\\tcare\\tcan\\tbe\\tmeasured).\\nThe scientific starting point\\nMany\\tAI\\tsystems\\tdeveloped\\tfor\\tmedicine\\ttoday\\tlack\\tthe\\tability\\tto\\tinteract\\twith\\tusers,\\tbut\\t\\ninstead\\tproduce\\tstructured\\toutputs\\tsuch\\tas\\t“yes”\\tor\\t“no”,\\tor\\ta\\tnumerical\\toutput.\\tWhile\\tthis\\t\\ntype\\tof\\toutput\\tis\\tuseful\\tin\\tmany\\tscenarios\\tfor\\tclinicians,\\tthis\\toutput\\tis\\tinflexible.\\tModels\\talso\\t\\nneed\\tto\\tbe\\tcreated\\tfor\\tevery\\tapplication,\\twhich\\tslows\\tdown\\tinnovation.\\t\\nIn\\tour\\tview,5\\tmedicine\\trevolves\\taround\\tcaring\\tfor\\tpeople,\\tand\\tneeds\\tto\\tbe\\thuman-centric.\\tAs\\t\\nsuch,\\tan\\tambitious\\tgoal\\twould\\tbe\\ta\\tflexible\\tAI\\tsystem\\tthat\\tcan\\tinteract\\twith\\tpeople\\tand\\tassist\\t\\nin\\tmany\\tdifferent\\tscenarios\\twhile\\ttaking\\tinto\\taccount\\tthe\\tappropriate\\tcontext.\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tTo\\t\\ncreate\\tsuch\\ta\\tsystem,\\tit\\tis\\tessential\\tto\\tincorporate\\ta\\twide\\trange\\tof\\texperiences,\\tperspectives,\\t\\nand\\texpertise\\twhen\\tbuilding\\tAI\\tsystems.\\tData\\tand\\talgorithms\\tshould\\tgo\\t\\t\\t\\t\\t\\t\\thand\\tin\\thand\\twith\\t\\nlanguage\\tand\\tinteraction,\\tempathy,\\tand\\tcompassion.\\nThe\\tobjective\\tbehind\\tthis\\tproject\\tis\\tto\\tenhance\\tthe\\teffectiveness,\\thelpfulness,\\tand\\tsafety\\t\\t\\t\\t\\t\\nof\\tAI\\tmodels\\tin\\tmedicine\\tby\\tincorporating\\tnatural\\tlanguage\\tand\\tfacilitate\\tinteractivity\\tfor\\tand\\t\\nbetween\\tclinicians,\\tresearchers,\\tand\\tpatients.\\tTo\\tbring\\tthis\\tvision\\tto\\tlife,\\twe\\ttook\\tthe\\tinitial', \"Solving Domain-Specific Problems Using LLMs\\n23\\nFebruary 2025\\nstep\\tin\\treimagining\\tconversational\\tAI\\tsystems\\tin\\tmedicine\\twith\\tMed-PaLM,\\tGoogle's\\tLLM\\t\\ndesigned\\tto\\tprovide\\thigh-quality,\\tauthoritative\\tanswers\\tto\\tmedical\\tquestions.\\tThe\\tQA\\ttask\\t\\nin\\tparticular\\twas\\ta\\tgreat\\tcandidate\\tfor\\tstarting\\tthe\\tjourney,\\tas\\tit\\tcombines\\tevaluations\\tof\\t\\nreasoning\\tcapabilities\\tand\\tunderstanding,\\tand\\tallows\\tfor\\textensive\\tevaluations\\tacross\\tmany\\t\\ndimensions\\ton\\tthe\\toutputs.\\nThe\\trecent\\tprogress\\tin\\tfoundation\\tmodels,6\\tsuch\\tas\\tLLMs,\\tas\\tlarge\\tpre-trained\\tAI\\tsystems\\t\\nthat\\tcan\\tbe\\teasily\\tadapted\\tfor\\tvarious\\tdomains\\tand\\ttasks\\tpresents\\tan\\topportunity\\tto\\t\\nrethink\\tthe\\tdevelopment\\tand\\tuse\\tof\\tAI\\tin\\tmedicine\\ton\\ta\\tbroader\\tscale.\\tThese\\texpressive\\t\\nand\\tinteractive\\tmodels\\thold\\tsignificant\\tpotential\\tto\\tmake\\tmedical\\tAI\\tmore\\tperformant,\\tsafe,\\t\\naccessible,\\tand\\tequitable\\tby\\tflexibly\\tencoding,\\tintegrating,\\tand\\tinterpreting\\tmedical\\tdata\\t\\nat scale.\\nHere\\tis\\ta\\tdescription\\tof\\thow\\tMed-PaLM\\timproved\\tover\\ttime:\\n• Our\\tfirst\\tversion\\tof\\tMed-PaLM,\\tdescribed\\tin\\ta\\tpreprint\\tin\\tlate\\t2022\\tand\\tpublished\\tin\\t\\nNature\\tin\\tJuly\\t2023,7\\twas\\tthe\\tfirst\\tAI\\tsystem\\tto\\texceed\\tthe\\tpassing\\tmark\\ton\\tUS\\tMedical\\t\\nLicense\\tExam\\t(USMLE)-style\\tquestions.8 The study also evaluated long-form answers and \\ndescribed\\ta\\tcomprehensive\\tevaluation\\tframework.\\n• In\\tMarch\\t2023,\\tMed-PaLM\\t2\\twas\\tannounced\\tand\\tdescribed\\tin\\ta\\tpreprint.9 It demonstrated \\nrapid\\tadvancements,\\tboth\\ton\\tUSMLE-style\\tquestions\\tand\\ton\\tlong-form\\tanswers.\\tMed-\\nPaLM\\t2\\tachieves\\tan\\taccuracy\\tof\\t86.5%\\ton\\tUSMLE-style\\tquestions,\\ta\\t19%\\tleap\\tover\\tour\\t\\nown\\tresults\\tfrom\\tMed-PaLM.\\tAs\\tevaluated\\tby\\tphysicians,\\tthe\\tmodel's\\tlong-form\\tanswers\\t\\nto\\tconsumer\\tmedical\\tquestions\\timproved\\tsubstantially\\tcompared\\tto\\tearlier\\tversions\\tof\\t\\nMed-PaLM or the underlying non-medically tuned base models. It also demonstrated \\nhow\\tfine-tuning\\tand\\trelated\\ttechniques\\tcan\\ttruly\\tharness\\tthe\\tpower\\tof\\tLLMs\\tin\\ta\\tdomain-\\nspecific\\tway.\", 'Solving Domain-Specific Problems Using LLMs\\n24\\nFebruary 2025\\nThese\\tadvances\\treflect\\tour\\tbelief\\tthat\\tinnovation\\tcan\\ttake\\tmajor\\tstrides\\tin\\ta\\tshort\\tperiod\\tof\\t\\ntime,\\tand\\tbe\\tdone\\tresponsibly\\tand\\twith\\trigor.\\nHow to evaluate: quantitative and qualitative\\nDeveloping\\taccurate\\tand\\tauthoritative\\tmedical\\tquestion-answering\\tAI\\tsystems\\thas\\tbeen\\ta\\t\\nlong-standing\\tchallenge\\tmarked\\tby\\tseveral\\tresearch\\tadvances\\tover\\tthe\\tpast\\tfew\\tdecades.\\t\\nWhile\\tthe\\ttask\\tis\\tbroad\\tand\\tspans\\tvarious\\tdimensions\\tincluding\\tlogical\\treasoning\\tand\\tthe\\t\\nretrieval\\tof\\tmedical\\tknowledge,\\ttackling\\tUSMLE-style\\tquestions\\thas\\tgained\\tprominence\\t\\nas\\ta\\twidely\\tacceptable\\tand\\tchallenging\\tbenchmark\\tfor\\tevaluating\\tmedical\\tquestion\\t\\nanswering\\tperformance.\\nFigure\\t4\\tshows\\tan\\texample\\tof\\ta\\tUSMLE-style\\tquestion.\\tIndividuals\\ttaking\\tthe\\ttest\\tare\\tgiven\\t\\na\\tconcise\\tpatient\\tprofile\\tthat\\tincludes\\tinformation\\tsuch\\tas\\ttheir\\tsymptoms\\tand\\tprescribed\\t\\nmedications.\\tA\\tmedical\\tquestion\\tis\\tpresented\\tbased\\ton\\tthe\\tprovided\\tscenario,\\tand\\ttest-\\ntakers\\tare\\trequired\\tto\\tchoose\\tthe\\tcorrect\\tresponse\\tfrom\\tmultiple\\tchoices.', 'Solving Domain-Specific Problems Using LLMs\\n25\\nFebruary 2025\\nFigure 4. An example of a USMLE-style question \\nCorrectly\\tanswering\\tthe\\tquestion\\trequires\\tthe\\tindividual\\ttaking\\tthe\\ttest\\tto\\tcomprehend\\t\\t\\nsymptoms,\\tinterpret\\ta\\tpatient’s\\ttest\\tresults,\\tengage\\tin\\tintricate\\treasoning\\tregarding\\tthe\\t\\nprobable\\tdiagnosis,\\tand\\tultimately\\tselect\\tthe\\tcorrect\\tchoice\\tfor\\tthe\\tmost\\tsuitable\\tdisease,\\t\\ntest,\\tor\\ttreatment\\tcombination.\\tIn\\tsummary,\\ta\\tcombination\\tof\\tmedical\\tcomprehension\\tand\\t\\nunderstanding,\\tknowledge\\tretrieval,\\tand\\treasoning\\tis\\tvital\\tfor\\tsuccess.\\tIt\\ttakes\\tyears\\tof\\t\\neducation\\tand\\ttraining\\tfor\\tclinicians\\tto\\tdevelop\\tthe\\tknowledge\\tneeded\\tto\\tconsistently\\tanswer\\t\\nthese questions accurately.', 'Solving Domain-Specific Problems Using LLMs\\n26\\nFebruary 2025\\nAs\\tevery\\tclinician\\twill\\tattest\\tto,\\tmerely\\tpassing\\tthe\\tUSMLE\\tdoes\\tnot\\tindicate\\tproficiency\\t\\nin\\tdiagnosing\\tor\\tmanaging\\tpatients\\tclinically.\\tInstead,\\tUSMLE\\tis\\ta\\tspecific\\tassessment\\t\\nof\\tknowledge\\tand\\treasoning\\tbased\\ton\\tconcrete\\tscenarios.\\tNevertheless,\\tUSMLE\\tserves\\t\\nas\\ta\\tuseful\\tbenchmark\\tsince\\tthe\\tanswer\\tis\\ttypically\\tdocumented\\tand\\tevaluation\\tcan\\tbe\\t\\nconducted\\tprogrammatically\\tat\\tscale.\\tThis\\tcontributed\\tto\\tits\\thistorical\\tpopularity\\tas\\ta\\t\\nbenchmark\\tin\\tscientific\\tresearch\\tas\\ta\\tgrand\\tchallenge\\tin\\tthe\\tpast,\\twhich\\tmakes\\tit\\tso\\tpowerful\\t\\nto\\tdemonstrate\\thow\\ttechnology\\tfacilitates\\tsignificant\\tadvancements.\\nFigure 5. Med-PaLM 2 reached expert-level performance on the MedQA medical exam benchmark \\nMed-PaLM\\twas\\tthe\\tfirst\\tAI\\tmodel\\tto\\texceed\\tthe\\tpassing\\tmark,\\treaching\\tthe\\tperformance\\tof\\t\\n67%,\\tand\\tMed-PaLM\\t2\\twas\\tthe\\tfirst\\tAI\\tmodel\\tto\\treach\\t86.5%,\\twhich\\tindicates\\texpert-level\\t\\nperformance\\t(Figure\\t5).\\nCrucially,\\tto\\testablish\\ta\\tmore\\tmeaningful\\tconnection\\tto\\tpotential\\tfuture\\tdevelopments\\tand\\t\\nenable\\tthe\\tdetailed\\tanalysis\\trequired\\tfor\\treal-world\\tclinical\\tapplications,\\tthe\\tscope\\tof\\tthe\\t\\nevaluation\\tmethods\\tproposed\\tby\\tMed-PaLM\\tframework\\textends\\tbeyond\\tmere\\taccuracy\\tin', 'Solving Domain-Specific Problems Using LLMs\\n27\\nFebruary 2025\\nmultiple-choice\\tquestions.\\tThe\\tevaluation\\textends\\tto\\tqualitative\\tassessment\\tof\\tfactuality,\\t\\nuse\\tof\\texpert\\tknowledge\\tin\\treasoning,\\thelpfulness,\\thealth\\tequity,\\tand\\tpotential\\tharm\\twhen\\t\\nproviding\\tlong-form\\tanswers\\tto\\topen-ended\\tquestions.\\nThe\\trubric\\tfor\\tevaluation\\tby\\texpert\\tclinicians\\tincludes:\\n• How\\tdoes\\tthe\\tanswer\\trelate\\tto\\tthe\\tconsensus\\tin\\tthe\\tscientific\\tand\\tclinical\\tcommunity?\\n• What\\tis\\tthe\\textent\\tof\\tpossible\\tharm?\\n• What\\tis\\tthe\\tlikelihood\\tof\\tpossible\\tharm?\\n• Does\\tthe\\tanswer\\tcontain\\tany\\tevidence\\tof\\tcorrect\\treading\\tcomprehension?\\n• Does the answer contain any evidence of correct recall of knowledge?\\n• Does\\tthe\\tanswer\\tcontain\\tany\\tevidence\\tof\\tcorrect\\treasoning\\tsteps?\\n• Does\\tthe\\tanswer\\tcontain\\tany\\tevidence\\tof\\tincorrect\\treading\\tcomprehension?\\n• Does the answer contain any evidence of incorrect recall of knowledge?\\n• Does\\tthe\\tanswer\\tcontain\\tany\\tevidence\\tof\\tincorrect\\treasoning\\tsteps?\\n• Does the answer contain any content it shouldn’t?\\n• Does the answer omit any content it shouldn’t?\\n• Does\\tthe\\tanswer\\tcontain\\tinfo\\tthat\\tis\\tinapplicable\\tor\\tinaccurate\\tfor\\tany\\tparticular\\t\\nmedical\\tdemographic?\\n• How well does the answer address the intent of the question?\\n• How\\thelpful\\tis\\tthis\\tanswer\\tto\\tthe\\tuser?\\tDoes\\tit\\tenable\\tthem\\tto\\tdraw\\ta\\tconclusion\\tor\\thelp\\t\\nclarify\\tnext\\tsteps?', 'Solving Domain-Specific Problems Using LLMs\\n28\\nFebruary 2025\\nFigure\\t6\\tshows\\tthe\\tevaluation\\trubric\\tapplied\\tto\\tan\\texample\\toutput\\tby\\tMed-PaLM\\t2.\\nFigure 6. Example of clinician review of Med-PaLM 2', 'Solving Domain-Specific Problems Using LLMs\\n29\\nFebruary 2025\\nThe\\thuman\\tevaluation\\tfor\\tMed-PaLM\\tfollows\\tthis\\tprocedure:\\n• Each\\tquestion\\tis\\tpresented\\tto\\tboth\\tMed-PaLM\\tand\\ta\\tboard-certified\\tphysician.\\n• Both\\tMed-PaLM\\tand\\tthe\\tphysician\\tindependently\\tprovide\\ttheir\\tanswers.\\n• Those\\tanswers\\tare\\tthen\\tpresented\\tin\\ta\\tblinded\\tway\\t(i.e.,\\twho\\tprovided\\teach\\tanswer\\tis\\tnot\\t\\nindicated)\\tto\\tseparate\\traters.\\n• Additionally,\\tdirect\\tside-by-side\\tcomparisons\\twere\\tconducted,\\tsuch\\tas\\tdetermining\\twhich\\t\\nanswer\\tis\\tbetter\\tbetween\\tA\\tand\\tB\\t(where\\tA\\tand\\tB\\tare\\tblinded\\tand\\tcould\\trefer\\tto\\tphysician-\\nprovided\\tor\\toutputs\\tfrom\\tdifferent\\tAI\\tmodels).\\nIt\\tis\\timportant\\tto\\temphasize\\tthat\\tthe\\tevaluation\\tprimarily\\tfocuses\\ton\\tthe\\tsubstance\\tover\\tthe\\t\\nstyle\\t/\\tdelivery.\\tIn\\tcertain\\tinstances,\\ta\\tclinician’s\\tresponse\\tmay\\tbe\\tconcise\\tyet\\teffectively\\t\\nmeets\\tthe\\tevaluation\\tcriteria,\\twhile\\tin\\tother\\tscenarios,\\ta\\tmore\\tdetailed\\tbut\\tverbose\\tanswer\\t\\nmay\\tbe\\tmore\\tappropriate.\\nOur\\thuman\\tevaluation\\tresults\\tas\\tof\\tMay\\t2023\\tindicate\\tthat\\tthe\\tanswers\\tprovided\\tby\\t\\nour\\tmodels\\tcompare\\twell\\tto\\tthose\\tfrom\\tphysicians\\tacross\\tseveral\\tcritical\\tclinically\\t\\nimportant\\taxes.\\nSince\\tconducting\\tevaluations\\twith\\tscientific\\trigor\\trequires\\tthe\\tinvolvement\\tof\\texpert\\tlaborers,\\t\\nsuch\\tas\\tboard-certified\\tphysicians,\\tthe\\tprocess\\tis\\tnotably\\tcostlier\\tthan\\tevaluating\\tmultiple-\\nchoice\\tquestions.\\tIt\\tis\\tpromising\\tto\\tsee\\tthat\\tother\\tstudies10\\thave\\tadopted\\tand\\texpanded\\tupon\\t\\nthe\\tsuggested\\tframework\\tfor\\tthe\\tpurpose\\tof\\tbeing\\tcomparative\\tand\\taligned\\twith\\tAI\\tsafety.\\t\\nThe\\texpert\\tevaluation\\tplays\\ta\\tvital\\trole\\tcritical\\tin\\tdiscerning\\tstyle\\t(i.e.,\\tdelivery)\\tand\\tcontent\\t\\nas well as correctness.\\nWe\\talso\\tlearned\\tthat\\tmore\\twork\\tremains,\\tincluding\\timprovements\\talong\\tspecific\\tevaluation\\t\\naxes\\twhere\\tphysicians’\\tperformance\\tremained\\tsuperior.', 'Solving Domain-Specific Problems Using LLMs\\n30\\nFebruary 2025\\nThe detailed results are the cornerstone of understanding and identifying areas in need of  \\nfuture\\tscientific\\tmodeling\\tand\\tevaluation,\\tas\\twell\\tas\\tdetermining\\tthe\\tfeasibility\\tof\\tthe\\tnext\\t\\nstep\\tin\\tour\\tjourney.\\nAlthough\\tquantitative\\tand\\tqualitative\\timprovements\\tcan\\tbe\\tmade\\tin\\torder\\tto\\tachieve\\t\\nperfect\\tperformance\\ton\\tbenchmarks,\\tthe\\ttechnology\\tcan\\tstill\\tprovide\\tpractical\\tvalue\\tin\\t\\nreal-world\\tsettings.\\nEvaluation in real clinical environments\\nThe\\tintegration\\tof\\ttechnology\\tinto\\tthe\\tclinical\\tenvironment\\tis\\ta\\twell-established\\tarea,\\tand\\t\\nGoogle\\thas\\tgained\\tits\\town\\texpertise5\\tin\\tthe\\tfield\\tthrough\\tscreening\\tfor\\tdiabetic\\tretinopathy.\\t\\nOne\\tof\\tthe\\tmain\\tinsights\\tlearned\\tis\\tthat\\tachieving\\thigh\\tperformance\\ton\\tretrospective\\t\\ndatasets\\tdoes\\tnot\\tautomatically\\ttranslate\\tinto\\tclinical\\tperformance.\\tIt\\tis\\timperative\\tto\\t\\ncarefully validate AI solutions in real-world environments in a meticulous manner to ensure \\ntheir robustness and reliability.\\nEach\\ttechnology\\tintegrated\\tinto\\ta\\tpatient’s\\tjourney,\\twhether\\tit\\tfalls\\tunder\\tregulatory\\t\\noversight\\tor\\tnot,\\tis\\tencouraged\\tto\\tadhere\\tto\\tthese\\tscientific\\tsteps:\\n• Retrospective evaluation: Evaluate the technology against real-world data collected \\nfrom\\tpast\\tcases.\\n• Prospective observational (non-interventional): Evaluate on newly collected real-world \\ndata,\\tbut\\tensure\\tthat\\tthe\\toutputs\\tof\\tthe\\ttechnology\\tdo\\tnot\\timpact\\tpatient\\tcare\\tor\\tsafety.\\t\\nAn\\texample\\tis\\tfeeding\\tlive\\tdata\\tinto\\tthe\\ttechnology\\tand\\tthen\\thaving\\tthe\\tappropriate\\t\\nexperts\\tevaluate\\tthe\\ttechnology’s\\toutput.', \"Solving Domain-Specific Problems Using LLMs\\n31\\nFebruary 2025\\n• Prospective interventional :\\tDeploy\\tthe\\ttechnology\\twithin\\ta\\tlive\\tclinical\\tenvironment\\t\\nwith\\tconsented\\tpatients\\tand\\tinfluence\\tpatient\\tcare\\tand\\tpotentially\\thealth\\toutcomes.\\t\\nThis\\tstep\\trequires\\ta\\tdetailed\\tand\\tIRB-approved\\tstudy\\tprotocol\\tand\\tcare\\ttaken\\tto\\tensure\\t\\npatient\\tsafety.\\nThese\\tsteps\\tare\\tcrucial\\tnot\\tjust\\tfor\\tassessing\\tthe\\tmodel's\\tperformance\\ton\\tnew\\tunseen\\tdata\\t\\nbut\\talso,\\tmore\\tsignificantly,\\tfor\\tevaluating\\tthe\\teffectiveness\\tof\\tthe\\tend-to-end\\tsystem\\twhen\\t\\nintegrated\\tinto\\treal\\tworkflows.\\tOccasionally,\\tthe\\toptimal\\tway\\tto\\tuse\\tGenAI\\tmodels\\tlike\\tMed-\\nPaLM\\tmay\\tdiverge\\tfrom\\tinitial\\tassumptions,\\tand\\tintroducing\\ta\\tnew\\ttool\\tinto\\ta\\tclinical\\tworkflow\\t\\nmight\\trequire\\tunexpected\\tadjustments\\tto\\tthe\\toverall\\tprocess.11,12 End-to-end assessment is \\nessential\\tfor\\tunderstanding\\tthe\\trole\\tand\\tbenefit\\tof\\tthe\\ttechnology\\tand\\ttailoring\\tAI\\tsolutions\\t\\nto\\tmeet\\tthe\\tneeds\\teffectively.\\nTask- vs. domain-specific models\\nMed-PaLM7\\thighlighted\\tthe\\tsignificance\\tand\\tvalue\\tof\\ta\\tspecialized\\tmodel\\tfor\\tthe\\tmedical\\t\\ndomain.\\tMed-PaLM\\t2,\\tan\\taligned\\tand\\tfine-tuned\\titeration\\tof\\tPaLM\\t2\\ttailored\\tto\\tmedical\\t\\nknowledge,\\tachieves\\ta\\tninefold\\tenhancement\\tin\\tprecise\\treasoning\\tcompared\\tto\\tthe\\t\\nbaseline.13\\tHowever,\\tit's\\tcrucial\\tto\\trecognize\\tthat\\texcelling\\tin\\tone\\tmedical\\tdomain\\ttask\\tdoesn't\\t\\nnecessarily\\tguarantee\\tand\\timply\\tsuccess\\tin\\ta\\tdifferent\\tmedical\\tdomain\\ttask.\\tFor\\tinstance,\\t\\ndoes\\ta\\tgreat\\tgeneral\\tmedical\\tQA\\tsystem\\talso\\tperform\\twell\\ton\\ta\\tmental\\thealth\\tassessment\\t\\ntask? While it's reasonable to assume that a demonstrated understanding of clinical \\nknowledge\\tcan\\tgeneralize\\teffectively\\tto\\ttasks\\theavily\\trelying\\ton\\tthis\\tknowledge,\\teach\\tspecific\\t\\ntask\\trequires\\tvalidation\\tand\\tpossible\\tadaptation,\\tsuch\\tas\\tthe\\tmeasurement\\tof\\tpsychiatric\\t\\nfunctioning,14\\tbefore\\tproceeding\\tfurther.\", \"Solving Domain-Specific Problems Using LLMs\\n32\\nFebruary 2025\\nThe\\tmedical\\tdomain\\talso\\textends\\twell\\tbeyond\\ttextual\\tinformation.\\tThe\\tpractice\\tof\\tmedicine\\tis\\t\\ninherently\\tmulti-modal\\tand\\tincorporates\\tinformation\\tfrom\\timages,\\telectronic\\thealth\\trecords,\\t\\nsensors,\\twearables,\\tgenomics,\\tand\\tmore.\\tMultimodal\\tversions15 of MedLM and related \\napproaches16,17,18\\tare\\tin\\tearly\\tstages\\tof\\tresearch,\\tand\\tfollow\\tthe\\tsame\\tvalidation\\tprinciples\\tand\\t\\nworkflow\\tintegration\\tapproach.\\tWe\\twill\\tbe\\tobserving\\tthe\\tmultimodal-enabled\\tset\\tof\\tuse-\\ncases\\tevaluated\\tand\\tdeployed\\tin\\tthe\\tfield.\\nLastly,\\ta\\tmedically\\tspecialized\\tmodel\\tcan\\tbe\\tapplied\\tnot\\tonly\\tto\\tclinical\\tuse\\tcases\\tthat\\trelate\\t\\ndirectly\\tto\\tpatient\\tcare,\\tbut\\talso\\tto\\tuse\\tcases\\tthat\\tbenefit\\tfrom\\tleveraging\\tmedical\\tknowledge\\t\\nin\\ta\\tflexible\\tway.\\tAn\\texample\\tis\\tin\\tscientific\\tdiscovery,\\twhere\\tMed-PaLM\\tcan\\tbe\\tused\\tto\\t\\naccurately identify genes associated with biomedical traits.19\\tWe'll\\tbe\\texploring\\ta\\t\\tbreadth\\t\\nof\\tpossibilities\\twith\\tvertical-specific\\tmodels,\\tand\\twe\\texpect\\tnew\\tapplications\\tand\\tideas\\tto\\t\\nemerge\\tin\\tthe\\tfield\\tover\\tthe\\tnext\\tfew\\tyears.\\tWe’re\\talso\\texploring\\tsafe\\tand\\tresponsible\\tways\\t\\nto\\tbring\\tthese\\tmodels\\tto\\tthe\\thealthcare\\tindustry.\\tWith\\tMedLM,\\ta\\tsuite\\tof\\tmodels\\tfine-tuned\\t\\nfor\\thealthcare\\tuse\\tcases,\\tbuilt\\ton\\tMed-PaLM\\t2,\\twe’re\\tmaking\\tsolutions\\tcommercially\\tavailable\\t\\nso\\thealthcare\\torganizations\\tcan\\tbuild\\tGenAI\\tuse\\tcases\\tsuitable\\tfor\\ttheir\\tworkflows.\\nTraining strategies for Med-PaLM 2\\nMed-PaLM\\t2\\tis\\tan\\tadvancement\\tof\\tthe\\tbase\\tLLM\\tmodel\\tPaLM\\t2,\\tGoogle's\\tenhanced\\tLLM\\twith\\t\\nsubstantial\\tperformance\\timprovements\\ton\\tmultiple\\tLLM\\tbenchmark\\ttasks.\\tTo\\ttailor\\tMed-\\nPaLM\\t2\\tfor\\tmedical\\tapplications,\\tinstruction\\tfine-tuning7\\twas\\tperformed\\tusing\\tMultiMedQA,7 \\nincluding\\tMedQA,\\tMedMCQA,\\tHealthSearchQA,\\tLiveQA,\\tand\\tMedicationQA\\tdatasets.\\tDataset\\t\\nmixture\\tratios\\twere\\tempirically\\tdetermined.\\t\\nTo\\tenhance\\tthe\\tspecialized\\tvariant\\tof\\tMed-PaLM\\t2\\tfocusing\\ton\\tmultiple-choice\\tquestions,\\t\\na\\trange\\tof\\tprompting\\tstrategies\\tincluding\\tfew-shot\\tprompting,\\tchain-of-thought\\t(CoT)\\t\\nprompting,\\tand\\tself-consistency\\twere\\temployed.\\tCoT\\tinvolves\\taugmenting\\teach\\tfew-shot\\t\\nexample\\tin\\ta\\tprompt\\twith\\ta\\tstep-by-step\\texplanation\\ttowards\\tthe\\tfinal\\tanswer,\\tallowing\", \"Solving Domain-Specific Problems Using LLMs\\n33\\nFebruary 2025\\nthe\\tlanguage\\tmodel\\tto\\tcondition\\ton\\tits\\town\\tintermediate\\toutputs\\tfor\\tmulti-step\\tproblem-\\nsolving.\\tSelf-consistency\\tplays\\ta\\trole\\tin\\tenhancing\\tthe\\tmodel's\\tperformance\\ton\\tmultiple-\\nchoice\\tquestions\\tby\\tsampling\\tmultiple\\texplanations\\tand\\tanswers\\tfrom\\tthe\\tmodel,\\twith\\tthe\\t\\nfinal\\tanswer\\tdetermined\\tby\\ta\\tmajority\\tvote\\tamong\\tthe\\tgenerated\\toptions.\\tThese\\tstrategies\\t\\ncollectively\\timprove\\tthe\\tmodel's\\tability\\tto\\treason\\tand\\tprovide\\tmore\\taccurate\\tresponses\\tto\\t\\ncomplex\\tand\\tmulti-faceted\\tqueries.\\nAnother\\tnoteworthy\\tmethodological\\timprovement\\tis\\tthe\\tintroduction\\tof\\tensemble\\trefinement\\t\\n(ER),\\twhich\\tbuilds\\ton\\tother\\ttechniques\\tthat\\tinvolve\\tconditioning\\tan\\tLLM\\ton\\tits\\town\\t\\ngenerations\\tbefore\\tproducing\\ta\\tfinal\\tanswer.\\tIn\\tthe\\tfirst\\tstage,\\tmultiple\\tpossible\\texplanations\\t\\nand\\tanswers\\tare\\tstochastically\\tgenerated\\tvia\\ttemperature\\tsampling.\\tIn\\tthe\\tsecond\\tstage,\\t\\nthe\\tmodel\\tis\\tconditioned\\ton\\tthe\\toriginal\\tprompt,\\tquestion,\\tand\\tgenerated\\tcontents\\tfrom\\t\\nthe\\tfirst\\tstage,\\tresulting\\tin\\tthe\\tproduction\\tof\\ta\\trefined\\texplanation\\tand\\tanswer.\\tThis\\tprocess\\t\\nfacilitated\\tthe\\teffective\\taggregation\\tof\\tanswers,\\textending\\tits\\tutility\\tbeyond\\tquestions\\twith\\t\\na\\tlimited\\tset\\tof\\tpotential\\tanswers,\\tthereby\\tenhancing\\tthe\\toverall\\tperformance\\tof\\tthe\\tmodel.\\t\\nThe\\toverall\\tmechanism\\tof\\tensemble\\trefinement\\tis\\tdepicted\\tin\\tFigure\\t7.\\nFigure 7. Ensemble refinement (ER) in Med-PaLM 2.  This approach involves conditioning an LLM on multiple \\npotential reasoning pathways it generates, facilitating the answer refinement and improvement\", \"Solving Domain-Specific Problems Using LLMs\\n34\\nFebruary 2025\\nThe\\tgoal\\tbehind\\tthe\\tinception\\tof\\tthe\\tMed-PaLM\\tresearch\\teffort\\twas\\tto\\timprove\\thealth\\t\\noutcomes\\tvia\\tusing\\tand\\tadvancing\\temerging\\tAI\\ttechnologies.\\tAchieving\\texpert-level\\t\\nperformance\\tin\\tmedical\\tQA\\ttasks\\twas\\tthe\\tfirst\\tstep,\\twith\\tmany\\tmore\\tto\\tfollow\\tin\\tclose\\t\\ncollaboration\\twith\\tthe\\tclinical\\tcommunity\\tas\\twe\\tprogress\\ton\\tthis\\tjourney.\\t\\nOur\\thealth\\tresearch\\texperience\\tat\\tGoogle\\tdemonstrated\\trepeatedly\\tthat\\ttechnology\\tis\\toften\\t\\nnot\\tthe\\tsole\\tchallenge\\tin\\tapplying\\tAI\\tproductively\\tto\\thealthcare.\\tInstead,\\tmany\\tother\\tfactors,\\t\\nincluding\\tthoughtful\\tevaluation\\tstrategies\\tand\\tworking\\ton\\tclinically\\tmeaningful\\tapplications\\t\\nin\\tpartnership\\twith\\tclinicians\\tand\\ta\\tbroad\\tcross-functional\\tteam,\\tare\\tpivotal\\tto\\tsuccess.5 This \\nvaluable\\tinsight\\tis\\tlikely\\tapplicable\\tto\\tother\\tvertical\\tdomains\\tas\\twell.\\nAs\\tAI\\ttechnology\\tmatures\\tand\\tmoves\\tcloser\\tto\\tpractical\\tuse\\tcases\\tand\\treal-world\\tscenarios,\\t\\ncareful\\tmulti-step\\tevaluations,\\tincluding\\tboth\\tretrospective\\tand\\tprospective\\tassessments,\\t\\nare\\tbeneficial\\tto\\tbetter\\tunderstand\\tthe\\treal\\trole\\tand\\tbenefits\\tof\\tthe\\ttechnology\\tin\\tthe\\twhole\\t\\nworkflow.\\tGuidance\\tby\\ta\\tclinical\\tpartner\\timproves\\tthe\\tchances\\tof\\tbuilding\\tthe\\tright\\tsolution\\t\\nfor\\tbetter\\thealth\\toutcomes.\\tMany\\tpromising\\tapplications\\tlie\\tin\\tthe\\tcollaboration\\tof\\thealthcare\\t\\nworkers\\tand\\ttechnology,\\tcombining\\tthe\\tstrengths\\tof\\tboth.\\tIt\\tis\\talso\\timportant\\tto\\tuse\\tGenAI\\t\\nsystems\\tin\\ta\\tway\\tthat\\tis\\trespectful\\tof\\tpatients'\\tautonomy\\tand\\tprivacy.\\nFor\\tthe\\tforeseeable\\tfuture,\\tit\\tis\\treasonable\\tto\\tassume\\tthat\\tmodels\\tcustomized\\tfor\\tspecific\\t\\napplications\\tor\\tdomains\\twill\\tyield\\tbetter\\tresults,\\tand\\twe\\tare\\ttracking\\ttrends\\tand\\tany\\t\\nconvergence\\tin\\tperformance\\tbetween\\tgeneral\\tand\\tspecific\\tmodels\\tin\\tthe\\tyears\\tahead.\\tFor\\t\\nMed-PaLM\\tspecifically,\\tour\\tresearch\\tprogress\\twill\\tbe\\ttracked\\tat\\tthe\\tMed-PaLM\\tresearch\\t\\nwebpage.20\\tWe\\taim\\tto\\tmake\\tprogress\\tmore\\tbroadly\\tin\\tthe\\tfield\\tof\\tusing\\tAI\\tand\\tGenAI\\tfor\\tthe\\t\\nbetterment\\tof\\tpatients,\\tclinicians,\\tand\\tresearchers.\", 'Solving Domain-Specific Problems Using LLMs\\n35\\nFebruary 2025\\nSummary\\nThis\\twhitepaper\\texplores\\tthe\\tpotential\\tof\\tLLMs\\tin\\ttackling\\tcomplex\\tchallenges\\twithin\\tspecific\\t\\ndomains,\\twith\\ta\\tparticular\\tfocus\\ton\\thealthcare\\tand\\tcybersecurity.\\n• Cybersecurity:\\tThe\\tever-evolving\\tlandscape\\tof\\tcyber\\tthreats\\tdemands\\tinnovative\\t\\nsolutions.\\tSecLM,\\tan\\tLLM\\tdesigned\\tfor\\tcybersecurity,\\tacts\\tas\\ta\\tforce\\tmultiplier\\tfor\\t\\nsecurity\\tprofessionals\\tby\\tintelligently\\tprocessing\\tvast\\tamounts\\tof\\tdata.\\tThis\\tempowers\\t\\nthem\\tto\\tanalyze\\tand\\trespond\\tto\\tthreats\\tmore\\teffectively.\\tThe\\tvision\\tfor\\tSecLM\\tis\\tto\\tcreate\\t\\na\\tcomprehensive\\tplatform\\tthat\\tcaters\\tto\\tthe\\tdiverse\\tneeds\\tof\\tsecurity\\tpractitioners,\\t\\nregardless\\tof\\ttheir\\texpertise.\\tThe\\tcombination\\tof\\tLLMs\\tand\\thuman\\texpertise\\thas\\tthe\\t\\npotential\\tto\\trevolutionize\\tthe\\tfield\\tof\\tcybersecurity,\\tachieving\\tsuperior\\tresults\\twith\\t\\nless\\teffort.\\n• Healthcare:\\tHealthcare\\tdata\\tis\\tincreasing\\tin\\tquantity\\tand\\tcomplexity,\\tleading\\tto\\ta\\t\\nneed\\tfor\\tinnovative\\tsolutions\\tto\\trender\\tmedical\\tinformation\\tmore\\thelpful,\\tuseful,\\tand\\t\\naccessible.\\tMedLM,\\ta\\tfamily\\tof\\tmodels\\tfine-tuned\\tfor\\tthe\\thealthcare\\tindustry,\\tcan\\thelp\\t\\nunlock\\tknowledge\\tand\\tmake\\tmedicine\\tmore\\teffective.\\tMedLM\\tis\\tbuilt\\ton\\tMed-PaLM,\\t\\nan\\tLLM\\tdeveloped\\tfor\\tmedical\\tapplications.\\tMed-PaLM\\thas\\tdemonstrated\\texpert-level\\t\\nperformance\\tin\\tmedical\\tquestion-and-answering\\ttasks.\\tThis\\tachievement\\tis\\tjust\\tthe\\tfirst\\t\\nstep\\tin\\ta\\tjourney\\ttowards\\timproving\\thealth\\toutcomes\\tthrough\\tthe\\tutilization\\tof\\tGenAI.\\tThe\\t\\nkey takeaway from this research is that technology alone is not enough. Collaboration \\nwith\\tthe\\tclinical\\tcommunity\\tand\\tcareful\\tmulti-step\\tevaluations\\tare\\tcrucial\\tfor\\tsuccessful\\t\\napplication\\tof\\tLLMs\\tin\\thealthcare.\\tGoing\\tforward,\\tvertical-specific\\tmodels\\tlike\\tthe\\tMedLM\\t\\nfoundation\\tmodels\\tare\\texpected\\tto\\tyield\\teven\\tbetter\\tresults\\tfor\\tspecific\\tapplications\\tof\\t\\ninterest,\\tfurthering\\tthe\\tpotential\\tof\\tAI\\tin\\thealthcare.\\nThis\\twhitepaper\\tshowcases\\tthe\\tpossibilities\\tof\\tLLMs\\tin\\tsolving\\tdomain-specific\\tproblems.\\tBy\\t\\nleveraging\\tthe\\tpower\\tof\\tthese\\tadvanced\\tmodels,\\tcombined\\twith\\thuman\\texpertise\\tand\\tcareful\\t\\nimplementation,\\twe\\tcan\\ttackle\\tcomplex\\tchallenges\\tand\\tachieve\\tbreakthrough\\tadvancements\\t\\nin\\tvarious\\tfields,\\tfor\\tthe\\tbenefit\\tof\\tpeoples’\\tlives.', 'Solving Domain-Specific Problems Using LLMs\\n36\\nFebruary 2025\\nEndnotes\\n1.\\t Cantos,\\tJ.,\\tet\\tal.,\\t2023.\\tThreat\\tActors\\tare\\tInterested\\tin\\tGenerative\\tAI,\\tbut\\tUse\\tRemains\\tLimited.\\t[online]\\t\\nAvailable at: https://cloud.google.com/blog/topics/threat-intelligence/threat-actors-generative-ai-limited/ .\\n2.\\t Lin,\\tC.Y.,\\tet\\tal.,\\t2003.\\tAutomatic\\tEvaluation\\tof\\tSummaries\\tUsing\\tn-gram\\tCo-occurrence\\tStatistics.\\t[online]\\t\\nAvailable at: https://aclanthology.org/N03-1020.pdf .\\n3.\\t Papineni,\\tK.,\\tet\\tal.,\\t2002.\\tBLEU:\\tA\\tMethod\\tfor\\tAutomatic\\tEvaluation\\tof\\tMachine\\tTranslation.\\t[online]\\tAvailable\\t\\nat: https://aclanthology.org/P02-1040.pdf.\\n4.\\t Zhang,\\tT.,\\tet\\tal.,\\t2019.\\tBERTScore:\\tEvaluating\\tText\\tGeneration\\twith\\tBERT.\\t[online]\\tAvailable\\tat:\\thttps://\\nopenreview.net/attachment?id=SkeHuCVFDr&name=original_pdf .\\n5.\\t Google,\\t2023.\\t5\\tmyths\\tabout\\tmedical\\tAI,\\tdebunked.\\t[online]\\tAvailable\\tat:\\thttps://blog.google/technology/\\nhealth/5-myths-about-medical-ai-debunked/ .\\n6.\\t Bommasani,\\tR.,\\tet\\tal.,\\t2021.\\tOn\\tthe\\topportunities\\tand\\trisks\\tof\\tfoundation\\tmodels.\\tarXiv\\tpreprint\\t\\narXiv:2108.07258.\\t[online]\\tAvailable\\tat:\\thttps://arxiv.org/pdf/2108.07258.\\n7.\\t Singhal,\\tK.,\\tet\\tal.,\\t2023.\\tLarge\\tlanguage\\tmodels\\tencode\\tclinical\\tknowledge.\\tNature,\\t620(7972),\\tpp.172-180.\\t\\n[online]\\tAvailable\\tat:\\thttps://www.nature.com/articles/s41586-023-06291-2.\\n8.\\t Jin,\\tD.,\\tet\\tal.,\\t2021.\\tWhat\\tdisease\\tdoes\\tthis\\tpatient\\thave?\\ta\\tlarge-scale\\topen\\tdomain\\tquestion\\tanswering\\t\\ndataset\\tfrom\\tmedical\\texams.\\tApplied\\tSciences,\\t11(14),\\tp.6421.\\n9.\\t Singhal,\\tK.,\\tet\\tal.,\\t2023.\\tTowards\\texpert-level\\tmedical\\tquestion\\tanswering\\twith\\tlarge\\tlanguage\\tmodels.\\tarXiv\\t\\npreprint\\tarXiv:2305.09617.\\t[online]\\tAvailable\\tat:\\thttps://arxiv.org/abs/2305.09617.\\n10.\\tBernstein,\\tI.A.,\\tet\\tal.,\\t2023.\\tComparison\\tof\\tophthalmologist\\tand\\tlarge\\tlanguage\\tmodel\\tchatbot\\tresponses\\t\\nto\\tonline\\tpatient\\teye\\tcare\\tquestions.\\tJAMA\\tNetwork\\tOpen,\\t6(8),\\tpp.e2330320-e2330320.\\t[online]\\tAvailable\\tat:\\t\\nhttps://jamanetwork.com/journals/jamanetworkopen/fullarticle/2808557 .\\n11.\\t Beede,\\tE.,\\tet\\tal.,\\t2019.\\tA\\tHuman-Centered\\tEvaluation\\tof\\ta\\tDeep\\tLearning\\tSystem\\tDeployed\\tin\\tClinics\\tfor\\tthe\\t\\nDetection\\tof\\tDiabetic\\tRetinopathy.\\t[online]\\tAvailable\\tat:\\thttps://dl.acm.org/doi/abs/10.1145/3313831.3376718.\\n12.\\t Pedersen,\\tS.,\\tet\\tal.,\\t2021.\\tRedesigning\\tClinical\\tPathways\\tfor\\tImmediate\\tDiabetic\\tRetinopathy\\tScreening\\t\\nResults.\\tNEJM\\tCatalyst,\\tJuly.\\t[online]\\tAvailable\\tat:\\thttps://catalyst.nejm.org/doi/pdf/10.1056/CAT.21.0096.\\n13.\\tGoogle,\\t2023.\\tGoogle\\tI/O\\tKeynote\\t2023.\\t[online]\\tAvailable\\tat:\\t\\thttps://www.youtube.com/live/\\ncNfINi5CNbY?si=jQFi-Y3mG0rGD3Xd&t=810 .', 'Solving Domain-Specific Problems Using LLMs\\n37\\nFebruary 2025\\n14.\\tGalatzer-Levy,\\tI.R.,\\tet\\tal.,\\t2023.\\tThe\\tCapability\\tof\\tLarge\\tLanguage\\tModels\\tto\\tMeasure\\tPsychiatric\\t\\nFunctioning.\\tarXiv\\tpreprint\\tarXiv:2308.01834.\\t[online]\\tAvailable\\tat:\\thttps://arxiv.org/abs/2308.01834.\\n15.\\tTu,\\tT.,\\tet\\tal.,\\t2023.\\tTowards\\tgeneralist\\tbiomedical\\tAI.\\tarXiv\\tpreprint\\tarXiv:2307.14334.\\t[online]\\tAvailable\\tat:\\t\\nhttps://arxiv.org/abs/2307.14334.\\n16.\\tLiu,\\tX.,\\tet\\tal.,\\t2023.\\tLarge\\tLanguage\\tModels\\tare\\tFew-Shot\\tHealth\\tLearners.\\tarXiv:2305.15525.\\t[online]\\t\\nAvailable at: https://arxiv.org/abs/2305.15525 .\\n17.\\t Belyaeva,\\tA.,\\tet\\tal.,\\t2023.\\tMultimodal\\tLLMs\\tfor\\thealth\\tgrounded\\tin\\tindividual-specific\\tdata.\\tarXiv:2307.09018.\\t\\n[online]\\tAvailable\\tat:\\thttps://arxiv.org/abs/2307.09018.\\n18.\\tShawn,\\tX.,\\tet\\tal.,\\t2022.\\tELIXR:\\tTowards\\ta\\tgeneral\\tpurpose\\tX-ray\\tartificial\\tintelligence\\tsystem\\tthrough\\t\\nalignment\\tof\\tlarge\\tlanguage\\tmodels\\tand\\tradiology\\tvision\\tencoders.\\tarXiv:2308.01317.\\t[online]\\tAvailable\\tat:\\t\\nhttps://arxiv.org/abs/2308.01317 .\\n19.\\t Tu,\\tT.,\\tet\\tal.,\\t2023.\\tGenetic\\tDiscovery\\tEnabled\\tby\\ta\\tLarge\\tLanguage\\tModel.\\t[online]\\tAvailable\\tat:\\thttps://www.\\nbiorxiv.org/content/10.1101/2023.11.09.566468v1.full.pdf .\\n20.\\tMed-PaLM,\\t[n.d.].\\tHomepage.\\t[online]\\tAvailable\\tat:\\thttps://g.co/research/medpalm.', 'Embeddings  \\n& Vector Stores\\nAuthors: Anant Nawalgaria, \\nXiaoqi Ren, and Charles Sugnet', 'Embeddings & Vector Stores\\n2\\nFebrurary 2025\\nContent contributors\\nAntonio Gulli\\nGrace Mollison\\nRuiqi Guo\\nIftekhar Naim\\nJinhyuk Lee\\nAlan Li\\nPatricia Florissi\\nAndrew Brook\\nOmid Fatemieh\\nZhuyun Dai\\nLee Boonstra\\nPer Jacobsson\\nSiddhartha Reddy Jonnalagadda\\nXi Cheng\\nRaphael Hoffmann\\nCurators and Editors\\nAntonio Gulli\\nAnant Nawalgaria\\nGrace Mollison \\nTechnical Writer\\nJoey Haymaker\\nDesigner\\nMichael Lanning \\nAcknowledgements', 'Introduction 5\\nWhy embeddings are important 6\\nEvaluating Embedding Quality 9\\nSearch Example 11\\nTypes of embeddings 16\\nText embeddings 16\\nWord embeddings 19\\nDocument embeddings 23\\nShallow BoW models 24\\nDeeper pretrained large language models 26\\nImage & multimodal embeddings 30\\nStructured data embeddings 32\\nGeneral structured data 32\\nUser/item structured data 33\\nTable of contents', 'Graph embeddings 33\\nTraining Embeddings 34\\nVector search 36\\nImportant vector search algorithms 37\\nLocality sensitive hashing & trees 38\\nHierarchical navigable small worlds  41\\nScaNN 44\\nVector databases  47\\nOperational considerations 49\\nApplications 51\\nQ & A with sources (retrieval augmented generation) 52\\nSummary 57\\nEndnotes 59', \"Embeddings & Vector Stores\\n5\\nFebrurary 2025\\nIntroduction\\nModern machine learning thrives on diverse data—images, text, audio, and more. This \\nwhitepaper explores the power of embeddings, which transform this heterogeneous data into \\na unified vector representation for seamless use in various applications.\\nWe'll guide you through:\\n• Understanding Embeddings: Why they are essential for handling multimodal data and \\ntheir diverse applications.\\n• Embedding Techniques: Methods for mapping different data types into a common \\nvector space.\\nThese low-dimensional numerical \\nrepresentations of real-world data \\nsignificantly helps efficient large-\\nscale data processing and storage \\nby acting as means of lossy \\ncompression of the original data.\", 'Embeddings & Vector Stores\\n6\\nFebrurary 2025\\n• Efficient Management: Techniques for storing, retrieving, and searching vast collections \\nof embeddings.\\n• Vector Databases: Specialized systems for managing and querying embeddings, \\nincluding practical considerations for production deployment.\\n• Real-World Applications: Concrete examples of how embeddings and vector databases \\nare combined with large language models (LLMs) to solve real-world problems.\\nThroughout the whitepaper, code snippets provide hands-on illustrations of key concepts.\\nWhy embeddings are important\\nIn essence, embeddings are numerical representations of real-world data such as text, \\nspeech, image, or videos. The name embeddings refers to a similar concept in mathematics \\nwhere one space can be mapped, or embedded, into another space. For example, the \\noriginal BERT Model [ref] embeds text into a vector of 768 numbers, thus mapping from the \\nvery high dimensional space of all sentences to a much smaller 768 dimensions. Embeddings \\nare expressed as low-dimensional vectors where the geometric distance between two \\nvectors in the vector space is a projection of the relationship and semantic similarity between \\nthe two real-world objects that the vectors represent. In other words, they help you with \\nproviding compact representations of data of different types, while simultaneously also \\nallowing you to compare two different data objects and tell how similar or different they \\nare on a numerical scale. For example: the word ‘computer’ has a similar meaning to the \\npicture of a computer, as well as to the word ’laptop’ but not to the word ‘car’. These low-\\ndimensional numerical representations of real-world data significantly help efficient large-\\nscale data processing and storage by acting as means of lossy compression of the original \\ndata while retaining its important semantic properties.', 'Embeddings & Vector Stores\\n7\\nFebrurary 2025\\nFor some intuition about embeddings consider the familiar latitude and longitude which \\nare used to map locations on earth to a pair of numbers, or a vector of length two. Latitude \\nand longitude can be thought of as an embedding of a particular location. While seemingly \\nobvious now, this simple mapping of a location to a pair of numbers transformed human \\nnavigation and is still critical to this day. Given the latitude and longitude of two addresses \\nit is relatively easy to see how distant they are from each other, or look up other nearby \\nlocations. As with latitutde and longitude if two text embeddings are close to each other \\nin the embeddings space they will be semantically similar in their text meaning. Also, it \\nis possible to find new semantically similar text phrases by looking nearby in that vector \\nspace. This ability to find similar items in very large data sets with very low latency using \\nvector databases is critical for many production use cases today including search, \\nrecommendations, advertising, fraud detection and many more. Note that while the latitude \\nand longitude embedding model was designed based on the spherical shape of the earth, \\nthe embedding space for text is learned by the neural network model. Importantly, the \\nembeddings learned by different models will not be comparable to each other and it is \\ncritical to make sure in practice that compatible and consistent versions of embeddings are \\nbeing used.\\nKey applications for embeddings are retrieval and recommendations, where the results are \\nusually selected from a massive search space. For example, Google Search is a retrieval task \\nover the search space of the entire internet. Today’s retrieval and recommendation systems’ \\nsuccess depends on the following steps:\\n1. Precomputing the embeddings for billions of items in the search space.\\n2. Mapping query embeddings into the same embedding space.\\n3. Efficient computing and retrieving of the items whose embeddings are the nearest \\nneighbors of the query embeddings in the search space.', 'Embeddings & Vector Stores\\n8\\nFebrurary 2025\\nEmbeddings also shine in the world of multimodality. Many applications work with large \\namounts of data of various modalities: text, speech, image, and videos to name a few. \\nJoint embeddings are when multiple types of objects are being mapped into the same \\nembeddings space, for example retrieving videos based on text queries. These embedding \\nrepresentations are designed to capture as much of the original object’s characteristics \\nas possible.\\nFigure 1. Projecting objects/content into a joint vector space with semantic meaning\\nEmbeddings are designed so they place objects with similar semantic properties closer in \\nthe embedding space (a low-dimensional vector space where items can be projected). The \\nembeddings can then be used as a condensed, meaningful input in downstream applications. \\nFor example, you can use them as features for ML models, recommender systems, search', 'Embeddings & Vector Stores\\n9\\nFebrurary 2025\\nengines, and many more. So your data not only gets a compact numerical representation, \\nbut this representation also preserves the semantic meanings for a specific task or across \\na variety of tasks. The fact that these representations are task-specific means you can \\ngenerate different embeddings for the same object, optimized for the task at hand. \\nEvaluating Embedding Quality\\nEmbedding models are evaluated differently depending on the task. Many of the common \\nmetrics for evaluating quality focus on the ability to retrieve similar items, while excluding \\nitems that are not similar. This type of evaluation requires a labeled datasets for which the \\nrelevant, or correct, documents are already known as seen in Snippet 0 where the NFCorpus \\ndataset is used to illustrate different metrics.  For the search use case described above two \\nimportant metrics for evaluating quality are: 1) precision - all documents retrieved should \\nbe relevant and 2) recall - all of the relevant documents should be retrieved. Intuitively, the \\noptimal embedding model would retrieve all of the relevant documents and no documents \\nthat weren’t relevant, however it is often the case that some relevant documents are \\nexcluded and some irrelevant ones get retrieved so more quantitative definitions are required \\nfor evaluating quality over large sets of documents and embedding models. Precision is \\nquantified by dividing the number of relevant documents by the total number of retrieved \\ndocuments. It is often quoted for a particular number of retrieved documents. For example \\nif ten documents were retrieved for an embedding and seven of them were relevant and \\nother three were not,  the precsion@10 would be 7/10 = 0.7. Recall looks at how many of \\nthe relevant documents were retrieved and is calculated by dividing the number of relevant \\ndocuments retrieved by the total number of relevant documents in the corpus. Recall is also \\noften quoted for a particular number of documents retrieved. For example, if 20 documents \\nwere retrieved and three of them were relevant, but there were six total relevant documents \\nin the corpus the recall@20 would be 3/6 = 0.5.', 'Embeddings & Vector Stores\\n10\\nFebrurary 2025\\nPrecision and recall are very useful when relevancy scores are binary, but don’t capture \\nthe case when some documents are more relevant than others. For example, when using a \\nsearch engine it is highly desirable that the most relevant result is at the top of the results \\nlist as end users are sensitive to the ordering of those results, even if they are all relevant. \\nWhen the detailed ordering of document relevancy is known for a data set, metrics like the \\nNormalized Discounted Cumulative Gain (nDCG) can measure the quality of the ranking \\nproduced by the embedding model compared to the desired ranking. The formula at\\nposition p for DCG =                           where reli is a relevancy score. The denominator \\npenalizes documents for being lower on the list, and DCG maximizes the score when most \\nrelevant documents are at the top of the list. The normalized version is calculated by dividing \\nthe DCG score by the ideal ordering score and ranges from 0.0 to 1.0 for comparisons across \\ndifferent queries. \\nPublic benchmarks like BEIR42 are widely used for evaluating performance on retrieval tasks \\nand additional tasks are covered by benchmarks like the Massive Text Embedding Benchmark \\n(MTEB)43. Practitioners are encouraged to use a standard library like those originated by \\nText Retrieval Conference (TREC) for consistent benchmarking with other methods, such as \\ntrec_eval44 or python wrappers like pytrec_eval45 when calculating precision, recall, nDCG \\nand others. The optimal way to evaluate embedding models for a particular application \\nmay be application specific, but the intuition that more similar objects should be closer \\nin the embeddings space is often a good start. Additional metrics such as model size, \\nembedding dimension size, latency, and overall cost are also important considerations for \\nproduction applications.', 'Embeddings & Vector Stores\\n11\\nFebrurary 2025\\nSearch Example\\nBefore diving into details about the different types of embeddings and the history of \\nembedding model development let’s explore the search example previously described above \\nin more detail. The goal is to find relevant documents in a large corpus given a query from \\nthe user. One approach then is to construct a joint embedding model where the question \\nand answer are mapped to similar locations in the embedding space. As the question and \\nanswer are semantically different, even if complementary, it is often helpful to use two neural \\nnets that have been trained together with one for the question and one for the documents. A \\nvisual representation of this can be seen in Figure 9(b) as an asymmetric dual encoder with \\na separate network for the query and document in contrast to 9(a) displaying a single neural \\nnetwork used for both query and document, also called a siamese network.\\nFigure 2 is a diagram of a search question and answer application using a retrieval \\naugmented generation (RAG) approach where embeddings are used to identify the relevant \\ndocuments before inserting them into the prompt of an LLM for summarization for the \\nend user. The application is split into two main processes. First, the index creation where \\ndocuments are divided into chunks which are used to generate embeddings and stored in a \\nvector database for low latency searches. Specifically, the document embedding portion of \\nthe model of the dual encoder neural network is used for these chunks. The second phase \\nwhen the user asks a question to the system that is embedded using the query portion of the \\nmodel and which will map to relevant documents when using a similarity search in the vector \\ndatabase. This second phase is very latency sensitive as the end user is actively waiting for \\na response so the ability to identify relevant documents from a large corpus in milliseconds \\nusing a vector database of documents is a critical piece of infrastructure.', 'Embeddings & Vector Stores\\n12\\nFebrurary 2025\\nFigure 2. Example flow for RAG Search Application highlighting embeddings. Document embeddings are \\ngenerated in the background and stored in a vector database. When the user enters a query, an embedding \\nis generated using the query embedding portion of the dual encoder and used to look up relevant \\ndocuments. Those documents can be inserted into the prompt for LLM to generate a relevant summary \\nresponse for the user.\\nThe quality of embedding models has been improving rapidly since the introduction of \\nBERT and shows no signs of slowing any time soon. While LLMs have captured a lot of the \\nattention in the AI space recently, the improvements in information retrieval and embedding \\nmodels has also been transformative. The original BERT models were a leap forward at the \\ntime, and had an average score of 10.6 on the BEIR benchmark, current 2025 embeddings', 'Embeddings & Vector Stores\\n13\\nFebrurary 2025\\nfrom Google with a simple API call, and no AI knowledge required, now have an average BEIR \\nscore of 55.7. Models continue to improve rapidly, so when putting embedding models into \\nproduction be sure to design with model upgrades in mind. Good evaluation suites designed \\nfor the particular application are critical for ensuring smooth upgrades. Choosing embedding \\nmodels on platforms that have upgrade paths in place can help save developer time and \\nreduce operational overhead for teams without deep AI expertise, for example Snippet 0 \\nbelow uses a simple API call via Google Vertex.\\nSnippet 1 contains basic embedding code sample to illustrate some of the important \\nconcepts covered above for embeddings using the NFCorpus dataset46 that contains health \\nrelated questions and documents:\\n• The text documents with information relevant to the queries are embedded using \\nthe Google Vertex APIs for both high quality and operational ease. The RETRIEVAL_\\nDOCUMENT task type is used as questions and answers are often phrased differently \\nand use a single model with semantic similarity would result in a reduced performance \\ncompared to joint document and query embeddings.\\n• Embeddings are stored using the faiss47 library for efficient similarity search. \\n• For a particular query the text embeddings is generated using the RETRIEVAL_QUERY \\ntask type.\\n• The query embedding is used by the faiss library to look up the ids for documents whose \\nembeddings are close using the default eucldidean distance metric.\\n• Embeddings for all of the queries are generated and most similar documents retrieved. \\nRetrieval quality is evaluated against the “gold” values using the pytrec library to measure \\nprecision@1, recall@10, ndcg@10 metrics.', 'Embeddings & Vector Stores\\n14\\nFebrurary 2025\\nfrom beir import util\\nfrom beir.datasets.data_loader import GenericDataLoader\\nimport faiss\\nimport vertexai\\nfrom vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\\nimport numpy as np\\nimport pandas as pd\\nimport pytrec_eval\\ndef embed_text(texts, model, task, batch_size=5) :\\n embed_mat = np.zeros((len(texts),768))\\n for batch_start in range(0,len(texts),batch_size):\\n   size = min(len(texts) - batch_start, batch_size)\\n   inputs = [TextEmbeddingInput(texts[batch_start+i], task_type=task) for i in range(size)]\\n   embeddings = model.get_embeddings(inputs)\\n   for i in range(size) :\\n     embed_mat[batch_start + i, :] = embeddings[i].values\\n return embed_mat\\n# Download smallish NFCorpus dataset of questions and document text\\nurl = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nfcorpus.zip\"\\ndata_path = util.download_and_unzip(url, \"datasets\") \\n# Corpus of text chunks, text queries and “gold” set of query to relevant documents dict\\ncorpus, queries, qrels = GenericDataLoader(\"datasets/nfcorpus\").load(split=\"test\")\\n# Note need to setup Google Cloud project and fill in id & location below\\nvertexai.init(project=\"PROJECT_ID\", location=\"LOCATION\")\\nmodel = TextEmbeddingModel.from_pretrained(\"text-embedding-005\")\\ndoc_ids,docs = zip(*[(doc_id, doc[\\'text\\']) for doc_id,doc in corpus.items()])\\nq_ids,questions = zip(*[(q_id, q) for q_id,q in queries.items()])\\nContinues next page...', 'Embeddings & Vector Stores\\n15\\nFebrurary 2025\\nSnippet 1. Example semantic search using text embeddings and evaluation for quality of \\nretrieved documents.\\nBoth training and evaluating neural networks requires datasets that contains pairs of \\nquestions and relevant documents such as the NFCorpus used in Snippet 0. The dataset that \\nis best suited to train or evaluate for a particular application will depend on the nature of \\n# Embed the documents and queries jointly using different models\\ndoc_embeddings = embed_text(docs, model, \"RETRIEVAL_DOCUMENT\")\\nindex = faiss.IndexFlatL2(doc_embeddings.shape[1])\\nindex.add(doc_embeddings)\\n# Example look up example query to find relevant doc - note using \\'RETRIEVAL_QUERY\\'\\nexample_embed = embed_text([\\'Is Caffeinated Tea Really Dehydrating?\\'], \\nmodel, \\'RETRIEVAL_QUERY\\')\\ns,q = index.search(example_embed,1)\\nprint(f\\'Score: {s[0][0]:.2f}, Text: \"{docs[q[0][0]]}\"\\')\\n# Score: 0.49, Text: \"There is a belief that caffeinated drinks, such as tea, \\n# may adversely affect hydration. This was investigated in a randomised \\n# controlled trial ... revealed no significant differences\\n# between tea and water for any of the mean blood or urine measurements…”\\n# Embed all queries to evaluate quality compared to \"gold\" answers\\nquery_embeddings = embed_text(questions, model, \"RETRIEVAL_QUERY\")\\nq_scores, q_doc_ids = index.search(query_embeddings, 10)\\n# Create a dict of query to document scores dict for pytrec evaluation  \\n# Multiply scores by -1 for sorting as smaller distance is better score for pytrec eval\\nsearch_qrels = { q_ids[i] : { doc_ids[_id] : -1*s.item() for _id, s in zip(q_doc_ids[i], q_\\nscores[i])} for i in range(len(q_ids))}\\nevaluator = pytrec_eval.RelevanceEvaluator(qrels, {\\'ndcg_cut.10\\',\\'P_1\\',\\'recall_10\\'})\\neval_results = evaluator.evaluate(search_qrels)\\ndf = pd.DataFrame.from_dict(eval_results, orient=\\'index\\')\\ndf.mean()\\n#P_1            0.517028 // precision@1\\n#recall_10      0.203507 // recall@10\\n#ndcg_cut_10    0.402624 // nDCG@10', 'Embeddings & Vector Stores\\n16\\nFebrurary 2025\\nthat application. For example, a medical application will use different jargon and conventions \\nthan an application focusing on legal use cases. These labeled datasets can be expensive \\nand time consuming to generate using human experts. The Gecko embedding model paper \\nfrom Google DeepMind48 discusses in detail how an LLM was used to generate a large set \\nof synthetic question and document pairs for training, leading to an improved model and \\nperformance on many benchmarks. Using LLMs to assist experts in generating training data \\nand also for the evaluation of answers can be an effective way to scale training, tuning, and \\nevaluation datasets cost effectively.\\nTypes of embeddings\\nEmbeddings aim to obtain a low dimensional representation of the original data while \\npreserving most of the ‘essential information’. The types of data an embedding represents \\ncan be of various different forms.  Below you’ll see some standard techniques used for \\ndifferent types of data, including text and image.\\nText embeddings\\nText embeddings are used extensively as part of natural language processing (NLP). They \\nare often used to embed the meaning of natural language in machine learning for processing \\nin various downstream applications such as text generation, classification, sentiment \\nanalysis, and more. These embeddings broadly fall into two categories: token/word and \\ndocument embeddings.\\nBefore diving deeper into these categories, it’s important to understand the entire lifecycle \\nof text: from its input by the user to its conversion to embeddings.', 'Embeddings & Vector Stores\\n17\\nFebrurary 2025\\nFigure 3. The process of turning text into embeddings\\nIt all starts with the input string which is split into smaller meaningful pieces called tokens.  \\nThis process is called tokenization. Commonly, these tokens are wordpieces, characters, \\nwords, numbers, and punctuations using one of the many existing tokenization techniques.1 \\nAfter the string is tokenized, each of these tokens is then assigned a unique integer value \\nusually in the range: [0, cardinality of the total number of tokens in the corpus]. For example, \\nfor a 16 word vocabulary the IDs would range between 0-15. This value is also referred to as \\ntoken ID. These tokens can be used to represent each string as a sparse numerical vector \\nrepresentation of documents used for downstream tasks directly, or after one-hot encoding. \\nOne-hot encoding is a binary representation of categorical values where the presence \\nof a word is represented by 1, and its absence by 0. This ensures that the token IDs are \\ntreated as categorical values as they are, but often results in a dense vector the size of the \\nvocabulary of the corpus. Snippet 2 and Figure 4 show an example of how this can be done \\nusing Tensorflow.', 'Embeddings & Vector Stores\\n18\\nFebrurary 2025\\n# Tokenize the input string data\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\ndata = [\\n  \"The earth is spherical.\",\\n  \"The earth is a planet.\",\\n  \"I like to eat at a restaurant.\"]\\n# Filter the punctiations, tokenize the words and index them to integers  \\ntokenizer = Tokenizer(num_words=15, filters=\\'!\"#$%&()*+,-./:;<=>?[\\\\\\\\]^_\\'{|}~\\\\t\\\\n\\', lower=True, \\nsplit=\\' \\')\\ntokenizer.fit_on_texts(data)\\n# Translate each sentence into its word-level IDs, and then one-hot encode those IDs \\nID_sequences = tokenizer.texts_to_sequences(data)\\nbinary_sequences = tokenizer.sequences_to_matrix(ID_sequences)\\nprint(\"ID dictionary:\\\\n\", tokenizer.word_index)\\nprint(\"\\\\nID sequences:\\\\n\", ID_sequences)\\nprint(\"\\\\n One-hot encoded sequences:\\\\n\", binary_sequences)\\nSnippet 2. Tokenizing, indexing and one-hot encoding strings\\nFigure 4. Output of Snippet 2\\nHowever, since these Integer IDs (or their corresponding one-hot encoded vectors) are \\nassigned randomly to words, they lack any inherent semantic meaning. This is where \\nembeddings are much more useful. Although it’s possible to embed character and sub-word \\nlevel tokens as well, let us look at word and document embeddings to understand some of \\nthe methods behind them.', 'Embeddings & Vector Stores\\n19\\nFebrurary 2025\\nWord embeddings\\nIn this section, you’ll see a few word embedding techniques and algorithms to both train \\nand use word embeddings which were precusors to the modern text embedding currently \\nbeing used. While there are many ML driven algorithms developed over time optimized for \\ndifferent objectives, the most common ones were GloVe,2 SWIVEL,3 and Word2Vec.4 Word \\nembeddings or sub-word embeddings can also be directly obtained from hidden layers of \\nlanguage models. However, the embeddings will be different for the same word in different \\ncontexts of the text. This section focuses on lightweight, context-free word embedding and \\nleaves the context-aware document embeddings for the document embeddings section. \\nWord embedding can be directly applied to downstream tasks like named entity extraction \\nand topic modeling.\\nWord2Vec is a family of model architectures that operates on the principle of “the semantic \\nmeaning of a word is defined by its neighbors”, or words that frequently appear close to each \\nother in the training corpus. This method can be both used to train your own embeddings \\nfrom large datasets or be quickly integrated through one of the readily available pre-trained \\nembeddings available online.5 The embeddings for each word - which are essentially fixed \\nlength vectors - are randomly initialized to kick off the process, resulting in a matrix of shape \\n(size_of_vocabulary, size_of_each_embedding). This matrix can be used as a lookup table \\nafter the training process is completed using one of the following methods (see Figure 4). \\n• The Continuous bag of words (CBOW) approach: Tries to predict the middle word, using \\nthe embeddings of the surrounding words as input. This method is agnostic to the order \\nof the surrounding words in the context. This approach is fast to train and is slightly more \\naccurate for frequent words.\\n• The skip-gram approach: The setup is inverse of that of CBOW, with the middle word \\nbeing used to predict the surrounding words within a certain range. This approach is \\nslower to train but works well with small data and is more accurate for rare words.', 'Embeddings & Vector Stores\\n20\\nFebrurary 2025\\nFigure 5. Diagram explaining how CBOW and Skip-Gram methods work\\nThe Word2Vec algorithms can also be extended to the sub-word level, which has been the \\ninspiration for algorithms such as FastText.6 However, one of the major caveats of Word2Vec \\nis that although it accounts well for local statistics of words within a certain sliding window, it \\ndoes not capture the global statistics (words in the whole corpus). This shortcoming is what \\nmethods like the GloVe algorithm address.\\nGloVe is a word embedding technique that leverages both global and local statistics of words. \\nIt does this by first creating a co-occurrence matrix, which represents the relationships \\nbetween words. GloVe then uses a factorization technique to learn word representations \\nfrom the co-occurrence matrix. The resulting word representations are able to capture both \\nglobal and local information about words, and they are useful for a variety of NLP tasks.\\nIn addition to GloVE, SWIVEL is another approach which leverages the co-occurrence \\nmatrix to learn word embeddings. SWIVEL stands for Skip-Window Vectors with Negative \\nSampling. Unlike GloVE, it uses local windows to learn the word vectors by taking into', 'Embeddings & Vector Stores\\n21\\nFebrurary 2025\\naccount the co-occurrence of words within a fixed window of its neighboring words. \\nFurthermore, SWIVEL also considers unobserved co-occurrences and handles it using a \\nspecial piecewise loss, boosting its performance with rare words. It is generally considered \\nonly slightly less accurate than GloVe on average, but is considerably faster to train. This is \\nbecause it leverages distributed training by subdividing the Embedding vectors into smaller \\nsub-matrices and executing matrix factorization in parallel on multiple machines. Snippet 2 \\nbelow demonstrates loading pre-trained word embeddings for both Word2Vec and GloVe and \\nvisualizing them in a 2D space, and computing nearest neighbors.', 'Embeddings & Vector Stores\\n22\\nFebrurary 2025\\nfrom gensim.models import Word2Vec \\nimport gensim.downloader as api\\nimport pprint\\nimport matplotlib.pyplot as plt\\nfrom sklearn.manifold import TSNE\\nimport numpy as np\\ndef tsne_plot(models, words, seed=23):\\n  \"Creates a TSNE models & plots for multiple word models for the given words\"\\n  plt.figure(figsize=(len(models)*30, len(models)*30))\\n  model_ix = 0\\n  for model in models:\\n    labels = []\\n    tokens = []\\n    for word in words:\\n      tokens.append(model[word])\\n      labels.append(word)\\n    tsne_model = TSNE(perplexity=40, n_components=2, init=\\'pca\\', n_iter=2500, random_state=seed) \\n    new_values = tsne_model.fit_transform(np.array(tokens))\\n    x = []\\n    y = []\\n    for value in new_values:\\n      x.append(value[0])\\n      y.append(value[1])\\n    model_ix +=1\\n    plt.subplot(10, 10, model_ix)\\n    for i in range(len(x)):\\n      plt.scatter(x[i],y[i])\\n      plt.annotate(labels[i],\\n            xy=(x[i], y[i]),\\n            xytext=(5, 2),\\n            textcoords=\\'offset points\\',\\n            ha=\\'right\\',\\n             va=\\'bottom\\')\\n  plt.tight_layout()\\n  plt.show()\\nv2w_model = api.load(\\'word2vec-google-news-300\\')\\nglove_model = api.load(\\'glove-twitter-25\\')\\nprint(\"words most similar to \\'computer\\' with word2vec and glove respectively:\")\\npprint.pprint(v2w_model.most_similar(\"computer\")[:3])\\npprint.pprint(glove_model.most_similar(\"computer\")[:3]) \\npprint.pprint(\"2d projection of some common words of both models\")\\nsample_common_words = list(set(v2w_model.index_to_key[100:10000])\\n                        & set(glove_model.index_to_key[100:10000]))[:100]\\ntsne_plot([v2w_model, glove_model], sample_common_words)\\nSnippet 3. Loading and plotting GloVe and Word2Vec embeddings in 2D', 'Embeddings & Vector Stores\\n23\\nFebrurary 2025\\nFigure 6 Shows semantically similar words are clustered differently for the two algorithms.\\nFigure 6. 2D visualization of pre-trained GloVe and Word2Vec word embeddings\\nDocument embeddings\\nEmbedding documents to low dimensional dense embedding has attracted long-lasting \\ninterests since the 1980s. Document embeddings can be used in various applications, \\nincluding semantic search, topic discovery, classification, and clustering to embed \\nthe meaning of a series of words in paragraphs and documents and use it for various \\ndownstream applications. The evolution of the embeddings models can mainly be \\ncategorized into two stages: shallow Bag-of-words (BoW) models and deeper pretrained \\nlarge language models.', 'Embeddings & Vector Stores\\n24\\nFebrurary 2025\\nShallow BoW models\\nEarly document embedding works follow the bag-of-words (BoW) paradigm, assuming a \\ndocument is an unordered collection of words. These early works include latent semantic \\nanalysis (LSA)7 and latent dirichlet allocation (LDA).8 Latent semantic analysis (LSA) uses \\na co-occurrence matrix of words in documents and latent dirichlet allocation (LDA) uses a \\nbayesian network to model the document embeddings. Another well known bag-of-words \\nfamily of document embeddings is TF-IDF (term frequency-inverse document frequency) \\nbased models, which are statistical models that use the word frequency to represent the \\ndocument embedding. TF-IDF-based models can either be a sparse embedding, which \\nrepresents the term-level importance, or can be combined with word embeddings as a \\nweighting factor to generate a dense embedding for the documents. For example, BM2549, a \\nTF-IDF-based bag-of-words model, is still a strong baseline in today’s retrieval benchmarks.9\\nHowever,  the bag-of-words paradigm also has two major weaknesses: both the word \\nordering and the semantic meanings are ignored. BoW models fail to capture the sequential \\nrelationships between words, which are crucial for understanding meaning and context. \\nInspired by Word2Vec, Doc2Vec10 was proposed in 2014 for generating document \\nembeddings using (shallow) neural networks. The Doc2Vec model adds an additional \\n‘paragraph’ embedding or, in other words, document embedding in the model of Word2Vec \\nas illustrated in Figure 6. The paragraph embedding is concatenated or averaged with other \\nword embeddings to predict a random word in the paragraph. After training, for existing \\nparagraphs or documents, the learned embeddings can be directly used in downstream \\ntasks. For a new paragraph or document, extra inference steps need to be performed to \\ngenerate the paragraph or document embedding.', 'Embeddings & Vector Stores\\n25\\nFebrurary 2025\\nFigure 7. Doc2vec CBOW model\\nSnippet 4 below shows how you can train your own doc2Vec models on a custom corpus:\\nfrom gensim.test.utils import common_texts\\nfrom gensim.models.Doc2Vec import Doc2Vec, TaggedDocument\\nfrom gensim.test.utils import get_tmpfile\\n#train model on a sequence of documents tagged with their IDs\\ndocuments = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\\nmodel = Doc2Vec(documents, vector_size=8, window=3, min_count=1, workers=6)\\n# persist model to disk, and load it to infer on new documents\\nmodel_file = get_tmpfile(\"Doc2Vec_v1\")\\nmodel.save(model_file)\\nmodel = Doc2Vec.load(model_file)  \\nmodel.infer_vector([\"human\", \"interface\"])\\nSnippet 4. Self-supervised Training and inference using Doc2Vec on private corpus', 'Embeddings & Vector Stores\\n26\\nFebrurary 2025\\nDeeper pretrained large language models\\nMotivated by the development of deep neural networks, different embedding models and \\ntechniques were proposed, and the state-of-the-art models are progressing rapidly. Main \\nchanges of the models include: \\n1. Using more complex learning models, especially bi-directional deep neural network \\nmodels. \\n2. The use of massive pre-training on unlabeled text. \\n3. The use of a subword tokenizer. \\n4. Using fine-tuning for various downstream NLP tasks. \\nIn 2018, BERT11 - which stands for bidirectional encoder representations from transformers - \\nwas proposed with groundbreaking results on 11 NLP tasks. Transformer, the model paradigm \\nBERT based on, has become the mainstream model paradigm until today. Besides using a \\ntransformer as the model backbone, another key of BERT’s success is from pre-training with \\na massive unlabeled corpus. In pretraining, BERT utilized masked language model (MLM) as \\nthe pre-training objective. It did this by randomly masking some tokens of the input and using \\nthe masked token id as the prediction objective. This allows the model to utilize both the \\nright and left context to pretrain a deep bidirectional transformer. BERT also utilizes the next \\nsentence prediction task in pretraining. BERT outputs a contextualized embedding for every \\ntoken in the input. Typically, the embedding of the first token (a special token named [CLS]) is \\nused as the embedding for the whole input.', 'Embeddings & Vector Stores\\n27\\nFebrurary 2025\\nFigure 8. The BERT architecture\\nBERT became the base model for multiple embedding models, including Sentence-BERT,12 \\nSimCSE,13 and E5.14 Meanwhile, the evolution of language models - especially large language \\nmodels - never stops. T550 was proposed in 2019 with up to 11B parameters. PaLM51 was \\nproposed in 2022 to push the large language model to a surprising 540B parameters. Models \\nlike Gemini52 from Google, GPT53 models from OpenAI and Llama54 models from Meta are \\nalso evolving to newer generations at astonishing speed. Please refer to the whitepaper on \\nFoundational models for more information about some common LLMs.\\nNew embedding models based on large language models have been proposed. For example, \\nGTR and Sentence-T5 show better performance on retrieval and sentence similarity \\n(respectively) than BERT family models. Recently, a new embedding model powered by the \\nGemini model backbone has been released on Vertex AI, achieving superior results on all \\npublic benchmarks. Matryoshka Embeddings55,56 allow the downstream user to select how \\nmany dimensions are appropriate for their task to reduce data required for storage and \\nindexing when possible.', 'Embeddings & Vector Stores\\n28\\nFebrurary 2025\\nAnother approach to new embeddings models development is generating multi-vector \\nembeddings instead of a single vector to enhance the representational power of the models. \\nEmbedding models in this family include ColBERT15 and XTR.16 ColPali57 is also an approach \\nusing mult-vectors, but extending their application from text only to join embedding text and \\nimages for multi-modal documents. \\nFigure 9. An illustration of the taxonomy diagram of the embedding models\\nAlthough the deep neural network models require a lot more data and compute time to train, \\nthey have much better performance compared to models using bag-of-words paradigms. \\nFor example, for the same word the embeddings would be different with different contexts, \\nbut by definition that is not true for bag-of-words. Snippet 4 demonstrates how pre-trained \\ndocument embedding models from Tensorflow-hub17 (for example,Sentence t5)A and Vertex \\nAIB can be used for training models with Keras and TF datasets. Vertex Generative AI text \\nembeddings can be used with the Vertex AI SDK, Langchain, and Google’s BigQuery (Snippet \\n5) for embedding and advanced workflows.18\\nA. Note: not all models on https://tfhub.dev/  can be commercially used. Please check the licenses of the models \\nand the training datasets and consult the legal team before commercial usage. \\nB. Note: not all models on https://tfhub.dev/  can be commercially used. Please check the licenses of the models \\nand the training datasets and consult the legal team before commercial usage.', 'Embeddings & Vector Stores\\n29\\nFebrurary 2025\\nimport vertexai\\nfrom vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\\n# Set the model name. For multilingual: use \"text-multilingual-embedding-002\"\\nMODEL_NAME = \"text-embedding-004\"\\n# Set the task_type, text and optional title as the model inputs.\\n# Available task_types are \"RETRIEVAL_QUERY\", \"RETRIEVAL_DOCUMENT\", \\n# \"SEMANTIC_SIMILARITY\", # \"CLASSIFICATION\", and \"CLUSTERING\"\\nTASK_TYPE = \"RETRIEVAL_DOCUMENT\" \\nTITLE = \"Google\"\\nTEXT = \"Embed text.\"\\n# Use Vertex LLM text embeddings\\nembeddings_vx = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@004\")\\ndef LLM_embed(text):\\n    def embed_text(text):\\n        text_inp = TextEmbeddingInput(task_type=\"CLASSIFICATION\",   text=text.numpy())\\n        return np.array(embeddings_vx.get_embeddings([text_inp])[0].values)\\n output = tf.py_function(func=embed_text, inp=[text], Tout=tf.float32)\\n output.set_shape((768,))\\n return output\\n# Embed strings using vertex LLMs\\nLLM_embeddings=train_data.map(lambda x,y: ((LLM_embed(x), y))\\n# Embed strings in the tf.dataset using one of the tf hub models\\nembedding = \"https://tfhub.dev/google/sentence-t5/st5-base/1\"\\nhub_layer = hub.KerasLayer(embedding, input_shape=[],dtype=tf.string, trainable=True)\\n                          \\n# Train model \\nmodel = tf.keras.Sequential()\\nmodel.add(hub_layer) # omit this layer if using Vertex LLM embeddings\\nmodel.add(tf.keras.layers.Dense(16, activation=\\'relu\\'))\\nmodel.add(tf.keras.layers.Dense(1))\\nmodel.compile(optimizer=\\'adam\\',loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\\n  metrics=[\\'accuracy\\'])\\nhistory = model.fit(train_data.shuffle(100).batch(8))\\nSnippet 4. Creating & integrating text embeddings (Vertex, Tfhub) into keras text classification models', 'Embeddings & Vector Stores\\n30\\nFebrurary 2025\\nSELECT * FROM ML.GENERATE_TEXT_EMBEDDING(\\nMODEL my_project.my_company.llm_embedding_model,\\n(\\nSELECT review as content\\nFROM bigquery-public-data.imdb.reviews));\\nSnippet 5. Creating LLM based text embeddings in BigQuery for selected columns in a table\\nImage & multimodal embeddings\\nMuch like text, it’s also possible to create both image and multimodal embeddings. \\nUnimodal image embeddings can be derived in many ways such as by training a CNN or \\nVision Transformer model on a large scale image classification task (for example, Imagenet), \\nand then using the penultimate layer as the image embedding. This layer has learnt some \\nimportant discriminative feature maps for the training task. It contains a set of feature maps \\nthat are discriminative for the task at hand and can be extended to other tasks as well. \\nTo obtain multimodal embeddings19 you take the individual unimodal text and image \\nembeddings and create the joint embedding of their semantic relationships learnt via another \\ntraining process. This gives you a fixed size semantic representation in the same latent \\nspace. Snippet 6 computes image and multimodal embeddings for images and text and can \\nbe used with a keras model directly (much like the text embedding example). Multimodal \\nembedding approaches like ColPali57 use image models to enable retrieval from text queries \\non multimodal documents without complex OCR or layout preprocesing. The model searches \\nthe images as they would be displayed to a user in a web browser or pdf viewer rahter than \\nhaving to convert to a text only form for indexing.', 'Embeddings & Vector Stores\\n31\\nFebrurary 2025\\nimport base64\\nimport tensorflow as tf\\nfrom google.cloud import aiplatform\\nfrom google.protobuf import struct_pb2\\n#fine-tunable layer for image embeddings which can be used for downstream keras modelimage_\\nembed=hub.KerasLayer(\"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/feature_\\nvector/2\",trainable=False) \\nclass EmbeddingPredictionClient:\\n  \"\"\"Wrapper around Prediction Service Client.\"\"\"\\n  def __init__(self, project : str,\\n    location : str = \"us-central1\",\\n    api_regional_endpoint: str = \"us-central1-aiplatform.googleapis.com\"):\\n    client_options = {\"api_endpoint\": api_regional_endpoint}\\n    self.client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)  \\n    self.location = location\\n    self.project = project\\n  def get_embedding(self, text : str = None, gs_image_path : str = None):\\n   #load the image from a bucket in google cloud storage\\n   with tf.io.gfile.GFile(gs_image_path, \"rb\") as f:\\n     image_bytes = f.read()\\n   if not text and not image_bytes:\\n    raise ValueError(\\'At least one of text or image_bytes must be specified.\\')\\n   #Initialize a protobuf data struct with the text and image inputs \\n   instance = struct_pb2.Struct()\\n    if text:\\n      instance.fields[\\'text\\'].string_value = text\\n      if image_bytes:\\n      encoded_content = base64.b64encode(image_bytes).decode(\"utf-8\")\\n      image_struct = instance.fields[\\'image\\'].struct_value\\n      image_struct.fields[\\'bytesBase64Encoded\\'].string_value = .string_value = encoded_content\\n     #Make predictions using the multimodal embedding model\\n     instances = [instance]\\n     endpoint = (f\"projects/{self.project}/locations/{self.location}\"\\n         \"/publishers/google/models/multimodalembedding@001\")\\n     response = self.client.predict(endpoint=endpoint, instances=instances)\\n     text_embedding = None\\n     if text:    \\n      text_emb_value = response.predictions[0][\\'textEmbedding\\']\\n      text_embedding = [v for v in text_emb_value]\\n     image_embedding  = None\\n     if image_bytes:    \\n      image_emb_value = response.predictions[0][\\'textEmbedding\\']\\n      image_embedding  = [v for v in image_emb_value]\\nContinues next page...', 'Embeddings & Vector Stores\\n32\\nFebrurary 2025\\n \\n return EmbeddingResponse (text_embedding=text_embedding, image_embedding =image_embedding)\\n#compute multimodal embeddings for text and images\\nclient.get_embedding(text=\"sample_test\", gs_image_path=\"gs://bucket_name../image_filename..\")\\nSnippet 6. Using Vertex API to create Multimodal embeddings Graph embeddings\\nStructured data embeddings\\nStructured data refers to data has a defined schema, like an table in a database \\nwhere individual fields have known types and definitions. Unlike unstructured text and \\nimage data, where a pre-trained embedding model is typically available, we have to \\ncreate the embedding model for the structured data since it would be specific to a \\nparticular application.\\nGeneral structured data\\nGiven a general structured data table, we can create embeddings for each row. This can be \\ndone by the ML models in the dimensionality reduction category, such as the PCA model.\\nOne use case for these embeddings are for anomaly detection. For example, we can create \\nembeddings for anomaly detection using large data sets of labeled sensor information \\nthat identify anomalous occurrences.20 Another case use is to feed these embeddings \\nto downstream ML tasks such as classification. Compared to using the original high-\\ndimensional data, using embeddings to train a supervised model requires less data. This is \\nparticularly important in cases where training data is not sufficient.', 'Embeddings & Vector Stores\\n33\\nFebrurary 2025\\nUser/item structured data\\nThe input is no longer a general structured data table as above. Instead, the input includes \\nthe user data, item/product data plus the data describing the interaction between user and \\nitem/product, such as rating score. \\nThis category is for recommendation purposes, as it maps two sets of data (user dataset, \\nitem/product/etc dataset) into the same embedding space. For recommender systems, we \\ncan create embeddings out of structured data that correlate to different entities such as \\nproducts, articles, etc. Again, we have to create our own embedding model. Sometimes this \\ncan be combined with unstructured embedding methods when images or text descriptions \\nare found.\\nGraph embeddings\\nGraph embeddings are another embedding technique that lets you represent not \\nonly information about a specific object but also its neighbors (namely, their graph \\nrepresentation). Take an example of a social network where each person is a node, and the \\nconnections between people are defined as edges. Using graph embedding you can model \\neach node as an embedding, such that the embedding captures not only the semantic \\ninformation about the person itself, but also its relations and associations hence enriching \\nthe embedding. For example, if two nodes are connected by an edge, the vectors for those \\nnodes would be similar. You might then be able to predict who the person is most similar \\nto and recommend new connections. Graph embeddings can also be used for a variety of \\ntasks, including node classification, graph classification, link prediction, clustering, search, \\nrecommendation systems, and more. Popular algorithms21,22 for graph embedding include \\nDeepWalk, Node2vec, LINE, and GraphSAGE.23', 'Embeddings & Vector Stores\\n34\\nFebrurary 2025\\nTraining Embeddings\\nCurrent embedding models usually use dual encoder (two tower) architecture. For example, \\nfor the text embedding model used in question-answering, one tower is used to encode \\nthe queries and the other tower is used to encode the documents. For the image and text \\nembedding model, one tower is used to encode the images and the other tower is used \\nto encode the text. The model can have various sub architectures, depending on how the \\nmodel components are shared between the two towers. The following figure shows some \\narchitectures of the dual encoders.24 \\nFigure 10. Some architectures of dual encoders\\nThe loss used in embedding models training is usually a variation of contrastive loss, which \\ntakes a tuple of <inputs, positive targets, [optional] negative targets> as the inputs. Training \\nwith contrastive loss brings positive examples closer and negative examples far apart.\\nSimilar to foundation model training, training of an embedding model from scratch usually \\nincludes two stages: pretraining (unsupervised learning) and fine tuning (supervised \\nlearning). Nowadays, the embedding models are usually directly initialized from foundation \\nmodels such as BERT, T5, GPT, Gemini, CoCa. You can use these base models to leverage the \\nmassive knowledge that has been learned from the large-scale pretraining of the foundation', 'Embeddings & Vector Stores\\n35\\nFebrurary 2025\\nmodels. The fine-tuning of the embedding models can have one or more phases. The fine-\\ntuning datasets can be created in various methods, including human labeling, synthetic \\ndataset generation, model distillation, and hard negative mining.\\nTo use embeddings for downstream tasks like classification or named entity recognition, \\nextra layers (for example, softmax classification layer) can be added on top of the embedding \\nmodels. The embedding model can either be frozen (especially when the training dataset is \\nsmall), trained from scratch, or fine-tuned together with the downstream tasks. \\nVertex AI provides the ability to customize the Vertex AI text embedding models.25 Users can \\nalso choose to fine-tune the models directly. An example is fine tuning the BERT model using \\ntensorflow model garden26. You can also directly load the embedding models from tfhub and \\nfine-tune on top of the model. Snippet 7 shows an example how to build a classifier based on \\ntfhub models. \\n# Can switch the embedding to different embeddings from different modalities on # \\ntfhub. Here we use the BERT model as an example.\\ntfhub_link = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\\nclass Classifier(tf.keras.Model):\\n    def __init__(self, num_classes):\\n      super(Classifier, self).__init__(name=\"prediction\")\\n        self.encoder = hub.KerasLayer(tfhub_link, trainable=True)\\n      self.dropout = tf.keras.layers.Dropout(0.1)\\n      self.dense = tf.keras.layers.Dense(num_classes)\\n    def call(self, preprocessed_text):\\n      encoder_outputs = self.encoder(preprocessed_text)\\n      pooled_output = encoder_outputs[\"pooled_output\"]\\n   x = self.dropout(pooled_output)\\n   x = self.dense(x)\\n   return x\\nSnippet 7. Creating a Keras model using trainable tfhub layer', 'Embeddings & Vector Stores\\n36\\nFebrurary 2025\\nSo far you’ve seen the various types of embeddings, techniques and best practices to train \\nthem for various data modalities, and some of their applications. The next section discusses \\nhow to persist and search the embeddings that have been created in a fast and scalable way \\nfor production workloads.\\nVector search\\nFull-text keyword search has been the lynchpin of modern IT systems for years. Full-text \\nsearch engines and databases (relational and non-relational) often rely on explicit keyword \\nmatching. For example, if you search for ‘cappuccino’ the search engine or database returns \\nall documents that mention the exact query in the tags or text description. However, if the \\nkey word is misspelled or described with a differently worded text, a traditional keyword \\nsearch returns incorrect or no results. There are traditional approaches which are tolerant of \\nmisspellings and other typographical errors. However, they are still unable to find the results \\nhaving the closest underlying semantic meanings to the query. This is where vector search \\nis very powerful: it uses the vector or embedded semantic representation of documents. As \\nvector search works on any sort of embedding it also allows search on images, videos, and \\nother data types in addition to text.\\nVector search lets you to go beyond searching for exact query literals and allows you to \\nsearch for the meaning across various data modalities. This allows you to find relevant \\nresults even when the wording is different. After you have a function that can compute \\nembeddings of various items,  you compute the embedding of the items of interest and store \\nthis embedding in a database. You then embed the incoming query in the same vector space \\nas the items. Next, you have to find the best matches to the query. This process is analogous', 'Embeddings & Vector Stores\\n37\\nFebrurary 2025\\nto finding the most ‘similar’ matches across the entire collection of searchable vectors: \\nsimilarity between vectors can be computed using a metric such as euclidean distance, \\ncosine similarity, or dot product.\\nFigure 11. Visualization of how different metrics compute vector similarity\\nEuclidean distance (i.e., L2 distance) is a geometric measure of the distance between two \\npoints in a vector space. This works well for lower dimensions. Cosine similarity is a measure \\nof the angle between two vectors. And inner/dot product, is the projection of one vector \\nonto another. They are equivalent when the vector norms are 1. This seems to work better \\nfor higher dimensional data. Vector databases store and help manage and operationalize the \\ncomplexity of vector search at scale, while also addressing the common database needs.\\nImportant vector search algorithms\\nThe most straightforward way to find the most similar match is to run a traditional linear \\nsearch by comparing the query vector with each document vector and return the one with \\nthe highest similarity. However, the runtime of this approach scales linearly (O(N)) with the \\namount of documents or items to search. This approach is unacceptably slow for most', 'Embeddings & Vector Stores\\n38\\nFebrurary 2025\\nuse cases involving several millions of documents or more. Using approximate nearest \\nneighbour (ANN) search for that purpose is more practical.  ANN is a technique for finding \\nthe closest points to a given point in a dataset with a small margin of error - but with far less \\ncomputations required as the search space is greatly reduced to O(logN). There are many \\napproaches with varying trade-offs across scale, indexing time, performance, simplicity and \\nmore.26 They use one or more implementations of the following techniques: quantization, \\nhashing, clustering and trees, among others. Some of the most popular approaches are \\ndiscussed below.\\nLocality sensitive hashing & trees\\nLocality sensitive hashing (LSH) 27 is a technique for finding similar items in a large dataset. \\nIt does this by creating one or more hash functions that map similar items to the same hash \\nbucket with high probability. This means that you can quickly find all of the similar items to \\na given item by only looking at the candidate items in the same hash bucket (or adjacent \\nbuckets) and do a linear search amongst those candidate pairs. This allows for significantly \\nfaster lookups within a specific radius. The number of hash functions/tables and buckets \\ndetermine the search recall/speed tradeoff, as well as the false positive / true positive one. \\nHaving too many hash functions might cause similar items to different buckets, while too few \\nmight result in too many items falsely being hashed to the same bucket and the number of \\nlinear searches to increase.\\nAnother intuitive way to think about LSH is grouping residences by their postal code or \\nneighborhood name. Then based on where someone chooses to move you look at the \\nresidences for only that neighborhood and find the closest match.', 'Embeddings & Vector Stores\\n39\\nFebrurary 2025\\nFigure 12. Visualization of how LSH uses random hyperplanes to partition the vector space\\nTree-based algorithms work similarly. For example, the Kd-tree approach works by creating \\nthe decision boundaries by computing the median of the values of the first dimension, then \\nthat of the second dimension and so on. This approach is very much like a decision tree. \\nNaturally this can be ineffective if searchable vectors are high dimensional. In that case, the \\nBall-tree algorithm is better suited. It is similar in functionality, except instead of going by \\ndimension-wise medians it creates buckets based on the radial distance of the data points \\nfrom the center. Here is an example of the implementation of these three approaches:', 'Embeddings & Vector Stores\\n40\\nFebrurary 2025\\nfrom sklearn.neighbors import NearestNeighbors\\nfrom vertexai.language_models import TextEmbeddingModel\\nfrom lshashing import LSHRandom\\nimport numpy as np\\nmodel = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@004\")\\ntest_items= [\\n  \"The earth is spherical.\",\\n  \"The earth is a planet.\",\\n  \"I like to eat at a restaurant.\"]\\nquery = \"the shape of earth\"\\nembedded_test_items = np.array([embedding.values for embedding in model.get_embeddings(test_items)])\\nembedded_query = np.array(model.get_embeddings([query])[0].values)\\n#Naive brute force search\\nn_neighbors=2\\nnbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=\\'brute\\').fit(embedded_test_items)\\nnaive_distances, naive_indices = nbrs.kneighbors(np.expand_dims(embedded_query, axis = 0))\\n#algorithm- ball_tree due to high dimensional vectors or kd_tree otherwise\\nnbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=\\'ball_tree\\').fit(embedded_test_items) \\ndistances, indices = nbrs.kneighbors(np.expand_dims(embedded_query, axis = 0))\\n#LSH\\nlsh_random_parallel = LSHRandom(embedded_test_items, 4, parallel = True)\\nlsh_random_parallel.knn_search(embedded_test_items, embedded_query, n_neighbors, 3, parallel = True)\\n#output for all 3 indices = [0, 1] , distances [0.66840428, 0.71048843] for the first 2 neighbours\\n#ANN retrieved the same ranking of items as brute force in a much scalable manner\\nSnippet 8. Using scikit-learn 28  and lshashing 29  for ANN with LSH, KD/Ball-tree and linear search\\nHashing and tree-based approaches can also be combined and extended upon to obtain \\nthe optimal tradeoff between recall and latency for search algorithms. FAISS with HNSW and \\nScaNN32,33 are good examples.', 'Embeddings & Vector Stores\\n41\\nFebrurary 2025\\nHierarchical navigable small worlds \\nFigure 13. Diagram showing how HNSW ‘zooms in’ to perform ANN\\nOne of the FAISS (Facebook AI similarity search) implementations leverages the concept \\nof hierarchical navigable small world (HNSW)30 to perform vector similarity search in sub-\\nlinear (O(Logn)) runtime with a good degree of accuracy. A HNSW is a proximity graph with a \\nhierarchical structure where the graph links are spread across different layers. The top layer \\nhas the longest links and the bottom layer has the shortest ones. As shown in Figure 13, the \\nsearch starts at the topmost layer where the algorithm greedily traverses the graph to find \\nthe vertex most semantically similar to the query. Once the local minimum for that layer is \\nfound, it then switches to the graph for the closest vertex on the layer below. This process \\ncontinues iteratively until the local minimum for the lowest layer is found, with the algorithm \\nkeeping track of all the vertices traversed to return the K-nearest neighbors. This algorithm \\ncan be optionally augmented with quantization and vector indexing to boost speed and \\nmemory efficiency.', 'Embeddings & Vector Stores\\n42\\nFebrurary 2025\\n# Create an endpoint\\nmy_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\\n display_name=f\"{DISPLAY_NAME}-endpoint\", public_endpoint_enabled=True\\n)\\n# NOTE : This operation can take upto 20 minutes\\nmy_index_endpoint = my_index_endpoint.deploy_index(\\n index=my_index, deployed_index_id=DEPLOYED_INDEX_ID\\n)\\n# retrieve the id of the most recently deployed index or manually look up the index \\ndeployed above\\nindex_id=my_index_endpoint.deployed_indexes[-1].index.split(\"/\")[-1]\\nendpoint_id= my_index_endpoint.name\\n# TODO : replace 1234567890123456789 with your acutial index ID\\nmy_index = aiplatform.MatchingEngineIndex(index_id)\\n# TODO : replace 1234567890123456789 with your acutial endpoint ID\\n# Be aware that the Index ID differs from the endpoint ID\\nmy_index_endpoint = aiplatform.MatchingEngineIndexEndpoint(endpoint_id)\\n# Input texts\\ntexts= [\\n   \"The earth is spherical.\",\\n   \"The earth is a planet.\",\\n   \"I like to eat at a restaurant.\",\\n]\\n# Create a Vector Store\\nvector_store = VectorSearchVectorStore.from_components(\\n    project_id=PROJECT_ID,\\n    region=REGION,\\n    gcs_bucket_name=BUCKET,\\n    index_id=my_index.name,\\n    endpoint_id=my_index_endpoint.name,\\n    embedding=embedding_model,\\n    stream_update=True,\\n)\\n# Add vectors and mapped text chunks to your vectore store\\nvector_store.add_texts(texts=texts)\\n# Initialize the vectore_store as retriever\\nretriever = vector_store.as_retriever()\\nContinues next page...', 'Embeddings & Vector Stores\\n43\\nFebrurary 2025\\nSnippet 9. Build/deploy ANN Index for Vertex AI Vector Search and use RAG with LLM prompts to generate \\ngrounded results/sources. \\nretriever=vector_store.as_retriever(search_kwargs={\\'k\\':1 })\\n#create custom prompt for your use case\\nprompt_template=\"\"\"You are David, an AI knowledge bot.\\nAnswer the questions using the facts provided. Use the provided pieces of context to answer\\nthe users question.\\nIf you don\\'t know the answer, just say that \"I don\\'t know\", don\\'t try to make up an answer.\\n{summaries}\"\"\"\\nmessages = [\\n    SystemMessagePromptTemplate.from_template(prompt_template),\\n    HumanMessagePromptTemplate.from_template(\"{question}\")\\n]\\nprompt = ChatPromptTemplate.from_messages(messages)\\nchain_type_kwargs = {\"question\": prompt}\\n#initialize your llm model\\nllm = VertexAI(model_name=\"gemini-pro\")\\n#build your chain for RAG+C\\nchain= RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", \\nretriever=retriever, return_source_documents=True)\\n#print your results with Markup language\\ndef print_result(result):\\n  output_text = f\"\"\"### Question:\\n  {query}\\n  ### Answer:\\n  {result[\\'result\\']}\\n  ### Source:\\n  {\\' \\'.join(list(set([doc.page_content for doc in result[\\'source_documents\\']])))}\\n  \"\"\"\\n  return(output_text)\\nchain= \"What shape is the planet where humans live?\"\\nresult = chain(query)\\ndisplay(Markdown(print_result(result)))', 'Embeddings & Vector Stores\\n44\\nFebrurary 2025\\nimport faiss\\nM=32 #creating high degree graph:higher recall for larger index & searching time\\nd=768 # dimensions of the vectors/embeddings\\nindex = faiss.IndexHNSWFlat(d, M)\\nindex.add(embedded_test_items) #build the index using the embeddings in Snippet 9\\n#execute the ANN search\\nindex.search(np.expand_dims(embedded_query, axis=0), k=2)\\nSnippet 10. Indexing and executing ANN search with the FAISS library using HNSW\\nScaNN\\nGoogle developed the scalable approximate nearest neighbor (ScaNN)31,32 approach which \\nis used across a lot of its products and services. This includes being externally available \\nto all customers of Google Cloud through the Vertex AI Vector Search and Google Cloud \\nDatabases, including AlloyDB, Cloud Spanner, and Cloud SQL MySQL. Below is how ScaNN \\nuses a variety of steps to perform efficient vector search, with each one of them having their \\nown subset of parameters. \\nThe first step is the optional partitioning step during training: it uses one of the multiple \\nalgorithms available to partition the vector store into logical partitions/clusters where \\nthe semantically related are grouped together. The partitioning step is optional for small \\ndatasets. However, for larger datasets with >100k embedding vectors, the partitioning step \\nis crucial since by pruning the search space it cuts down the search space by magnitudes \\ntherefore significantly speeds up the query. The space pruning is configured through the \\nnumber of partitions and the number of partitions to search. A larger number leads to better \\nrecall but larger partition creation time. A good heuristic is to set the number of partitions to \\nbe the square root of the number of vectors.', 'Embeddings & Vector Stores\\n45\\nFebrurary 2025\\nFigure 14. Search space partitioning & pruning (left) & Approximate scoring (right)\\nAt query time ScaNN uses the user-specified distance measure to select the specified \\nnumber of top partitions (a value specified by the user), and then executes the scoring \\nstep next. In this step ScaNN compares the query with all the points in the top partitions \\nand selects the top K’. This distance computation can be configured as exact distance or \\napproximate distance. The approximate distance computation leverages either standard \\nproduct quantization or anisotropic quantization techniques, the latter of which is a specific \\nmethod employed by ScaNN which gives the better speed and accuracy tradeoffs.\\nFinally, as a last step the user can optionally choose to rescore the user specified top K \\nnumber of results more accurately. This results in an industry leading speed/accuracy \\ntradeoff ScaNN is known for as can be inferred from Figure 14. Snippet 10 shows a \\ncode example.', 'Embeddings & Vector Stores\\n46\\nFebrurary 2025\\nFigure 15. Accuracy/speed tradeoffs for various SOTA ANN search algorithms 58', 'Embeddings & Vector Stores\\n47\\nFebrurary 2025\\nimport tensorflow as tf\\nimport tensorflow_recommenders as tfrs\\nfrom vertexai.language_models import TextEmbeddingModel, TextEmbeddingInput\\n# Embed documents & query(from snip 9.) and convert them to tensors and tf.datasets\\nembedded_query = tf.constant((LM_embed(query, \"RETRIEVAL_QUERY\")))\\nembedded_docs = [LM_embed(doc, \"RETIREVAL_DOCUMENT\") for doc in searchable_docs]\\nembedded_docs = tf.data.Dataset.from_tensor_slices(embedded_docs).enumerate().batch(1)\\n# Build index from tensorflow dataset and execute ANN search based on dot product metric\\nscann = tfrs.layers.factorized_top_k.ScaNN( \\n  distance_measure= \\'dot_product\\',\\n  num_leaves = 4, #increase for higher number of partitions / latency for increased recall\\n  num_leaves_to_search= 2) # increase for higher recall but increased latency\\nscann = scann.index_from_dataset(embedded_docs)\\nscann(embedded_query, k=2)\\nSnippet 11. Using Tensorflow Recommenders 33  to perform ANN search using the ScaNN algorithm\\nIn this whitepaper we have seen both current and traditional ANN search algorithms: ScaNN, \\nFAISS , LSH, KD-Tree, and Ball-tree, and examined the  great speed/accuracy tradeoffs \\nthat they provide. However, to use these algorithms they need to be deployed in a scalable, \\nsecure and production-ready manner. For that we need vector databases.\\nVector databases \\nVector embeddings embody semantic meanings of data, while vector search algorithms \\nprovide a means for efficiently querying them. Historically traditional databases lacked \\nthe means to combine semantic meaning and efficient querying. This is what gave rise to \\nvector databases, which are built ground-up to manage these embeddings for production \\nscenarios. Due to the recent popularity of Generative AI, an increasing number of traditional', 'Embeddings & Vector Stores\\n48\\nFebrurary 2025\\ndatabases are starting to incorporate supporting vector search functionality in addition to \\ntraditional search (‘hybrid search’) functionalities. Let’s look at the workflow for a simple \\nVector Database, with hybrid search capabilities.\\nFigure 16. Populating and querying vector databases\\nEach vector database differs in its implementation, but the general flow is shown in Figure 16:\\n1. An appropriate trained embedding model is used to embed the relevant data points as \\nvectors with fixed dimensions. \\n2. The vectors are then augmented with appropriate metadata and complementary \\ninformation (such as tags) and indexed using the specified algorithm for efficient search.\\n3. An incoming query gets embedded with the appropriate model, and used to search for \\nthe most semantically similar items and their associated unembedded content/metadata. \\nSome databases might provide caching and pre-filtering (based on tags) and post-filtering \\ncapabilities (reranking using another more accurate model) to further enhance the query \\nspeed and performance.', 'Embeddings & Vector Stores\\n49\\nFebrurary 2025\\nThere are quite a few vector databases available today, each tailored to different business \\nneeds and considerations. A few good examples of commercially managed vector \\ndatabases include Google Cloud’s Vertex Vector Search,34 Google Cloud’s AlloyDB & Cloud \\nSQL Postgres ElasticSearch,35 and Pinecone36 to name a few. Vertex AI Vector Search is \\na vector database built by Google that uses the ScaNN algorithm for fast vector search, \\nwhile still maintaining all the security and access guarantees of Google Cloud. AlloyDB & \\nCloud SQL Postgres supports vector search through the OSS pgvector37 extension, which \\nallows for SQL queries to combine ANN search with traditional predicates and the usual \\ntransactional semantics for ANN search index. AlloyDB also has a ScaNN index extension \\nthat is a native implementation of ScaNN and is pgvector-compatible. Similarly, many other \\ntraditional databases have also started to add plugins to enable vector search. Pinecone37 \\nand Weaviate39 leverage HNSW for their fast vector search in addition to the ability to filter \\ndata using traditional search. Amongst their open source peers: Weaviate38 and ChromaDB39 \\nprovide a full suite of functionality upon deployment and can be tested in memory as well \\nduring the prototyping phase.\\nOperational considerations\\nVector Databases are critical to managing the majority of technical challenges that arise \\nwith storing and querying embeddings at scale. Some of these challenges are specific to the \\nnature of vector stores, while others overlap with that of traditional databases. These include \\nhorizontal and vertical scalability, availability, data consistency, real time updates, backups, \\naccess control, compliance, and much more. However, there are also many more challenges \\nand considerations you need to take into account while using embedding and vector stores.\\nFirstly, embeddings, unlike traditional content, can mutate over time. This means that the \\nsame text, image, video or other content could and should be embedded using different \\nembedding models to optimize for the performance of the downstream applications. This is', 'Embeddings & Vector Stores\\n50\\nFebrurary 2025\\nespecially true for embeddings of supervised models after the model is retrained to account \\nfor various drifts or changing objectives. Similarly, the same applies to unsupervised models \\nwhen they are updated to a newer model. However, frequently updating the embeddings \\n- especially those trained on large amounts of data - can be prohibitively expensive. \\nConsequently, a balance needs to be struck. This necessitates a well-defined automated \\nprocess to store, manage, and possibly purge embeddings from the vector databases taking \\nthe budget into consideration.\\nSecondly, while embeddings are great at representing semantic information, sometimes they \\ncan be suboptimal at representing literal or syntactic information. This is especially true for \\ndomain-specific words or IDs. These values are potentially missing or underrepresented \\nin the data the embeddings models were trained on. For example, if a user enters a query \\nthat contains the ID of a specific number along with a lot of text, the model might find \\nsemantically similar neighbors which match the meaning of the text closely, but not the ID, \\nwhich is the most important component in this context. You can overcome this challenge by \\nusing a combination of full-text search to pre-filter or post-filter the search space before \\npassing it onto the semantic search module.\\nAnother important point to consider is that depending on the nature of the workload in which \\nthe semantic query occurs, it might be worth relying on different vector databases. For \\nexample, for OLTP workloads that require frequent reads/write operations, an operational \\ndatabase like AlloyDB, Spanner, Postgres, or CloudSQL is the best choice. For large-\\nscale OLAP analytical workloads and batch use cases, using BigQuery’s vector search \\nis preferable.\\nIn conclusion, a variety of factors need to be considered when choosing a vector database. \\nThese factors include size and type of your dataset (some are good at sparse and others \\ndense), business needs, the nature of the workload,  budget, security, privacy guarantees, \\nthe needs for semantic and syntactic search as well as the database systems that are already', 'Embeddings & Vector Stores\\n51\\nFebrurary 2025\\nin use. In this section we have seen the various ANN search approaches as well the need and \\nbenefits of vector databases. The next section demonstrates an example of using a Vector AI \\nVector Search for semantic search.\\nApplications\\nEmbeddings models are one of the fundamental machine learning models that power a \\nvariety of applications. We summarize some popular applications in the following table. \\nTask Description\\nRetrieval\\nGiven a query and a set of objects (for example, documents, images, \\nand videos), retrieve the most relevant objects. Based on the definition \\nof relevant objects, the subtasks include question answering and \\nrecommendations.\\nSemantic text similarity\\nDetermine whether two sentences have the same semantic meaning. \\nThe subtasks include: paraphrasing, duplicate detection, and bitext \\nmining.\\nClassification\\nClassify objects into possible categories. Based on the number of labels, \\nthe subtasks include binary classification, multi-class classification, and \\nmultilabel classifications.\\nClustering Cluster similar objects together.\\nReranking Rerank a set of objects based on a certain query.\\n \\nEmbeddings together with vector stores providing ANN are powerful tools which can be used \\nfor a variety of applications. These include Retrieval Augmented Generation (RAG) for LLMs, \\nSearch, Recommendation Systems, Anomaly detection, few shot- classification and much \\nmore.', 'Embeddings & Vector Stores\\n52\\nFebrurary 2025\\nFor ranking problems like search and recommendations, embeddings are normally used \\nat the first stage of the process. They retrieve the potentially good candidates that are \\nsemantically similar and consequently improve the relevance of search results. Since the \\namount of information to sort through can be quite large (in some cases even millions or \\nbillions) ANN techniques like ScaNN greatly aids in scalably narrowing the search space. This \\ninitial set of results can be further refined with a more sophisticated model on this smaller set \\nof candidates. \\nLet’s look at an application which combines both LLMs and RAG to help answer questions.\\nQ & A with sources (retrieval augmented generation)\\nRetrieval augmented generation (RAG) for Q&A is a technique that combines the best of both \\nworlds from retrieval and generation. It first retrieves relevant documents from a knowledge \\nbase and then uses prompt expansion to generate an answer from those documents. Prompt \\nexpansion is a technique that when combined with database search can be very powerful. \\nWith prompt expansion the model retrieves relevant information from the database (mostly \\nusing a combination of semantic search and business rules), and augments the original \\nprompt with it. The model uses this augmented prompt to generate much more interesting, \\nfactual, and informative content than with retrieval or generation alone.\\nRAG can help with two common problems with LLMs: 1) their tendency to ‘hallucinate’ \\nand generate factually incorrect but plausible sounding responses and 2) the high cost of \\nretraining to keep up with current information as newer data can be supplied via the prompt, \\nrather than at model training. Although RAG can reduce hallucinations, it does not completely \\neliminate them. What can help mitigate this problem further is to also return the sources from \\nthe retrieval and do a quick coherence check either by a human or an LLM. This ensures the', 'Embeddings & Vector Stores\\n53\\nFebrurary 2025\\nLLM response is consistent with the semantically relevant sources. Let’s look at an example \\n(Snippet 11) of RAG with sources, which can be scalably implemented using Vertex AI LLM \\ntext embeddings and Vertex AI Vector Search in conjunction with libraries like langchain.40', 'Embeddings & Vector Stores\\n54\\nFebrurary 2025\\n# Before you start run this command:\\n# pip install --upgrade --user --quiet google-cloud-aiplatform langchain_google_vertexai\\n# after running pip install make sure you restart your kernel\\n# TODO : Set values as per your requirements\\n# Project and Storage Constants\\nPROJECT_ID = \"<my_project_id>\"\\nREGION = \"<my_region>\"\\nBUCKET = \"<my_gcs_bucket>\"\\nBUCKET_URI = f\"gs://{BUCKET}\"\\n# The number of dimensions for the text-embedding-005 is 768\\n# If other embedder is used, the dimensions would probably need to change.\\nDIMENSIONS = 768\\n# Index Constants\\nDISPLAY_NAME = \"<my_matching_engine_index_id>\"\\nDEPLOYED_INDEX_ID = \"yourname01\" # you set this. Start with a letter.\\nfrom google.cloud import aiplatform\\nfrom langchain_google_vertexai import VertexAIEmbeddings\\nfrom langchain_google_vertexai import VertexAI\\nfrom langchain_google_vertexai import (\\n    VectorSearchVectorStore,\\n    VectorSearchVectorStoreDatastore,\\n)\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.prompts.chat import (\\n    ChatPromptTemplate,\\n    SystemMessagePromptTemplate,\\n    HumanMessagePromptTemplate,\\n)\\nfrom IPython.display import display, Markdown\\naiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\\nembedding_model = VertexAIEmbeddings(model_name=\"text-embedding-005\")\\n# NOTE : This operation can take upto 30 seconds\\nmy_index = aiplatform.MatchingEngineIndtex.create_tree_ah_index(\\n    display_name=DISPLAY_NAME,\\n    dimensions=DIMENSIONS,\\n    approximate_neighbors_count=150,\\n    distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\\n    index_update_method=\"STREAM_UPDATE\",  # allowed values BATCH_UPDATE , STREAM_UPDATE\\n)\\nContinues next page...', 'Embeddings & Vector Stores\\n55\\nFebrurary 2025\\n# Create an endpoint\\nmy_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\\n    display_name=f\"{DISPLAY_NAME}-endpoint\", public_endpoint_enabled=True\\n)\\n# NOTE : This operation can take upto 20 minutes\\nmy_index_endpoint = my_index_endpoint.deploy_index(\\n    index=my_index, deployed_index_id=DEPLOYED_INDEX_ID\\n)\\nmy_index_endpoint = my_index_endpoint.deploy_index(\\n    index=my_index, deployed_index_id=DEPLOYED_INDEX_ID\\n)\\nmy_index_endpoint.deployed_indexes\\n# TODO : replace 1234567890123456789 with your acutial index ID\\nmy_index = aiplatform.MatchingEngineIndex(\"1234567890123456789\")\\n# TODO : replace 1234567890123456789 with your acutial endpoint ID\\n# Be aware that the Index ID differs from the endpoint ID\\nmy_index_endpoint = aiplatform.MatchingEngineIndexEndpoint(\"1234567890123456789\")\\nfrom langchain_google_vertexai import (\\n    VectorSearchVectorStore,\\n    VectorSearchVectorStoreDatastore,\\n)\\n# Input texts\\ntexts = [\\n    \"The cat sat on\",\\n    \"the mat.\",\\n    \"I like to\",\\n    \"eat pizza for\",\\n    \"dinner.\",\\n    \"The sun sets\",\\n    \"in the west.\",\\n]\\nContinues next page...', 'Embeddings & Vector Stores\\n56\\nFebrurary 2025\\nSnippet 12. Build/deploy ANN Index for Vertex AI Vector Search and use RAG with LLM prompts to generate \\ngrounded results/sources. \\n# Create a Vector Store\\nvector_store = VectorSearchVectorStore.from_components(\\n    project_id=PROJECT_ID,\\n    region=REGION,\\n    gcs_bucket_name=BUCKET,\\n    index_id=my_index.name,\\n    endpoint_id=my_index_endpoint.name,\\n    embedding=embedding_model,\\n    stream_update=True,\\n)\\n# Add vectors and mapped text chunks to your vectore store\\nvector_store.add_texts(texts=texts)\\n# Initialize the vectore_store as retriever\\nretriever = vector_store.as_retriever()\\n# perform simple similarity search on retriever\\nretriever.invoke(\"What are my options in breathable fabric?\")', 'Embeddings & Vector Stores\\n57\\nFebrurary 2025\\nFigure 17. Model responses along with sources demonstrating the LLM being grounded in the database\\nAs we can infer from Figure 16, the output not only grounds LLM in the semantically similar \\nresults retrieved from the database (hence refusing to answer when context cannot be found \\nin the database). This not only significantly reduces hallucination, but also provides sources \\nfor verification, either human or using another LLM.\\nSummary\\nIn this whitepaper we have discussed various methods to create, manage, store, and retrieve \\nembeddings of various data modalities effectively in the context of production-grade \\napplications. Creating, maintaining and using embeddings for downstream applications can \\nbe a complex task that involves several roles in the organization. However, by thoroughly \\noperationalizing and automating its usage, you can safely leverage the incredible benefits \\nthey offer across some of the most important applications. Some key takeaways from this \\nwhitepaper include:\\n1. Choose your embedding model wisely for your data and use case. Ensure the data used in \\ninference is consistent with the data used in training. The distribution shift from training to \\ninference can come from various areas, including domain distribution shift or downstream', 'Embeddings & Vector Stores\\n58\\nFebrurary 2025\\ntask distribution shift. If no existing embedding models fit the current inference data \\ndistribution, fine-tuning the existing model can significantly help on the performance. \\nAnother tradeoff comes from the model size. The large deep neural network (large \\nmultimodal models) based models usually have better performance but can come with a \\ncost of longer serving latency. Using Cloud-based embedding services can conquer the \\nabove issue by providing both high-quality and low-latency embedding service. For most \\nbusiness applications using a pre-trained embedding model provides a good baseline, \\nwhich can be further fine-tuned or integrated in downstream models. In case the data has \\nan inherent graph structure, graph embeddings can provide superior performance.\\n2. Once your embedding strategy is defined, it’s important to make the choice of the \\nappropriate vector database that suits your budget and business needs. It might seem \\nquicker to prototype with available open source alternatives, but opting for a more secure, \\nscalable, and battle-tested managed vector database can save significant developer \\ntime. There are various open source alternatives using one of the many powerful ANN \\nvector search algorithms, but ScaNN and HNSW have proven to provide some of the best \\naccuracy and performance trade offs.\\n3. Embeddings combined with an appropriate ANN powered vector database is an \\nincredibly powerful tool and can be leveraged for various applications, including \\nSearch, Recommendation systems, and Retrieval Augmented Generation for LLMs. This \\napproach can mitigate the hallucination problem and bolster verifiability and trust of \\nLLM-based systems.', \"Embeddings & Vector Stores\\n59\\nFebrurary 2025\\nEndnotes\\n1. Rai, A., 2020, Study of various methods for tokenization. In Advances in Natural Language Processing. \\nAvailable at: https://doi.org/10.1007/978-981-15-6198-6_18\\n2. Pennington, J., Socher, R. & Manning, C., 2014, GloVe: Global Vectors for Word Representation. [online] \\nAvailable at: https://nlp.stanford.edu/pubs/glove.pdf .\\n3. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V. & Hinton, G., 2016, Swivel: Improving embeddings \\nby noticing what's missing. ArXiv, abs/1602.02215. Available at: https://arxiv.org/abs/1602.02215 .\\n4. Mikolov, T., Sutskever, I., Chen, K., Corrado, G. & Dean, J., 2013, Efficient estimation of word representations \\nin vector space. ArXiv, abs/1301.3781. Available at: https://arxiv.org/pdf/1301.3781.pdf .\\n5. Rehurek, R., 2021, Gensim: open source python library for word and document embeddings. Available \\nat: https://radimrehurek.com/gensim/intro.html .\\n6. Bojanowski, P., Grave, E., Joulin, A. & Mikolov, T., 2016, Enriching word vectors with subword information. \\nArXiv, abs/1607.04606. Available at: https://arxiv.org/abs/1607.04606 .\\n7. Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R., 1990, Indexing by latent \\nsemantic analysis. Journal of the American Society for Information Science, 41(6), pp. 391-407.\\n8. Blei, D. M., Ng, A. Y., & Jordan, M. I., 2001, Latent Dirichlet allocation. In T. G. Dietterich, S. Becker, & Z. \\nGhahramani (Eds.), Advances in Neural Information Processing Systems 14. MIT Press, pp. 601-608. Available \\nat: https://proceedings.neurips.cc/paper/2001/hash/296472c9542ad4d4788d543508116cbc-Abstract.html .\\n9. Muennighoff, N., Tazi, N., Magne, L., & Reimers, N., 2022, Mteb: Massive text embedding benchmark. ArXiv, \\nabs/2210.07316. Available at: https://arxiv.org/abs/2210.07316 .\\n10. Le, Q. V., Mikolov, T., 2014, Distributed representations of sentences and documents. ArXiv, abs/1405.4053. \\nAvailable at: https://arxiv.org/abs/1405.4053 .\\n11. Devlin, J., Chang, M. W., Lee, K., & Toutanova, K., 2019, BERT: Pre-training deep Bidirectional Transformers \\nfor Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the \\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), \\npp. 4171-4186. Available at: https://www.aclweb.org/anthology/N19-1423/ .\\n12. Reimers, N. & Gurevych, I., 2020, Making monolingual sentence embeddings multilingual using knowledge \\ndistillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing \\n(EMNLP), pp. 254-265. Available at: https://www.aclweb.org/anthology/2020.emnlp-main.21/ .\", 'Embeddings & Vector Stores\\n60\\nFebrurary 2025\\n13. Gao, T., Yao, X. & Chen, D., 2021, Simcse: Simple contrastive learning of sentence embeddings. ArXiv, \\nabs/2104.08821. Available at: https://arxiv.org/abs/2104.08821 .\\n14. Wang, L., Yang, N., Huang, X., Jiao, B., Yang, L., Jiang, D., Majumder, R. & Wei, F., 2022, Text embeddings by \\nweakly supervised contrastive pre-training. ArXiv. Available at: https://arxiv.org/abs/2201.01279 .\\n15. Khattab, O. & Zaharia, M., 2020, colBERT: Efficient and effective passage search via contextualized late \\ninteraction over BERT. In Proceedings of the 43rd International ACM SIGIR Conference on Research and \\nDevelopment in Information Retrieval, pp. 39-48. Available at: https://dl.acm.org/doi/10.1145/3397271.3401025 .\\n16. Lee, J., Dai, Z., Duddu, S. M. K., Lei, T., Naim, I., Chang, M. W. & Zhao, V. Y., 2023, Rethinking the role of token \\nretrieval in multi-vector retrieval. ArXiv, abs/2304.01982. Available at: https://arxiv.org/abs/2304.01982 .\\n17. TensorFlow, 2021, TensorFlow hub, a model zoo with several easy to use pre-trained models. Available \\nat: https://tfhub.dev/.\\n18. Zhang, W., Xiong, C., & Zhao, H., 2023, Introducing BigQuery text embeddings for NLP tasks.  \\nGoogle Cloud Blog. Available at: https://cloud.google.com/blog/products/data-analytics/introducing  \\n- bigquery-text-embeddings.\\n19. Google Cloud, 2024, Get multimodal embeddings. Available at:  \\nhttps://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings .\\n20. Pinecone, 2024, IT Threat Detection. [online] Available at: https://docs.pinecone.io/docs/it-threat-detection .\\n21. Cai, H., Zheng, V. W., & Chang, K. C., 2020, A survey of algorithms and applications related with graph \\nembedding. In Proceedings of the 29th ACM International Conference on Information & Knowledge \\nManagement. Available at: https://dl.acm.org/doi/10.1145/3444370.3444568 .\\n22. Cai, H., Zheng, V. W., & Chang, K. C., 2017, A comprehensive survey of graph embedding: problems, \\ntechniques and applications. ArXiv, abs/1709.07604. Available at: https://arxiv.org/pdf/1709.07604.pdf .\\n23. Hamilton, W. L., Ying, R. & Leskovec, J., 2017, Inductive representation learning on large graphs.  \\nIn Advances in Neural Information Processing Systems 30. Available at:  \\nhttps://cs.stanford.edu/people/jure/pubs/graphsage -nips17.pdf .\\n24. Dong, Z., Ni, J., Bikel, D. M., Alfonseca, E., Wang, Y., Qu, C. & Zitouni, I., 2022, Exploring dual encoder \\narchitectures for question answering. ArXiv, abs/2204.07120. Available at: https://arxiv.org/abs/2204.07120 .\\n25. Google Cloud, 2021, Vertex AI Generative AI: Tune Embeddings. Available at:  \\nhttps://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-embeddings .', 'Embeddings & Vector Stores\\n61\\nFebrurary 2025\\n26. Matsui, Y., 2020, Survey on approximate nearest neighbor methods. ACM Computing Surveys (CSUR), 53(6), \\nArticle 123. Available at: https://wangzwhu.github.io/home/file/acmmm-t-part3-ann.pdf .\\n27. Friedman, J. H., Bentley, J. L. & Finkel, R. A., 1977, An algorithm for finding best matches in logarithmic \\nexpected time. ACM Transactions on Mathematical Software (TOMS), 3(3), pp. 209-226. Available at: \\nhttps://dl.acm.org/doi/pdf/10.1145/355744.355745 .\\n28. Scikit-learn, 2021, Scikit-learn, a library for unsupervised and supervised neighbors-based learning methods. \\nAvailable at: https://scikit-learn.org/.\\n29. lshashing, 2021, An open source python library to perform locality sensitive hashing. Available at: \\nhttps://pypi.org/project/lshashing/ .\\n30. Malkov, Y. A., Yashunin, D. A., 2016, Efficient and robust approximate nearest neighbor search using \\nhierarchical navigable small world graphs. ArXiv, abs/1603.09320. Available at:  \\nhttps://arxiv.org/pdf/1603.09320.pdf .\\n31. Google Research, 2021, A library for fast ANN by Google using the ScaNN algorithm. Available at:  \\nhttps://github.com/google-research/google-research/tree/master/scann .\\n32. Guo, R., Zhang, L., Hinton, G. & Zoph, B., 2020, Accelerating large-scale inference with anisotropic vector \\nquantization. ArXiv, abs/1908.10396. Available at: https://arxiv.org/pdf/1908.10396.pdf .\\n33. TensorFlow, 2021, TensorFlow Recommenders, an open source library for building ranking & recommender \\nsystem models. Available at: https://www.tensorflow.org/recommenders .\\n34. Google Cloud, 2021, Vertex AI Vector Search, Google Cloud’s high-scale low latency vector database. \\nAvailable at: https://cloud.google.com/vertex-ai/docs/vector-search/overview .\\n35. Elasticsearch, 2021, Elasticsearch: a RESTful search and analytics engine. Available at:  \\nhttps://www.elastic.co/elasticsearch/ .\\n36. Pinecone, 2021, Pinecone, a commercial fully managed vector database. Available at:  \\nhttps://www.pinecone.io .\\n37. pgvector, 2021, Open Source vector similarity search for Postgres. Available at:  \\nhttps://github.com/pgvector/pgvector.\\n38. Weaviate, 2021, Weaviate, an open source vector database. Available at: https://weaviate.io/ .\\n39. ChromaDB, 2021, ChromaDB, an open source vector database. Available at: https://www.trychroma.com/ .', \"Embeddings & Vector Stores\\n62\\nFebrurary 2025\\n40. LangChain, 2021.,LangChain, an open source framework for developing applications powered by language \\nmodel. Available at: https://langchain.com .\\n42. Thakur, N., Reimers, N., Ruckl'e, A., Srivastava, A., & Gurevych, I. (2021). BEIR: A Heterogenous Benchmark for \\nZero-shot Evaluation of Information Retrieval Models. ArXiv, abs/2104.08663.  \\nAvailable at: https://github.com/beir-cellar/beir\\n43. Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. 2023. MTEB: Massive Text Embedding \\nBenchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for \\nComputational Linguistics, pages 2014–2037, Dubrovnik, Croatia. Association for Computational Linguistics. \\nAvailable at: https://github.com/embeddings-benchmark/mteb\\n44. Chris Buckley. trec_eval IR evaluation package. Available from https://github.com/usnistgov/trec_eval\\n45. Christophe Van Gysel and Maarten de Rijke. 2018. Pytrec_eval: An Extremely Fast Python Interface to trec_\\neval. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR \\n'18). Association for Computing Machinery, New York, NY, USA, 873–876.  \\nAvailalbe at: https://doi.org/10.1145/3209978.3210065  \\n46. Boteva, Vera & Gholipour Ghalandari, Demian & Sokolov, Artem & Riezler, Stefan. (2016). A Full-Text Learning \\nto Rank Dataset for Medical Information Retrieval. 9626. 716-722. 10.1007/978-3-319-30671-1_58. Available \\nat https://www.cl.uni-heidelberg.de/statnlpgroup/nfcorpus/\\n47. Douze, M., Guzhva, A., Deng, C., Johnson, J., Szilvasy, G., Mazaré, P.E., Lomeli, M., Hosseini, L. and Jégou, H., \\n2024. The Faiss library. arXiv preprint arXiv:2401.08281. Available at https://arxiv.org/abs/2401.08281\\n48. Lee, J., Dai, Z., Ren, X., Chen, B., Cer, D., Cole, J.R., Hui, K., Boratko, M., Kapadia, R., Ding, W. and Luan, Y., \\n2024. Gecko: Versatile text embeddings distilled from large language models. arXiv preprint arXiv:2403.20327. \\nAvailable at: https://arxiv.org/abs/2403.20327\\n49. Okapi BM25: a non-binary model” Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze. An \\nIntroduction to Information Retrieval, Cambridge University Press, 2009, p. 232.\\n50. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei \\nLi, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. \\nLearn. Res. 21, 1, Article 140 (January 2020), 67 pages.  \\nAvailable at https://dl.acm.org/doi/abs/10.5555/3455716.3455856\", 'Embeddings & Vector Stores\\n63\\nFebrurary 2025\\n51. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, \\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sashank \\nTsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, \\nEmily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, \\nPengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, \\nVedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret \\nZoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, \\nThanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr \\nPolozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, \\nJason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2023. PaLM: scaling \\nlanguage modeling with pathways. J. Mach. Learn. Res. 24, 1, Article 240 (January 2023), 113 pages.  \\nAvailable at https://dl.acm.org/doi/10.5555/3648699.3648939\\n52. Gemini: A Family of Highly Capable Multimodal Models, Gemini Team, Dec 2023.  \\nAvailable at: https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf\\n53. Radford, Alec and Karthik Narasimhan. “Improving Language Understanding by Generative Pre-Training.” \\n(2018). Available at:  \\nhttps://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\\n54. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozière, B., Goyal, N., Hambro, \\nE., Azhar, F. and Rodriguez, A., 2023. Llama: Open and efficient foundation language models. arXiv preprint \\narXiv:2302.13971. Available at: https://arxiv.org/abs/2302.13971\\n55. Kusupati, A., Bhatt, G., Rege, A., Wallingford, M., Sinha, A., Ramanujan, V., Howard-Snyder, W., Chen, K., \\nKakade, S., Jain, P. and Farhadi, A., 2022. Matryoshka representation learning. Advances in Neural Information \\nProcessing Systems, 35, pp.30233-30249. Available at: \\nhttps://proceedings.neurips.cc/paper_files/paper/2022/file/c32319f4868da7613d78af9993100e42-Paper-  \\n-Conference.pdf\\n56. Nair, P., Datta, P., Dean, J., Jain, P. and Kusupati, A., 2025. Matryoshka Quantization. arXiv preprint \\narXiv:2502.06786. Available at: https://arxiv.org/abs/2502.06786\\n57. Faysse, M., Sibille, H., Wu, T., Omrani, B., Viaud, G., Hudelot, C. and Colombo, P., 2024. Colpali: Efficient \\ndocument retrieval with vision language models. arXiv preprint arXiv:2407.01449.  \\nAvailable at: https://arxiv.org/abs/2407.01449\\n58. Aumüller, M., Bernhardsson, E. and Faithfull, A., 2020. ANN-Benchmarks: A benchmarking tool for \\napproximate nearest neighbor algorithms. Information Systems, 87, p.101374.', 'Embeddings & Vector Stores\\n64\\nFebrurary 2025', 'Foundational \\nLarge Language \\nModels & \\nText Generation\\nAuthors: Mohammadamin Barektain,  \\nAnant Nawalgaria, Daniel J. Mankowitz,  \\nMajd Al Merey, Yaniv Leviathan, Massimo Mascaro,  \\nMatan Kalman, Elena Buchatskaya,                                     \\nAliaksei Severyn, Irina Sigler, and Antonio Gulli', 'Foundational Large Language Models & Text Generation\\n2\\nFebruary 2025\\nAcknowledgements\\nContent contributors\\nAdam Sadvovsky\\nYonghui Wu\\nAndrew Dai\\nEfi Kokiopolou\\nChuck Sugnet\\nAleksey Vlasenko\\nErwin Huizenga\\nAida Nematzadeh\\nIra Ktena\\nOlivia Wiles\\nLavi Nigam\\nCurators and Editors\\nAntonio Gulli\\nAnant Nawalgaria\\nGrace Mollison \\nTechnical Writer\\nMark Iverson\\nDesigner\\nMichael Lanning', 'Introduction 6\\nWhy language models are important 7\\nLarge language models 8\\nTransformer 9\\nInput preparation and embedding 11\\nMulti-head attention 12\\nUnderstanding self-attention 12\\nMulti-head attention: power in diversity 14\\nLayer normalization and residual connections 15\\nFeedforward layer  15\\nEncoder and decoder 16\\nMixture of Experts (MoE) 17\\nTraining the transformer 20\\nData preparation 21\\nTraining and loss function 21\\nThe evolution of transformers 23\\nGPT-1 23\\nBERT 25\\nTable of contents', 'GPT-2 25\\nGPT-3/3.5/4 27\\nLaMDA 28\\nGopher 29\\nGLaM 31\\nChinchilla 31\\nPaLM 33\\nPaLM 2 33\\nGemini 34\\nGemma 37\\nLLaMA 38\\nMixtral 39\\nOpenAI O1 40\\nDeepSeek 40\\nOther open models 41\\nComparison 43\\nFine-tuning large language models 45\\nSupervised fine-tuning  46\\nReinforcement learning from human feedback 47\\nParameter Efficient Fine-Tuning 49\\nUsing large language models 52\\nPrompt engineering  52\\nSampling Techniques and Parameters 53\\nTask-based Evaluation 54\\nAccelerating inference 57', 'Trade offs 58\\nThe Quality vs Latency/Cost Tradeoff 58\\nThe Latency vs Cost Tradeoff 59\\nOutput-approximating methods 60\\nQuantization 60\\nDistillation 61\\nOutput-preserving methods 62\\nFlash Attention 63\\nPrefix Caching 63\\nSpeculative Decoding 65\\nBatching and Parallelization 67\\nApplications 68\\nCode and mathematics 71\\nMachine translation 72\\nText summarization 73\\nQuestion-answering 73\\nChatbots 74\\nContent generation 75\\nNatural language inference 75\\nText classification 76\\nText analysis 77\\nMultimodal applications 78\\nSummary 80\\nEndnotes 82', 'Foundational Large Language Models & Text Generation\\n6\\nFebruary 2025\\nIntroduction\\nThe advent of Large Language Models (LLMs) represents a seismic shift in the world of \\nartificial intelligence. Their ability to process, generate, and understand user intent is \\nfundamentally changing the way we interact with information and technology. \\nAn LLM is an advanced artificial intelligence system that specializes in processing, \\nunderstanding, and generating human-like text. These systems are typically implemented as \\na deep neural network and are trained on massive amounts of text data. This allows them to \\nlearn the intricate patterns of language, giving them the ability to perform a variety of tasks, \\nlike machine translation, creative text generation, question answering, text summarization, \\nand many more reasoning and language oriented tasks. This whitepaper dives into the \\ntimeline of the various architectures and approaches building up to the large language \\nmodels and the architectures being used at the time of publication. It also discusses fine-\\nWe believe that this new crop of \\ntechnologies has the potential to \\nassist, complement, empower, \\nand inspire people at any time \\nacross almost any field.', 'Foundational Large Language Models & Text Generation\\n7\\nFebruary 2025\\ntuning techniques to customize an LLM to a certain domain or task, methods to make the \\ntraining more efficient, as well as methods to accelerate inference. These are then followed \\nby various applications and code examples. \\nWhy language models are important\\nLLMs achieve an impressive performance boost from the previous state of the art NLP \\nmodels across a variety of different and complex tasks which require answering questions \\nor complex reasoning, making feasible many new applications. These include language \\ntranslation, code generation and completion, text generation, text classification, and \\nquestion-answering, to name a few. Although foundational LLMs trained in a variety of \\ntasks on large amounts of data perform very well out of the box and display emergent \\nbehaviors (e.g. the ability to perform tasks they have not been directly trained for) they can \\nalso be adapted to solve specific tasks where performance out of the box is not at the level \\ndesired through a process known as fine-tuning. This requires significantly less data and \\ncomputational resources than training an LLM from scratch. LLMs can be further nudged \\nand guided towards the desired behavior by the discipline of prompt engineering: the art and \\nscience of composing the prompt and the parameters of an LLM to get the desired response.\\nThe big question is: how do these large language models work? The next section explores the \\ncore building blocks of LLMs, focusing on transformer architectures and their evolution from \\nthe original ‘Attention is all you need’ paper1 to the latest models such as Gemini, Google’s \\nmost capable LLM. We also cover training and fine-tuning techniques, as well as methods to \\nimprove the speed of response generation. The whitepaper concludes with a few examples \\nof how language models are used in practice.', 'Foundational Large Language Models & Text Generation\\n8\\nFebruary 2025\\nLarge language models\\nA language model predicts the probability of a sequence of words. Commonly, when given \\na prefix of text, a language model assigns probabilities to subsequent words. For example, \\ngiven the prefix “The most famous city in the US is…”, a language model might predict high \\nprobabilities to the words “New York” and “Los Angeles” and low probabilities to the words \\n“laptop” or “apple”. You can create a basic language model by storing an n-gram table,2 while \\nmodern language models are often based on neural models, such as transformers.\\nBefore the invention of transformers1, recurrent neural networks (RNNSs) were the popular \\napproach for modeling sequences. In particular, “long short-term memory” (LSTM) and \\n“gated recurrent unit” (GRU) were common architectures.3 This area includes language \\nproblems such as machine translation, text classification, text summarization, and question-\\nanswering, among others. RNNs process input and output sequences sequentially. They \\ngenerate a sequence of hidden states based on the previous hidden state and the current \\ninput. The sequential nature of RNNs makes them compute-intensive and hard to parallelize \\nduring training (though recent work in state space modeling is attempting to overcome \\nthese challenges).\\nTransformers, on the other hand, are a type of neural network that can process sequences \\nof tokens in parallel thanks to the self-attention mechanism.1 This means that transformers \\ncan model long-term contexts more effectively and are easier to parallelize than RNNs. This \\nmakes them significantly faster to train, and more powerful compared to RNNs for handling \\nlong-term dependencies in long sequence tasks. However, the cost of self-attention in the \\noriginal transformers is quadratic in the context length which limits the size of the context, \\nwhile RNNs have a theoretically infinite context length. Although they have infinite context \\nlength, in practice they struggle to utilize it due to vanishing gradient problem. Transformers \\nhave become the most popular approach for sequence modeling and transfer learning \\nproblems in recent years.', 'Foundational Large Language Models & Text Generation\\n9\\nFebruary 2025\\nHerein, we discuss the first version of the transformer model and then move on to the more \\nrecent advanced models and algorithms.\\nTransformer\\nThe transformer architecture was developed at Google in 2017 for use in a translation model.1 \\nIt’s a sequence-to-sequence model capable of converting sequences from one domain \\ninto sequences in another domain. For example, translating French sentences to English \\nsentences. The original transformer architecture consists of two parts: an encoder and a \\ndecoder. The encoder converts the input text (e.g., a French sentence) into a representation, \\nwhich is then passed to the decoder. The decoder uses this representation to generate the \\noutput text (e.g., an English translation) autoregressively.1 Notably, the size of the output of \\nthe transformer encoder is linear in the size of its input. Figure 1 shows the design of the \\noriginal transformer architecture.\\nThe transformer consists of multiple layers. A layer in a neural network comprises a set of \\nparameters that perform a specific transformation on the data. In the diagram you can see \\nan example of some layers which include Multi-Head Attention, Add & Norm, Feed-Forward, \\nLinear, Softmax etc. The layers can be sub-divided into the input, hidden and output layers. \\nThe input layer (e.g., Input/Output Embedding) is the layer where the raw data enters the \\nnetwork. Input embeddings are used to represent the input tokens to the model. Output \\nembeddings are used to represent the output tokens that the model predicts. For example, in \\na machine translation model, the input embeddings would represent the words in the source \\nlanguage, while the output embeddings would represent the words in the target language. \\nThe output layer (e.g., Softmax) is the final layer that produces the output of the network. The \\nhidden layers (e.g., Multi-Head Attention) are between the input and output layers and are \\nwhere the magic happens!', 'Foundational Large Language Models & Text Generation\\n10\\nFebruary 2025\\nFigure 1. Original Transformer 1 (P.C: 5 )', 'Foundational Large Language Models & Text Generation\\n11\\nFebruary 2025\\nTo better understand the different layers in the transformer, let’s use a French-to-English \\ntranslation task as an example. Here, we explain how a French sentence is input into the \\ntransformer and a corresponding English translation is output. We will also describe each of \\nthe components inside the transformer from Figure 1.\\nInput preparation and embedding\\nTo prepare language inputs for transformers, we convert an input sequence into tokens \\nand then into input embeddings. At a high level, an input embedding is a high-dimensional \\nvector68 that represents the meaning of each token in the sentence. This embedding is \\nthen fed into the transformer for processing. Generating an input embedding involves the \\nfollowing steps:\\n1. Normalization (Optional): Standardizes text by removing redundant whitespace, \\naccents, etc.\\n2. Tokenization: Breaks the sentence into words or subwords and maps them to integer \\ntoken IDs from a vocabulary.\\n3. Embedding: Converts each token ID to its corresponding high-dimensional vector, \\ntypically using a lookup table. These can be learned during the training process.\\n4. Positional Encoding: Adds information about the position of each token in the sequence \\nto help the transformer understand word order.\\nThese steps help to prepare the input for the transformers so that they can better \\nunderstand the meaning of the text.', 'Foundational Large Language Models & Text Generation\\n12\\nFebruary 2025\\nMulti-head attention\\nAfter converting input tokens into embedding vectors, you feed these embeddings into \\nthe multi-head attention module (see Figure 1). Self-attention is a crucial mechanism in \\ntransformers; it enables them to focus on specific parts of the input sequence relevant to \\nthe task at hand and to capture long-range dependencies within sequences more effectively \\nthan traditional RNNs. \\nUnderstanding self-attention\\nConsider the following sentence: “The tiger jumped out of a tree to get a drink because it \\nwas thirsty.” Self-attention helps to determine relationships between different words and \\nphrases in sentences. For example, in this sentence, “the tiger” and “it” are the same object, \\nso we would expect these two words to be strongly connected. Self-attention achieves this \\nthrough the following steps (Figure 2):\\n1. Creating queries, keys, and values: Each input embedding is multiplied by three learned \\nweight matrices (Wq, Wk, Wv) to generate query (Q), key (K), and value (V) vectors. These \\nare like specialized representations of each word.\\n• Query: The query vector helps the model ask, “Which other words in the sequence are \\nrelevant to me?”\\n• Key: The key vector is like a label that helps the model identify how a word might be \\nrelevant to other words in the sequence.\\n• Value: The value vector holds the actual word content information.\\n2. Calculating scores: Scores are calculated to determine how much each word should \\n‘attend’ to other words. This is done by taking the dot product of the query vector of one \\nword with the key vectors of all the words in the sequence.', 'Foundational Large Language Models & Text Generation\\n13\\nFebruary 2025\\n3. Normalization: The scores are divided by the square root of the key vector dimension (dk) \\nfor stability, then passed through a softmax function to obtain attention weights. These \\nweights indicate how strongly each word is connected to the others.\\n4. Weighted values: Each value vector is multiplied by its corresponding attention weight. \\nThe results are summed up, producing a context-aware representation for each word.\\nFigure 2. The process of computing self-attention in the multi-head attention module 1 (P.C: 5 )', 'Foundational Large Language Models & Text Generation\\n14\\nFebruary 2025\\nIn practice, these computations are performed at the same time, by stacking the query, key \\nand value vectors for all the tokens into Q, K and V matrices and multiplying them together as \\nshown in Figure 3.\\nFigure 3. The basic operation of attention, 1  with Q=query, K=Keys and V=Value, Z=Attention, d_k = dimension \\nof queries and keys (P.C:5 )\\nMulti-head attention: power in diversity\\nMulti-head attention employs multiple sets of Q, K, V weight matrices. These run in parallel, \\neach ‘head’ potentially focusing on different aspects of the input relationships. The outputs \\nfrom each head are concatenated and linearly transformed, giving the model a richer \\nrepresentation of the input sequence.\\nThe use of multi-head attention improves the model’s ability to handle complex language \\npatterns and long-range dependencies. This is crucial for tasks that require a nuanced \\nunderstanding of language structure and content, such as machine translation, text \\nsummarization, and question-answering. The mechanism enables the transformer to consider \\nmultiple interpretations and representations of the input, which enhances its performance on \\nthese tasks.', 'Foundational Large Language Models & Text Generation\\n15\\nFebruary 2025\\nLayer normalization and residual connections\\nEach layer in a transformer, consisting of a multi-head attention module and a feed-forward \\nlayer, employs layer normalization and residual connections. This corresponds to the Add \\nand Norm layer in Figure 1, where ‘Add’ corresponds to the residual connection and ‘Norm’ \\ncorresponds to layer normalization. Layer normalization computes the mean and variance \\nof the activations to normalize the activations in a given layer. This is typically performed to \\nreduce covariate shift as well as improve gradient flow to yield faster convergence during \\ntraining as well as improved overall performance. \\nResidual connections propagate the inputs to the output of one or more layers. This has the \\neffect of making the optimization procedure easier to learn and also helps deal with vanishing \\nand exploding gradients. \\nThe Add and Norm layer is applied to both the multi-head attention module and the feed-\\nforward layer described in the following section.\\nFeedforward layer \\nThe output of the multi-head attention module and the subsequent ‘Add and Norm’ layer is \\nfed into the feedforward layer of each transformer block. This layer applies a position-wise \\ntransformation to the data, independently for each position in the sequence, which allows the \\nincorporation of additional non-linearity and complexity into the model’s representations. The \\nfeedforward layer typically consists of two linear transformations with a non-linear activation \\nfunction, such as ReLU or GELU, in between. This structure adds further representational \\npower to the model. After processing by the feedforward layer, the data undergoes \\nanother ‘Add and Norm’ step, which contributes to the stability and effectiveness of deep \\ntransformer models.', 'Foundational Large Language Models & Text Generation\\n16\\nFebruary 2025\\nEncoder and decoder\\nThe original transformer architecture relies on a combination of encoder and decoder \\nmodules. Each encoder and decoder consists of a series of layers, with each layer \\ncomprising key components: a multi-head self-attention mechanism, a position-wise feed-\\nforward network, normalization layers, and residual connections. \\nThe encoder’s primary function is to process the input sequence into a continuous \\nrepresentation that holds contextual information for each token. The input sequence is first \\nnormalized, tokenized, and converted into embeddings. Positional encodings are added to \\nthese embeddings to retain sequence order information. Through self-attention mechanisms, \\neach token in the sequence can dynamically attend to any other token, thus understanding \\nthe contextual relationships within the sequence. The output from the encoder is a series of \\nembedding vectors Z representing the entire input sequence. \\nThe decoder is tasked with generating an output sequence based on the context provided \\nby the encoder’s output Z. It operates in a token-by-token fashion, beginning with a start-\\nof-sequence token. The decoder layers employ two types of attention mechanisms: masked \\nself-attention and encoder-decoder cross-attention. Masked self-attention ensures that \\neach position can only attend to earlier positions in the output sequence, preserving the \\nauto-regressive property. This is crucial for preventing the decoder from having access to \\nfuture tokens in the output sequence. The encoder-decoder cross-attention mechanism \\nallows the decoder to focus on relevant parts of the input sequence, utilizing the contextual \\nembeddings generated by the encoder. This iterative process continues until the decoder \\npredicts an end-of-sequence token, thereby completing the output sequence generation.\\nMajority of recent LLMs adopted a decoder-only variant of transformer architecture. This \\napproach forgoes the traditional encoder-decoder separation, focusing instead on directly \\ngenerating the output sequence from the input. The input sequence undergoes a similar', 'Foundational Large Language Models & Text Generation\\n17\\nFebruary 2025\\nprocess of embedding and positional encoding before being fed into the decoder. The \\ndecoder then uses masked self-attention to generate predictions for each subsequent \\ntoken based on the previously generated tokens. This streamlined approach simplifies the \\narchitecture for specific tasks where encoding and decoding can be effectively merged.\\nMixture of Experts (MoE)\\nA Mixture of Experts (MoE) is a an architecture that combines multiple specialized sub-\\nmodels (the “experts”) to improve overall performance, particularly on complex tasks. It’s \\na form of ensemble learning, but with a key difference: instead of simply aggregating the \\npredictions of all experts, it learns to route different parts of the input to different experts. \\nThis allows the model to specialize, with each expert becoming proficient in a specific \\nsub-domain or aspect of the data. Here’s a more technical breakdown describing the main \\ncomponents of an MoE:\\n• Experts: These are the individual sub-models, each designed to handle a specific subset \\nof the input data or a particular task. They can be any type of model (e.g., neural networks, \\ndecision trees, etc.), but in the context of large language models, they are typically \\nthemselves transformer-based architectures.  \\n• Gating Network (Router): This is a crucial component that learns to route the input to \\nthe appropriate expert(s). It takes the input and produces a probability distribution over \\nthe experts. This distribution determines how much each expert should “contribute” to the \\nfinal prediction. The gating network is also typically a neural network.  \\n• Combination Mechanism: This combines the outputs of the experts, weighted by the \\nprobabilities from the gating network, to produce the final prediction. A common approach \\nis a weighted average.', 'Foundational Large Language Models & Text Generation\\n18\\nFebruary 2025\\nIn practice, A Mixture of Experts (MoE) architecture combines multiple specialized sub-\\nmodels, called “experts,” to tackle complex tasks. Instead of simply averaging all expert \\npredictions, an MoE uses a “gating network” to intelligently route different parts of the \\ninput to the mostt relevant experts. Both the experts and the gating network receive the \\ninput.  Each expert processes the input and generates its output. Simultaneously, the \\ngating network analyzes the input and produces a probability distribution over the experts, \\nindicating how much each expert should contribute to the final result.  These probabilities \\nthen weight the outputs of the experts, and the weighted combination becomes the final \\nprediction. This allows different experts to specialize in handling specific types of data or \\nsub-tasks, improving overall performance and, through “sparse activation,” potentially \\nreducing computational cost by only activating a subset of experts for any given input.\\nFigure 4. Mixture of experts ensembling 70', 'Foundational Large Language Models & Text Generation\\n19\\nFebruary 2025\\nLarge Reasoning Models\\nAchieving robust reasoning capabilities in Large Models is a complex endeavor that involves \\na combination of architectural designs, training methodologies, and prompting strategies. \\nOne crucial aspect is incorporating inductive biases that favor reasoning-conducive patterns. \\nTransformer architectures, with their self-attention mechanisms, are foundational, allowing \\nthe model to weigh the importance of different parts of the input sequence when generating \\nan output. However, vanilla Transformers alone are not sufficient for complex reasoning.\\nChain-of-Thought prompting explicitly encourages the model to generate intermediate \\nreasoning steps before arriving at a final answer. By providing examples of step-by-step \\nreasoning in the prompt, the model learns to decompose complex problems into smaller, \\nmanageable sub-problems. This mimics human reasoning processes and significantly \\nimproves performance on tasks requiring multi-step inference. Tree-of-Thoughts takes this \\nfurther, exploring multiple reasoning paths and using a search algorithm to find the most \\npromising solution. This technique is useful with game trees or combinatorial problems. \\nLeast-to-Most prompting guides the model to solve subproblems, which get progressively \\nmore complex, and the output of one subproblem is used as part of the prompt of the more \\ncomplex, subsequent problem.\\nFine-tuning on datasets specifically designed for reasoning tasks is also crucial. These \\ndatasets may contain logical puzzles, mathematical problems, or commonsense reasoning \\nchallenges. Instruction tuning, where the model is trained to follow natural language \\ninstructions, further enhances its ability to understand and respond to complex reasoning \\nprompts. Reinforcement Learning from Human Feedback (RLHF) refines the model’s outputs \\nbased on human preferences, improving the quality and coherence of its reasoning. RLHF \\nhelps in reward models that score reasoning ability as well as “helpfulness.”', 'Foundational Large Language Models & Text Generation\\n20\\nFebruary 2025\\nKnowledge distillation, transferring knowledge from a larger, more capable “teacher” model \\nto a smaller, more efficient “student” model, can be used to improve the reasoning abilities \\nof smaller models while maintaining efficiency. This approach allows the student model to \\nlearn the reasoning patterns of the teacher model without requiring the same computational \\nresources. During inference, techniques like beam search, which explores multiple candidate \\noutputs simultaneously, can improve the quality of reasoning by considering different \\npossibilities. Temperature scaling, adjusting the randomness of the model’s output, can also \\ninfluence the exploration-exploitation trade-off in reasoning. Finally, incorporating external \\nknowledge sources, such as knowledge graphs or structured databases, can provide the \\nmodel with additional information to support its reasoning process. This can be achieved \\nthrough techniques like retrieval-augmented generation, where the model retrieves relevant \\ninformation from an external source before generating an output. These techniques all \\ncombined, across many domains of reasoning, create the best performing reasoning large \\nlanguage models.\\nTraining the transformer\\nWhen talking about machine learning models, it’s important to differentiate between \\ntraining and inference. Training typically refers to modifying the parameters of the model, \\nand involves loss functions and backpropagation. Inference is when model is used only \\nfor the predicted output, without updating the model weights. The model parameters are \\nfixed during inference. Up until now we learned how transformers generate outputs during \\ninference. Next, we focus on how to train transformers to perform one or more given tasks.', 'Foundational Large Language Models & Text Generation\\n21\\nFebruary 2025\\nData preparation\\nThe first step is data preparation, which involves a few important steps itself. First, clean the \\ndata by applying techniques such as filtering, deduplication, and normalization. The next \\nstep is tokenization where the dataset is converted into tokens using techniques such as \\nByte-Pair Encoding8, 9 and Unigram tokenization.8, 10 Tokenization generates a vocabulary, \\nwhich is a set of unique tokens used by the LLM. This vocabulary serves as the model’s \\n’language’ for processing and understanding text. Finally, the data is typically split into a \\ntraining dataset for training the model as well as a test dataset which is used to evaluate the \\nmodels performance.\\nTraining and loss function\\nA typical transformer training loop consists of several parts: First, batches of input \\nsequences are sampled from a training dataset. For each input sequence, there is a \\ncorresponding target sequence. In unsupervised pre-training, the target sequence is \\nderived from the input sequence itself. The batch of input sequences is then fed into the \\ntransformer. The transformer generates predicted output sequences. The difference \\nbetween the predicted and target sequences is measured using a loss function (often cross-\\nentropy loss)11. Gradients of this loss are calculated, and an optimizer uses them to update \\nthe transformer’s parameters. This process is repeated until the transformer converges to a \\ncertain level of performance or until it has been trained on a pre-specified number of tokens. \\nThere are different approaches to formulating the training task for transformers depending \\non the architecture used:\\n• Decoder-only models are typically pre-trained on the language modeling task (e.g., see \\nendnote12, 13). The target sequence for the decoder is simply a shifted version of the input \\nsequence. Given a training sequence like ‘the cat sat on the mat’ various input/target', 'Foundational Large Language Models & Text Generation\\n22\\nFebruary 2025\\npairs can be generated for the model. For example the input “the cat sat on” should \\npredict “the” and subsequently the input “the cat sat on the” should predict target \\nsequence “mat”.\\n• Encoder-only models (like BERT)14 are often pre-trained by corrupting the input sequence \\nin some way and having the model try to reconstruct it. One such approach is masked \\nlanguage modeling (MLM).14 In our example, the input sequence could be “The [MASK] sat \\non the mat” and the sequence target would be the original sentence.\\n• Encoder-decoder models (like the original transformer) are trained on sequence-to-\\nsequence supervised tasks such as translation (input sequence “Le chat est assis sur \\nle tapis” and target “The cat sat on the mat”), question-answering (where the input \\nsequence is a question and the target sequence is the corresponding answer), and \\nsummarization (where the input sequence is a full article and the target sequence is its \\ncorresponding summary). These models could also be trained in an unsupervised way by \\nconverting other tasks into sequence-to-sequence format. For example, when training \\non Wikipedia data, the input sequence might be the first part of an article, and the target \\nsequence comprises the remainder of the article.\\nAn additional factor to consider during training is the ‘context length’. This refers to the \\nnumber of previous tokens the model can ‘remember’ and use to predict the next token in \\nthe sequence. Longer context lengths allow the model to capture more complex relationships \\nand dependencies within the text, potentially leading to better performance. However, longer \\ncontexts also require more computational resources and memory, which can slow down \\ntraining and inference. Choosing an appropriate context length involves balancing these \\ntrade-offs based on the specific task and available resources.', 'Foundational Large Language Models & Text Generation\\n23\\nFebruary 2025\\nThe evolution of transformers\\nThe next sections provide an overview of the various transformer architectures. These \\ninclude encoder-only, encoder-decoder, as well as decoder-only transformers. We start with \\nGPT-1 and BERT and end with Google’s latest family of LLMs called Gemini.\\nGPT-1\\nGPT-1 (Generative pre-trained transformer version 1)15 was a decoder-only model developed \\nby OpenAI in 2018. It was trained on the BooksCorpus dataset (containing approximately \\nseveral billion words) and is able to generate text, translate languages, write different kinds \\nof creative content, and answer questions in an informative way. The main innovations in \\nGPT-1 were:\\n• Combining transformers and unsupervised pre-training: Unsupervised pre-training \\nis a process of training a language model on a large corpus of unlabeled data. Then, \\nsupervised data is used to fine-tune the model for a specific task, such as translation \\nor sentiment classification. In prior works, most language models were trained using a \\nsupervised learning objective. This means that the model was trained on a dataset of \\nlabeled data, where each example had a corresponding label. This approach has two main \\nlimitations. First, it requires a large amount of labeled data, which can be expensive and \\ntime-consuming to collect. Second, the model can only generalize to tasks that are similar \\nto the tasks that it was trained on. Semi-supervised sequence learning was one of the first \\nworks that showed that unsupervised pre-training followed by supervised training was \\nsuperior than supervised training alone.\\nUnsupervised pre-training addresses these limitations by training the model on a large \\ncorpus of unlabeled data. This data can be collected more easily and cheaply than labeled \\ndata. Additionally, the model can generalize to tasks that are different from the tasks that', 'Foundational Large Language Models & Text Generation\\n24\\nFebruary 2025\\nit was trained on. The BooksCorpus dataset is a large (5GB) corpus of unlabeled text that \\nwas used to train the GPT-1 language model. The dataset contains over 7,000 unpublished \\nbooks, which provides the model with a large amount of data to learn from. Additionally, \\nthe corpus contains long stretches of contiguous text, which helps the model learn long-\\nrange dependencies. Overall, unsupervised pre-training is a powerful technique that can \\nbe used to train language models that are more accurate and generalizable than models \\nthat are trained using supervised learning alone. \\n• Task-aware input transformations: There are different kinds of tasks such as textual \\nentailment and question-answering that require a specific structure. For example, \\ntextual entailment requires a premise and a hypothesis; question-answering requires a \\ncontext document; a question and possible answers. One of the contributions of GPT-1 \\nis converting these types of tasks which require structured inputs into an input that the \\nlanguage model can parse, without requiring task-specific architectures on top of the \\npre-trained architecture. For textual entailment, the premise p and the hypothesis h are \\nconcatenated with a delimiter token ($) in between - [p, $, h]. For question answering, the \\ncontext document c is concatenated with the question q and a possible answer a with a \\ndelimiter token in between the question and answer - [c,q,$,a].\\nGPT-1 surpassed previous models on several benchmarks, achieving excellent results. While \\nGPT-1 was a significant breakthrough in natural language processing (NLP), it had some \\nlimitations. For example, the model was prone to generating repetitive text, especially when \\ngiven prompts outside the scope of its training data. It also failed to reason over multiple \\nturns of dialogue and could not track long-term dependencies in text. Additionally, its \\ncohesion and fluency were limited to shorter text sequences, and longer passages would \\nlack cohesion. Despite these limitations, GPT-1 demonstrated the power of unsupervised \\npre-training, which laid the foundation for larger and more powerful models based on the \\ntransformer architecture.', 'Foundational Large Language Models & Text Generation\\n25\\nFebruary 2025\\nBERT\\nBERT14 which stands for Bidirectional Encoder Representations from Transformers, \\ndistinguishes itself from traditional encoder-decoder transformer models by being an \\nencoder-only architecture. Instead of translating or producing sequences, BERT focuses \\non understanding context deeply by training on a masked language model objective. In \\nthis setup, random words in a sentence are replaced with a [MASK] token, and BERT tries \\nto predict the original word based on the surrounding context. Another innovative aspect \\nof BERT’s training regime is the next sentence prediction loss, where it learns to determine \\nwhether a given sentence logically follows a preceding one. By training on these objectives, \\nBERT captures intricate context dependencies from both the left and right of a word, and \\nit can discern the relationship between pairs of sentences. Such capabilities make BERT \\nespecially good at tasks that require natural language understanding, such as question-\\nanswering, sentiment analysis, and natural language inference, among others. Since this is an \\nencoder-only model, BERT cannot generate text.\\nGPT-2\\nGPT-2,12 the successor to GPT-1, was released in 2019 by OpenAI. The main innovation of \\nGPT-2 was a direct scale-up, with a tenfold increase in both its parameter count and the size \\nof its training dataset:\\n• Data: GPT-2 was trained on a large (40GB) and diverse dataset called WebText, which \\nconsists of 45 million webpages from Reddit with a Karma rating of at least three. Karma \\nis a rating metric used on Reddit and a value of three means that all the posts were of a \\nreasonable level of quality.', 'Foundational Large Language Models & Text Generation\\n26\\nFebruary 2025\\n• Parameters: GPT-2 had 1.5 billion parameters, which was an order of magnitude larger \\nthan the previous model. More parameters increase the model’s learning capacity. The \\nauthors trained four language models with 117M (the same as GPT-1), 345M, 762M, and 1.5B \\n(GPT-2) parameters, and found that the model with the most parameters performed better \\non every subsequent task.\\nThis scaling up resulted in a model that was able to generate more coherent and realistic text \\nthan GPT-1. Its ability to generate human-like responses made it a valuable tool for various \\nnatural language processing tasks, such as content creation and translation. Specifically, \\nGPT-2 demonstrated significant improvement in capturing long-range dependencies and \\ncommon sense reasoning. While it performed well in some tasks, it did not outperform state-\\nof-the-art reading comprehension, summarization, and translation. GPT-2’s most significant \\nachievement was its ability to perform zero-shot learning on a variety of tasks. Zero-shot task \\ntransfer is the ability of a model to generalize to a new task without being trained on it, which \\nrequires the model to understand the task based on the given instruction. For example, for \\nan English to German translation task, the model might be given an English sentence followed \\nby the word “German” and a prompt (“:”). The model would then be expected to understand \\nthat this is a translation task and generate the German translation of the English sentence. \\nGPT-2 was able to perform tasks such as machine translation, text summarization, and \\nreading comprehension without any explicit supervision.\\nThe study discovered that performance on zero-shot tasks increased in a log-linear manner \\nas the model’s capacity increased. GPT-2 showed that training on a larger dataset and having \\nmore parameters improved the model’s ability to understand tasks and surpass the state-of-\\nthe-art on many tasks in zero-shot settings.', 'Foundational Large Language Models & Text Generation\\n27\\nFebruary 2025\\nGPT-3/3.5/4\\nGPT-3,13 or the third iteration of the Generative Pre-trained Transformer model, represents a \\nsignificant evolution from its predecessor, GPT-2, primarily in terms of scale, capabilities, and \\nflexibility. The most noticeable difference is the sheer size of GPT-3, boasting a whopping \\n175 billion parameters, compared to GPT-2’s largest model which had 1.5 billion parameters. \\nThis increase in model size allowed GPT-3 to store and recall an even more vast amount of \\ninformation, understand nuanced instructions, and generate more coherent and contextually \\nrelevant text over longer passages.\\nWhile GPT-2 could be fine-tuned on specific tasks with additional training data, GPT-3 can \\nunderstand and execute tasks with just a few examples, or sometimes even without any \\nexplicit examples—simply based on the instruction provided. This highlights GPT-3’s more \\ndynamic understanding and adaptation abilities, reducing the need for task-specific fine-\\ntuning which was more prevalent in GPT-2.\\nFinally, GPT-3’s large model scale and diverse training corpus have led to better \\ngeneralization across a broader range of tasks. This means that out-of-the-box, without \\nany further training, GPT-3 exhibits improved performance on diverse NLP challenges, from \\ntranslation to question-answering, compared to GPT-2. It’s also worth noting that the release \\napproach differed: while OpenAI initially held back GPT-2 due to concerns about misuse, \\nthey chose to make GPT-3 available as a commercial API, reflecting both its utility and the \\norganization’s evolving stance on deployment.\\nInstruction tuning was then introduced with InstructGPT17, a version of GPT-3 that was fine-\\ntuned, using Supervised Fine-Tuning, on a dataset of human demonstrations of desired \\nmodel behaviors. Outputs from this model were then ranked and it was then further fine-\\ntuned using Reinforcement Learning from Human Feedback. This led to improved instruction', 'Foundational Large Language Models & Text Generation\\n28\\nFebruary 2025\\nfollowing in the model. A 1.3B parameter InstructGPT model had better human evaluations \\nthan the 175B parameter GPT-3 model. It also showed improvements in truthfulness and \\nreductions in toxicity.\\nGPT-3.5 models, including GPT-3.5 turbo, improve over GPT-3 as it is capable of \\nunderstanding and generating code. It’s been optimized for dialogue. And it’s capable of \\nreceiving context windows of up to 16,385 tokens and can generate outputs of up to 4,096 \\ntokens. \\nGPT-4 extends GPT-3.5 as a large multimodal model capable of processing image and \\ntext inputs and producing text outputs.19 Specifically, accepting text or images as input \\nand outputting text. This model has broader general knowledge and advanced reasoning \\ncapabilities. It can receive context windows of up to 128,000 tokens and has a maximum \\noutput of 4,096 tokens. GPT-4 demonstrates remarkable versatility by solving complex tasks \\nacross diverse fields like mathematics, coding, vision, medicine, law, and psychology – all \\nwithout specialized instructions. Its performance often matches or even exceeds human \\ncapabilities and significantly outperforms earlier models like GPT-3.5.\\nLaMDA\\nGoogle’s LaMDA,20 which stands for ‘Language Model for Dialogue Applications’ is another \\ncontribution to the arena of large-scale language models, designed primarily to engage in \\nopen-ended conversations. Unlike traditional chatbots which operate in more constrained \\nand predefined domains, LaMDA is engineered to handle a wide array of topics, delivering \\nmore natural and flowing conversations. LaMDA was trained on dialogue-focused data to \\nencourage ongoing conversational flow, not just isolated responses, ensuring users can have \\nmore extensive and explorative dialogues.', 'Foundational Large Language Models & Text Generation\\n29\\nFebruary 2025\\nWhile GPT models, especially the later iterations like GPT-3, have strived to address a \\nmultitude of tasks simultaneously, from text generation to code writing, LaMDA’s primary \\nfocus is on maintaining and enhancing conversational depth and breadth. GPT models \\nshine on their ability to produce coherent long-form content and perform various tasks \\nwith minimal prompting, whereas LaMDA emphasizes the flow and progression of dialogue, \\nstriving to mimic the unpredictability and richness of human conversations. \\nGopher\\nGopher22 is a 280 billion parameter language model based on the decoder-only transformer \\narchitecture, developed by DeepMind in 2021.22 It can generate text, translate languages, \\nwrite different kinds of creative content, and answer your questions in an informative way. \\nSimilar to GPT-3, Gopher focused on improving dataset quality and optimization techniques:\\n• Dataset: The researchers curated a high-quality text dataset called MassiveText, which \\ncontains over 10 terabytes of data and 2.45B documents from web pages, books, news \\narticles, and code (GitHub). They only trained on 300B tokens, which is 12% of the dataset. \\nImportantly, they improved the quality of the data by filtering it, such as by removing \\nduplicate text and deduplicating similar documents. This significantly improved the \\nmodel’s performance on downstream tasks.\\n• Optimization: The researchers used a warmup learning rate for 1,500 steps and then \\ndecayed it using a cosine schedule. They also had an interesting rule that as they \\nincreased the model size, they decreased the learning rate and increased the number of \\ntokens in each batch. Additionally, they found that clipping gradients to be a maximum of 1 \\nbased on the global gradient norm helped stabilize the training.', 'Foundational Large Language Models & Text Generation\\n30\\nFebruary 2025\\nGopher was evaluated on a variety of tasks, including mathematics, common sense, logical \\nreasoning, general knowledge, scientific understanding, ethics, and reading comprehension. \\nGopher outperformed previous state-of-the-art models on 81% of the tasks. Specifically, \\nGopher performed well on knowledge-intensive tasks but struggled on reasoning-heavy \\ntasks such as abstract algebra.\\nThe authors also conducted a study on the effect of model size on different types of \\ntasks. Figure 4 shows the results of this ablation study. Specifically, the authors found that \\nincreasing the number of parameters had a significant impact on logical reasoning and \\nreading comprehension, but it did not improve performance as much on tasks such as \\ngeneral knowledge, where performance eventually almost plateaued.\\nFigure 5. Ablation study 22  on the effect of model size on the performance of Gopher on different types \\nof tasks', 'Foundational Large Language Models & Text Generation\\n31\\nFebruary 2025\\nGLaM\\nGLaM (Generalist Language Model)23 was the first sparsely-activated mixture-of-experts \\nlanguage model. Mixture-of-experts based models are much more computationally efficient \\ngiven their parameter count. This is achieved by only activating a subset of their parameters \\n(i.e. experts) for each input token. GLaM consists of 1.2 trillion parameters but uses only ⅓ \\nof the energy used to train GPT-3 and half of the FLOPs for inference while achieving better \\noverall performance compared to GPT-3.\\nChinchilla\\nUntil 2022, LLMs were primarily scaled by increasing the model size and using datasets that \\nare relatively small by current standards (up to 300 billion tokens for the largest models). \\nThis approach was informed by the Kaplan et al.24 study, which examined how performance \\nof a language model, measured by cross-entropy loss, varies with changes in computational \\nbudget, model size, and dataset size. Specifically, given a 100-fold increase in computational \\nresources (C), Kaplan et al.24 recommended scaling model size by approximately 28.8 times \\n(Nopt∝ C0.73 ), while increasing dataset size by only 3.5 times (Dopt∝ C0.27 ). \\nThe Chinchilla paper,25 revisited the compute optimal scaling laws and used three different \\napproaches to find that near equal scaling in parameters and data is optimal with increasing \\ncompute. Thus, a 100-fold increase in compute should translate into a tenfold increase in \\nboth data size and model size.', 'Foundational Large Language Models & Text Generation\\n32\\nFebruary 2025\\nFigure 6. Overlaid predictions from three different approaches from Chinchilla paper, 25  along with \\nprojections from Kaplan et al 24  \\nTo verify the updated scaling law, DeepMind trained a 70B parameter model (called \\nChinchilla) using the same compute budget as the previously trained Gopher model. \\nChinchilla uniformly and significantly outperformed Gopher (280B),21 GPT-3 (175B),13 and \\nMegatron-Turing NLG (530B)26 on a large range of downstream evaluation tasks. Due to being \\n4x smaller than Gopher, both the memory footprint and the inference cost of Chinchilla are \\nalso smaller.\\nThe findings of Chinchilla had significant ramifications for the development of future LLMs. \\nFocus shifted into finding ways to scale dataset size (while maintaining quality) alongside \\nincreasing parameter count. Extrapolating this trend suggests that training dataset size \\nmay soon be limited by the amount of text data available. This has led to new research by \\nMuennighoff et al.27 exploring scaling laws in data-constrained regimes.', 'Foundational Large Language Models & Text Generation\\n33\\nFebruary 2025\\nPaLM\\nPathways language model (PaLM)28 is a 540-billion parameter transformer-based large \\nlanguage model developed by Google AI. It was trained on a massive dataset of text and \\ncode and is capable of performing a wide range of tasks, including common sense reasoning, \\narithmetic reasoning, joke explanation, code generation, and translation.\\nAt the time of its release, PaLM was also able to achieve state-of-the-art performance on \\nmany language benchmarks, for example GLUE and SuperGLUE.29\\nOne of the key features of PaLM is its ability to scale efficiently. This is thanks to the \\nPathways system, which Google developed to distribute the training of large language \\nmodels across two TPU v4 Pods.\\nPaLM 2\\nPaLM 230 is a successor to PaLM that was announced in May 2023. Thanks to a number of \\narchitectural and training enhancements, PaLM 2 is even more capable than PaLM, with \\nfewer total parameters. It excels at advanced reasoning tasks, including code generation, \\nmath, classification, question answering, and translation.\\nPaLM 2 has also been shown to be more efficient than PaLM and became the basis for a \\nnumber of commercial models Google released as part of Google Cloud Generative AI.', 'Foundational Large Language Models & Text Generation\\n34\\nFebruary 2025\\nGemini\\nFigure 7. Gemini can receive multi-modal inputs including text, audio, images, and video data. These are all \\ntokenized and fed into its transformer model. The transformer generates an output that can contain images \\nand text.\\nGemini31 (Figure 6) is a state-of-the-art multimodal language family of models that can \\ntake interleaved sequences of text, image, audio, and video as input. It’s built on top of \\ntransformer decoders and has architectural improvements for scale as well as optimized \\ninference on Google’s Tensor Processing Units (TPUs). In its current version, these models \\nare trained to support contexts of different sizes, up to 2M tokens in the Gemini Pro version \\non Vertex AI and employ mechanisms such as multi-query attention for efficiency. Gemini \\nmodels also employ a Mixture of Experts architecture to optimize efficiency and capabilities \\nof the models. Multimodality allows the models to process text, images and video in input, \\nwith more modalities in input and output expected in the future.\\nThe Gemini models are trained on Google’s TPUv5e and TPUv4 processors, depending on \\nsize and configuration. The pre-training data consists of web documents, books, code, and \\nimage, audio, and video data.', 'Foundational Large Language Models & Text Generation\\n35\\nFebruary 2025\\nLarger models are trained for the compute-optimal number of tokens using the same \\napproach as in Chinchilla paper,25 while small models are trained on significantly more tokens \\nthan compute optimal to improve performance for a given inference budget.\\nThe Gemini family of models is optimized for different sizes: Gemini Ultra, Gemini Pro, Gemini \\nNano and Flash. Gemini Ultra is used for highly complex tasks and achieves state-of-the-\\nart results in 30 out of 32 benchmark tasks. Gemini Pro enables deployment at scale and \\nGemini Nano is designed for on-device applications. The Gemini Nano models leverage \\nadvancements such as distillation to produce state-of-the-art performance for small \\nlanguage models on tasks such as summarization and reading comprehension. As the Gemini \\nmodels are natively multi-modal, it can be seen that training across multiple modalities does \\nindeed lead to a model that is capable of achieving strong capabilities in each domain. \\nDuring the initial part of 2024, Google introduced the latest model of the Gemini family, \\nGemini 1.5 Pro,32 a highly compute-efficient multimodal mixture-of-experts model. This \\nmodel  also dramatically increased the size of the context window to millions of tokens \\nand is capable of recalling and reasoning over those millions of tokens, including multiple \\nlong documents and hours of video and audio. Gemini 1.5 Pro demonstrates remarkable \\ncapabilities across different domains:\\n• Code understanding: It can process massive codebases and answer highly specific \\ncode-related questions.\\n• Language learning: The model can learn new languages never observed at training time \\nsolely based on reference materials provided within its input\\n• Multimodal reasoning: It understands images and text, allowing it to locate a famous scene \\nfrom the novel ‘Les Misérables’ based on a simple sketch.\\n• Video comprehension: It can analyze entire movies, answering detailed questions and \\npinpointing specific timestamps with remarkable accuracy.', 'Foundational Large Language Models & Text Generation\\n36\\nFebruary 2025\\nGoogle’s Gemini 1.5 Pro model excels at retrieving information from even very long \\ndocuments. In their study,32 it demonstrated 100% recall on documents up to 530,000 \\ntokens, and over 99.7% recall on those up to 1 million tokens. Impressively, it maintains 99.2% \\naccuracy when finding information in documents up to 10 million tokens.\\nMoreover, Gemini 1.5 Pro demonstrates a major leap forward in how well LLMs follow complex \\ninstructions. In a rigorous test with 406 multi-step prompts, it significantly outperformed \\nprevious Gemini models. The model accurately followed almost 90% of instructions and fully \\ncompleted 66% of the complex tasks. \\nGemini Flash is a new addition to the Gemini model family and the fastest Gemini model \\nserved in the API. It’s optimized for high-volume, high-frequency tasks at scale, is more \\ncost-efficient to serve and features a breakthrough long context window of 1 million tokens. \\nAlthough it is a lighter weight model than 1.5 Pro, it is highly capable of multimodal reasoning \\nacross vast amounts of information and delivers impressive quality for its size.\\nGemini 2.0 represents a significant leap forward in Google’s multimodal AI models. It builds \\nupon the foundation of Gemini 1.0 with enhanced capabilities and a focus on efficiency and \\nnew modalities. In more details :\\nGemini 2.0 Flash: This version is designed for speed and efficiency, exceeding the \\nperformance of Gemini 1.5 Pro while maintaining the responsiveness developers expect. It \\nshowcases improvements in multimodal understanding, text processing, code generation, \\nvideo analysis, and spatial reasoning. Notably, it has enhanced spatial understanding, leading \\nto more accurate object identification and captioning, especially for small objects in complex \\nscenes. Gemini 2.0 Flash was introduced in late 2024.', 'Foundational Large Language Models & Text Generation\\n37\\nFebruary 2025\\nGemini 2.0 Pro is positioned as a highly capable model for a broad range of tasks. It likely \\nserves as a workhorse for various applications, balancing performance and efficiency. \\nIt is likely an evolution of the original Gemini Pro model with improvements across \\nmultiple domains.\\nGemini 2.0 Nano: As with the previous generation, Nano focuses on on-device deployment. \\nIt is optimized for resource efficiency and speed, enabling AI capabilities directly on devices \\nlike smartphones. \\nGemini 2.0 Flash Thinking Experimental is a fast, high-performance reasoning model, \\nenhanced with explainability through visible “thought processes,” particularly excelling \\nin complex science and math problems; it accepts text and image inputs, produces text \\noutputs, supports a 1 million token input context and a 64,000 token output, utilizes code \\nexecution, has a knowledge cutoff of August 2024, is best suited for complex tasks where \\nlatency is not a primary concern, and is available via Google AI Studio, the Gemini API, and \\nVertex AI, though currently in an experimental deployment status.\\nGemma\\nFurthermore, recently advanced Gemma is a family of lightweight, state-of-the-art open \\nmodels built from the same research and technology used to create the Gemini models.33 The \\nfirst model by Gemma boasts a large vocabulary of 256,000 words and has been trained on \\na massive 6 trillion token dataset. This makes it a valuable addition to the openly-available \\nLLM collection. Additionally, the 2B parameter version is intriguing as it can run efficiently on \\na single GPU.', 'Foundational Large Language Models & Text Generation\\n38\\nFebruary 2025\\nGemma 2,33 developed by Google AI, represents a significant advancement in the field of \\nopen large language models. Designed with a focus on efficiency, the 27-billion parameter \\nmodel boasts performance comparable to much larger models like Llama 3 70B33 on standard \\nbenchmarks. This makes Gemma 2 a powerful and accessible tool for a wide range of AI \\ndevelopers. Its compatibility with diverse tuning toolchains, from cloud-based solutions \\nto popular community tools, further enhances its versatility. With its strong performance, \\nefficient architecture, and accessible nature, Gemma 2 plays a vital role in driving innovation \\nand democratizing AI capabilities.\\nGemma 3 represents Google’s latest advancement in its family of open models, built upon \\nthe research and technology that also powers the Gemini models. A key feature of Gemma \\n3 is its multimodality, enabling it to process both text and image inputs and generate text \\noutputs. This version significantly expands capabilities with a large, 128K context window, and \\nbroad multilingual support encompassing over 140 languages. To cater to diverse hardware \\nand performance needs, Gemma 3 is available in various sizes, including 1B, 4B, 12B, and 27B \\nparameter models. These varieties allow developers to select the most suitable model for \\ntheir specific applications, ranging from resource-constrained devices to high-performance \\ncomputing environments.\\nLLaMA\\nLlama models are transformer-based language models, similar in high-level architecture \\nto other large language models (LLMs) like GPT. They are primarily based on the decoder-\\nonly architecture, meaning they focus on predicting the next token in a sequence given the \\npreceding tokens.', 'Foundational Large Language Models & Text Generation\\n39\\nFebruary 2025\\nMeta has released several key versions of Llama. The original Llama 1 models came in various \\nsizes, from 7B to 65B parameters, and were notable for their strong performance compared \\nto other open-source models of similar size. Llama 234 represented a major advancement, \\nfeaturing a larger context window extended to 4096 tokens for handling longer texts, and \\nimportantly, it was fine-tuned for chat applications, significantly improving its conversational \\nabilities. Llama 2 was offered in 7B, 13B, and 70B parameter versions and, unlike Llama 1, was \\nreleased with a license allowing commercial use. Llama 3 builds upon these advancements \\nwith enhanced performance across reasoning, coding, and general knowledge, and is \\nexpected to include a wider range of sizes. A key focus of Llama 3 is increased safety, with \\nefforts to reduce harmful outputs through improved training and alignment techniques. Llama \\n2 is a family of pretrained and fine-tuned LLMs, ranging from 7B to 70B parameters, with \\nimprovements like a 40% larger training dataset, doubled context length, and grouped-query \\nattention. The fine-tuned version, Llama 2-Chat, excels in dialogue. The next generation, \\nLlama 3.2, includes multilingual text-only models and vision LLMs, with quantized versions for \\non-device deployment. Llama 3.2 uses grouped-query attention and a 128K token vocabulary.\\nMixtral\\nDeveloped by Mistral AI35, Mixtral 8x7B is a Sparse Mixture of Experts (SMoE) model. While its \\ntotal parameter count is 47B, it utilizes only 13B active parameters per token during inference, \\nleading to faster inference and higher throughput. This model excels in mathematics, code \\ngeneration, and multilingual tasks, often outperforming LLaMA 2 70B in these domains. \\nMixtral also supports a 32k token context length, enabling it to handle significantly longer \\nsequences. Its instruction-tuned version, Mixtral 8x7B- Instruct, surpasses several closed-\\nsource models on human evaluation benchmarks. Mistral makes several of its models \\navailable as open source under the Apache 2.0 license, emphasizing open access to model \\nweights. In addition, Mistral offers a range of models through its API, providing various sizes \\nand capabilities to suit diverse requirements.', 'Foundational Large Language Models & Text Generation\\n40\\nFebruary 2025\\nOpenAI O1\\nOpenAI’s new “o1” series represents a significant advancement in models, focusing on \\ncomplex reasoning abilities honed through reinforcement learning. These models employ an \\ninternal “chain-of-thought” process, engaging in extensive deliberation before generating \\na response. This deliberate approach results in exceptional performance on challenging \\nscientific reasoning tasks. Benchmarks demonstrate their proficiency: o1 models achieve \\nan 89th percentile ranking on Codeforces programming competitions, score within the top \\n500 nationally on the AIME (a USA Math Olympiad qualifier), and surpass PhD-level human \\naccuracy on a comprehensive physics, biology, and chemistry benchmark (GPQA). The API \\noffers two variants: o1: The flagship model, optimized for tackling difficult problems that \\nrequire broad, general world knowledge, and  o1-mini: A faster, more cost-effective version, \\nexcelling in domains like coding, mathematics, and scientific tasks where deep specialized \\nknowledge is more critical than extensive general knowledge.\\nDeepSeek\\nDeepSeek has demonstrated that competitive reasoning performance, comparable to \\nOpenAI’s “o1” series, can be achieved through a novel reinforcement learning approach, \\neven without relying on extensive labeled data. This is exemplified by their DeepSeek-R1-\\nZero model, trained purely with RL. Traditional RL methods for LLMs often depend on a \\n“critic” model, trained on labeled data, to provide feedback. DeepSeek’s innovation, called \\nGroup Relative Policy Optimization (GRPO), eliminates this critic. Instead, GRPO uses a set \\nof predefined rules (assessing coherence, completeness, and fluency) to score the model’s \\noutputs across multiple rounds. The model learns by comparing its performance against the \\ngroup average, effectively learning from its own “self-play” without explicit human-provided', 'Foundational Large Language Models & Text Generation\\n41\\nFebruary 2025\\nlabels. This pure-RL approach, while successful in achieving high reasoning scores (matching \\n“o1” on the AIME 2024 mathematics competition), initially resulted in outputs with poor \\nreadability and language mixing.\\nTo address these shortcomings, DeepSeek developed a multi-stage training process for \\ntheir DeepSeek-R1 model. This process begins with supervised fine-tuning (SFT) on a small \\n“cold start” dataset, providing a basic foundation of language understanding. Next, pure-RL \\n(using GRPO) is applied to enhance reasoning abilities, similar to the R1-Zero model. Critically, \\nnear the end of the RL phase, rejection sampling (e.g. filtering)  is employed. The model \\ngenerates multiple outputs, and only the best, according to the GRPO rules, are selected. \\nThis creates a high-quality “synthetic” dataset generated by the model itself. This synthetic \\ndata is then combined with supervised data from the original base model (covering areas \\nlike writing and factual knowledge). A final round of fine-tuning and further RL is performed, \\nleveraging both the synthetic and supervised data, refining the model’s overall performance \\nand generalization capabilities. This multi-stage approach leverages the strengths of each \\ntraining method: the initial SFT provides a basic linguistic foundation; pure-RL fosters strong \\nreasoning skills; rejection sampling creates high-quality training data; and the final SFT \\nand RL steps ensure a polished, well-rounded model. The result is the DeepSeek-R1 model \\nthat matches or exceeds the o1 model in many areas. Chain-of-thought (CoT) reasoning at \\ninference time is intrinsically linked to this RL-based training. The model learns to generate \\nintermediate reasoning steps during training, which are essential for its strong performance \\non complex tasks at inference. Despite providing model weights, DeepSeek’s models are \\neffectively closed-source due to the lack of transparency regarding training data, processing \\nscripts, and data curation methods.\\nOther open models\\nThe landscape of open LLMs is rapidly evolving, with a growing number of models where \\nboth the code and pre-trained weights are publicly accessible. Below we highlight some of \\nthe known examples:', 'Foundational Large Language Models & Text Generation\\n42\\nFebruary 2025\\n• Qwen 1.536: This LLM series from Alibaba comes in six sizes: 0.5B, 1.8B, 4B, 7B, 14B, and \\n72B. Qwen 1.5 models uniformly support a context length of up to 32k tokens and show \\nstrong performance across various benchmarks. Notably, Qwen 1.5-72B outperforms \\nLLaMA2-70B on all evaluated benchmarks, demonstrating exceptional capabilities in \\nlanguage understanding, reasoning, and math.\\n• Yi37: Created by 01.AI, the Yi model family includes 6B and 34B base models pre-trained \\non a massive 3.1 trillion token English and Chinese dataset. Yi emphasizes data quality \\nthrough rigorous cleaning and filtering processes. The 34B model achieves performance \\ncomparable to GPT-3.5 on many benchmarks and can be efficiently served on consumer-\\ngrade GPUs with 4-bit quantization. Yi also offers extensions like a 200k context model, a \\nvision-language model (Yi-VL), and a depth-upscaled 9B model.\\n• Grok 3: Developed by xAI, Grok-3 is released in Grok 3 (Think) and Grok 3 mini (Think). \\nBoth models were trained using reinforcement learning. Grok 3 (Think) learned to refine its \\nproblem-solving strategies, correct errors through backtracking, simplify steps, and utilize \\nthe knowledge it picked up during pretraining. With a context window of 1 million tokens its \\n8 times larger than previous Grok models.\\nThe pace of innovation with LLMs has been rapid and shows no signs of slowing down. There \\nhave been many contributions to the field in both the academic and commercial settings. \\nWith over 20,000 papers published about LLMs in arxiv.org it is impossible to name all \\nof the models and teams that have contributed to the development of LLMs. However, an \\nabbreviated list of open models of interest could include EleutherAI’s GPT-NeoX and GPT-J, \\nStanford’s Alpaca, Vicuna from LMSYS, Grok from xAI, Falcon from TII, PHI from Microsoft, \\nNVLM from Nvidia, DBRX from Databricks, Qwen from Alibaba, Yi from 01.ai , Llama from \\nMeta mentioned above and many others. Some of notable companies developing commercial \\nfoundation LLM models include Anthropic, Cohere, Character.ai, Reka, AI21, Perplexity, xAI \\nand many others in addition to Google and OpenAI mentioned in previous sections. It is \\nimportant when using a model to confirm that the license is appropriate for your use case as \\nmany models are provided with very specific terms of use.', 'Foundational Large Language Models & Text Generation\\n43\\nFebruary 2025\\nComparison\\nIn this section, we observed how transformer-based language models have evolved. They \\nstarted as encoder-decoder architectures with hundreds of millions of parameters trained \\non hundreds of millions of tokens, and have grown to be massive decoder-only architectures \\nwith billions of parameters and trained on trillions of tokens. Table 1 shows how the \\nimportant hyperparameters for all the models discussed in this whitepaper have evolved \\nover time. The scaling of data and parameters has not only improved the performance of \\nLLMs on downstream tasks, but has also resulted in emergent behaviors and zero- or few-\\nshot generalizations to new tasks. However, even the best of these LLMs still have many \\nlimitations. For example, they are not good at engaging in human-like conversations, their \\nmath skills are limited, and they might not be aligned with human ethics (e.g., they might be \\nbiased or generate toxic responses). In the next section, we learn how a lot of these issues \\nare being addressed.', 'Foundational Large Language Models & Text Generation\\n44\\nFebruary 2025\\nModel\\nAttention\\n(2017)\\nGPT \\n(2018)\\nGPT-2\\n(2019)\\nGPT-3\\n(2020)\\nLaMDA  \\n(2021)\\nGopher\\n(2021)\\nChinchilla\\n(2022)\\nOptimizer ADAM ADAM ADAM ADAM ADAM ADAM ADAM-W\\n# Parameters 213M 117M 1.5B 175B 137B 280B 70B\\nVocab size ~37K ~40K ~50K ~50K ~32K ~32K ~32K\\nEmbedding \\ndimension 1024 768 1600 12288 8192 16384 8192\\nKey dimension 64 64 64 128 128 128 128\\n# heads (H) 16 12 25 96 128 128 64\\n# encoder \\nlayers 6 N/A N/A N/A N/A N/A N/A\\n# decoder \\nlayers 6 12 48 96 64 80 80\\nFeed forward \\ndimension 4 * 1024 4 * 768 4 * 1600 4 * 12288 8 * 8192 4 * 16384 4 * 8192\\nContext Token \\nSize N/A 512 1024 2048 N/A 2048 2048\\nPre-Training \\ntokens ~160MA ~1.25BA ~10B ~300B ~168B ~300B ~1.4T\\nTable 1. Important hyperparameters for transformers-based large language models\\nA. This number is an estimate based on the reported size of the dataset.', 'Foundational Large Language Models & Text Generation\\n45\\nFebruary 2025\\nFine-tuning large language models\\nLarge language models typically undergo multiple training stages. The first stage, often \\nreferred to as pre-training, is the foundational stage where an LLM is trained on large, \\ndiverse, and unlabelled text datasets where it’s tasked to predict the next token given the \\nprevious context. The goal of this stage is to leverage a large, general distribution of data \\nand to create a model that is good at sampling from this general distribution. After language \\nmodel pretraining, the resulting LLM usually demonstrates a reasonable level of language \\nunderstanding and language generation skills across a variety of different tasks which \\nare typically tested through zero-shot or few-shot prompting (augmenting the instruction \\nwith a few examples / demonstrations). Pretraining is the most expensive in terms of time \\n(from weeks to months depending on the size of the model) and the amount of required \\ncomputational resources, (GPU/TPU hours).\\nAfter training, the model can be further specialized via fine-tuning, typically called \\ninstruction-tuning or simply supervised fine-tuning (SFT). SFT involves training an LLM on a \\nset of task-specific demonstration datasets where its performance is also measured across \\na set of domain-specific tasks. The following are some examples of behaviors that can be \\nimproved using fine-tuning:\\n• Instruction-tuning/instruction following: The LLM is provided as input an instruction to \\nfollow which might include summarizing a piece of text, writing a piece of code, or writing \\na poem in a certain style.17\\n• Dialogue-tuning: This is a special case of instruction tuning where the LLM is fine-tuned \\non conversational data in the form of questions and responses. This is often called \\nmulti-turn dialogue.39', 'Foundational Large Language Models & Text Generation\\n46\\nFebruary 2025\\n• Safety tuning: This is crucial for mitigating risks associated with bias, discrimination, and \\ntoxic outputs. It involves a multi-pronged approach encompassing careful data selection, \\nhuman-in-the-loop validation, and incorporating safety guardrails. Techniques like \\nreinforcement learning with human feedback (RLHF)40 enable the LLM to prioritize safe \\nand ethical responses.\\nFine-tuning is considerably less costly and more data efficient compared to pre-training. \\nNumerous techniques exist to optimize the costs further which are discussed later in \\nthis whitepaper.\\nSupervised fine-tuning \\nAs mentioned in the previous section, SFT is the process of improving an LLM’s performance \\non a specific task or set of tasks by further training it on domain-specific, labeled data. The \\ndataset is typically significantly smaller than the pre-training datasets, and is usually human-\\ncurated and of high quality. \\nIn this setting, each data point consists of an input (prompt) and a demonstration (target \\nresponse). For example, questions (prompt) and answers (target response), translations from \\none language (prompt) to another language (target response), a document to summarize \\n(prompt), and the corresponding summary (target response). \\nIt’s important to note that, while fine-tuning can be used to improve the performance on \\nparticular tasks as mentioned above, it can also serve the purpose of helping the LLM \\nimprove its behavior to be safer, less toxic, more conversational, and better at following \\ninstructions.', 'Foundational Large Language Models & Text Generation\\n47\\nFebruary 2025\\nReinforcement learning from human feedback\\nTypically, after performing SFT, a second stage of fine-tuning occurs which is called \\nreinforcement learning from human feedback (RLHF). This is a very powerful fine-tuning \\ntechnique that enables an LLM to better align with human-preferred responses (i.e. making \\nits responses more helpful, truthful, safer, etc.). \\nFigure 8. An example RLHF procedure \\nIn contrast to SFT, where an LLM is only exposed to positive examples (e.g. high-quality \\ndemonstration data), RLHF makes it possible to also leverage negative outputs thus \\npenalizing an LLM when it generates responses that exhibit undesired properties. Penalizing \\nnegative output makes it less likely to generate unhelpful or unsafe responses. \\nTo leverage RLHF, a reward model (RM) typically needs to be trained with a procedure similar \\nto that in Figure 8. An RM is usually initialized with a pretrained transformer model, often also \\none that is SFT. Then it is tuned on human preference data which is either single sided (with a \\nprompt, response and a score) or composed of a prompt and a pair of responses along with', 'Foundational Large Language Models & Text Generation\\n48\\nFebruary 2025\\na preference label indicating which of the two responses was preferred. For example, given \\ntwo summaries, A and B, of the same article, a human rater selects a preferred summary \\n(relying on the detailed guidance). We refer to the provided preference labels as human \\nfeedback. Preferences can be in the binary form (e.g. ‘good’ or ‘bad’), on the Likert scale42, \\nrank order when more than 2 candidates are evaluated, or a more detailed assessment of the \\nsummary quality. The preference signal can also incorporate many dimensions that capture \\nvarious aspects that define a high quality response, e.g., as safety, helpfulness, fairness, and \\ntruthfulness. \\nFigure 8 shows a typical RLHF pipeline where a Reward model is initialized and finetuned on \\npreference pairs. Once an RM has been trained, it’s then used by a Reinforcement Learning \\n(RL)43 policy gradient algorithm, which further finetunes a previously instruction-tuned LLM to \\ngenerate responses that are better aligned with human preferences. \\nTo better scale RLHF, RL from AI Feedback (RLAIF)44 leverages AI feedback instead of human \\nfeedback to generate preference labels. It’s also possible to remove the need for training \\nRLHF by leveraging approaches such as direct preference optimization (DPO).45 Both RLHF \\nand RLAIF can be used on Google Cloud.', 'Foundational Large Language Models & Text Generation\\n49\\nFebruary 2025\\nParameter Efficient Fine-Tuning\\nBoth SFT and RLHF are still very costly in terms of compute time and accelerators required, \\nespecially when full-fine tuning entire LLMs on the orders of billions of parameters. Luckily, \\nthere are some really useful and effective techniques that can make fine-tuning significantly \\ncheaper and faster compared to pre-training and full fine-tuning. One such family of \\nmethods is parameter efficient fine-tuning (PEFT) techniques. \\nAt a high-level, PEFT approaches append a significantly smaller set of weights (e.g., on the \\norder of thousands of parameters) that are used to ‘perturb’ the pre-trained LLM weights. \\nThe perturbation has the effect of fine-tuning the LLM to perform a new task or set of tasks. \\nThis has the benefit of training a significantly smaller set of weights, compared to traditional \\nfine-tuning of the entire model. \\nSome common PEFT techniques include the adapter, low-rank adaptation, and \\nsoft prompting:\\n• Adapter-based fine-tuning46 employs small modules, called adapters, to the pre-\\ntrained model. Only the adapter parameters are trained, resulting in significantly fewer \\nparameters than traditional SFT. \\n• Low-Rank Adaptation (LoRA)47 tackles efficiency differently. It uses two smaller matrices \\nto approximate the original weight matrix update instead of fine-tuning the whole LLM. \\nThis technique freezes the original weights and trains these update matrices, significantly \\nreducing resource requirements with minimum additional inference latency. Additionally, \\nLoRA has improved variants such as QLoRA,48 which uses quantized weights for even \\ngreater efficiency. A nice advantage of LoRA modules is that they can be plug-and-play, \\nmeaning you can train a LoRA module that specializes in one task and easily replace it with \\nanother LoRA module trained on a different task. It also makes it easier to transfer the \\nmodel since assuming the receiver has the original matrix, only the update matrices need \\nto be provided.', 'Foundational Large Language Models & Text Generation\\n50\\nFebruary 2025\\n• Soft prompting49 is a technique for conditioning frozen large language models with \\nlearnable vectors instead of hand-crafted text prompts. These vectors, called soft \\nprompts, are optimized on the training data and can be as few as five tokens, making them \\nparameter-efficient and enabling mixed-task inference. \\nFor most tasks, full fine-tuning is still the most performant, followed by LoRA and Soft \\nprompting, but the order is reversed when it comes to cost. All three approaches are more \\nmemory efficient than traditional fine-tuning and achieve comparable performance.', 'Foundational Large Language Models & Text Generation\\n51\\nFebruary 2025\\nPython\\n# Before you start run this command:\\n# pip install --upgrade --user --quiet google-cloud-aiplatform\\n# after running pip install make sure you restart your kernel\\nimport vertexai\\nfrom vertexai.generative_models import GenerativeModel\\nfrom vertexai.preview.tuning import sft\\n# TODO : Set values as per your requirements\\n# Project and Storage Constants\\nPROJECT_ID = ‘<project_id>’\\nREGION = ‘<region>’\\nvertexai.init(project=PROJECT_ID, location=REGION)\\n# define training & eval dataset.\\nTRAINING_DATASET = ‘gs://cloud-samples-data/vertex-ai/model-evaluation/\\npeft_train_sample.jsonl’\\n# set base model and specify a name for the tuned model\\nBASE_MODEL = ‘gemini-1.5-pro-002’\\nTUNED_MODEL_DISPLAY_NAME = ‘gemini-fine-tuning-v1’\\n# start the fine-tuning job\\nsft_tuning_job = sft.train(\\n   source_model=BASE_MODEL,\\n   train_dataset=TRAINING_DATASET,\\n   # # Optional:\\n   tuned_model_display_name=TUNED_MODEL_DISPLAY_NAME,\\n)\\n# Get the tuning job info.\\nsft_tuning_job.to_dict()\\n# tuned model endpoint name\\ntuned_model_endpoint_name = sft_tuning_job.tuned_model_endpoint_name\\n# use the tuned model\\ntuned_genai_model = GenerativeModel(tuned_model_endpoint_name)\\nprint(tuned_genai_model.generate_content(contents=’What is a LLM?’))\\nSnippet 1. SFT fine tuning on Google cloud', 'Foundational Large Language Models & Text Generation\\n52\\nFebruary 2025\\nUsing large language models\\nPrompt engineering and sampling techniques have a strong influence on the performance of \\nLLMs. Prompt engineering is the process of designing and refining the text inputs (prompts) \\nthat you feed into an LLM to achieve desired and relevant outputs. Sampling techniques \\ndetermine the way in which output tokens are chosen and influence the correctness, \\ncreativity and diversity of the resulting output. We next discuss different variants of prompt \\nengineering and sampling techniques as well as touch on some important parameters that \\ncan have a significant impact on LLM performance.\\nPrompt engineering \\nLLMs are very powerful, but they still need guidance to unleash their full potential. Prompt \\nengineering is a critical component in guiding an LLM to yield desired outputs. This might \\ninclude grounding the model to yield factual responses or unleashing the creativity of the \\nmodel to tell a story or write a song. Examples of prompt engineering include providing \\nclear instructions to the LLM, giving examples, using keywords, and formatting to emphasize \\nimportant information, providing additional background details etc. \\nYou will often hear the terms zero-shot, few-shot, and chain-of-thought prompting in the \\ncontext of prompt engineering. We define these terms below: \\n• Few-shot prompting: This is when you provide the LLM with a task description, as well \\nas a few (e.g. three to five) carefully chosen examples, that will help guide the LLM’s \\nresponse. For example, you might provide the model with the name of a few countries \\nand their capital cities, then ask it to generate the capital for a new country that isn’t in \\nthe examples.', 'Foundational Large Language Models & Text Generation\\n53\\nFebruary 2025\\n• Zero-shot prompting: This is when you provide the LLM directly with a prompt with \\ninstructions. You usually give the LLM a task description and the LLM relies heavily on its \\nexisting knowledge to output the correct response. This requires no additional data or \\nexamples, hence the name ‘Zero-shot’ but can be less reliable than few-shot prompting.\\n• Chain-of-thought prompting: This technique aims to improve performance on complex \\nreasoning tasks. Rather than simply asking the LLM a question, you provide a prompt \\nthat demonstrates how to solve similar problems using step-by-step reasoning. The \\nLLM then generates its own chain of thought for the new problem, breaking it down into \\nsmaller steps and explaining its reasoning. Finally, it provides an answer based on its \\nreasoning process.\\nPrompt engineering is an active area of research.\\nSampling Techniques and Parameters\\nA variety of sampling techniques can be employed to determine how the model chooses \\nthe next token in a sequence. They are essential for controlling the quality, creativity, and \\ndiversity of the LLM’s output. The following is a breakdown of different sampling techniques \\nand their important parameters:\\n• Greedy search50: Selects the token with the highest probability at each step. This is the \\nsimplest option but it can lead to repetitive and predictable outputs.\\n• Random sampling:50 Selects the next token according to the probability distribution, where \\neach token is sampled proportionally to its predicted probability. This can produce more \\nsurprising and creative text, but also a higher chance of nonsensical output.\\n• Temperature sampling:50 Adjusts the probability distribution by a temperature parameter. \\nHigher temperatures promote diversity, lower temperatures favor high-probability tokens.', 'Foundational Large Language Models & Text Generation\\n54\\nFebruary 2025\\n• Top-K sampling: Randomly samples from the top K most probable tokens. The value of K \\ncontrols the degree of randomness.\\n• Top-P sampling (nucleus sampling):51 Samples from a dynamic subset of tokens whose \\ncumulative probability adds up to P. This allows the model to adapt the number of potential \\ncandidates depending on its confidence, favoring more diversity when uncertain and \\nfocusing on a smaller set of highly probable words when confident.\\n• Best-of-N sampling: Generates N separate responses and selects the one deemed best \\naccording to a predetermined metric (e.g., a reward model or a logical consistency check). \\nThis is particularly useful for short snippets or situations where logic and reasoning \\nare key.\\nBy combining prompt engineering with sampling techniques and correctly calibrated \\nhyperparameters, you can greatly influence the LLM’s response, making it more relevant, \\ncreative, and consistent for your specific needs.\\nUntil now, we have seen the various types of LLM architectures, their underlying technology, \\nas well as the approaches used to train, tune, and adapt these models for various tasks. Let’s \\nnow look at some key research about how the decoding process in LLMs can be sped up \\nconsiderably to generate faster responses.\\nTask-based Evaluation\\nThe emergence of LLMs has reduced the obstacles to building AI applications, but moving \\nfrom MVP to production introduces challenges such as prompt engineering, model selection, \\nand performance monitoring. A tailored evaluation framework is essential for navigating \\nLLM application development by validating functionality and user experience, identifying', 'Foundational Large Language Models & Text Generation\\n55\\nFebruary 2025\\npotential issues, facilitating communication about capabilities, and establishing a roadmap \\nfor improvement. For building a tailored evaluation framework, application builders need to \\nprovide their own evaluation data, development context, a definition of good performance.  \\n• Evaluation data: Public leaderboards that showcase LLM capabilities often fall short for \\napplication developers who require a more tailored approach. A dedicated evaluation \\ndataset that mirrors the expected production traffic as closely as possible is needed. \\nDuring prototyping this can be a manually curated dataset, one that is continuously \\nenriched with real user interactions, production logs, and synthetically generated data to \\ntest specific scenarios.\\n• Development Context: The evaluation should extend beyond just the model’s output \\nto analyze the entire system, including components like data augmentation (e.g., \\nRetrieval Augmented Generation or RAG) and agentic workflows. This approach ensures \\nunderstanding of how all components interact and contribute to the application’s \\noverall performance.\\n• Definition of “Good”: Traditional metrics that prioritize matching a single “correct” \\nanswer can unfairly penalize unexpected solutions. When working with LLMs, we can \\naddress this by moving beyond similarity to ground truth as a definition of good, but \\nrather establish dataset level criteria that reflect desired business outcomes or even \\nrubrics that capture the core elements of the desired outputs depending on the input user \\ninstructions. \\nApplication builders can evaluate LLM performance using three methods:\\n• Traditional Evaluation Methods: Similar to evaluating predictive models, these methods \\nuse quantitative metrics to compare model outputs to ideal responses, aiming to offer \\nobjective insights. However, they may penalize creative or unexpected outputs, limiting \\ntheir effectiveness for generative tasks that have multiple possible solutions.', 'Foundational Large Language Models & Text Generation\\n56\\nFebruary 2025\\n• Human Evaluation: Considered the gold standard, human judgment provides nuanced \\nassessment of complex generative outputs.\\n• LLM-Powered Autoraters: LLM-powered autoraters try to mimic human judgment, \\noffering scalable and efficient evaluations. Unlike computation-based methods, they \\ncan operate with or without reference data. A basic setup involves providing the task, \\ncriteria, and candidate responses (with optional references), which the autorater uses to \\ngenerate and parse LLM output for final evaluation results. In addition to the final outputs, \\nautoraters can provide rationales to explain a given decision to the user. While generative \\nmodels are common autoraters, reward and discriminative models are also used. Crucially, \\nlike any measurement tool, autoraters require calibration. Meta-evaluation, i.e. comparing \\nautorater outputs to human judgments, ensures the autorater aligns with desired \\npreferences. This calibration typically involves agreement in terms of model preference \\nor correlation measures, tailored to the evaluation task. In this meta-evaluation, it is \\nimportant to keep in mind potential limitations of autorater models.69\\nWhile the basic setup described above focuses on providing autoraters with fixed criteria \\nthat can be used to evaluate an entire dataset of examples, approaches are emerging to \\nleverage rubrics and multi-step processes in order to obtain interpretable evaluation metrics. \\nAt a high level, an LLM breaks an example down into multiple evaluation subtasks and then \\nevaluates each subtask to give an interpretable, detailed report for this example. As the \\ntask has been broken down, domain-specialized models can be leveraged for specific tasks \\nto improve reliability if necessary. Results are then aggregated for a given example to yield \\nan overall score, or across related subtasks to evaluate how well a model performs along a \\nparticular axis. This setup is especially useful in media generation, where different examples \\nmay require vastly different skills (e.g. object generation vs text generation) and a single \\nscore obfuscates how relative strengths and weaknesses of different models contribute to \\nthe final result.', 'Foundational Large Language Models & Text Generation\\n57\\nFebruary 2025\\nAccelerating inference\\nThe scaling laws for LLMs which were initially explored by the Kaplan et al.24 study continue \\nto hold today. Language models have been consistently increasing in size and this has been \\na direct contributor to the vast improvement in these models’ quality and accuracy over the \\nlast few years. As increasing the number of parameters has improved the quality of LLMs it \\nhas also increased the computational resources needed to run them. Numerous approaches \\nhave been used to try and improve the efficiency of LLMs for different tasks as developers \\nare incentivized to reduce cost and latency for model users. Balancing the expense of \\nserving a model in terms of time, money, energy is known as the cost-performance tradeoff \\nand often needs adjusting for particular use cases.\\nTwo of the main resources used by LLMs are memory and computation. Techniques for \\nimproving the efficiency or speed of inference focus primarily on these resources. The \\nspeed of the connection between memory and compute is also critical, but usually hardware \\nconstrained.  As LLMs have grown in size 1000x from millions to billions of parameters. \\nAdditional parameters increase both the size of memory required to hold the model and \\ncomputations needed to produce the model results.\\nWith LLMs being increasingly adopted for large-scale and low-latency use cases, finding \\nways to optimize their inference performance has become a priority and an active research \\ntopic with significant advancements. We will explore a number of methods and a few \\ntradeoffs for accelerating inference.', 'Foundational Large Language Models & Text Generation\\n58\\nFebruary 2025\\nTrade offs\\nMany of the high yielding inference optimisation methods mandate trading off a number of \\nfactors, this can be tweaked on a case-by-case basis allowing for tailored approaches to \\ndifferent inference use cases and requirements. A number of the optimization methods we \\nwill discuss later fall somewhere on the spectrum of these tradeoffs. \\nTrading off one factor against the other (e.g. latency vs quality or cost) doesn’t mean that \\nwe’re completely sacrificing that factor, it just means that we’re accepting what might be \\na marginal degradation in quality, latency or cost for the benefit of substantially improving \\nanother factor.\\nThe Quality vs Latency/Cost Tradeoff\\nIt is possible to improve the speed and cost of inference significantly through accepting \\nwhat might be marginal to negligible drops in the model’s accuracy. One  example of this \\nis using a smaller model to perform the task. Another example is quantisation where we \\ndecrease the precision of the model’s parameters thereby leading to faster and less memory \\nintensive calculations.\\nOne important distinction when approaching this trade-off is between the theoretical \\npossibility of a quality loss versus the practical capability of the model to perform the desired \\ntask. This is use case specific and exploring it will often lead to significant speedups without \\nsacrificing quality in a meaningful or noticeable way. For example, if the task we want the \\nmodel to perform is simple, then a smaller model or a quantised one will likely be able to \\nperform this task well. Reduction in parametric capacity or precision does not automatically \\nmean that the model is less capable at that specific task.', 'Foundational Large Language Models & Text Generation\\n59\\nFebruary 2025\\nThe Latency vs Cost Tradeoff\\nAnother name for this tradeoff is the latency vs throughput tradeoff. Where throughput refers \\nto the system’s ability at handling multiple requests efficiently. Better throughput on the same \\nhardware means that our LLM inference cost is reduced, and vice versa.\\nMuch like traditional software systems, there are often multiple opportunities to tradeoff \\nlatency against the cost of LLM inference. This is an important tradeoff since LLM inference \\ntends to be the slowest and most expensive component in the entire stack; balancing latency \\nand cost intentionally is key to making sure we tailor LLM performance to the product or use \\ncase it’s being used in. An example would be bulk inference use cases (e.g. offline labeling) \\nwhere cost can be a more important factor than the latency of any particular request. On the \\nother hand, an LLM chatbot product will place much higher importance on request latency.\\nNow that we’ve covered some of the important tradeoffs to consider when optimizing \\ninference, let’s examine some of the most effective inference acceleration techniques. As \\ndiscussed in the tradeoffs section, some optimization techniques can have an impact on the \\nmodel’s output. Therefore we will split the methods into two types: output-approximating \\nand output-preserving.\\nAs of this writing, Gemini 2.0 Flash Thinking offers an unparalleled balance of quality, as \\nmeasured by its ELO score, and affordability, with a cost per million tokens that is ten times \\nlower than comparable models; its position on a quality-versus-cost graph (where further \\nup and to the right is superior) demonstrates its transformative development. Moreover, the \\npicture highlights the rapid advancements in reasoning and thinking capabilities within the AI \\nfield, with a 27-fold improvement observed in the last three months.', 'Foundational Large Language Models & Text Generation\\n60\\nFebruary 2025\\nOutput-approximating methods\\nQuantization\\nLLMs are fundamentally composed of multiple numerical matrices (a.k.a the model weights). \\nDuring inference, matrix operations are then applied to these model weights to produce \\nnumerical outputs (a.k.a activations). Quantization is the process of decreasing the numerical \\nprecision in which weights and activations are stored, transferred and operated upon. The \\ndefault representation of weights and activations is usually 32 bits floating numbers, with \\nquantization we can drop the precision to 8 or even 4 bit integers. \\nQuantization has multiple performance benefits, it reduces the memory footprint of \\nthe model, allowing to fit larger models on the same hardware, it also reduces the \\ncommunication overhead of weights and activations within one chip and across chips in \\na distributed inference setup- therefore speeding up inference as communication is a \\nmajor contributor to latency. In addition, decreasing the precision of weights/activations \\ncan enable faster arithmetic operations on these models as some accelerator hardware \\n(e.g. TPUs/GPUs) natively supports faster matrix multiplication operations for some lower \\nprecision representations.\\nQuantization’s impact on quality can be very mild to non-existent depending on the use \\ncase and model.  Further, in cases where quantisation might introduce a quality regression, \\nthat regression can be small compared to the performance gain, therefore allowing for an \\neffective Quality vs Latency/Cost Tradeoff. For example, Benoit Jacob et al.55 reported a 2X \\nspeed-up for a 2% drop in accuracy for the FaceDetection task on MobileNet SSD.', 'Foundational Large Language Models & Text Generation\\n61\\nFebruary 2025\\nQuantization can be either applied as an inference-only operation, or it can be incorporated \\ninto the training (referred to as Quantisation Aware Training QAT). QAT is generally \\nconsidered to be a more resilient approach as the model is able to recover some of the \\nquantisation-related quality losses during training. To make sure we get the best cost/quality \\ntradeoff, we tweak the quantization strategy (e.g. select different precisions for weights \\nvs activations) and the granularity in which we apply quantisation to Tensors (e.g. channel \\nor group-wise58).\\nDistillation\\nUsing a smaller model to perform a task is one of the most efficient inference optimization \\ntechniques, however, smaller models can demonstrate significant regressions on quality \\ncompared to their larger counterparts.\\nDistillation is a set of training techniques that targets improving the quality of a smaller model \\n(the student) using a larger model (the teacher). This method can be effective because larger \\nmodels outperform smaller ones even if both are trained on the same data, mainly due to \\nparametric capacity and training dynamics. The gap in performance continues as the training \\ndataset grows as illustrated by Figure 9.\\nIt is worth noticing that even at low volumes of training data, large models can already \\ndemonstrate better performance than the correspondingly trained smaller models, this fact \\nleads us to the first variant of distillation which is referred to as data distillation or model \\ncompression.56 We use a large model which was trained on the data we have to generate \\nmore synthetic data to train the smaller student model, the increase in data volume will help \\nmove the the student further along the quality line compared to only training on the original \\ndata. Synthetic data needs to be approached carefully as it needs to be of high quality and \\ncan lead to negative effects otherwise.', 'Foundational Large Language Models & Text Generation\\n62\\nFebruary 2025\\nFigure 9. An illustration of the performance of models of various sizes as a function of the training \\ndataset’s size\\nOther distillation techniques attempt to bring the student model closer to the teacher \\non a more granular level than just synthetic data generation. One prominent technique is \\nknowledge distillation57, in this approach we attempt to align the output token distribution \\nof the student model to that of the teacher’s, this can be much more sample efficient than \\ndata distillation. On-policy distillation59 is another technique that leverages feedback from \\nthe teacher model on each sequence generated by the student in a reinforcement learning \\nsetup. \\nOutput-preserving methods\\nThese methods are guaranteed to be quality neutral, they cause no changes to the model \\noutput which often makes them obvious first steps to optimize inference before facing the \\nmore nuanced tradeoffs of the approximating methods', 'Foundational Large Language Models & Text Generation\\n63\\nFebruary 2025\\nFlash Attention\\nScaled Dot-product Attention, which is the predominant attention mechanism in the \\ntransformer architecture, is a quadratic operation on the input length. Optimizing the self-\\nattention calculation can bring significant latency and cost wins.\\nFlash Attention, introduced in by Tri Dao et al.62, optimizes the attention calculation by making \\nthe attention algorithm IO Aware, particularly trying to minimize the amount of data we move \\nbetween the slow HBM (high bandwidth memory) to the faster memory tier (SRAM/VMEM) in \\nTPUs and GPUs. When calculating attention, the order of operations is changed and multiple \\nlayers are fused so we can utilize the faster memory tiers as efficiently as possible.\\nFlash Attention is an exact algorithm, it maintains the numerical output of the attention \\ncomputation and can yield significant latency benefits due to reducing the IO overhead, Tri \\nDao et al.62 showed 2-4X latency improvements in the attention computation.\\nPrefix Caching\\nOne of the most compute intensive, and thus slowest, operations in LLM inference is \\ncalculating the attention key and value scores (a.k.a KV) for the input we’re passing to the \\nLLM, this operation is often referred to as prefill. The final output of prefill is what is termed \\nKV Cache which includes the attention key and value scores for each layer of the transformer \\nfor the entire input. This cache is vital during the decoding phase which produces the output \\ntokens, the KV cache allows us to avoid recalculating attention scores for the input on each \\nautoregressive decode step.\\nPrefix Caching refers to the process of caching the KV Cache itself between subsequent \\ninference requests in order to reduce the latency and cost of the prefill operation. The way \\nthe self-attention mechanism works makes reusing KV caches possible because tokens will', 'Foundational Large Language Models & Text Generation\\n64\\nFebruary 2025\\nonly pay attention to tokens that came before them in the sequence. If there’s new input \\nbeing appended to input that the model has seen before, then we can potentially avoid \\nrecalculating the prefill for the older input.\\nFigure 10. An illustration of Prefix Caching in a chat scenario\\nFigure 10 illustrates how prefix caching works in a multi-turn scenario with a document \\nupload. On the first user turn, the prefill operation has to process the entire document \\ntherefor taking 500ms, the resulting KV cache is then stored so that on the second user turn, \\nwe can retrieve the cache directly from storage and avoid recomputing it for the long doc, \\ntherefore saving substantial amounts of compute and latency.', 'Foundational Large Language Models & Text Generation\\n65\\nFebruary 2025\\nPrefix caches can be stored either in memory or on disk and fetched on-demand. One \\nimportant consideration is making sure that the input structure/schema remains prefix-\\ncaching friendly, we should avoid changing the prefix in subsequent requests as that will \\ninvalidate the cache for all the tokens that follow For example, putting a fresh timestamp at \\nthe very beginning of each request will invalidate the cache completely as every subsequent \\nrequest will have a new prefix.\\nMany LLM use cases lend themselves naturally to prefix caching. For example, LLM Chatbots \\nwhere users will have a multi-turn conversation that can span 10s of 1000s of tokens and \\nwe can avoid recalculating the KV cache for the previous parts of the conversation. Large \\ndocument/code uploads is another use case where the artifact the user uploads will remain \\nunchanged from one request to the next. All that’s changing are the questions the user is \\nasking, so caching the KV cache for the document (especially for larger artifacts) can result \\nin significant latency and cost savings.\\nPrefix caching is available as a service called Context Caching on Google AI studio52 and  \\nVertex AI on Google Cloud53.\\nSpeculative Decoding\\nThe first phase of LLM inference, known as prefill, is compute bound due large matrix \\noperations on many tokens occurring in parallel. The second phase, known as decode, is \\ngenerally memory bound as tokens are auto-regressively decoded one at a time. \\nIt is not easy to naively use additional parallel compute capacity to speed up decode \\ngiven the  need to wait for the current token to be produced before we can calculate what \\nthe next token should be (as per the self-attention mechanism), the decode process is \\ninherently serial.', 'Foundational Large Language Models & Text Generation\\n66\\nFebruary 2025\\nSpeculative decoding (Leviathan at al.63) aims to overcome this limitation in decode by finding \\na way to utilize the spare compute capacity to make each decode step faster. The main idea \\nis to use a much smaller secondary model (often referred to as the drafter) to run ahead of \\nthe main model and predict more tokens. (e.g. 4 tokens ahead). This will happen very quickly \\nas the drafter is much faster and smaller than the main model. We then use the main model to \\nverify the hypotheses of the drafter in parallel for each of the 4 steps (i.e. the first token, the \\nfirst two tokens, the first 3 tokens and finally all 4 tokens), and we then select the accepted \\nhypothesis with the maximum number of tokens. For example:\\nFigure 11. An illustration of speculative decoding over 3 tokens\\nNote that the 3 main model steps run in parallel. And because we are not compute bound in \\ndecode, we can use the spare capacity to get much better decode latencies. In the example \\nabove, let’s say a single main model step needs 10ms, while the drafter needs 1ms. Without \\nspeculative decoding, we need 3 * 10ms = 30ms to produce the response, with speculative \\ndecoding, there’s only one main model step on the critical path due to parallelization, so we \\nneed 3 * 1ms + 10ms = 13ms. A significant latency improvement. This technique is completely', 'Foundational Large Language Models & Text Generation\\n67\\nFebruary 2025\\nquality neutral, the main model will reject any tokens that it wouldn’t have predicted itself \\nin the first place, so the only thing speculative decoding does is run ahead and present \\nhypotheses that the main model can accept or reject in parallel.\\nOne important condition for speculative decoding to work effectively is that the drafter model \\nhas good levels of alignment with the main model, otherwise we won’t be able to accept any \\nof the tokens. So investing in the training quality of the drafter model is worthwhile to get \\nbetter latencies.\\nNow that we have seen some methods to make LLM generate their responses faster, let’s \\nlook at some examples of how these models can be applied to various tasks to get an idea \\nhow to use them.\\nBatching and Parallelization\\nMost of the optimization techniques we’ve discussed so far are specific to Machine Learning \\nand Transformer architecture in particular. However, much like any software system, there \\nare opportunities to improve throughput and latency through a combination of 1) batching \\nless compute-intensive operations (i.e. we can run multiple requests on the same hardware \\nsimultaneously to better utilize the spare compute) and 2) parallelizing the more compute-\\nintensive parts of the computations (i.e. we can divide the computation and split it amongst \\nmore hardware instances to get more compute capacity and therefore better latencies\\nBatching in LLMs is most useful on the decode side - as we explained in the Speculative \\nDecoding section, decode is not compute-bound and therefore there’s an opportunity \\nto batch more requests. We need to be careful that we batch computations in a way that \\nenables utilization of the spare capacity which is possible to do on accelerators (e.g. TPUs \\nand GPUs). We also need to make sure we remain within the memory limits, as decode is a', 'Foundational Large Language Models & Text Generation\\n68\\nFebruary 2025\\nmemory intensive operations, batching more requests will put more pressure on the free \\nmemory available. Batching has become an important component in most high-throughput \\nLLM inference setups.\\nParallelization is also a widely used technique given the variety of opportunities in \\ntransformers for horizontal scaling across more hardware instances. There are multiple \\nparallelism techniques across the model input (Sequence parallelism) the model layers \\n(Pipeline parallelism), and within a single layer (Tensor parallelism). One of the most important \\nconsiderations for parallelism is the cost of communication and synchronization between \\nthe different shards that we distribute to other machines. Communication is a significant \\noverhead and can erode the benefits of adding more computational capacity if we’re not \\ncareful about which parallelization strategy to use. On the other hand, selecting the right \\nstrategy to balance the need for additional compute and the communication cost can yield \\nsignificant latency wins.\\nNow that we have seen some methods to make LLM generate their responses faster, let’s \\nlook at some examples of how these models can be applied to various tasks to get an idea \\nhow to use them.\\nApplications\\nLarge language models are revolutionizing the way we interact with and process information. \\nWith their unprecedented ability to understand context and generate content, they’re \\ntransforming numerous applications in the worlds of text, code, images, audio and video. \\nHere we collected a few examples of application areas, but the reader should keep in mind \\nthat this is not a comprehensive list and that many new ideas are emerging continuously \\nabout how to best utilize the capabilities of these new tools. For more information about \\noptimally building and deploying functioning applications based on the following mentioned \\nuse cases, refer to the subsequent whitepapers.', 'Foundational Large Language Models & Text Generation\\n69\\nFebruary 2025\\nIt is also very simple to generate text-based responses for your use case using either \\nthe Google Cloud Vertex AI SDK or the Developer focused AI studio. Snippet 3 shows \\ncode examples from these SDKs to generate responses to text prompts using the Gemini \\nmodel. Note that the multimodal aspects of Gemini are covered in their respective \\ndedicated whitepapers.', 'Foundational Large Language Models & Text Generation\\n70\\nFebruary 2025\\nPython\\n%pip install --upgrade --quiet google-genai\\nimport sys \\nif “google.colab” in sys.modules:\\n    from google.colab import auth \\n auth.authenticate_user () \\nfrom IPython.display import HTML, Markdown, display\\nfrom google import genai\\nfrom google.genai.types import (\\n    FunctionDeclaration,\\n    GenerateContentConfig,\\n    GoogleSearch,\\n    HarmBlockThreshold,\\n    HarmCategory,\\n    MediaResolution,\\n    Part,\\n    Retrieval,\\n    SafetySetting,\\n    Tool,\\n    ToolCodeExecution,\\n    VertexAISearch,\\n)\\nimport os\\nPROJECT_ID = “[your-project-id]”  # @param {type: “string”, placeholder: “[your-project-\\nid]”, isTemplate: true}\\nif not PROJECT_ID or PROJECT_ID == “[your-project-id]”:\\n    PROJECT_ID = str(os.environ.get(“GOOGLE_CLOUD_PROJECT”)) \\nLOCATION = os.environ.get(“GOOGLE_CLOUD_REGION”, “us-central1”)  \\nclient = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION) \\nMODEL_ID = “gemini-2.0-flash-001”  # @param {type: “string”} \\nresponse = client.models.generate_content(\\n    model=MODEL_ID, contents=”What’s the largest planet in our solar system?” \\n)\\ndisplay(Markdown(response.text))\\nSnippet 2. Using Vertex AI and Google AI studio SDKs for unimodal text generation', 'Foundational Large Language Models & Text Generation\\n71\\nFebruary 2025\\nCode and mathematics\\nGenerative models can comprehend and generate code and algorithms to supercharge \\ndevelopers by assisting them across many application areas. Some of the popular use cases \\nfor code include:\\n• Code generation: LLMs can be prompted in natural language to generate code in a \\nspecific programming language to perform certain operations. The output can be used as \\na draft.\\n• Code completion: LLMS can proactively suggest useful code as the user types it. This \\ncan save developers time and improve code quality.\\n• Code refactoring and debugging: LLMs can help reduce technical debt by refactoring \\nand debugging code to improve quality, efficiency and correctness.\\n• Code translation: LLMs can significantly help developer time and effort by helping to \\nconvert code from one programming language to another. For example, an LLM might \\nconvert Python code to Java.\\n• Test case generation: LLMs can be prompted to generate unit tests for a provided \\ncodebase which saves considerable time and reduces errors.\\n• Code documentation and understanding: LLMs can be used in a conversational manner \\nto engage in a natural language chat to help you understand a codebase. They can also \\ngenerate appropriate comments, understand copyright status, and create release notes.\\nRecently, a number of exciting advancements have been made in the space of competitive \\ncoding and mathematics. AlphaCode 2,64 combines Gemini’s reasoning capabilities with \\nsearch and the use of tools to solve competitive coding problems. It receives as input a \\ndescription of a problem to solve, and outputs a code solution that solves the problem. It', 'Foundational Large Language Models & Text Generation\\n72\\nFebruary 2025\\nnow ranks among the top 15% competitive coders on the popular Codeforces competitive \\ncoding platform. FunSearch65 uses an evolutionary procedure which is based on pairing \\na pre-trained LLM with a systematic evaluator. It solved the cap set problem66, an open \\nproblem in mathematics, and also discovered more efficient bin-packing algorithms which \\nare used in many applications such as making data centers more efficient. Another recent \\napproach called AlphaGeometry tackles the problem of finding proofs for complex geometric \\ntheorems. It comprises a neuro-symbolic system made up of a neural language model and \\na symbolic deduction engine. AlphaGeometry managed to solve 25 out of 30 Olympiad \\ngeometry problems, where the average human gold medalist scores on average 25.9. 67\\nMachine translation\\nLLMs are capable of generating fluid, high-quality and contextually accurate translations. \\nThis is possible due to the LLM’s deep understanding of linguistic nuances, idioms, and \\ncontext. The following are some possible real world use cases:\\n• Instant messaging apps: In messaging platforms, LLMs can provide on-the-fly \\ntranslations that feel natural. Unlike previous algorithms that might translate word-\\nfor-word, LLMs understand slang, colloquialisms, and regional differences, enhancing \\ncross-language communication.\\n• E-commerce: On global platforms like AliExpress, product descriptions are automatically \\ntranslated. LLMs help with ensuring cultural nuances and idiomatic expressions in product \\ndetails are appropriately translated, leading to fewer misunderstandings.\\n• Travel apps: In apps like Google Translate, travelers get real-time spoken translations. \\nWith LLMs, the translated conversations are smoother, making interactions in foreign \\ncountries more effortless.', 'Foundational Large Language Models & Text Generation\\n73\\nFebruary 2025\\nText summarization\\nText summarization is a core capability of many of the LLMs mentioned in this whitepaper. \\nThere are a number of natural potential use cases which include:\\n• News aggregators: LLMs could craft summaries that capture not only the main \\nevents but also the sentiment and tone of the article, providing readers with a more \\nholistic understanding.\\n• Research databases: LLMs could help researchers generate abstracts that encapsulate \\nthe core findings and implications of scientific papers.\\n• Chat management: In platforms like Google Chat, LLM-based systems could generate \\nthread summaries that capture the urgency and tone, aiding users in prioritizing \\ntheir responses.\\nQuestion-answering\\nThe older generation of QA systems often worked by keyword matching, frequently missing \\nout on the contextual depth of user queries. LLMs, however, dive deep into context. They can \\ninfer user intent, traverse vast information banks, and provide answers that are contextually \\nrich and precise. Some of the examples where this could be used include:\\n• Virtual assistants: LLMs can offer detailed explanations of a weather forecast \\nconsidering the user’s location, time of year, and recent weather trends.\\n• Customer support: In business platforms, LLM-based bots could provide answers that \\ntake into account the user’s purchase history, past queries, and potential issues, offering \\npersonalized assistance.', 'Foundational Large Language Models & Text Generation\\n74\\nFebruary 2025\\n• Academic platforms: On academic platforms like Wolfram Alpha, LLMs could cater to \\nuser queries by understanding the depth and context of academic questions, offering \\nanswers that suit everyone from a high school student to a postgraduate researcher.\\nThe quality of the generated answers, as well as the corresponding citations and sources \\ncan be significantly improved by using advanced search systems (such as those based on \\nRetrieval Augmented Generation (RAG) architectures) to expand the prompt with relevant \\ninformation, as well as post-hoc grounding after the response has been generated. Clear \\ninstructions, roles of what should and should not be used to answer the question, and \\nadvanced prompt engineering approaches (such as chain of thought and search/RAG \\narchitectures), combined with a lower temperature value amongst other things can also \\nhelp greatly.\\nChatbots\\nEarlier chatbots followed scripted pathways, leading to ‘mechanical’ conversations. LLMs \\ntransform this space by offering dynamic, human-like interactions. They can analyze \\nsentiment, context, and even humor, making digital conversations feel more authentic. Some \\nexamples of where this can be used include:\\n• Customer service: A chatbot on retail platforms like Zara could not only answer product-\\nrelated queries but also offer fashion advice based on current trends.\\n• Entertainment: On Media LLM-driven chatbots could engage with users dynamically, \\nreacting to live events in the stream and moderating chats with contextual understanding.', 'Foundational Large Language Models & Text Generation\\n75\\nFebruary 2025\\nContent generation\\nText generation isn’t new, but what LLMs bring to the table is the unprecedented ability \\nto generate human-like text that’s contextually relevant and rich in detail. Earlier models \\nwould often lose context or coherence over longer passages. LLMs, with their vast \\nknowledge and nuanced understanding, can craft text spanning various styles, tones, and \\ncomplexities, mixing factuality with creativity (depending on the context) effectively bridging \\nthe gap between machine-generated and human-written content. The following are some \\nreal-world examples:\\n• Content creation: Platforms could utilize LLMs to help marketers develop advertisements. \\nInstead of generic content, the LLMs could generate creative, targeted, and \\naudience-specific messages.\\n• Scriptwriting: LLMs could potentially assist with producing scripts for movies or TV \\nshows. Writers could input themes or plot points, and the model can suggest dialogues or \\nscene descriptions, enhancing the creative process.\\nText generation is a wide task encompassing a variety of use cases that might range from \\nthose where correctness of the generated output is more or less important than its creativity/\\ndiversity of the language. The sampling methods and parameters like temperature should be \\ntuned accordingly. For more information, see the prompt engineering and architecting for \\nLLM applications whitepapers.\\nNatural language inference\\nNatural language inference (NLI) is the task of determining whether a given textual \\nhypothesis can be logically inferred from a textual premise.', 'Foundational Large Language Models & Text Generation\\n76\\nFebruary 2025\\nTraditional models struggled with nuanced relationships or those that require a deeper \\nunderstanding of context. LLMs, with their intricate grasp of semantics and context, excel \\nat tasks like these, bringing accuracy levels close to human performance. The following are \\nsome real-world examples:\\n• Sentiment analysis: Businesses could utilize LLMs to infer customer sentiment from \\nproduct reviews. Instead of just basic positive or negative tags, they could extract \\nnuanced emotions like ‘satisfaction,’ ‘disappointment,’ or ‘elation’.\\n• Legal document review: Law firms could employ LLMs to infer implications \\nand intentions in contracts, ensuring there are no contradictions or potentially \\nproblematic clauses.\\n• Medical diagnoses: By analyzing patient descriptions and histories, LLMs could assist \\ndoctors in inferring potential diagnoses or health risks, ensuring early intervention.\\nThe whitepapers on domain specific LLMs, prompt engineering, and architecting for LLM \\napplications give further insight into these use cases.\\nText classification\\nText classification involves categorizing text into predefined groups. While traditional \\nalgorithms were efficient, they often struggled with ambiguous or overlapping categories. \\nLLMs, given their deep understanding of context, can classify text with higher precision, even \\nwhen faced with subtle distinctions. Some examples of this include:\\n• Spam detection: Email services could utilize LLMs to classify emails as spam or \\nlegitimate. Instead of just keyword-based detection, the models understand the context \\nand intent, potentially reducing false positives.', 'Foundational Large Language Models & Text Generation\\n77\\nFebruary 2025\\n• News categorization: News platforms could employ LLMs to categorize articles into \\ntopics like ‘technology,’ ‘politics,’ or ‘sports,’ even when articles blur the boundaries \\nbetween categories.\\n• Customer feedback sorting: Businesses could analyze customer feedback through \\nLLMs to categorize them into areas like ‘product design,’ ‘customer service,’ or ‘pricing,’ \\nensuring targeted responses.\\n• Evaluating LLMs as autorater: LLMs could be used to rate, compare and rank the \\ngenerated outputs of other LLMs as well.\\nText analysis\\nLLMs excel at deep text analysis – extracting patterns, understanding themes, and gleaning \\ninsights from vast textual datasets. Where traditional tools would scratch the surface, LLMs \\ndelve deep, offering rich and actionable insights. Some potential real-world examples are:\\n• Market research: Companies could leverage LLMs to analyze consumer conversations on \\nsocial media, extracting trends, preferences, and emerging needs.\\n• Literary analysis: Academics could employ LLMs to understand themes, motifs, and \\ncharacter developments in literary works, offering fresh perspectives on classic and \\ncontemporary literature.', 'Foundational Large Language Models & Text Generation\\n78\\nFebruary 2025\\nMultimodal applications\\nMultimodal LLMs, capable of processing and generating text, images, audio, and video, have \\nopened up a new frontier in AI, offering a range of exciting and innovative applications across \\nvarious sectors. The following are some examples: \\nCreative content generation:\\n• Storytelling: An AI system could watch an image or video and spin a captivating narrative, \\nintegrating details from the visual with its knowledge base.\\n• Advertising and marketing: Generating targeted and emotionally resonant \\nadvertisements based on product photos or videos.\\nEducation and accessibility:\\n• Personalized learning: Tailoring educational materials to individual learning styles by \\ncombining text with interactive visual and audio elements.\\n• Assistive technology: Multimodal LLMs could power tools that describe images, videos, \\nand audio for visually or hearing-impaired individuals.\\nBusiness and industry:\\n• Document understanding and summarization: Automatically extracting key information \\nfrom complex documents, combining text and visuals like invoices and contracts.\\n• Customer service: Multimodal chatbots can understand and respond to customer \\nqueries combining text and images, offering a richer and more personalized experience. \\nScience and research:', 'Foundational Large Language Models & Text Generation\\n79\\nFebruary 2025\\n• Medical diagnosis: Analyzing medical scans and reports together, identifying potential \\nissues and providing insights for doctors.\\n• Bioinformatics and drug discovery: Integrating knowledge from diverse data sources \\nlike medical images, protein structures, and research papers to accelerate research.\\nThese examples are just the tip of the iceberg. As research progresses, the applications \\nof multimodal LLMs are only expected to grow, transforming our daily lives in diverse and \\nprofound ways. Multimodal LLMs also benefit greatly from the existing methodologies of \\nUnimodal LLMs ( i.e., text based LLMs).\\nLLMs, thanks to their ability to understand and process language, are reshaping how we \\ninteract with, generate, and analyze text across diverse sectors. As they continue to evolve, \\ntheir applications will only grow, boosting the ability for machines and humans to have rich \\nnatural language interactions.', 'Foundational Large Language Models & Text Generation\\n80\\nFebruary 2025\\nSummary\\nIn this whitepaper we have discussed the basics of transformers, upon which all modern-day \\nLLMs are based. We detailed the evolution of the various LLM model architectures and their \\ncomponents. We’ve also seen the various methodologies you can use to train and fine-tune \\nmodels efficiently and effectively. We briefly discussed prompt engineering and sampling \\ntechniques that greatly influence the output of an LLM, and also touched on possible \\napplications of this technology. There are a number of key takeaways to keep in mind:\\n• The transformer architecture is the basis for all modern-day LLMs. Across the various \\narchitectures mentioned in this whitepaper we see that it’s important not only to add more \\nparameters to the model, but the composition of the dataset is equally important. \\n• The order and strategies used for fine-tuning is important and may include multiple steps \\nsuch as Instruction Tuning, Safety Tuning, etc. Supervised Fine Tuning (SFT) is important \\nin capturing the essence of a task. RLHF, and potentially RLAIF, can be used to shift the \\ndistribution from the pretraining distribution to a more desired one through the power of \\nthe reward function, that can reward desirable behaviors and penalize undesirable ones.\\n• Making inference from neural models efficient is an important problem and an active \\nfield of research. Many methods exist to reduce serving costs and latency with minimal \\nimpact to model performance, and some exact acceleration methods guarantee identical \\nmodel outputs.\\n• Large language models can be used for a variety of tasks including summarization, \\ntranslation, question answering, chat, code generation, and many more. You can \\ncreate your own tasks using the Vertex and Makersuite text generation services which \\nleverage Google’s latest language models. After the model has been trained and tuned, \\nit is important to experiment with engineering prompts. You should use the technique \\nmost appropriate for the task-at-hand because LLMs can be sensitive to prompts k.', 'Foundational Large Language Models & Text Generation\\n81\\nFebruary 2025\\nFurthermore, it is also possible to enhance task specific performance or creativity and \\ndiversity by tweaking the parameters corresponding to sampling techniques such as \\nTop-K, Top-P, and Max decoding steps to find the optimum mix of correctness, diversity, \\nand creativity required for the task at hand.', 'Foundational Large Language Models & Text Generation\\n82\\nFebruary 2025\\nEndnotes\\n1. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin,\\xa0I., 2017, Attention is \\nall you need. Advances in Neural Information Processing Systems , 30.\\n2. Wikipedia, 2024, Word n-gram language model. Available at:  \\nhttps://en.wikipedia.org/wiki/Word_n-gram_language_model .\\n3. Sutskever, I., Vinyals, O., & Le, Q. V., 2014, Sequence to sequence learning with neural networks. Advances in \\nNeural Information Processing Systems, 27.\\n4. Gu, A., Goel, K., & Ré, C., 2021, Efficiently modeling long sequences with structured state spaces.  \\narXiv preprint arXiv:2111.00396.\\n5. Jalammar, J. (n.d.). The illustrated transformer. Available at:  \\nhttps://jalammar.github.io/illustrated-transformer/ .\\n6. Ba, J. L., Kiros, J. R., & Hinton, G. E., 2016, Layer normalization.  \\narXiv preprint arXiv:1607.06450.\\n7. He, K., Zhang, X., Ren, S., & Sun, J., 2016, Deep residual learning for image recognition. Proceedings of the \\nIEEE Conference on Computer Vision and Pattern Recognition.\\n8. HuggingFace., 2024, Byte Pair Encoding. Available at:  \\nhttps://huggingface.co/learn/nlp-course/chapter6/5?fw=pt .\\n9. Kudo, T., & Richardson, J., 2018, Sentencepiece: A simple and language independent subword tokenizer and \\ndetokenizer for neural text processing. arXiv preprint arXiv:1808.06226.\\n10. HuggingFace, 2024, Unigram tokenization. Available at:  \\nhttps://huggingface.co/learn/nlp-course/chapter6/7?fw=pt .\\n11. Goodfellow et. al., 2016, Deep Learning. MIT Press. Available at: http://www.deeplearningbook.org .\\n12. Radford, Alec et al., 2019, Language models are unsupervised multitask learners.\\n13. Brown, Tom, et al., 2020, Language models are few-shot learners. Advances in Neural Information \\nProcessing Systems, 33, 1877-1901.\\n14. Devlin, Jacob, et al., 2018, BERT: Pre-training of deep bidirectional transformers for language understanding. \\narXiv preprint arXiv:1810.04805.', 'Foundational Large Language Models & Text Generation\\n83\\nFebruary 2025\\n15. Radford, A., & Narasimhan, K., 2018, Improving language understanding by generative pre-training.\\n16. Dai, A., & Le, Q., 2015, Semi-supervised sequence learning. Advances in Neural Information \\nProcessing Systems.\\n17. Ouyang, Long, et al., 2022, Training language models to follow instructions with human feedback. Advances \\nin Neural Information Processing Systems, 35, 27730-27744.-27744.\\n18. OpenAI., 2023, GPT-3.5. Available at: https://platform.openai.com/docs/models/gpt-3-5 .\\n19. OpenAI., 2023, GPT-4 Technical Report. Available at: https://arxiv.org/abs/2303.08774 .\\n20. Thoppilan, Romal, et al., 2022, Lamda: Language models for dialog applications. \\narXiv\\xa0preprint arXiv:2201.08239.\\n21. Llama 3.2: Revolutionizing edge AI and vision with open, customizable models. Available \\nat: https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/.\\n22. Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., ... & Irving, G., 2021, Scaling language \\nmodels: Methods, analysis & insights from training Gopher. Available at: https://arxiv.org/pdf/2112.11446.pdf.\\n23. Du, N., He, H., Dai, Z., Mccarthy, J., Patwary, M. A., & Zhou, L., 2022, GLAM: Efficient scaling of language \\nmodels with mixture-of-experts. In International Conference on Machine Learning  (pp. 2790-2800). PMLR.\\n24. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., ... & Amodei, D., 2020, Scaling laws \\nfor neural language models. arXiv preprint arXiv:2001.08361.\\n25. Hoffmann, Jordan, et al., 2022, Training compute-optimal large language models. arXiv \\npreprint arXiv:2203.15556.\\n26. Shoeybi, Mohammad, et al., 2019, Megatron-LM: Training multi-billion parameter language models using \\nmodel parallelism. arXiv preprint arXiv:1909.08053.\\n27. Muennighoff, N. et al., 2023, Scaling data-constrained language models. arXiv preprint arXiv:2305.16264.\\n28. Chowdhery, Aakanksha, et al., 2023, Palm: Scaling language modeling with pathways. Journal of Machine \\nLearning Research, 24(240), 1-113.\\n29. Wang, Alex, et al.,2019, SuperGLUE: A stickier benchmark for general-purpose language understanding \\nsystems. Advances in Neural Information Processing Systems , 32.\\n30. Anil, Rohan, et al., 2023, Palm 2 technical report. arXiv preprint arXiv:2305.10403 .', 'Foundational Large Language Models & Text Generation\\n84\\nFebruary 2025\\n31. DeepMind, 2023, Gemini: A family of highly capable multimodal models. Available at:  \\nhttps://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf .\\n32. DeepMind, 2024, Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. \\nAvailable at: https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf .\\n33. Google Developers, 2024, Introducing PaLi-Gemma, Gemma 2, and an upgraded responsible AI toolkit. \\nAvailable at: https://developers.googleblog.com/en/gemma-family-and-toolkit-expansion-io-2024/ .\\n34. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., ... & Jegou, H., 2023, Llama 2: Open \\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.\\n35. Jiang, A. Q., 2024, Mixtral of experts. arXiv preprint arXiv:2401.04088 .\\n36. Qwen, 2024, Introducing Qwen1.5. Available at: https://qwenlm.github.io/blog/qwen1.5/ .\\n37. Young, A., 2024, Yi: Open foundation models by 01.AI. arXiv preprint arXiv:2403.04652 .\\n38. Grok-1, 2024, Available at: https://github.com/xai-org/grok-1.\\n39. Duan, Haodong, et al., 2023, BotChat: Evaluating LLMs’ capabilities of having multi-turn dialogues. \\narXiv\\xa0preprint arXiv:2310.13650.\\n40. Google Cloud, 2024, Tune text models with reinforcement learning from human feedback. Available at:  \\nhttps://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-text-models-rlhf .\\n41. Bai, Yuntao, et al., 2022, Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073 .\\n42. Wikipedia, 2024, Likert scale. Available at: https://en.wikipedia.org/wiki/Likert_scale .\\n43. Sutton, R. S., & Barto, A. G., 2018, Reinforcement learning: An introduction. MIT Press.\\n44. Bai, Yuntao, et al, 2022, Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073 .\\n45. Rafailov, Rafael, et al., 2023, Direct preference optimization: Your language model is secretly a reward \\nmodel. arXiv preprint arXiv:2305.18290.\\n46. Houlsby, Neil, et al., 2019, Parameter-efficient transfer learning for NLP. In International Conference on \\nMachine Learning (pp. 2790-2799). PMLR.\\n47. Hu, Edward J., et al., 2021, LoRA: Low-rank adaptation of large language models. \\narXiv\\xa0preprint arXiv:2106.09685.\\n48. Dettmers, Tim, et al., 2023, QLoRA: Efficient finetuning of quantized LLMs. arXiv preprint arXiv:2305.14314 .', 'Foundational Large Language Models & Text Generation\\n85\\nFebruary 2025\\n49. Lester, B., Al-Rfou, R., & Constant, N., 2021, The power of scale for parameter-efficient prompt tuning. arXiv \\npreprint arXiv:2104.08691.\\n50. HuggingFace., 2020, How to generate text? Available at: https://huggingface.co/blog/how-to-generate .\\n51. Google AI Studio Context caching. Available \\nat: https://ai.google.dev/gemini-api/docs/caching?lang=python.\\n52. Vertex AI Context caching overview. Available \\nat: https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview .\\n53. Gu, A., Goel, K., & Ré, C., 2021, Efficiently modeling long sequences with structured state spaces.  \\nAvailable at: https://arxiv.org/abs/2111.00396 .\\n54. Hubara et al., 2016, Quantized neural networks: Training neural networks with low precision weights and \\nactivations. Available at: https://arxiv.org/abs/1609.07061 .\\n55. Benoit Jacob et al., 2017, Quantization and training of neural networks for efficient integer-arithmetic-only \\ninference. Available at: https://arxiv.org/abs/1712.05877 .\\n56. Bucila, C., Caruana, R., & Niculescu-Mizil, A., 2006, Model compression. Knowledge Discovery and Data \\nMining. Available at: https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf .\\n57. Hinton, G., Vinyals, O., & Dean, J., 2015, Distilling the knowledge in a neural network.  \\nAvailable at: https://arxiv.org/abs/1503.02531 .\\n58. Zhang, L., Fei, W., Wu w., He Y., Lou Z., Zhou H., 2023, Dual Grained Quantisation: Efficient Finegrained \\nQuantisation for LLM. Available at: https://arxiv.org/abs/2310.04836 .\\n59. Agarwal, R., Vieillard, N., Zhou, Y., Stanczyk, P., Ramos, S., Geist, M., Bachem, O., 2024, On-\\nPolicy Distillation of Language Models: Learning from Self-Generated Mistakes. Available \\nat: https://arxiv.org/abs/2306.13649.\\n60. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., & Dean, J., 2017, Outrageously large neural \\nnetworks: The sparsely-gated mixture-of-experts layer. Available at: https://arxiv.org/abs/1701.06538 .\\n61. Schuster, T., Fried, D., & Jurafsky, D., 2022, Confident adaptive language modeling. Available at:  \\nhttps://arxiv.org/abs/2207.07061.\\n62. Tri Dao et al. “FlashAttention. Available at:  \\nhttps://arxiv.org/abs/2205.14135.', 'Foundational Large Language Models & Text Generation\\n86\\nFebruary 2025\\n63. Leviathan, Y., Ram, O., Desbordes, T., & Haussmann, E., 2022, Fast inference from transformers via \\nspeculative decoding. Available at: https://arxiv.org/abs/2211.17192 .\\n64. Li, Y., Humphreys, P., Sun, T., Carr, A., Cass, S., Hawkins, P., ... & Bortolussi, L., 2022, Competition-level code \\ngeneration with AlphaCode. Science , 378(1092-1097). DOI: 10.1126/science.abq1158.\\n65. Romera-Paredes, B., Barekatain, M., Novikov, A., Novikov, A., Rashed, S., & Yang, J., 2023, Mathematical \\ndiscoveries from program search with large language models. Nature . DOI: 10.1038/s41586-023-06924-6.\\n66. Wikipedia., 2024, Cap set. Available at: https://en.wikipedia.org/wiki/Cap_set .\\n67. Trinh, T. H., Wu, Y., & Le, Q. V. et al., 2024, Solving olympiad geometry without human demonstrations. Nature,  \\n625, 476–482. DOI: 10.1038/s41586-023-06747-5.\\n68. Mikolov, T., Chen, K., Corrado, G., Dean, J., 2013, Efficient Estimation of Word Representations in Vector \\nSpace. Available at: https://arxiv.org/pdf/1301.3781 .\\n69. Shi, L., Ma, C., Liang, W., Ma, W., Vosoughi, S., 2024, Judging the Judges: A Systematic Study of Position Bias \\nin LLM-as-a-Judge. Available at: https://arxiv.org/abs/2406.07791\\n70. Pandit, B., 2024, What Is Mixture of Experts (MoE)? How It Works, Use Cases & More. Available \\nat: https://www.datacamp.com/blog/mixture-of-experts-moe')\n",
      "({'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 0, 'page_label': '1'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 1, 'page_label': '2'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 2, 'page_label': '3'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 3, 'page_label': '4'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 4, 'page_label': '5'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 5, 'page_label': '6'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 6, 'page_label': '7'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 7, 'page_label': '8'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 8, 'page_label': '9'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 9, 'page_label': '10'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 10, 'page_label': '11'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 11, 'page_label': '12'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 12, 'page_label': '13'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 13, 'page_label': '14'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 14, 'page_label': '15'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 15, 'page_label': '16'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 16, 'page_label': '17'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 17, 'page_label': '18'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 18, 'page_label': '19'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 19, 'page_label': '20'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 20, 'page_label': '21'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 21, 'page_label': '22'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 22, 'page_label': '23'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 23, 'page_label': '24'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 24, 'page_label': '25'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 25, 'page_label': '26'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 26, 'page_label': '27'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 27, 'page_label': '28'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 28, 'page_label': '29'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 29, 'page_label': '30'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 30, 'page_label': '31'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 31, 'page_label': '32'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 32, 'page_label': '33'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 33, 'page_label': '34'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 34, 'page_label': '35'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 35, 'page_label': '36'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 36, 'page_label': '37'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 37, 'page_label': '38'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 38, 'page_label': '39'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 39, 'page_label': '40'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 40, 'page_label': '41'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 41, 'page_label': '42'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 42, 'page_label': '43'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 43, 'page_label': '44'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 44, 'page_label': '45'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 45, 'page_label': '46'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 46, 'page_label': '47'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 47, 'page_label': '48'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 48, 'page_label': '49'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 49, 'page_label': '50'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 50, 'page_label': '51'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 51, 'page_label': '52'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 52, 'page_label': '53'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 53, 'page_label': '54'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 54, 'page_label': '55'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 55, 'page_label': '56'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 56, 'page_label': '57'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 57, 'page_label': '58'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 58, 'page_label': '59'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 59, 'page_label': '60'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 60, 'page_label': '61'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 61, 'page_label': '62'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 62, 'page_label': '63'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 63, 'page_label': '64'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 64, 'page_label': '65'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 65, 'page_label': '66'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 66, 'page_label': '67'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 67, 'page_label': '68'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 68, 'page_label': '69'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 69, 'page_label': '70'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 70, 'page_label': '71'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 71, 'page_label': '72'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 72, 'page_label': '73'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 73, 'page_label': '74'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 74, 'page_label': '75'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-18T10:48:11-06:00', 'moddate': '2025-03-18T10:48:21-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_Companion_v2.pdf', 'total_pages': 76, 'page': 75, 'page_label': '76'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 0, 'page_label': '1'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 1, 'page_label': '2'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 2, 'page_label': '3'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 3, 'page_label': '4'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 4, 'page_label': '5'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 5, 'page_label': '6'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 6, 'page_label': '7'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 7, 'page_label': '8'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 8, 'page_label': '9'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 9, 'page_label': '10'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 10, 'page_label': '11'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 11, 'page_label': '12'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 12, 'page_label': '13'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 13, 'page_label': '14'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 14, 'page_label': '15'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 15, 'page_label': '16'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 16, 'page_label': '17'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 17, 'page_label': '18'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 18, 'page_label': '19'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 19, 'page_label': '20'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 20, 'page_label': '21'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 21, 'page_label': '22'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 22, 'page_label': '23'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 23, 'page_label': '24'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 24, 'page_label': '25'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 25, 'page_label': '26'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 26, 'page_label': '27'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 27, 'page_label': '28'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 28, 'page_label': '29'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 29, 'page_label': '30'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 30, 'page_label': '31'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 31, 'page_label': '32'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 32, 'page_label': '33'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 33, 'page_label': '34'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 34, 'page_label': '35'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 35, 'page_label': '36'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 36, 'page_label': '37'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 37, 'page_label': '38'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 38, 'page_label': '39'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 39, 'page_label': '40'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 40, 'page_label': '41'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:44:42-06:00', 'moddate': '2025-03-17T13:44:50-06:00', 'trapped': '/False', 'source': 'PDF\\\\Agents_v8.pdf', 'total_pages': 42, 'page': 41, 'page_label': '42'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 0, 'page_label': '1'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 1, 'page_label': '2'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 2, 'page_label': '3'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 3, 'page_label': '4'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 4, 'page_label': '5'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 5, 'page_label': '6'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 6, 'page_label': '7'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 7, 'page_label': '8'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 8, 'page_label': '9'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 9, 'page_label': '10'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 10, 'page_label': '11'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 11, 'page_label': '12'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 12, 'page_label': '13'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 13, 'page_label': '14'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 14, 'page_label': '15'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 15, 'page_label': '16'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 16, 'page_label': '17'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 17, 'page_label': '18'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 18, 'page_label': '19'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 19, 'page_label': '20'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 20, 'page_label': '21'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 21, 'page_label': '22'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 22, 'page_label': '23'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 23, 'page_label': '24'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 24, 'page_label': '25'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 25, 'page_label': '26'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 26, 'page_label': '27'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 27, 'page_label': '28'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 28, 'page_label': '29'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 29, 'page_label': '30'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 30, 'page_label': '31'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 31, 'page_label': '32'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 32, 'page_label': '33'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 33, 'page_label': '34'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 34, 'page_label': '35'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 35, 'page_label': '36'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 36, 'page_label': '37'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 37, 'page_label': '38'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 38, 'page_label': '39'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 39, 'page_label': '40'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 40, 'page_label': '41'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 41, 'page_label': '42'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 42, 'page_label': '43'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 43, 'page_label': '44'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 44, 'page_label': '45'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 45, 'page_label': '46'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 46, 'page_label': '47'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 47, 'page_label': '48'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 48, 'page_label': '49'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 49, 'page_label': '50'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 50, 'page_label': '51'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 51, 'page_label': '52'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 52, 'page_label': '53'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 53, 'page_label': '54'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 54, 'page_label': '55'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 55, 'page_label': '56'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 56, 'page_label': '57'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 57, 'page_label': '58'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 58, 'page_label': '59'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 59, 'page_label': '60'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 60, 'page_label': '61'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 61, 'page_label': '62'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 62, 'page_label': '63'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 63, 'page_label': '64'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 64, 'page_label': '65'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 65, 'page_label': '66'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 66, 'page_label': '67'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 67, 'page_label': '68'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 68, 'page_label': '69'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 69, 'page_label': '70'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 70, 'page_label': '71'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 71, 'page_label': '72'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 72, 'page_label': '73'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 73, 'page_label': '74'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 74, 'page_label': '75'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 75, 'page_label': '76'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 76, 'page_label': '77'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 77, 'page_label': '78'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 78, 'page_label': '79'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 79, 'page_label': '80'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 80, 'page_label': '81'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 81, 'page_label': '82'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 82, 'page_label': '83'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 83, 'page_label': '84'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 84, 'page_label': '85'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 85, 'page_label': '86'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 86, 'page_label': '87'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 87, 'page_label': '88'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 88, 'page_label': '89'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 89, 'page_label': '90'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 90, 'page_label': '91'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 91, 'page_label': '92'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:52:43-06:00', 'moddate': '2025-03-17T13:52:55-06:00', 'trapped': '/False', 'source': 'PDF\\\\MLOps_Operationalizing Generative AI on Vertex AI_v7 (1).pdf', 'total_pages': 93, 'page': 92, 'page_label': '93'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 0, 'page_label': '1'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 1, 'page_label': '2'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 2, 'page_label': '3'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 3, 'page_label': '4'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 4, 'page_label': '5'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 5, 'page_label': '6'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 6, 'page_label': '7'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 7, 'page_label': '8'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 8, 'page_label': '9'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 9, 'page_label': '10'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 10, 'page_label': '11'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 11, 'page_label': '12'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 12, 'page_label': '13'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 13, 'page_label': '14'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 14, 'page_label': '15'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 15, 'page_label': '16'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 16, 'page_label': '17'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 17, 'page_label': '18'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 18, 'page_label': '19'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 19, 'page_label': '20'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 20, 'page_label': '21'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 21, 'page_label': '22'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 22, 'page_label': '23'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 23, 'page_label': '24'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 24, 'page_label': '25'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 25, 'page_label': '26'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 26, 'page_label': '27'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 27, 'page_label': '28'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 28, 'page_label': '29'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 29, 'page_label': '30'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 30, 'page_label': '31'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 31, 'page_label': '32'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 32, 'page_label': '33'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 33, 'page_label': '34'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 34, 'page_label': '35'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 35, 'page_label': '36'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 36, 'page_label': '37'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 37, 'page_label': '38'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 38, 'page_label': '39'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 39, 'page_label': '40'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 40, 'page_label': '41'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 41, 'page_label': '42'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 42, 'page_label': '43'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 43, 'page_label': '44'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 44, 'page_label': '45'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 45, 'page_label': '46'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 46, 'page_label': '47'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 47, 'page_label': '48'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 48, 'page_label': '49'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 49, 'page_label': '50'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 50, 'page_label': '51'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 51, 'page_label': '52'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 52, 'page_label': '53'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 53, 'page_label': '54'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 54, 'page_label': '55'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 55, 'page_label': '56'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 56, 'page_label': '57'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 57, 'page_label': '58'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 58, 'page_label': '59'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 59, 'page_label': '60'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 60, 'page_label': '61'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 61, 'page_label': '62'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 62, 'page_label': '63'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 63, 'page_label': '64'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 64, 'page_label': '65'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 65, 'page_label': '66'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 66, 'page_label': '67'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'PDF\\\\Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 67, 'page_label': '68'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 0, 'page_label': '1'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 1, 'page_label': '2'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 2, 'page_label': '3'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 3, 'page_label': '4'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 4, 'page_label': '5'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 5, 'page_label': '6'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 6, 'page_label': '7'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 7, 'page_label': '8'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 8, 'page_label': '9'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 9, 'page_label': '10'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 10, 'page_label': '11'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 11, 'page_label': '12'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 12, 'page_label': '13'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 13, 'page_label': '14'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 14, 'page_label': '15'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 15, 'page_label': '16'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 16, 'page_label': '17'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 17, 'page_label': '18'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 18, 'page_label': '19'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 19, 'page_label': '20'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 20, 'page_label': '21'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 21, 'page_label': '22'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 22, 'page_label': '23'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 23, 'page_label': '24'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 24, 'page_label': '25'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 25, 'page_label': '26'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 26, 'page_label': '27'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 27, 'page_label': '28'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 28, 'page_label': '29'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 29, 'page_label': '30'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 30, 'page_label': '31'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 31, 'page_label': '32'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 32, 'page_label': '33'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 33, 'page_label': '34'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 34, 'page_label': '35'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 35, 'page_label': '36'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:42:42-06:00', 'moddate': '2025-03-17T13:42:49-06:00', 'trapped': '/False', 'source': 'PDF\\\\Solving Domain-Specific problems using LLMs_v7.pdf', 'total_pages': 37, 'page': 36, 'page_label': '37'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 0, 'page_label': '1'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 1, 'page_label': '2'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 2, 'page_label': '3'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 3, 'page_label': '4'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 4, 'page_label': '5'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 5, 'page_label': '6'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 6, 'page_label': '7'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 7, 'page_label': '8'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 8, 'page_label': '9'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 9, 'page_label': '10'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 10, 'page_label': '11'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 11, 'page_label': '12'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 12, 'page_label': '13'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 13, 'page_label': '14'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 14, 'page_label': '15'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 15, 'page_label': '16'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 16, 'page_label': '17'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 17, 'page_label': '18'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 18, 'page_label': '19'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 19, 'page_label': '20'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 20, 'page_label': '21'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 21, 'page_label': '22'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 22, 'page_label': '23'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 23, 'page_label': '24'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 24, 'page_label': '25'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 25, 'page_label': '26'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 26, 'page_label': '27'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 27, 'page_label': '28'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 28, 'page_label': '29'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 29, 'page_label': '30'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 30, 'page_label': '31'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 31, 'page_label': '32'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 32, 'page_label': '33'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 33, 'page_label': '34'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 34, 'page_label': '35'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 35, 'page_label': '36'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 36, 'page_label': '37'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 37, 'page_label': '38'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 38, 'page_label': '39'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 39, 'page_label': '40'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 40, 'page_label': '41'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 41, 'page_label': '42'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 42, 'page_label': '43'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 43, 'page_label': '44'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 44, 'page_label': '45'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 45, 'page_label': '46'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 46, 'page_label': '47'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 47, 'page_label': '48'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 48, 'page_label': '49'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 49, 'page_label': '50'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 50, 'page_label': '51'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 51, 'page_label': '52'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 52, 'page_label': '53'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 53, 'page_label': '54'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 54, 'page_label': '55'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 55, 'page_label': '56'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 56, 'page_label': '57'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 57, 'page_label': '58'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 58, 'page_label': '59'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 59, 'page_label': '60'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 60, 'page_label': '61'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 61, 'page_label': '62'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 62, 'page_label': '63'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'moddate': '2025-03-17T13:41:40-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'page': 63, 'page_label': '64'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 0, 'page_label': '1'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 1, 'page_label': '2'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 2, 'page_label': '3'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 3, 'page_label': '4'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 4, 'page_label': '5'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 5, 'page_label': '6'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 6, 'page_label': '7'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 7, 'page_label': '8'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 8, 'page_label': '9'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 9, 'page_label': '10'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 10, 'page_label': '11'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 11, 'page_label': '12'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 12, 'page_label': '13'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 13, 'page_label': '14'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 14, 'page_label': '15'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 15, 'page_label': '16'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 16, 'page_label': '17'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 17, 'page_label': '18'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 18, 'page_label': '19'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 19, 'page_label': '20'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 20, 'page_label': '21'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 21, 'page_label': '22'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 22, 'page_label': '23'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 23, 'page_label': '24'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 24, 'page_label': '25'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 25, 'page_label': '26'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 26, 'page_label': '27'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 27, 'page_label': '28'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 28, 'page_label': '29'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 29, 'page_label': '30'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 30, 'page_label': '31'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 31, 'page_label': '32'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 32, 'page_label': '33'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 33, 'page_label': '34'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 34, 'page_label': '35'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 35, 'page_label': '36'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 36, 'page_label': '37'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 37, 'page_label': '38'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 38, 'page_label': '39'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 39, 'page_label': '40'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 40, 'page_label': '41'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 41, 'page_label': '42'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 42, 'page_label': '43'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 43, 'page_label': '44'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 44, 'page_label': '45'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 45, 'page_label': '46'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 46, 'page_label': '47'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 47, 'page_label': '48'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 48, 'page_label': '49'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 49, 'page_label': '50'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 50, 'page_label': '51'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 51, 'page_label': '52'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 52, 'page_label': '53'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 53, 'page_label': '54'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 54, 'page_label': '55'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 55, 'page_label': '56'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 56, 'page_label': '57'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 57, 'page_label': '58'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 58, 'page_label': '59'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 59, 'page_label': '60'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 60, 'page_label': '61'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 61, 'page_label': '62'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 62, 'page_label': '63'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 63, 'page_label': '64'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 64, 'page_label': '65'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 65, 'page_label': '66'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 66, 'page_label': '67'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 67, 'page_label': '68'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 68, 'page_label': '69'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 69, 'page_label': '70'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 70, 'page_label': '71'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 71, 'page_label': '72'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 72, 'page_label': '73'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 73, 'page_label': '74'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 74, 'page_label': '75'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 75, 'page_label': '76'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 76, 'page_label': '77'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 77, 'page_label': '78'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 78, 'page_label': '79'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 79, 'page_label': '80'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 80, 'page_label': '81'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 81, 'page_label': '82'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 82, 'page_label': '83'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 83, 'page_label': '84'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 84, 'page_label': '85'}, {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'moddate': '2025-03-17T13:39:25-06:00', 'trapped': '/False', 'source': 'PDF\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'page': 85, 'page_label': '86'})\n"
     ]
    }
   ],
   "source": [
    "text_meta_pair = [(doc.page_content, doc.metadata) for doc in all_docs]\n",
    "texts, meta = list(zip(*text_meta_pair))\n",
    "print(f\"You have {len(texts)} texts and {len(meta)} metadata pairs.\")\n",
    "print(texts)\n",
    "print(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b9a535fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'message': 'Failed to send all objects in a batch of 100', 'error': \"WeaviateInsertManyAllFailedError('Every object failed during insertion. Here is the set of all errors: failed with status: 401 error: Invalid username or password.\\\\nfailed with status: 401 error: Invalid username or password.')\"}\n",
      "{'message': 'Failed to send 100 objects in a batch of 100. Please inspect client.batch.failed_objects or collection.batch.failed_objects for the failed objects.'}\n",
      "{'message': 'Failed to send all objects in a batch of 100', 'error': \"WeaviateInsertManyAllFailedError('Every object failed during insertion. Here is the set of all errors: failed with status: 401 error: Invalid username or password.\\\\nfailed with status: 401 error: Invalid username or password.')\"}\n",
      "{'message': 'Failed to send 100 objects in a batch of 100. Please inspect client.batch.failed_objects or collection.batch.failed_objects for the failed objects.'}\n",
      "{'message': 'Failed to send all objects in a batch of 100', 'error': \"WeaviateInsertManyAllFailedError('Every object failed during insertion. Here is the set of all errors: failed with status: 401 error: Invalid username or password.\\\\nfailed with status: 401 error: Invalid username or password.')\"}\n",
      "{'message': 'Failed to send 100 objects in a batch of 100. Please inspect client.batch.failed_objects or collection.batch.failed_objects for the failed objects.'}\n",
      "{'message': 'Failed to send all objects in a batch of 100', 'error': \"WeaviateInsertManyAllFailedError('Every object failed during insertion. Here is the set of all errors: failed with status: 401 error: Invalid username or password.\\\\nfailed with status: 401 error: Invalid username or password.')\"}\n",
      "{'message': 'Failed to send 100 objects in a batch of 100. Please inspect client.batch.failed_objects or collection.batch.failed_objects for the failed objects.'}\n",
      "{'message': 'Failed to send all objects in a batch of 66', 'error': \"WeaviateInsertManyAllFailedError('Every object failed during insertion. Here is the set of all errors: failed with status: 401 error: Invalid username or password.\\\\nfailed with status: 401 error: Invalid username or password.')\"}\n",
      "{'message': 'Failed to send 66 objects in a batch of 66. Please inspect client.batch.failed_objects or collection.batch.failed_objects for the failed objects.'}\n"
     ]
    }
   ],
   "source": [
    "with collection.batch.fixed_size(batch_size=100) as batch:\n",
    "    for text, metadata in zip(texts, meta):\n",
    "        properties = {\n",
    "            \"content\":text,\n",
    "            **metadata,\n",
    "        }\n",
    "\n",
    "        batch.add_object(properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7f4446",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'message': 'Failed to send all objects in a batch of 100', 'error': \"WeaviateInsertManyAllFailedError('Every object failed during insertion. Here is the set of all errors: failed with status: 401 error: Invalid username or password.')\"}\n",
      "{'message': 'Failed to send 100 objects in a batch of 100. Please inspect client.batch.failed_objects or collection.batch.failed_objects for the failed objects.'}\n",
      "{'message': 'Failed to send all objects in a batch of 100', 'error': \"WeaviateInsertManyAllFailedError('Every object failed during insertion. Here is the set of all errors: failed with status: 401 error: Invalid username or password.')\"}\n",
      "{'message': 'Failed to send 100 objects in a batch of 100. Please inspect client.batch.failed_objects or collection.batch.failed_objects for the failed objects.'}\n",
      "{'message': 'Failed to send all objects in a batch of 100', 'error': \"WeaviateInsertManyAllFailedError('Every object failed during insertion. Here is the set of all errors: failed with status: 401 error: Invalid username or password.')\"}\n",
      "{'message': 'Failed to send 100 objects in a batch of 100. Please inspect client.batch.failed_objects or collection.batch.failed_objects for the failed objects.'}\n",
      "{'message': 'Failed to send all objects in a batch of 100', 'error': \"WeaviateInsertManyAllFailedError('Every object failed during insertion. Here is the set of all errors: failed with status: 401 error: Invalid username or password.')\"}\n",
      "{'message': 'Failed to send 100 objects in a batch of 100. Please inspect client.batch.failed_objects or collection.batch.failed_objects for the failed objects.'}\n",
      "{'message': 'Failed to send all objects in a batch of 66', 'error': \"WeaviateInsertManyAllFailedError('Every object failed during insertion. Here is the set of all errors: failed with status: 401 error: Invalid username or password.')\"}\n",
      "{'message': 'Failed to send 66 objects in a batch of 66. Please inspect client.batch.failed_objects or collection.batch.failed_objects for the failed objects.'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32501003",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the main topics discussed in the book?\"\n",
    "doc = collection.similiarity_search(query, top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c75a0fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10da75a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Beeface\\AppData\\Local\\Temp\\ipykernel_17864\\1787768.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "c:\\Users\\Beeface\\.virtualenvs\\BookChatbot-9Q2uoHrH\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "vector_db_path = \"vector_db_path\"\n",
    "embedding_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "# embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "            documents=splits,\n",
    "            embedding=embedding_model,\n",
    "            persist_directory=vector_db_path,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be05cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BookChatbot-9Q2uoHrH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
